[
  {
    "arxiv_id": "2312.04956v5",
    "title": "An Explainable Ensemble-based Intrusion Detection System for Software-Defined Vehicle Ad-hoc Networks",
    "authors": "Shakil Ibne Ahsan; Phil Legg; S M Iftekharul Alam",
    "abstract": "Intrusion Detection Systems (IDS) are widely employed to detect and mitigate external network security events. Vehicle ad-hoc Networks (VANETs) continue to evolve, especially with developments related to Connected Autonomous Vehicles (CAVs). In this study, we explore the detection of cyber threats in vehicle networks through ensemble-based machine learning, to strengthen the performance of the learnt model compared to relying on a single model. We propose a model that uses Random Forest and CatBoost as our main investigators, with Logistic Regression used to then reason on their outputs to make a final decision. To further aid analysis, we use SHAP (SHapley Additive exPlanations) analysis to examine feature importance towards the final decision stage. We use the Vehicular Reference Misbehavior (VeReMi) dataset for our experimentation and observe that our approach improves classification accuracy, and results in fewer misclassifications compared to previous works. Overall, this layered approach to decision-making combining teamwork among models with an explainable view of why they act as they do can help to achieve a more reliable and easy-to-understand cyber security solution for smart transportation networks.",
    "published_date": "2023-12-08",
    "pdf_link": "https://arxiv.org/pdf/2312.04956v5",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Explainable ensemble-based IDS for Software-Defined VANETs (misbehavior detection using VeReMi)",
      "attack_types": [
        "Position falsification (Constant Attack - type 1)",
        "Position falsification (Constant Offset Attack - type 2)",
        "Position falsification (Random Attack - type 4)",
        "Position falsification (Random Offset Attack - type 8)",
        "Position falsification (Eventual Stop Attack - type 16)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Ensemble (Stacking)",
        "specific": "Random Forest + CatBoost base learners with Logistic Regression meta-learner",
        "novel_contribution": "Layered ensemble that combines Random Forest and CatBoost as 'investigators' and uses Logistic Regression to reason on their outputs for final decision in SDN-based VANET IDS"
      },
      {
        "type": "primary",
        "category": "Tree-based",
        "specific": "Random Forest",
        "novel_contribution": "Used as one of two main base learners due to handling high-dimensional and categorical data in VANET context"
      },
      {
        "type": "primary",
        "category": "Gradient Boosting",
        "specific": "CatBoost",
        "novel_contribution": "Used as one of two main base learners; chosen for categorical/high-dimensional robustness"
      },
      {
        "type": "primary",
        "category": "Linear Model",
        "specific": "Logistic Regression",
        "novel_contribution": "Meta-learner operating on the outputs of RF and CatBoost to make final classification"
      },
      {
        "type": "primary",
        "category": "Explainability/XAI",
        "specific": "SHAP (SHapley Additive exPlanations)",
        "novel_contribution": "Explains feature importance contributing to the final ensemble decision and analysis of misclassifications"
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "VeReMi (Vehicular Reference Misbehavior) dataset",
        "type": "synthetic",
        "domain": "vehicular_network_messages (OBU logs: timestamps, IDs, positions, speeds, reception times)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Accuracy",
      "Misclassification count"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can an ensemble of Random Forest and CatBoost with a Logistic Regression meta-learner improve IDS accuracy for SDN-based VANETs compared to single models?",
        "Can SHAP-based explainability provide insight into feature importance and reasons for remaining misclassifications in VANET IDS?"
      ],
      "gaps_identified": [
        "Lack of interpretability in IDS for fast-paced, real-time VANET environments",
        "High false positives and false negatives in existing IDS deployments",
        "Publicly available VANET datasets with attacks are limited; no dataset enabling 100% accuracy",
        "No fast, inexpensive, and effective in-built privacy-preserving ML-based IDS evident",
        "VANET topology is dynamic and unpredictable with frequent disconnections and high mobility, complicating IDS design",
        "VeReMi focuses on position-falsifying attacks and lacks broader network threats and advanced malicious strategies; it is synthetic and lacks real-life cases",
        "IDS and malware detection systems still cannot detect unknown, encrypted, and inference attacks with 100% accuracy"
      ],
      "limitations": [
        "Use of a synthetic dataset (VeReMi) limited to position-falsifying attacks; lacks coverage of all network threats and advanced strategies",
        "Results are limited to simulated, city-scale scenarios rather than real-world deployments"
      ],
      "future_work": [],
      "motivation": "Improve reliability and transparency of IDS in SDN-based VANETs by combining ensemble learning with explainability to reduce misclassifications and provide actionable insights.",
      "potential_research_ideas": [
        "Extend to broader VANET threat taxonomy (e.g., Sybil, message fabrication, DDoS on SDN controllers) and multi-attack scenarios beyond position falsification",
        "Incorporate temporal modeling of beacon sequences (e.g., sequence models or time-aware features) for trajectory-consistency checks",
        "Domain adaptation and transfer learning from synthetic (VeReMi) to real-world V2X datasets; evaluate cross-scenario robustness",
        "Semi-supervised or self-supervised representation learning to detect unknown/zero-day VANET attacks",
        "Federated or decentralized learning for privacy-preserving collaborative IDS across vehicles and RSUs",
        "Adversarial robustness studies and defenses against poisoning/evasion targeting tree ensembles in VANET settings",
        "Graph-based modeling of vehicular communication (e.g., GNNs over dynamic V2V/V2I graphs) for contextual misbehavior detection",
        "Online learning and concept drift detection for evolving VANET conditions and attacker strategies",
        "Calibration and human-in-the-loop explanations to reduce operator workload and false positives"
      ],
      "architectural_improvement_recommendations": [
        "Use rigorous stacking with out-of-fold meta-features to prevent information leakage when training the Logistic Regression meta-learner",
        "Calibrate output probabilities (e.g., Platt scaling or isotonic regression) to improve decision thresholds and reduce false positives",
        "Augment base learners with additional gradient-boosting variants (e.g., XGBoost, LightGBM) and perform model selection via cross-validation",
        "Engineer temporal and kinematic consistency features (speed/acceleration/heading continuity) and neighborhood-consensus features from nearby vehicles/RSUs",
        "Adopt cost-sensitive learning or class-weighting to handle class imbalance across attack types",
        "Incorporate uncertainty estimation and SHAP interaction values to diagnose model brittleness",
        "Evaluate ensemble diversity explicitly (e.g., Q-statistic, disagreement) and prune/weight base models accordingly",
        "Add a lightweight anomaly detection stage (e.g., isolation forest) before supervised classifier to flag novel behaviors"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "SDN-based VANET (simulated, city-scale via VeReMi/VEINS)",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Dynamic topology with high mobility and frequent disconnections in VANETs",
        "Potential vulnerability of centralized SDN controllers to compromise/impersonation",
        "Gap between synthetic datasets and real-world traffic/attack distributions",
        "Need to balance accuracy, explainability, and runtime for real-time operations"
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes a complementary ensemble approach integrating Random Forest, CatBoost, and Logistic Regression to balance accuracy, explainability, and runtime for IDS in SDN-based VANETs.",
      "Demonstrates the approach on the VeReMi dataset across multiple attack classes and integrates SHAP analysis to explain how individual models and the ensemble inform decision-making, including reasoning behind remaining misclassifications.",
      "Motivates the choice of Random Forest and CatBoost for handling high-dimensional and categorical data typical in VANETs."
    ]
  },
  {
    "arxiv_id": "2311.14342v2",
    "title": "AI-based Attack Graph Generation",
    "authors": "Sangbeom Park; Jaesung Lee; Jeong Do Yoo; Min Geun Song; Hyosun Lee; Jaewoong Choi; Chaeyeon Sagong; Huy Kang Kim",
    "abstract": "With the advancement of IoT technology, many electronic devices are interconnected through networks, communicating with each other and performing specific roles. However, as numerous devices join networks, the threat of cyberattacks also escalates. Preventing and detecting cyber threats are crucial, and one method of preventing such threats involves using attack graphs. Attack graphs are widely used to assess security threats within networks. However, a drawback emerges as the network scales, as generating attack graphs becomes time-consuming. To overcome this limitation, artificial intelligence models can be employed. By utilizing AI models, attack graphs can be created within a short period, approximating optimal outcomes. AI models designed for attack graph generation consist of encoders and decoders, trained using reinforcement learning algorithms. After training the AI models, we confirmed the model's learning effectiveness by observing changes in loss and reward values. Additionally, we compared attack graphs generated by the AI model with those created through conventional methods.",
    "published_date": "2023-11-24",
    "pdf_link": "https://arxiv.org/pdf/2311.14342v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Security Risk Assessment",
      "specific_problem": "AI-accelerated attack graph generation and attack path inference to approximate optimal results faster than conventional exhaustive methods",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "GNN",
        "specific": "Graph Attention Network (GAT) encoder-decoder",
        "novel_contribution": "Custom encoder-decoder using GAT for node embedding and attention-based neighbor selection to construct attack paths/graphs"
      },
      {
        "type": "primary",
        "category": "Reinforcement Learning",
        "specific": "Policy gradient with baseline (variance-reduction) and greedy rollout-style decoding",
        "novel_contribution": "Trains the GAT decoder to select next-node actions using rewards derived from accumulated path weights (Attack Path Score) with a bespoke DFS-guided exploration mechanism"
      },
      {
        "type": "baseline",
        "category": "Classical Search",
        "specific": "Brute-force DFS enumeration",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Heuristic Search",
        "specific": "Greedy rollout baseline (for training baseline updates; not an external competitor)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Optimization",
        "specific": "Adam optimizer",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Reinforcement Learning"
    ],
    "datasets": [
      {
        "name": "Synthetic random weighted undirected graphs (NetworkX-generated)",
        "type": "synthetic",
        "domain": "attack_graphs/topology_graphs",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Brute Force (DFS enumeration of all paths for Attack Path Score)",
        "paper_reference": null,
        "metric": "Attack Path Score similarity between reconstructed (AI) and original/exhaustive results",
        "their_result": "“두 결과는 비슷했으며 동일한 경우도 있었다.”",
        "baseline_result": "Exact Attack Path Score from exhaustive DFS"
      }
    ],
    "performance_metrics_used": [
      "training_loss",
      "reward",
      "attack_path_score",
      "convergence_epochs",
      "qualitative_runtime_claim"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can a GAT-based encoder-decoder trained with reinforcement learning generate attack graphs quickly while approximating optimal results?",
        "Does the learned model’s path/graph selection produce Attack Path Scores comparable to exhaustive brute-force methods?"
      ],
      "gaps_identified": [
        "Conventional attack graph generation becomes prohibitively slow as network size and connectivity grow.",
        "Lack of fast approximate generators usable on large or complex networks."
      ],
      "limitations": [
        "Evaluation performed only on synthetic, undirected random graphs (no real network/attack graph datasets).",
        "No quantitative runtime comparisons versus classical generators are reported.",
        "No standard accuracy metrics (e.g., exact-match rate, path overlap) beyond qualitative Attack Path Score comparisons.",
        "Graph semantics simplified: undirected edges and random weights used as proxies for attack feasibility; does not model real vulnerability/privilege states or preconditions.",
        "Scalability not quantified across graph sizes; figures show 100-node example but no systematic scaling study.",
        "No comparisons against alternative learning-based solvers (e.g., pointer networks, GATv2, transformers) or classical heuristic algorithms (e.g., Dijkstra-based, A* variants for weighted path enumeration).",
        "No treatment of directed/conditional dependencies typical in attack graphs (e.g., multi-step exploits, privilege escalation states).",
        "No discussion of robustness to noise/adversarial manipulation of inputs.",
        "No public code or dataset provided to reproduce results."
      ],
      "future_work": [],
      "motivation": "Reduce the time cost of attack graph generation for large-scale, IoT-rich networks by learning to approximate optimal attack paths/graphs using AI.",
      "potential_research_ideas": [
        "Extend from undirected random graphs to directed, stateful attack graphs encoding privileges, vulnerabilities, and conditional preconditions (e.g., Bayesian attack graphs).",
        "Integrate real-world vulnerability and network configuration data to estimate edge/transition probabilities (e.g., using CVE/CVSS, asset criticality) and train on realistic distributions.",
        "Benchmark against classical and learning-based solvers on standardized graph sets and controlled scalability studies to quantify speed-accuracy trade-offs.",
        "Incorporate multi-objective optimization (e.g., maximize risk while minimizing detection likelihood) using multi-criteria rewards.",
        "Introduce curriculum learning from small to large graphs and domain randomization to improve generalization to complex topologies.",
        "Explore meta-RL or offline RL to rapidly adapt to new networks with limited interaction.",
        "Use constrained decoding or post-processing to enforce valid attack graph structure (acyclicity, state transitions) and security semantics.",
        "Develop uncertainty estimation and calibration for predicted path scores to support risk-aware decision-making.",
        "Investigate adversarial robustness to manipulated topologies or weights (evasion attempts by attackers)."
      ],
      "architectural_improvement_recommendations": [
        "Replace/augment GAT with GATv2 or transformer-based graph attention with positional/relative encodings for better expressive power.",
        "Adopt pointer network or graph-transformer decoder with masking for invalid actions and beam search for higher-quality path sets.",
        "Switch from REINFORCE with baseline to Advantage Actor-Critic (A2C/A3C) or PPO with a learned critic for lower-variance updates.",
        "Incorporate Graph Isomorphism Network (GIN) or message-passing with higher-order neighborhood aggregation to better capture complex dependencies.",
        "Add constraint-aware reward shaping and penalties (e.g., cycles, invalid preconditions) and Monte Carlo Tree Search (MCTS) guided decoding.",
        "Scale to large graphs with neighborhood sampling (GraphSAGE) and sparse attention; enable batching over variable-size graphs.",
        "Introduce directed-edge modeling and state nodes (assets, privileges) to align with attack graph semantics; predict edges and states jointly."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "PyTorch",
        "NetworkX"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Mapping real vulnerabilities, privileges, and policies to graph structure and edge probabilities.",
        "Handling directed, conditional dependencies and multi-step exploit chains.",
        "Ensuring validity and interpretability of generated attack graphs for security analysts.",
        "Robustness to noisy or incomplete network inventory and configuration data.",
        "Integration with existing enterprise tooling and dynamic network changes."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes an AI-based attack graph generation approach using a GAT encoder-decoder trained with reinforcement learning.",
      "Introduces a decoder that outputs attention-based neighbor selection probabilities used in a DFS-guided path exploration mechanism.",
      "Provides a synthetic data generation pipeline for random weighted undirected graphs using NetworkX and PyTorch.",
      "Empirically validates convergence via loss and reward curves and compares Attack Path Scores between AI-generated and brute-force results, reporting they are similar and sometimes identical."
    ]
  },
  {
    "arxiv_id": "2312.06555v1",
    "title": "On the Impact of CDL and TDL Augmentation for RF Fingerprinting under Impaired Channels",
    "authors": "Omer Melih Gul; Michel Kulhandjian; Burak Kantarci; Claude D'Amours; Azzedine Touazi; Cliff Ellement",
    "abstract": "Cyber-physical systems have recently been used in several areas (such as connected and autonomous vehicles) due to their high maneuverability. On the other hand, they are susceptible to cyber-attacks. Radio frequency (RF) fingerprinting emerges as a promising approach. This work aims to analyze the impact of decoupling tapped delay line and clustered delay line (TDL+CDL) augmentation-driven deep learning (DL) on transmitter-specific fingerprints to discriminate malicious users from legitimate ones. This work also considers 5G-only-CDL, WiFi-only-TDL augmentation approaches. RF fingerprinting models are sensitive to changing channels and environmental conditions. For this reason, they should be considered during the deployment of a DL model. Data acquisition can be another option. Nonetheless, gathering samples under various conditions for a train set formation may be quite hard. Consequently, data acquisition may not be feasible. This work uses a dataset that includes 5G, 4G, and WiFi samples, and it empowers a CDL+TDL-based augmentation technique in order to boost the learning performance of the DL model. Numerical results show that CDL+TDL, 5G-only-CDL, and WiFi-only-TDL augmentation approaches achieve 87.59%, 81.63%, 79.21% accuracy on unobserved data while TDL/CDL augmentation technique and no augmentation approach result in 77.81% and 74.84% accuracy on unobserved data, respectively.",
    "published_date": "2023-12-11",
    "pdf_link": "https://arxiv.org/pdf/2312.06555v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Wireless Security",
      "subdomain": "RF Fingerprinting / Device Identification",
      "specific_problem": "Transmitter identification under varying channel conditions using CDL/TDL-based data augmentation on raw I/Q signals",
      "attack_types": [
        "spoofing",
        "impersonation",
        "replay",
        "jamming/DoS"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Data Augmentation",
        "specific": "Decoupled CDL+TDL channel-model augmentation",
        "novel_contribution": "Applies CDL to 5G and TDL to WiFi only (LTE not augmented) and studies impact vs uniform CDL/TDL augmentation and no augmentation on real I/Q data collected on different days."
      },
      {
        "type": "primary",
        "category": "Neural Network",
        "specific": null,
        "novel_contribution": "Supervised classifier on raw I/Q samples adopted from prior work [17]; used to evaluate augmentation strategies."
      },
      {
        "type": "baseline",
        "category": "Data Augmentation",
        "specific": "Uniform TDL/CDL augmentation (same transform applied to all waveform types)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "No Augmentation",
        "specific": "Training on Day-1 data only without channel-model augmentation",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Data Augmentation",
        "specific": "5G-only-CDL augmentation",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Data Augmentation",
        "specific": "WiFi-only-TDL augmentation",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "POWDER RF fingerprinting dataset (Reus-Muns et al. [17])",
        "type": "public",
        "domain": "rf_iq_samples",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Without Augmentation",
        "paper_reference": null,
        "metric": "Accuracy on Day-2 (%)",
        "their_result": "87.59",
        "baseline_result": "74.84"
      },
      {
        "method_name": "TDL/CDL-based Augmentation (uniform)",
        "paper_reference": "[19]",
        "metric": "Accuracy on Day-2 (%)",
        "their_result": "87.59",
        "baseline_result": "77.81"
      },
      {
        "method_name": "5G-only-CDL Augmentation",
        "paper_reference": null,
        "metric": "Accuracy on Day-2 (%)",
        "their_result": "87.59",
        "baseline_result": "81.63"
      },
      {
        "method_name": "WiFi-only-TDL Augmentation",
        "paper_reference": null,
        "metric": "Accuracy on Day-2 (%)",
        "their_result": "87.59",
        "baseline_result": "79.21"
      }
    ],
    "performance_metrics_used": [
      "Accuracy"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can decoupling CDL and TDL augmentation across waveform types (5G vs WiFi) improve RF fingerprinting generalization to unobserved channel conditions (Day-2) compared to uniform augmentation or no augmentation?",
        "What is the effect of applying CDL only to 5G or TDL only to WiFi on cross-day performance?",
        "How robust is an RF fingerprinting classifier trained on Day-1 when tested on Day-2 with different propagation conditions?"
      ],
      "gaps_identified": [
        "RF fingerprinting models are sensitive to changing channels and environmental conditions; collecting sufficiently diverse training data is often infeasible.",
        "Most prior works use synthetic/modulated data or same-day evaluations; few evaluate cross-day generalization with real I/Q data.",
        "Lack of standard dataset structure/organization for RF fingerprinting hampers reuse across ML frameworks (simulation–reality gap).",
        "Impact of receiver hardware (phase noise, clock offsets, IQ imbalance, sampling rate, antenna configuration) on learned fingerprints remains underexplored.",
        "RFFS vulnerabilities (impersonation/spoofing, DoS/jamming, obfuscation) need deeper study."
      ],
      "limitations": [
        "Classifier architecture details are inherited from prior work and not deeply explored/optimized here.",
        "LTE waveform data is not augmented in the proposed CDL+TDL strategy.",
        "Evaluation limited to four base stations and one public dataset (two collection days).",
        "No adversarial robustness or privacy analysis; focus is accuracy under channel variation.",
        "No code/artifacts released; augmentation depends on MATLAB 5G Toolbox parameters."
      ],
      "future_work": [
        "Address the simulation–reality gap with standardized datasets and structures for RF I/Q data.",
        "Systematically study receiver hardware effects on fingerprint stability and algorithm independence.",
        "Investigate robustness to impersonation/spoofing and DoS/jamming; explore fingerprint obfuscation schemes recoverable only by legitimate receivers.",
        "Design specialized LOS/NLOS models and scale augmentation for desired RMS delay spread.",
        "Extend augmentation to LTE and additional waveform types; evaluate across more days and environments."
      ],
      "motivation": "Improve RF fingerprinting robustness to channel impairments when collecting diverse training data is infeasible by leveraging channel-model-based data augmentation.",
      "potential_research_ideas": [
        "Learned augmentation policy/search to automatically select CDL/TDL parameter distributions per waveform and environment.",
        "Domain-adversarial or contrastive representation learning to disentangle channel effects from transmitter impairments across days.",
        "Multi-receiver training with invariance objectives to factor out receiver hardware artifacts.",
        "Self-supervised pretraining on large unlabeled I/Q corpora followed by fine-tuning with limited labeled emitters.",
        "Augment LTE with appropriate models (e.g., TDL/EVA/ETU) and study tri-tech decoupling strategies.",
        "Incorporate differentiable channel layers and curriculum schedules that gradually increase channel severity.",
        "Adversarial training against spoofing (e.g., GAN-based forgeries) and jamming-aware training.",
        "Uncertainty estimation and calibration to flag low-confidence identifications under severe channel shift."
      ],
      "architectural_improvement_recommendations": [
        "Replace/augment the baseline classifier with 1D complex-valued CNN or ResNet blocks and attention over time to better capture waveform-specific features.",
        "Add domain-adversarial heads to enforce channel-invariant embeddings between Day-1 and augmented samples.",
        "Use multi-branch encoders per waveform type with shared bottleneck to encourage cross-tech generalization.",
        "Introduce differentiable CDL/TDL layers in the training graph to co-train augmentation parameters with the classifier.",
        "Leverage metric learning (triplet/contrastive losses) jointly with cross-entropy to tighten same-emitter clusters across days.",
        "Model uncertainty (MC dropout or deep ensembles) and temperature scaling for calibrated decisions."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "MATLAB 5G Toolbox"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "POWDER wireless testbed with four base stations (Day-1/Day-2 collections)",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Channel variability across days/environments causing accuracy degradation without augmentation.",
        "Data acquisition under diverse conditions is difficult/expensive.",
        "Receiver hardware variability may confound fingerprints.",
        "Bridging the simulation–reality gap for synthetic augmentations."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "First work applying different types of data augmentation on raw time-domain signals in heterogeneous environments with real-world data.",
      "Decouples TDL and CDL augmentation across waveform types (CDL for 5G, TDL for WiFi) and evaluates against uniform augmentation and no augmentation.",
      "Introduces and evaluates 5G-only-CDL and WiFi-only-TDL augmentation variants.",
      "Demonstrates improved accuracy on unobserved (Day-2) data: CDL+TDL 87.59% vs uniform TDL/CDL 77.81% and no augmentation 74.84%.",
      "Provides qualitative t-SNE visualizations illustrating class separability changes under different augmentations."
    ]
  },
  {
    "arxiv_id": "2401.00563v3",
    "title": "KernelGPT: Enhanced Kernel Fuzzing via Large Language Models",
    "authors": "Chenyuan Yang; Zijie Zhao; Lingming Zhang",
    "abstract": "Bugs in operating system kernels can affect billions of devices and users all over the world. As a result, a large body of research has been focused on kernel fuzzing, i.e., automatically generating syscall (system call) sequences to detect potential kernel bugs or vulnerabilities. Kernel fuzzing aims to generate valid syscall sequences guided by syscall specifications that define both the syntax and semantics of syscalls. While there has been existing work trying to automate syscall specification generation, this remains largely manual work, and a large number of important syscalls are still uncovered.   In this paper, we propose KernelGPT, the first approach to automatically synthesizing syscall specifications via Large Language Models (LLMs) for enhanced kernel fuzzing. Our key insight is that LLMs have seen massive kernel code, documentation, and use cases during pre-training, and thus can automatically distill the necessary information for making valid syscalls. More specifically, KernelGPT leverages an iterative approach to automatically infer the specifications, and further debug and repair them based on the validation feedback. Our results demonstrate that KernelGPT can generate more new and valid specifications and achieve higher coverage than state-of-the-art techniques. So far, by using newly generated specifications, KernelGPT has already detected 24 new unique bugs in Linux kernel, with 12 fixed and 11 assigned with CVE numbers. Moreover, a number of specifications generated by KernelGPT have already been merged into the kernel fuzzer Syzkaller, following the request from its development team.",
    "published_date": "2023-12-31",
    "pdf_link": "https://arxiv.org/pdf/2401.00563v3",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "System Security",
      "subdomain": "Fuzzing",
      "specific_problem": "Automatic synthesis of syscall specifications (syzlang) for kernel fuzzing of Linux drivers and sockets",
      "attack_types": [
        "kernel crash",
        "out-of-bounds write"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Large Language Model (Transformer)",
        "specific": null,
        "novel_contribution": "Iterative multi-stage prompting with few-shot syzlang guidance to infer identifier values, argument types, and dependencies from kernel source; LLM-in-the-loop validation-driven debugging/repair of generated specifications"
      }
    ],
    "learning_paradigm": [
      "Prompt-based",
      "In-context/few-shot"
    ],
    "datasets": [
      {
        "name": "Linux kernel source codebase (drivers and sockets)",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Syzkaller syscall specifications (syzlang)",
        "type": "public",
        "domain": "syscall_specifications",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "SyzDescribe",
        "paper_reference": "[25] (static analysis for syscall specification generation)",
        "metric": "coverage; number of new and valid syscall specifications",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Syzkaller (existing manual/available specifications)",
        "paper_reference": "[5] Syzkaller",
        "metric": "coverage; number of new and valid syscall specifications",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "code coverage",
      "number of new syscall specifications",
      "validity rate of generated specifications",
      "number of bugs found",
      "number of CVEs assigned"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can we automate and improve the learning of various rules for generating high-quality syscall specifications from the kernel codebase with minimal effort?"
      ],
      "gaps_identified": [
        "L-1: Incomplete modeling of diverse kernel code patterns by rule-based static analysis",
        "L-2: Poor readability of auto-generated specifications hindering validation and maintenance",
        "L-3: Difficulty comprehending textual information (comments, docs) with static tools"
      ],
      "limitations": [
        "Inherent LLM constraints such as context size restrictions and hallucinations; mitigated via few-shot prompting, iterative analysis, and validation-driven repair"
      ],
      "future_work": [],
      "motivation": "Manual syscall specification authoring is difficult and incomplete; automated generation has been desired for years, but static approaches are brittle, hard to maintain, and often unreadable.",
      "potential_research_ideas": [
        "Extend LLM-guided specification synthesis beyond drivers/sockets to filesystems, eBPF, networking stacks, and other OSes (e.g., BSD, Windows)",
        "Combine LLM reasoning with static program analysis/slicing to reduce hallucinations and improve precision",
        "Incorporate symbolic execution or constraint solving to verify and refine LLM-produced types and dependencies",
        "Train or fine-tune a code LLM on syzlang corpora and kernel diffs to specialize for syscall specification generation",
        "Actively learn from fuzzing feedback (coverage, crash triage) to iteratively refine specifications (closed-loop RL or bandit formulations)",
        "RAG over kernel code, Kconfig, documentation, and mailing lists to ground LLM outputs",
        "Automated detection and repair of drift as kernel evolves (change-aware specification maintenance)",
        "Cross-project transfer: leverage learned patterns to bootstrap specs for newly added drivers"
      ],
      "architectural_improvement_recommendations": [
        "Adopt retrieval-augmented generation with program slicing to construct minimal, relevant contexts for the LLM",
        "Use structured decoding with a syzlang grammar/AST constraints to ensure syntactic correctness",
        "Introduce a verifier module (type checker/SMT checks) before validation runs to catch inconsistencies early",
        "Integrate differential testing against headers/macros to validate inferred identifiers (e.g., ioctl cmd values)",
        "Cache and reuse inferred schemas across related handlers (e.g., shared ioctl families)",
        "Leverage function summaries and call-graph traversal to systematically discover dependencies",
        "Fine-tune a small open LLM for kernel-spec tasks to reduce reliance on proprietary models"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/ise-uiuc/KernelGPT",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Kernel fuzzing of upstream Linux via Syzkaller",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "LLM hallucinations leading to incorrect specifications",
        "Context size limitations when analyzing large code regions",
        "Evolving kernel codebase causing specification drift"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "First fully automated approach using LLMs to synthesize syscall specifications for kernel fuzzing, integrating with mature fuzzing frameworks",
      "Iterative strategy to infer identifier values, types, and dependencies; followed by validation-driven repair using LLMs",
      "Empirical demonstration of higher coverage and more new/valid specifications than state-of-the-art baselines (SyzDescribe, Syzkaller)",
      "Detected 24 previously unknown Linux kernel bugs (12 fixed, 11 assigned CVEs); several generated specifications merged into Syzkaller"
    ]
  },
  {
    "arxiv_id": "2312.04818v1",
    "title": "Using Program Knowledge Graph to Uncover Software Vulnerabilities",
    "authors": "M. Xie; T. Rahat; W. Wang; Y. Tian",
    "abstract": "In an increasingly interconnected and data-driven world, the importance of robust security measures cannot be overstated. A knowledge graph constructed with information extracted from the system along with the desired security behavior can be utilized to identify complex security vulnerabilities hidden underneath the systems. Unfortunately, existing security knowledge graphs are constructed from coarse-grained information extracted from publicly available vulnerability reports, which are not equipped to check actual security violations in real-world system implementations. In this poster, we present a novel approach of using Program Knowledge Graph that is embedded with fine-grained execution information of the systems (e.g., callgraph, data-flow, etc.) along with information extracted from the public vulnerability and weakness datasets (e.g., CVE and CWE). We further demonstrate that our custom security knowledge graph can be checked against the standard queries generated by LLM, providing a powerful way to identify security vulnerabilities and weaknesses in critical systems.",
    "published_date": "2023-12-08",
    "pdf_link": "https://arxiv.org/pdf/2312.04818v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Static Analysis and Vulnerability Detection",
      "specific_problem": "Detecting CWE/CVE-relevant weaknesses in program implementations by integrating program graphs with vulnerability knowledge and auto-generated queries",
      "attack_types": [
        "CWE-242 Use of inherently dangerous function",
        "CWE-401 Missing release of memory after effective lifetime",
        "CWE-415 Double free",
        "CWE-467 Use of sizeof() on a pointer type",
        "CWE-477 Use of obsolete function",
        "CWE-479 Signal handler use of a non-reentrant function",
        "CWE-558 Use of getlogin() in multi-threaded application",
        "CWE-1341 Multiple releases of same resource or handle"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer LLM",
        "specific": null,
        "novel_contribution": "Prompt-tuned LLM is used to automatically generate Cypher queries that check for specific vulnerabilities over the Program Knowledge Graph."
      },
      {
        "type": "primary",
        "category": "Knowledge Graph",
        "specific": "Program Knowledge Graph (integrating program call graph with CWE/CVE data)",
        "novel_contribution": "Introduces a Program Knowledge Graph that embeds fine-grained program execution structure (e.g., call graph) with security knowledge (CWE/CVE) to enable vulnerability queries."
      }
    ],
    "learning_paradigm": [
      "Prompt-based generation"
    ],
    "datasets": [
      {
        "name": "CWE (Common Weakness Enumeration)",
        "type": "public",
        "domain": "security_knowledge_base",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CVE (Common Vulnerabilities and Exposures)",
        "type": "public",
        "domain": "security_knowledge_base",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Collected C/C++ CWE code samples (15 examples across 8 CWE categories)",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "Program Call Graphs for collected code samples",
        "type": "synthetic",
        "domain": "program_graphs",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "CSV-formatted CVE/CWE datasets (processed for graph ingestion)",
        "type": "public",
        "domain": "security_knowledge_base",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Detection success count",
      "Number of benchmarks detected vs total"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can a Program Knowledge Graph that integrates fine-grained program structure with vulnerability knowledge uncover real vulnerabilities in software implementations?",
        "Can LLMs be leveraged to automatically generate complex graph queries to detect specific CWEs/CVEs without manual query authoring?"
      ],
      "gaps_identified": [
        "Existing security knowledge graphs are constructed from coarse-grained, publicly available vulnerability reports and are not equipped to check actual security violations in real-world system implementations.",
        "Prior work focused on extraction/integration of vulnerability data, not on detection of vulnerabilities in code using the graph.",
        "Manual construction of queries is a barrier for practitioners."
      ],
      "limitations": [
        "Evaluation limited to 15 small C/C++ code examples from CWE sites.",
        "Current program graph integration primarily uses call graphs; fails to detect weaknesses that require data-flow information (e.g., CWE-401).",
        "No apple-to-apple baseline comparison since prior work emphasized data integration rather than detection.",
        "LLM details, reliability, and robustness of query generation are not fully explored."
      ],
      "future_work": [
        "Expand to include additional program graph types (e.g., data flow graphs, control flow graphs) to detect more complex vulnerabilities.",
        "Systematically automate the LLM-based query generation process.",
        "Extend evaluation and compare against existing vulnerability detection methods."
      ],
      "motivation": "Bridge the gap between abstract vulnerability knowledge (CWE/CVE) and concrete detection of vulnerabilities in actual software implementations by combining program graphs with security knowledge and automating query generation.",
      "potential_research_ideas": [
        "Integrate interprocedural data-flow/taint graphs with the Program Knowledge Graph to detect memory/resource lifetime and injection-related CWEs.",
        "Develop a retrieval-augmented LLM pipeline that conditions on CWE/CVE descriptions and schema to produce verified Cypher queries with constraint checking.",
        "Design a learning-based query synthesizer (e.g., sequence-to-Cypher) fine-tuned on pairs of vulnerability specs and correct queries.",
        "Incorporate dynamic traces (runtime call/data-flow) and differential analysis to detect environment-dependent vulnerabilities.",
        "Use GNNs over program graphs to predict vulnerable nodes and then verify with graph queries as a hybrid approach.",
        "Create a larger, standardized benchmark of code examples with ground-truth CWE labels and query suites for reproducible comparisons."
      ],
      "architectural_improvement_recommendations": [
        "Augment the Program Knowledge Graph with data-flow and control-flow edges and unify them under a typed, versioned schema.",
        "Introduce a query verification loop: generate candidate queries via LLM, validate on synthetic counterexamples, and refine using self-consistency or constraint solving.",
        "Add a mapping layer from CWE/CVE taxonomies to code-level event schemas (APIs, sinks, sources) to standardize query generation.",
        "Implement provenance and confidence scoring for each detection by tracking which nodes/edges and which LLM prompt versions contributed.",
        "Cache and parameterize queries per CWE family to enable scalable reuse across projects."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "Neo4j",
        "Cypher",
        "LLM (unspecified)"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Low; graph storage and traversal in Neo4j for small code examples; LLM access required for query generation."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Graph database (Neo4j) with static analysis outputs (program call graphs) ingested",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Extracting accurate and complete program graphs (call/data/control-flow) from large real-world codebases.",
        "Aligning CWE/CVE semantics with concrete code-level events across diverse languages and libraries.",
        "Ensuring correctness and reliability of LLM-generated queries; preventing hallucinations.",
        "Managing graph size and query performance at scale for enterprise codebases."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces the Program Knowledge Graph that integrates program graphs (e.g., call graphs) with CWE/CVE security data to enable vulnerability detection.",
      "Leverages prompt-tuned LLMs to automatically generate Cypher queries for vulnerability checking, alleviating manual query authoring.",
      "Implements a prototype on Neo4j and demonstrates detection on collected C/C++ CWE examples, reporting: “we successfully identified the weakness of 14 out of 15 benchmarks.”",
      "Provides processed CSV datasets (CVE/CWE) and constructed call graphs for the evaluation examples."
    ]
  },
  {
    "arxiv_id": "2312.07696v2",
    "title": "Real-time Network Intrusion Detection via Decision Transformers",
    "authors": "Jingdi Chen; Hanhan Zhou; Yongsheng Mei; Gina Adam; Nathaniel D. Bastian; Tian Lan",
    "abstract": "Many cybersecurity problems that require real-time decision-making based on temporal observations can be abstracted as a sequence modeling problem, e.g., network intrusion detection from a sequence of arriving packets. Existing approaches like reinforcement learning may not be suitable for such cybersecurity decision problems, since the Markovian property may not necessarily hold and the underlying network states are often not observable. In this paper, we cast the problem of real-time network intrusion detection as casual sequence modeling and draw upon the power of the transformer architecture for real-time decision-making. By conditioning a causal decision transformer on past trajectories, consisting of the rewards, network packets, and detection decisions, our proposed framework will generate future detection decisions to achieve the desired return. It enables decision transformers to be applied to real-time network intrusion detection, as well as a novel tradeoff between the accuracy and timeliness of detection. The proposed solution is evaluated on public network intrusion detection datasets and outperforms several baseline algorithms using reinforcement learning and sequence modeling, in terms of detection accuracy and timeliness.",
    "published_date": "2023-12-12",
    "pdf_link": "https://arxiv.org/pdf/2312.07696v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Real-time packet-level network intrusion detection with an accuracy–timeliness tradeoff using decision transformers",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "Decision Transformer (causal, GPT-style)",
        "novel_contribution": "Applies decision transformers to packet-level NIDS for real-time decision-making and optimizes a novel reward trading off accuracy and timeliness"
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "Continuous-Time Decision Transformer",
        "novel_contribution": "Adapts a continuous-time transformer formulation to handle irregular packet arrival times for network security policies learned from offline traffic"
      },
      {
        "type": "primary",
        "category": "Autoencoder",
        "specific": null,
        "novel_contribution": "Compresses arbitrary-length packet payload bytes into compact embeddings used as sequential inputs to the decision transformer"
      },
      {
        "type": "primary",
        "category": "Sampling/Weighting",
        "specific": "Importance Sampling",
        "novel_contribution": "Weights different network traces to mitigate DT’s dependence on behavior policy quality and improve intrusion detection performance"
      }
    ],
    "learning_paradigm": [
      "Offline Reinforcement Learning",
      "Supervised",
      "Autoregressive sequence modeling"
    ],
    "datasets": [
      {
        "name": "UNSW-NB15",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "accuracy",
      "timeliness"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can network intrusion detection be cast as causal sequence modeling and solved effectively with decision transformers?",
        "Can a decision transformer enable a controllable tradeoff between detection accuracy and detection timeliness at the packet level?",
        "How does DT perform on public NIDS datasets compared to reinforcement learning and sequence modeling baselines?"
      ],
      "gaps_identified": [
        "RL-based NIDS often assume Markovian dynamics and observability, which may not hold in practical cybersecurity settings.",
        "Existing deep learning NIDS struggle with real-time assimilation of features from new packets as they first appear.",
        "Offline RL applications to NIDS are sparse, and prior works have not demonstrated packet-level attack detection."
      ],
      "limitations": [
        "Decision Transformer performance depends on the quality of the behavior policy in training data, though claimed to be alleviated with well-labeled NIDS datasets like UNSW-NB15."
      ],
      "future_work": [
        "Incorporating techniques like importance sampling to strategically weight different network traces to refine detection performance."
      ],
      "motivation": "Provide a simple, scalable, and real-time decision-making approach for packet-level NIDS without relying on the Markov property, enabling earlier and more efficient detection via sequence modeling.",
      "potential_research_ideas": [
        "Develop hierarchical packet-to-flow transformers that jointly model packet bytes and flow-level context for earlier and more accurate detection.",
        "Explore uncertainty-aware decision transformers (e.g., ensemble DTs or Bayesian heads) to calibrate when to wait versus decide.",
        "Integrate pretrained byte-level or protocol-aware encoders (e.g., byte-token transformers) instead of training an autoencoder from scratch for payload compression.",
        "Extend the reward design with cost-sensitive timeliness penalties that adapt to attack criticality or service-level constraints.",
        "Apply constrained RL or safe policy optimization to enforce latency budgets while maximizing detection accuracy.",
        "Evaluate robustness under distribution shift and adversarial evasion (e.g., payload perturbations) for DT-based NIDS."
      ],
      "architectural_improvement_recommendations": [
        "Replace or augment the autoencoder with a pretrained byte-level transformer encoder to derive richer payload embeddings.",
        "Introduce hierarchical attention: per-packet payload attention feeding a flow-level temporal transformer for long-horizon dependencies.",
        "Add an auxiliary head to predict time-to-detect or detection confidence to better govern the 'wait' versus 'decide' action.",
        "Use curriculum or prioritized sequence sampling based on novelty/rarity to improve learning from scarce attack trajectories.",
        "Leverage contrastive learning on packet/flow pairs to improve representation quality before DT training."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Partial observability and non-Markovian traffic patterns complicate real-time decisions.",
        "Balancing accuracy and timeliness requires careful reward shaping and policy control.",
        "High-throughput packet parsing and embedding generation from PCAP in production environments.",
        "Handling irregular, bursty packet arrival in continuous time within strict latency budgets."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Formulates packet-level network intrusion detection as sequence modeling for decision-making using Decision Transformers.",
      "Introduces an autoencoder-based approach to compress packet payloads into compact embeddings for causal transformer input.",
      "Designs a reward function that penalizes delayed decisions to explicitly trade off detection accuracy and timeliness.",
      "Adapts a continuous-time transformer to handle temporal information and irregular packet timings for offline network traffic.",
      "Highlights DT’s applicability to NIDS even with non-optimal trajectories and proposes importance sampling to weight traces.",
      "Evaluates on public NIDS datasets and reports outperforming RL and sequence modeling baselines in detection accuracy and timeliness."
    ]
  },
  {
    "arxiv_id": "2312.12575v3",
    "title": "LLMs Cannot Reliably Identify and Reason About Security Vulnerabilities (Yet?): A Comprehensive Evaluation, Framework, and Benchmarks",
    "authors": "Saad Ullah; Mingji Han; Saurabh Pujar; Hammond Pearce; Ayse Coskun; Gianluca Stringhini",
    "abstract": "Large Language Models (LLMs) have been suggested for use in automated vulnerability repair, but benchmarks showing they can consistently identify security-related bugs are lacking. We thus develop SecLLMHolmes, a fully automated evaluation framework that performs the most detailed investigation to date on whether LLMs can reliably identify and reason about security-related bugs. We construct a set of 228 code scenarios and analyze eight of the most capable LLMs across eight different investigative dimensions using our framework. Our evaluation shows LLMs provide non-deterministic responses, incorrect and unfaithful reasoning, and perform poorly in real-world scenarios. Most importantly, our findings reveal significant non-robustness in even the most advanced models like `PaLM2' and `GPT-4': by merely changing function or variable names, or by the addition of library functions in the source code, these models can yield incorrect answers in 26% and 17% of cases, respectively. These findings demonstrate that further LLM advances are needed before LLMs can be used as general purpose security assistants.",
    "published_date": "2023-12-19",
    "pdf_link": "https://arxiv.org/pdf/2312.12575v3",
    "paper_types": [
      "benchmark",
      "empirical_analysis",
      "new_dataset"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Vulnerability Detection (Static Analysis)",
      "specific_problem": "Evaluating whether chat-based LLMs can identify and reason about software vulnerabilities in source code",
      "attack_types": [
        "CWE-787 Out-of-bounds Write",
        "CWE-79 Cross-site Scripting (XSS)",
        "CWE-89 SQL Injection",
        "CWE-416 Use After Free",
        "CWE-22 Path Traversal",
        "CWE-476 NULL Pointer Dereference",
        "CWE-190 Integer Overflow/Wraparound",
        "CWE-77 Command Injection"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Evaluation Framework",
        "specific": "SecLLMHolmes",
        "novel_contribution": "A fully automated, multi-dimensional evaluation framework with 17 prompt templates to test LLMs on vulnerability detection accuracy, reasoning faithfulness, robustness to code augmentations, determinism, difficulty levels, and real-world CVEs"
      },
      {
        "type": "baseline",
        "category": "Transformer LLM",
        "specific": "GPT-4",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer LLM",
        "specific": "gpt-3.5-turbo-16k",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer LLM",
        "specific": "PaLM2 codechat-bison@001",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer LLM",
        "specific": "PaLM2 chat-bison@001",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer LLM",
        "specific": "CodeLlama-7b-instruct",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer LLM",
        "specific": "CodeLlama-13b-instruct",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer LLM",
        "specific": "CodeLlama-34b-instruct",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer LLM",
        "specific": "StarChat-beta (StarCoder+)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Zero-shot",
      "Few-shot",
      "Prompt-based",
      "Chain-of-Thought"
    ],
    "datasets": [
      {
        "name": "SecLLMHolmes Benchmark Suite (overall 228 scenarios)",
        "type": "public",
        "domain": "source_code",
        "link": "https://github.com/ai4cloudops/SecLLMHolmes",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "Hand-crafted CWE Scenarios (48 scenarios: vulnerable/patched pairs across 8 CWEs, C and Python)",
        "type": "public",
        "domain": "source_code",
        "link": "https://github.com/ai4cloudops/SecLLMHolmes",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "Real-World CVE Scenarios (30 scenarios: 15 CVEs from 4 OSS projects, 2023)",
        "type": "public",
        "domain": "source_code",
        "link": "https://github.com/ai4cloudops/SecLLMHolmes",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "Code Augmentations Set (150 augmented scenarios: whitespace, renaming, library substitutions)",
        "type": "public",
        "domain": "source_code",
        "link": "https://github.com/ai4cloudops/SecLLMHolmes",
        "is_new_contribution": true,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "GPT-4",
        "paper_reference": null,
        "metric": "Non-robustness rate under simple code augmentations (variable/function renaming or added library functions)",
        "their_result": "17% incorrect answers when augmented",
        "baseline_result": null
      },
      {
        "method_name": "PaLM2 (codechat-bison/chat-bison)",
        "paper_reference": null,
        "metric": "Non-robustness rate under simple code augmentations",
        "their_result": "26% incorrect answers when augmented",
        "baseline_result": null
      },
      {
        "method_name": "All evaluated LLMs (8 models)",
        "paper_reference": null,
        "metric": "False Positive Rate (flagging patched code as vulnerable)",
        "their_result": "High FPR across models (exact values not provided in excerpt)",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "False Positive Rate (FPR)",
      "Determinism across runs (answer change rate)",
      "Robustness to code augmentations (error rate under perturbations)",
      "Reasoning faithfulness (qualitative and automatic)",
      "ROUGE (reasoning similarity)",
      "Cosine similarity (reasoning similarity)",
      "LLM (GPT-4) evaluator for reasoning quality"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can LLMs be used as helpful security assistants for vulnerability detection?"
      ],
      "gaps_identified": [
        "No standardized and automated approach to evaluate LLMs for identifying vulnerable code",
        "Prior evaluations are limited in number of LLMs, coverage of vulnerabilities, diversity of prompts, robustness testing, and often require manual evaluation",
        "Existing studies largely perform only binary evaluation and do not assess reasoning/root-cause quality",
        "LLM-based vulnerability detection tools exhibit non-robustness to trivial code augmentations and high FPR in leaderboards"
      ],
      "limitations": [],
      "future_work": [
        "Use SecLLMHolmes as a benchmark suite to track meaningful progress of future LLMs on vulnerability detection",
        "Broaden coverage to additional programming languages and vulnerability types (implied by framework design)"
      ],
      "motivation": "Assess whether current LLMs can reliably identify and reason about security-related bugs and provide a comprehensive, automated benchmark to fill the evaluation gap.",
      "potential_research_ideas": [
        "Integrate program analysis (CFG/DFG/CPG) with LLM prompting to improve data/control-flow reasoning for vulnerability detection",
        "Adversarial training or robustness tuning of code LLMs using systematic code augmentations (renaming, whitespace, library swaps) from the benchmark",
        "Develop verification-augmented chain-of-thought that checks intermediate reasoning steps against static analyses for faithfulness",
        "Fine-tune or instruction-tune LLMs on post-2023 CVE corpora with aligned reasoning annotations specific to CWEs",
        "Design retrieval-augmented code understanding that pulls CWE definitions, secure patterns, and project-specific APIs into prompts",
        "Create ensemble decision protocols combining multiple prompts and models to reduce non-determinism and FPR",
        "Build a calibrated uncertainty estimator for LLM vulnerability judgments to gate warnings in CI pipelines"
      ],
      "architectural_improvement_recommendations": [
        "Hybrid neuro-symbolic pipeline: LLM for hypothesis generation + static/dynamic analysis for validation and root-cause attribution",
        "Reasoning scaffolds aligned to security-expert workflows (overview → critical components → detailed checks → verdict) with explicit subtask verification",
        "Prompt engineering with role/task conditioning plus CWE-specific definitions and few-shot exemplars, optimized per model via automated search",
        "Robustness-oriented training using code transformations (identifier renaming, API substitutions, whitespace) and contrastive objectives",
        "Long-context segmentation and hierarchical attention to handle real-world repositories without truncation"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/ai4cloudops/SecLLMHolmes",
      "frameworks": [
        "OpenAI API",
        "Google PaLM API",
        "Hugging Face Transformers"
      ],
      "reproducibility_score": "high",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "High false positive rates leading to developer burden",
        "Non-deterministic outputs across runs",
        "Lack of robustness to trivial code augmentations (e.g., variable/function renaming, whitespace, library additions)",
        "Poor performance on real-world CVEs"
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": true,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Develop SecLLMHolmes, a fully automated, comprehensive framework to evaluate LLMs for vulnerability detection across eight dimensions",
      "Construct and release a benchmark of 228 code scenarios (48 hand-crafted, 30 real-world CVEs, 150 augmented) and 17 prompting techniques",
      "Evaluate eight state-of-the-art LLMs and show high FPR, non-determinism, unfaithful reasoning, lack of robustness, and poor real-world performance",
      "Identify and enumerate key shortcomings of current LLMs for vulnerability detection and provide a checklist for future research"
    ]
  },
  {
    "arxiv_id": "2312.11301v1",
    "title": "Ensuring Cross-Device Portability of Electromagnetic Side-Channel Analysis",
    "authors": "Lojenaa Navanesana; Nhien-An Le-Khac; Mark Scanlon; Kasun De Zoysa; Asanka P. Sayakkara",
    "abstract": "Investigation on smart devices has become an essential subdomain in digital forensics. The inherent diversity and complexity of smart devices pose a challenge to the extraction of evidence without physically tampering with it, which is often a strict requirement in law enforcement and legal proceedings. Recently, this has led to the application of non-intrusive Electromagnetic Side-Channel Analysis (EM-SCA) as an emerging approach to extract forensic insights from smart devices. EM-SCA for digital forensics is still in its infancy, and has only been tested on a small number of devices so far. Most importantly, the question still remains whether Machine Learning (ML) models in EM-SCA are portable across multiple devices to be useful in digital forensics, i.e., cross-device portability. This study experimentally explores this aspect of EM-SCA using a wide set of smart devices. The experiments using various iPhones and Nordic Semiconductor nRF52-DK devices indicate that the direct application of pre-trained ML models across multiple identical devices does not yield optimal outcomes (under 20% accuracy in most cases). Subsequent experiments included collecting distinct samples of EM traces from all the devices to train new ML models with mixed device data; this also fell short of expectations (still below 20% accuracy). This prompted the adoption of transfer learning techniques, which showed promise for cross-model implementations. In particular, for the iPhone 13 and nRF52-DK devices, applying transfer learning techniques resulted in achieving the highest accuracy, with accuracy scores of 98% and 96%, respectively. This result makes a significant advancement in the application of EM-SCA to digital forensics by enabling the use of pre-trained models across identical or similar devices.",
    "published_date": "2023-12-18",
    "pdf_link": "https://arxiv.org/pdf/2312.11301v1",
    "paper_types": [
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Digital Forensics",
      "subdomain": "Side-Channel Forensics",
      "specific_problem": "Cross-device portability of ML models for electromagnetic side-channel analysis (EM-SCA) to recognize software activities on smart devices and IoT boards",
      "attack_types": [
        "electromagnetic side-channel analysis",
        "cross-device portability",
        "transfer learning for SCA"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Deep Neural Network (classifier)",
        "specific": null,
        "novel_contribution": "Adoption of transfer learning by retraining the output layer of a pre-trained EM-SCA model to enable cross-device portability, achieving high accuracy across identical devices."
      },
      {
        "type": "baseline",
        "category": "Supervised classifier (per-device training)",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Supervised classifier trained on mixed-device data",
        "specific": null,
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Transfer Learning (fine-tuning)"
    ],
    "datasets": [
      {
        "name": "Smartphone EM-SCA traces (iPhones: 4S, 6S, 8, 13×3 units, 14 Pro) with 10 activities",
        "type": "private",
        "domain": "side_channel_em_traces_smartphone",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "IoT EM-SCA traces (Nordic nRF52-DK, 2 devices) with 8 activities",
        "type": "private",
        "domain": "side_channel_em_traces_iot_embedded",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Direct application of a pre-trained per-device model to an identical device (zero-shot cross-device)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "Transfer learning: 98% (iPhone 13) and 96% (nRF52-DK)",
        "baseline_result": "Under 20% accuracy in most cases"
      },
      {
        "method_name": "Training new models on mixed data from multiple identical devices (pooled/mixed-device supervised)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "Transfer learning: 98% (iPhone 13) and 96% (nRF52-DK)",
        "baseline_result": "Still below 20% accuracy"
      },
      {
        "method_name": "Applying a pre-trained model to different samples of the same device collected at different times (temporal generalization)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "Transfer learning (when applied): high accuracy on targeted devices (up to 98%/96%)",
        "baseline_result": "Did not align with expectations; under ~20%"
      }
    ],
    "performance_metrics_used": [
      "Accuracy"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Are ML models for EM-SCA portable across multiple devices (even identical make/model) for digital forensics?",
        "Does training on mixed-device data restore cross-device performance?",
        "Can transfer learning enable cross-device portability for EM-SCA on smartphones and IoT devices?"
      ],
      "gaps_identified": [
        "EM-SCA for digital forensics has only been tested on a small number of devices; cross-device portability remains unanswered.",
        "Models trained on one device often fail to generalize to other identical devices due to device variability, environmental factors, and architectural differences (e.g., multi-core SoCs).",
        "Prior work suggested transfer learning but lacked validation on complex smartphone SoCs."
      ],
      "limitations": [
        "This study is confined to experiments on identical devices and diverse samples of the same devices.",
        "Manual probe placement and manual gain setting; no automated optimization of sensor position.",
        "Dataset is not released publicly; reproducibility limited by data access."
      ],
      "future_work": [],
      "motivation": "Enable non-invasive EM-SCA to be practically useful in legal/forensic contexts by ensuring models can be reused across identical or similar devices without re-collecting large per-device datasets.",
      "potential_research_ideas": [
        "Unsupervised or self-supervised representation learning on EM traces to learn device-invariant features before fine-tuning on small labeled sets.",
        "Domain-adversarial training (e.g., DANN) to explicitly enforce device-invariant embeddings across identical and closely-related devices.",
        "Few-shot/meta-learning for rapid personalization of EM-SCA models to new devices with minimal labeled traces.",
        "Automated probe-position calibration using active scanning and optimization to reduce location sensitivity and variability.",
        "Contrastive multi-session learning to mitigate temporal drift (different collection times) and environment changes.",
        "Style-transfer or data augmentation (e.g., GAN-based) to simulate device-specific EM ‘styles’ for robustness.",
        "Multi-sensor fusion (EM + power + acoustic if available) to improve robustness and portability.",
        "Cross-frequency and multi-resolution time–frequency architectures to capture invariant spectral–temporal patterns.",
        "Benchmarking portability across broader phone/SoC families and vendors; standardized portability protocols for EM-SCA."
      ],
      "architectural_improvement_recommendations": [
        "Replace/reinforce the classifier with a time–frequency CNN/Transformer using spectrogram inputs with attention over frequency bands sensitive to SoC activity.",
        "Introduce domain-adversarial heads to learn device-invariant representations; add device-ID confusion loss.",
        "Use instance/layer normalization tuned for varying EM amplitude distributions; apply spectral augmentation (SpecAugment-like) and band-drop regularization.",
        "Adopt multi-branch, multi-scale feature extractors to capture both narrowband clock harmonics and broadband noise patterns.",
        "Apply few-shot adapters or LoRA-style lightweight adapters for rapid on-device transfer to new units.",
        "Incorporate temporal drift calibration layers using batch norm statistics per-session and calibration tokens.",
        "Explore contrastive pretraining (SimCLR/MoCo) on large unlabeled EM traces, followed by small labeled fine-tunes."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "GNU Radio"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Device-to-device variability even for identical make/model reduces direct portability.",
        "Manual probe placement and gain tuning critically affect signal quality and repeatability.",
        "Environmental noise and temporal drift between capture sessions impact model generalization.",
        "Complex SoC multi-core behavior influences EM patterns and classifier reliability.",
        "Lack of publicly available datasets hinders benchmarking and reproducibility."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Experimentally investigates cross-device behavior of EM-SCA ML models using multiple real smartphones (iPhones) and IoT devices (nRF52-DK) with complex SoC architectures.",
      "Shows that direct application of pre-trained models and training on mixed-device data do not yield good cross-device performance (\"under 20% accuracy in most cases\").",
      "Demonstrates transfer learning effectiveness for cross-device portability, reporting: \"for the iPhone 13 and nRF52-DK devices, applying transfer learning techniques resulted in achieving the highest accuracy, with accuracy scores of 98% and 96%, respectively.\""
    ]
  },
  {
    "arxiv_id": "2312.07921v1",
    "title": "BinGo: Identifying Security Patches in Binary Code with Graph Representation Learning",
    "authors": "Xu He; Shu Wang; Pengbin Feng; Xinda Wang; Shiyu Sun; Qi Li; Kun Sun",
    "abstract": "A timely software update is vital to combat the increasing security vulnerabilities. However, some software vendors may secretly patch their vulnerabilities without creating CVE entries or even describing the security issue in their change log. Thus, it is critical to identify these hidden security patches and defeat potential N-day attacks. Researchers have employed various machine learning techniques to identify security patches in open-source software, leveraging the syntax and semantic features of the software changes and commit messages. However, all these solutions cannot be directly applied to the binary code, whose instructions and program flow may dramatically vary due to different compilation configurations. In this paper, we propose BinGo, a new security patch detection system for binary code. The main idea is to present the binary code as code property graphs to enable a comprehensive understanding of program flow and perform a language model over each basic block of binary code to catch the instruction semantics. BinGo consists of four phases, namely, patch data pre-processing, graph extraction, embedding generation, and graph representation learning. Due to the lack of an existing binary security patch dataset, we construct such a dataset by compiling the pre-patch and post-patch source code of the Linux kernel. Our experimental results show BinGo can achieve up to 80.77% accuracy in identifying security patches between two neighboring versions of binary code. Moreover, BinGo can effectively reduce the false positives and false negatives caused by the different compilers and optimization levels.",
    "published_date": "2023-12-13",
    "pdf_link": "https://arxiv.org/pdf/2312.07921v1",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "benchmark",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Vulnerability Management and Patch Analysis",
      "specific_problem": "Identifying security patches in binary code between pre- and post-patch versions",
      "attack_types": [
        "N-day exploitation",
        "Vulnerability exploitation"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "GNN",
        "specific": "Siamese graph convolutional network with multi-head attention over heterogeneous edges (CFG/CDG/DDG)",
        "novel_contribution": "Siamese architecture on pre/post CPGs with a multi-head attention convolution that treats each edge type (CFG, CDG, DDG) as an individual convolution channel and aggregates across channels."
      },
      {
        "type": "primary",
        "category": "Transformer LM for code",
        "specific": "BERT-like instruction language model (LM-based embedding on basic blocks)",
        "novel_contribution": "Embedding assembly instruction sequences per basic block using a language-model-based encoder to capture instruction semantics robust to compiler/optimization changes."
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "BinGo binary security patch dataset (Linux kernel)",
        "type": "synthetic",
        "domain": "binary_code",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "accuracy",
      "F1 score",
      "false negative ratio (FNR)",
      "false positive ratio (FPR)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can security patches in binaries be automatically identified by comparing pre- and post-patch versions when compilation configurations change instruction sequences and control/data flows?",
        "Does combining CPG-based program-flow relations with LM-based basic-block semantics improve robustness to compiler and optimization-level variability?"
      ],
      "gaps_identified": [
        "Existing security patch identification approaches rely on source code diffs/metadata and cannot be directly applied to binaries.",
        "Binary code instructions and program flow vary dramatically across compilers and optimization levels, breaking sequential models and simple graph statistics.",
        "Prior graph-based representations embed only statistical features of statements and miss subtle code changes in security patches, leading to false negatives.",
        "Lack of an existing binary security patch dataset for training/evaluation."
      ],
      "limitations": [
        "Assumption: There is only one patch between the two compared binary versions; all modified basic blocks come from the same patch.",
        "Assumption: A security patch fixes one vulnerability and all patch-related basic blocks are connected via data or control dependencies.",
        "Evaluation dataset is constructed from Linux kernel patches; generalization to other software ecosystems and architectures is not shown in provided text."
      ],
      "future_work": [],
      "motivation": "Vendors may silently apply security patches without CVEs or clear changelogs; identifying hidden security patches in released binaries is critical to mitigate N-day attacks, but source-based ML methods do not transfer to binaries due to compilation-induced variability and no available binary patch datasets.",
      "potential_research_ideas": [
        "Extend to multi-patch scenarios and develop a patch clustering/segmentation method to handle multiple concurrent patches between versions.",
        "Cross-architecture generalization (x86, ARM, MIPS, RISC-V) via domain-adaptive or contrastive representation learning on instruction embeddings and graph structure.",
        "Contrastive pretraining on large corpora of binaries with cross-compiler/opt-level views to learn compiler-invariant embeddings before supervised fine-tuning.",
        "Integrate dynamic analysis (execution traces) or symbolic execution features with the CPG to better capture semantic differences introduced by security checks.",
        "Severity estimation and patch-type taxonomy classification (security vs non-security plus CWE class) to aid triage and prioritization.",
        "Unsupervised detection of hidden security patches across long version histories via change-point detection on graph embeddings.",
        "Active learning or weak supervision using CVE references and commit metadata when available to reduce labeling burden."
      ],
      "architectural_improvement_recommendations": [
        "Adopt heterogeneous GNN operators (e.g., relational graph convolution or edge-type-specific attention with learned edge embeddings) instead of simple channel aggregation.",
        "Use a joint encoder that fuses basic-block LM embeddings with structural encoders via gated feature fusion or cross-attention.",
        "Introduce graph-level contrastive losses across compiler/optimization variants of the same patch to enforce compiler invariance.",
        "Replace/augment LM with instruction-specific pretrained models (e.g., Token-level BPE on opcodes/operands, masked LM pretraining on large disassembly corpora).",
        "Incorporate positional and structural encodings (dominance/post-dominance, loop nesting, distance-to-patch) into node/edge features.",
        "Add a patch localization head to jointly learn to localize and classify patches, potentially improving representation quality.",
        "Leverage graph matching or differentiable alignment to explicitly align pre/post graphs before siamese comparison."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Variability across compilers and optimization levels causing structural and instruction-level differences in binaries.",
        "Handling stripped binaries or missing symbols; reliance on robust diffing to find patch-related blocks.",
        "Assumptions of single-patch and connectedness may not hold for complex releases.",
        "Potential high overhead of inter-procedural diffing; mitigated by modified DeepBinDiff but still a concern for large binaries."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "A new binary security patch detection system (BinGo) that identifies potential hidden security patches in newly released binaries.",
      "A joint representation that integrates code property graphs (CFG, CDG, DDG) with LM-based basic-block embeddings to capture program flow and instruction semantics.",
      "A siamese graph-learning model with multi-head attention convolution over heterogeneous edge types to compare pre- and post-patch graphs.",
      "A new benchmark dataset constructed by compiling pre- and post-patch Linux kernel code; BinGo achieves \"up to 80.8% accuracy with an F1 score of 0.76\" and shows FNR 29.15% and FPR 11.82%, remaining effective across compilers and optimization levels."
    ]
  },
  {
    "arxiv_id": "2312.01681v1",
    "title": "Malicious Lateral Movement in 5G Core With Network Slicing And Its Detection",
    "authors": "Ayush Kumar; Vrizlynn L. L. Thing",
    "abstract": "5G networks are susceptible to cyber attacks due to reasons such as implementation issues and vulnerabilities in 3GPP standard specifications. In this work, we propose lateral movement strategies in a 5G Core (5GC) with network slicing enabled, as part of a larger attack campaign by well-resourced adversaries such as APT groups. Further, we present 5GLatte, a system to detect such malicious lateral movement. 5GLatte operates on a host-container access graph built using host/NF container logs collected from the 5GC. Paths inferred from the access graph are scored based on selected filtering criteria and subsequently presented as input to a threshold-based anomaly detection algorithm to reveal malicious lateral movement paths. We evaluate 5GLatte on a dataset containing attack campaigns (based on MITRE ATT&CK and FiGHT frameworks) launched in a 5G test environment which shows that compared to other lateral movement detectors based on state-of-the-art, it can achieve higher true positive rates with similar false positive rates.",
    "published_date": "2023-12-04",
    "pdf_link": "https://arxiv.org/pdf/2312.01681v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Mobile Network Security",
      "subdomain": "5G Core and Network Slicing Security",
      "specific_problem": "Detection of malicious lateral movement across network functions and slices in a 5G Core using host/container logs",
      "attack_types": [
        "Lateral Movement",
        "Container escape",
        "Credential abuse (SSH/remote access)",
        "Container/Kubernetes misconfiguration exploitation",
        "Cross-slice pivoting"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Graph-based anomaly detection",
        "specific": "Threshold-based scoring of host-container path anomalies",
        "novel_contribution": "Constructs a host-container access graph from 5G Core logs; computes a multiplicative path score using historical edge probabilities, slice transitions, and hop count; flags anomalies using a threshold learned from benign training data"
      }
    ],
    "learning_paradigm": [
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "OAI-based 5G Core test environment logs with attack campaigns (5GLatte evaluation dataset)",
        "type": "proprietary",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "2015 LANL Comprehensive Cyber Security Events dataset",
        "type": "public",
        "domain": "log_files",
        "link": "https://csr.lanl.gov/data/cyber1/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "True Positive Rate (TPR)",
      "False Positive Rate (FPR)"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "What are feasible malicious lateral movement strategies within a 5G Core with network slicing?",
        "Can host and NF container logs be leveraged to detect lateral movement in a 5G Core?",
        "How does a graph/path-based threshold anomaly detector perform versus state-of-the-art LM detectors in this context?"
      ],
      "gaps_identified": [
        "Absence of existing datasets containing lateral movement in a 5G Core environment",
        "Prior LM detection focuses on enterprise IT networks and does not address 5G Core specifics (NF containers, network slices)",
        "Existing slice security management focuses on resource anomalies or risk assessment, not exploitation of NF containers orchestrating slices"
      ],
      "limitations": [
        "Implementation not designed for real-time operation",
        "Evaluation performed on a lab OAI-based 5G Core testbed; generalizability to production 5GCs not demonstrated",
        "Training used a single day of benign data; sensitivity to training duration and concept drift not evaluated",
        "Approach assumes attacker movement is reflected in host/container logs; evasion via log tampering or living-off-the-land not considered",
        "Dataset and code are not released, limiting reproducibility"
      ],
      "future_work": [],
      "motivation": "5G Core with network slicing introduces new attack surfaces; attackers can pivot across NFs and slices. There is a need to characterize such lateral movement and to detect it effectively using available telemetry.",
      "potential_research_ideas": [
        "Create and release a standardized, labeled 5G Core LM dataset covering diverse vendors/topologies and attack tactics to catalyze research benchmarking",
        "Develop temporal/dynamic graph learning models (e.g., TGNNs) tailored to 5G NF/slice semantics for LM detection",
        "Design log integrity and provenance mechanisms (e.g., secure logging, attestation) to harden the pipeline against log forgery/evasion",
        "Fuse multi-modal telemetry (control/user plane traffic, API calls, kube events) with host/container logs for robust detection",
        "Online/streaming LM detection with adaptive thresholds and drift detection for changing 5G operations",
        "Attack simulation framework for 5G cores to generate varied LM campaigns aligned to MITRE ATT&CK and FiGHT",
        "Explainable LM detection that highlights suspicious subpaths, slice transitions, and rare edge contributions to aid analysts"
      ],
      "architectural_improvement_recommendations": [
        "Replace heuristic multiplicative path score with a learned scoring function using representation learning over paths/subgraphs",
        "Incorporate temporal modeling (e.g., Hawkes processes or temporal GNNs) to capture sequence dynamics and inter-slice transitions",
        "Use robust anomaly scoring and calibration (e.g., Extreme Value Theory, Isolation Forest/LOF on path features) for threshold selection",
        "Profile per-slice and per-NF baselines to reduce false positives and capture slice-specific behaviors",
        "Implement streaming graph construction and windowed inference for near real-time detection",
        "Augment features with privilege change indicators and container capability sets to better capture container-escape signals"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "Python",
        "networkx"
      ],
      "reproducibility_score": "low",
      "computational_requirements": "Evaluated on VMs (Intel Xeon Silver 4216 @2.10GHz, 8 cores, 16GB RAM) in a VMware ESXi-hosted OAI 5GC testbed; no GPU reported"
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "OAI-based 5G Core test environment with network slicing (AMF, AUSF, NSSF, NRF, SMF, UPF) and NR-UE/gNB emulators",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Requires collection and correlation of host authentication and container event logs across NF hosts",
        "Not designed for real-time operation in current implementation",
        "Variability in 5GC vendor implementations and slice orchestration may affect portability",
        "Potential exposure to log tampering or incomplete logging in adversarial settings"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposed and studied malicious lateral movement strategies in 5G Cores with network slicing",
      "Designed 5GLatte, a threshold-based anomaly detection system operating on a host-container access graph built from 5GC logs",
      "Built a 5GC OAI-based testbed and collected a realistic dataset (benign and attack campaigns aligned with MITRE ATT&CK and FiGHT) to evaluate 5GLatte",
      "Reported that 5GLatte achieved higher true positive rates with similar false positive rates compared to state-of-the-art lateral movement detectors (quantitative details not provided in the excerpt)"
    ]
  },
  {
    "arxiv_id": "2312.17300v5",
    "title": "Improving Intrusion Detection with Domain-Invariant Representation Learning in Latent Space",
    "authors": "Padmaksha Roy; Tyler Cody; Himanshu Singhal; Kevin Choi; Ming Jin",
    "abstract": "Zero-day anomaly detection is critical in industrial applications where novel, unforeseen threats can compromise system integrity and safety. Traditional detection systems often fail to identify these unseen anomalies due to their reliance on in-distribution data. Domain generalization addresses this gap by leveraging knowledge from multiple known domains to detect out-of-distribution events. In this work, we introduce a multi-task representation learning technique that fuses information across related domains into a unified latent space. By jointly optimizing classification, reconstruction, and mutual information regularization losses, our method learns a minimal(bottleneck), domain-invariant representation that discards spurious correlations. This latent space decorrelation enhances generalization, enabling the detection of anomalies in unseen domains. Our experimental results demonstrate significant improvements in zero-day or novel anomaly detection across diverse anomaly detection datasets.",
    "published_date": "2023-12-28",
    "pdf_link": "https://arxiv.org/pdf/2312.17300v5",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Zero-day/novel anomaly (OOD) intrusion detection via domain-invariant representation learning across multiple related domains",
      "attack_types": [
        "RARE (CSE-CIC-IDS2018 rare attacks)",
        "SLOWHTTPS",
        "HOIC",
        "BENIGN (held-out day)",
        "INFILTRATION",
        "BOTNET",
        "DoS",
        "DDoS",
        "RECON",
        "WEB",
        "MIRAI",
        "SPOOFING",
        "MQTT (IoMT attack category)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Autoencoder",
        "specific": "Multi-task Latent Space Regularized Encoder-Decoder (MTLS-RED)",
        "novel_contribution": "Joint optimization of classification (cross-entropy), reconstruction loss, and matrix-based mutual information regularization to learn a minimal, domain-invariant latent bottleneck"
      },
      {
        "type": "primary",
        "category": "Information Bottleneck / Mutual Information Regularization",
        "specific": "Matrix-based Rényi’s second-order entropy MI divergence between input and latent RBF-kernel spaces",
        "novel_contribution": "Minimizes MI(Kx; Kz) to decorrelate input–latent dependencies and discard spurious domain-specific correlations"
      },
      {
        "type": "primary",
        "category": "Kernel Methods",
        "specific": "RBF kernels over input and latent batches with learnable bandwidths",
        "novel_contribution": "Batchwise kernel MI penalty with learnable σ to enforce invariance"
      },
      {
        "type": "primary",
        "category": "Multi-task Learning",
        "specific": "Joint training across multiple source and cross domains",
        "novel_contribution": "Mixing data from domains with differing correlation structures to induce domain-invariant representations"
      },
      {
        "type": "baseline",
        "category": "Domain Adaptation",
        "specific": "CORAL (Correlation Alignment)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Autoencoder",
        "specific": "MTAE (Multi-task Autoencoder)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Autoencoder with Distribution Alignment",
        "specific": "MMD-AE (MMD-Autoencoder)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Supervised Autoencoder",
        "specific": "NSAE (Noise-Enhanced Supervised Autoencoder)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Domain Generalization",
        "specific": "DIFEX (Domain-invariant Feature Exploration)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Multi-task Learning",
      "Zero-shot OOD Detection",
      "Regularized Representation Learning"
    ],
    "datasets": [
      {
        "name": "CSE-CIC-IDS2018",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CICIoT2023",
        "type": "public",
        "domain": "iot_network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CICIoMT 2024",
        "type": "public",
        "domain": "iomt_network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Arrhythmia (ECG beats: N, S, V, F, Q)",
        "type": "public",
        "domain": "healthcare_ecg",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "CORAL (Correlation Alignment)",
        "paper_reference": null,
        "metric": "accuracy, precision, recall, AUC-ROC",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "MTAE (Multi-task Autoencoder)",
        "paper_reference": null,
        "metric": "accuracy, precision, recall, AUC-ROC",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "MMD-AE (MMD Autoencoder)",
        "paper_reference": null,
        "metric": "accuracy, precision, recall, AUC-ROC",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "NSAE (Noise-Enhanced Supervised Autoencoder)",
        "paper_reference": null,
        "metric": "accuracy, precision, recall, AUC-ROC",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "DIFEX (Domain-invariant Feature Exploration)",
        "paper_reference": null,
        "metric": "accuracy, precision, recall, AUC-ROC",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "accuracy",
      "precision",
      "recall",
      "AUC-ROC"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can a joint objective over classification, reconstruction, and mutual information minimization learn a minimal, domain-invariant latent representation that generalizes to unseen (OOD) anomaly classes?",
        "Does mixing data from multiple source and cross domains with differing correlation structures improve zero-shot OOD anomaly detection?",
        "Can decorrelating input–latent dependencies via matrix-based MI improve detection of rare and novel attacks?"
      ],
      "gaps_identified": [
        "Traditional intrusion/anomaly detectors rely on in-distribution data and fail on unseen anomalies (zero-day).",
        "Few-shot/meta-learning approaches often require target domain data during training or risk embedding domain-specific biases.",
        "High-dimensional feature spaces suffer from concentration of distances, degrading generalization.",
        "Adversarial domain adaptation can suffer from negative transfer that hurts generalization."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Improve zero-day anomaly detection by learning domain-invariant, minimal latent representations that discard spurious domain-specific correlations, enabling generalization to unseen domains without access to target domain data.",
      "potential_research_ideas": [
        "Incorporate self-supervised or contrastive pretraining (e.g., InfoNCE/SimCLR) to further strengthen invariant feature learning before supervised multi-task training.",
        "Extend to continual/online learning to adapt the latent space as new domains or attacks are encountered without catastrophic forgetting.",
        "Develop calibrated OOD scoring on top of the latent space (energy-based, ODIN-like perturbations) for threshold-free deployment.",
        "Investigate causal invariance methods to explicitly separate causal from spurious correlations in traffic features.",
        "Explore encrypted traffic settings and side-channel features to test invariance under limited visibility.",
        "Assess adversarial robustness of the latent invariance (evasion attacks) and add adversarial training in latent space.",
        "Scale MI estimation with approximate/mini-batch-friendly estimators (e.g., HSIC, MINE) for large batches and streaming data.",
        "Apply sequence/temporal architectures (TCN/Transformers) to model flow series with the same MI bottleneck principle."
      ],
      "architectural_improvement_recommendations": [
        "Replace or complement MI penalty with HSIC-based independence regularization for computational efficiency and stability.",
        "Adopt a Variational Information Bottleneck (VIB) or β-VAE-style stochastic bottleneck to tightly control information flow.",
        "Introduce mixture-of-experts or domain-specific adapters with a shared invariant core, trained with the MI penalty.",
        "Use learned dynamic weighting (e.g., uncertainty-based) for balancing CE, reconstruction, and MI losses per domain/batch.",
        "Employ temporal models (1D CNN/TCN/Transformer) for traffic sequences rather than static MLP encoders.",
        "Add prototype/center loss in latent space to compact class clusters and improve OOD separability.",
        "Implement bilevel optimization or meta-learning to tune kernel bandwidths σ for MI computation robustly across domains."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Distribution shift across networks/devices and domains in practice",
        "Potential negative transfer if source–cross domain mix is poorly chosen",
        "Computational overhead of batchwise kernel MI estimation",
        "Requirement for labeled data in multiple source and cross domains to train the classifier"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes a multi-task latent-space encoder–decoder framework with mutual information regularization and reconstruction to retain only relevant features for OOD classification, yielding a compressed, domain-invariant representation.",
      "Demonstrates that integrating data from multiple sources and cross-domains with varying correlation patterns enhances coverage and generalization to unseen domains.",
      "Reports empirical gains: “Experiments demonstrate an 8%-15% increase in average precision, and recall and a 4%-9% improvement in average AUC-ROC across all source/IN, cross-domain, and OOD datasets.”",
      "Provides ablation indicating higher MI regularization improves rare-class detection (e.g., 10–20% improvement and stability for the RARE class metrics)."
    ]
  },
  {
    "arxiv_id": "2311.07460v2",
    "title": "KnowSafe: Combined Knowledge and Data Driven Hazard Mitigation in Artificial Pancreas Systems",
    "authors": "Xugui Zhou; Maxfield Kouzel; Chloe Smith; Homa Alemzadeh",
    "abstract": "Significant progress has been made in anomaly detection and run-time monitoring to improve the safety and security of cyber-physical systems (CPS). However, less attention has been paid to hazard mitigation. This paper proposes a combined knowledge and data driven approach, KnowSafe, for the design of safety engines that can predict and mitigate safety hazards resulting from safety-critical malicious attacks or accidental faults targeting a CPS controller. We integrate domain-specific knowledge of safety constraints and context-specific mitigation actions with machine learning (ML) techniques to estimate system trajectories in the far and near future, infer potential hazards, and generate optimal corrective actions to keep the system safe. Experimental evaluation on two realistic closed-loop testbeds for artificial pancreas systems (APS) and a real-world clinical trial dataset for diabetes treatment demonstrates that KnowSafe outperforms the state-of-the-art by achieving higher accuracy in predicting system state trajectories and potential hazards, a low false positive rate, and no false negatives. It also maintains the safe operation of the simulated APS despite faults or attacks without introducing any new hazards, with a hazard mitigation success rate of 92.8%, which is at least 76% higher than solely rule-based (50.9%) and data-driven (52.7%) methods.",
    "published_date": "2023-11-13",
    "pdf_link": "https://arxiv.org/pdf/2311.07460v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Medical Device Security",
      "subdomain": "Cyber-Physical Systems (CPS) Resilience and Safety",
      "specific_problem": "Run-time hazard prediction and mitigation for controller-targeting attacks/faults in Artificial Pancreas Systems",
      "attack_types": [
        "Controller integrity manipulation (change insulin delivery amount)",
        "Controller availability manipulation (stop insulin delivery, stop refreshing state variables)",
        "Input tampering to controller (change controller input values)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "RNN",
        "specific": "LSTM encoder-decoder (multivariate regression sequence-to-sequence)",
        "novel_contribution": "Custom knowledge-guided loss that penalizes predictions outside domain-defined reachable sets in a transformed state space; two-level long-/short-term prediction pipeline (PredNet-l and a near-term model) integrated with reachability-based hazard inference"
      },
      {
        "type": "primary",
        "category": "Planning / Motion Planning",
        "specific": "RRT* variant",
        "novel_contribution": "Knowledge-guided mitigation path planning that enforces application-specific safety constraints while minimizing time-to-safe-region under a mitigation deadline"
      },
      {
        "type": "baseline",
        "category": "Rule-based",
        "specific": "Solely rule-based mitigation",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Data-driven (unspecified baseline)",
        "specific": "Solely data-driven mitigation",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "",
        "type": "public",
        "domain": "physiological_sensor_time_series",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      },
      {
        "name": "",
        "type": "public",
        "domain": "physiological_sensor_time_series",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      },
      {
        "name": "Closed-loop APS Testbed 1 (simulation with real control software and adverse event simulator)",
        "type": "synthetic",
        "domain": "physiological_sensor_time_series",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      },
      {
        "name": "Closed-loop APS Testbed 2 (simulation with real control software and adverse event simulator)",
        "type": "synthetic",
        "domain": "physiological_sensor_time_series",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      }
    ],
    "baselines": [
      {
        "method_name": "Solely rule-based mitigation",
        "paper_reference": null,
        "metric": "Mitigation success rate",
        "their_result": "92.8%",
        "baseline_result": "50.9%"
      },
      {
        "method_name": "Solely data-driven mitigation",
        "paper_reference": null,
        "metric": "Mitigation success rate",
        "their_result": "92.8%",
        "baseline_result": "52.7%"
      },
      {
        "method_name": "State-of-the-art recovery methods (unspecified)",
        "paper_reference": null,
        "metric": "RMSE of system state trajectory prediction",
        "their_result": "28.1% lower RMSE than baselines",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "RMSE (state trajectory prediction)",
      "False positive rate (hazard prediction)",
      "False negative rate (hazard prediction)",
      "Mitigation success rate",
      "Hazard prediction accuracy"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can domain knowledge of safety constraints and context-specific mitigation actions be integrated with ML to predict hazards earlier and more accurately than thresholded anomaly detection?",
        "Can a combined knowledge- and data-driven safety engine generate optimal corrective actions that keep APS operation safe under controller-targeted attacks/faults?"
      ],
      "gaps_identified": [
        "Limited attention to hazard mitigation compared to anomaly detection and run-time monitoring in CPS",
        "Model-based approaches struggle to capture complex human physiology and accumulate approximation error over time",
        "Black-box ML methods often ignore safety constraints, lack transparency/robustness, and react too late",
        "Cyber-physical state explosion hampers generation of corrective action sequences",
        "Lack of realistic closed-loop testbeds to safely evaluate mitigation strategies"
      ],
      "limitations": [
        "Assumes sensor measurements to the safety engine are uncompromised (relies on separate sensor anomaly detection)",
        "Assumes attacks/faults are transient and occur once within a duration",
        "Safety engine assumed tamper-proof and isolated; attacks compromising it are out of scope",
        "Mitigation performance may depend on the effectiveness of upstream sensor anomaly detection"
      ],
      "future_work": [
        "Study mitigation when attackers can evade existing sensor anomaly detection methods",
        "Study scenarios where the safety engine could be compromised"
      ],
      "motivation": "Address the underexplored problem of hazard mitigation in CPS, especially for APS controllers, by combining domain knowledge with ML to enable early hazard prediction and safe, optimal corrective actions under strict timing and safety constraints.",
      "potential_research_ideas": [
        "Integrate uncertainty estimation (e.g., Bayesian RNNs or conformal prediction) to calibrate hazard predictions and adapt mitigation deadlines",
        "Jointly learn safety-aware dynamics with physics-informed or hybrid models to reduce model mismatch in physiology",
        "Extend to robust operation under compromised sensor inputs via secure sensor fusion and adversarial training",
        "Personalized safety constraint learning from individual patient data with continual/online learning",
        "Formal verification of the integrated ML-and-planning pipeline to provide guarantees on no new hazards",
        "Replace or augment RRT* with risk-aware MPC or safe RL (e.g., constrained RL with control barrier functions) for faster online mitigation",
        "Hierarchical planning: coarse long-horizon planning with fast local controllers for real-time deadlines",
        "Cross-device generalization: adapt to diverse APS controllers via meta-learning or domain adaptation"
      ],
      "architectural_improvement_recommendations": [
        "Upgrade the long-horizon predictor from LSTM to Transformer-based sequence models with encoder-decoder and multiscale temporal attention",
        "Add probabilistic outputs and uncertainty quantification (Monte Carlo dropout or deep ensembles) feeding into reachability and planning",
        "Use learned reachable set estimators or set-membership filters (e.g., zonotope/ellipsoid reachability) to tighten constraints dynamically",
        "Integrate a fast MPC layer for near-term control while RRT* plans a safe global path",
        "Incorporate control barrier functions to enforce hard safety constraints during action generation",
        "Employ patient-specific latent embeddings with personalization layers to improve trajectory prediction",
        "GPU-accelerate planning and prediction with batching and parallel tree expansion to meet strict timing"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Artificial Pancreas System safety engine wrapper integrated near the actuator (e.g., pump) in medical CPS",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Strict timing constraints for early hazard prediction and timely mitigation",
        "Cyber-physical state space explosion for reachability and planning",
        "Ensuring tamper-proof isolation of the safety engine",
        "Dependence on integrity of sensor inputs and upstream anomaly detection",
        "Avoiding introduction of new hazards while mitigating ongoing ones"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Integration of domain knowledge of safety constraints with RNNs via a customized loss to enforce realistic, safe trajectory predictions",
      "Two-level regression RNN (long- and short-term) with reachability-based hazard prediction for preemptive mitigation and near-term accuracy",
      "Knowledge-guided mitigation path planning using a variant of RRT* to quickly return the system to the safe region under application constraints",
      "Framework for formal specification of context-dependent mitigation requirements integrated with ML for compliant action generation",
      "Comprehensive evaluation on two realistic closed-loop APS testbeds and two real-world clinical/patient datasets, showing 28.1% lower RMSE, low FPR with no FNs, and 92.8% mitigation success rate (≥76% higher than rule-based and data-driven baselines)"
    ]
  },
  {
    "arxiv_id": "2312.13530v1",
    "title": "HW-V2W-Map: Hardware Vulnerability to Weakness Mapping Framework for Root Cause Analysis with GPT-assisted Mitigation Suggestion",
    "authors": "Yu-Zheng Lin; Muntasir Mamun; Muhtasim Alam Chowdhury; Shuyu Cai; Mingyu Zhu; Banafsheh Saber Latibari; Kevin Immanuel Gubbi; Najmeh Nazari Bavarsad; Arjun Caputo; Avesta Sasan; Houman Homayoun; Setareh Rafatirad; Pratik Satam; Soheil Salehi",
    "abstract": "The escalating complexity of modern computing frameworks has resulted in a surge in the cybersecurity vulnerabilities reported to the National Vulnerability Database (NVD) by practitioners. Despite the fact that the stature of NVD is one of the most significant databases for the latest insights into vulnerabilities, extracting meaningful trends from such a large amount of unstructured data is still challenging without the application of suitable technological methodologies. Previous efforts have mostly concentrated on software vulnerabilities; however, a holistic strategy incorporates approaches for mitigating vulnerabilities, score prediction, and a knowledge-generating system that may extract relevant insights from the Common Weakness Enumeration (CWE) and Common Vulnerability Exchange (CVE) databases is notably absent. As the number of hardware attacks on Internet of Things (IoT) devices continues to rapidly increase, we present the Hardware Vulnerability to Weakness Mapping (HW-V2W-Map) Framework, which is a Machine Learning (ML) framework focusing on hardware vulnerabilities and IoT security. The architecture that we have proposed incorporates an Ontology-driven Storytelling framework, which automates the process of updating the ontology in order to recognize patterns and evolution of vulnerabilities over time and provides approaches for mitigating the vulnerabilities. The repercussions of vulnerabilities can be mitigated as a result of this, and conversely, future exposures can be predicted and prevented. Furthermore, our proposed framework utilized Generative Pre-trained Transformer (GPT) Large Language Models (LLMs) to provide mitigation suggestions.",
    "published_date": "2023-12-21",
    "pdf_link": "https://arxiv.org/pdf/2312.13530v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Hardware Security",
      "subdomain": "Vulnerability Management and Analysis",
      "specific_problem": "Mapping hardware CVEs to CWEs for root-cause analysis, trend/story evolution over time, CVSS exploit/impact score prediction, and GPT-assisted mitigation suggestion for IoT/hardware vulnerabilities",
      "attack_types": [
        "Software-assisted hardware attacks",
        "Rowhammer",
        "IoT device vulnerabilities"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Ontology Learning / Knowledge Graph",
        "specific": null,
        "novel_contribution": "Ontology-driven storytelling framework with automatic ontology updating tailored to hardware/IoT vulnerabilities and CVE↔CWE mapping"
      },
      {
        "type": "primary",
        "category": "NLP (classical)",
        "specific": null,
        "novel_contribution": "Use of n-grams and POS tagging to extract relevant records and relationships from CVE/CWE text for user-driven queries and pattern discovery"
      },
      {
        "type": "primary",
        "category": "Classical ML (scoring/regression or classification)",
        "specific": null,
        "novel_contribution": "Machine-learning-based determination of CVSS exploitability and impact scores from GUI-derived threat vectors and cleaned features"
      },
      {
        "type": "primary",
        "category": "Transformer (LLM)",
        "specific": "OpenAI GPT (unspecified version)",
        "novel_contribution": "GPT-assisted mitigation suggestion via prompt engineering that blends CWE web-crawled mitigation info with user-provided vulnerability description"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised",
      "Zero-shot / Prompt-based"
    ],
    "datasets": [
      {
        "name": "National Vulnerability Database (NVD) - CVE records",
        "type": "public",
        "domain": "vulnerability_records",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Common Weakness Enumeration (CWE)",
        "type": "public",
        "domain": "vulnerability_records",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "SARD (Software Assurance Reference Dataset)",
        "type": "public",
        "domain": "source_code_vulnerabilities",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "BID (Bugtraq ID)",
        "type": "public",
        "domain": "vulnerability_records",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CNNVD (China National Vulnerability Database)",
        "type": "public",
        "domain": "vulnerability_records",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "IBM X-Force",
        "type": "public",
        "domain": "vulnerability_records",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "VARIoT (Vulnerability and Attack Repository for the Internet of Things)",
        "type": "public",
        "domain": "vulnerability_records",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Li et al. (2019)",
        "paper_reference": "[26]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Neuhaus et al.",
        "paper_reference": "[29]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Blinowski et al.",
        "paper_reference": "[7]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Murtaza et al.",
        "paper_reference": "[28]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Williams et al.",
        "paper_reference": "[44]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Hassan et al.",
        "paper_reference": "[12]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Kuhen et al. (OVANA)",
        "paper_reference": "[23]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Janiszewski et al. (VARIoT)",
        "paper_reference": "[18]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Guo et al. (IBM X-Force severity prediction)",
        "paper_reference": "[11]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "CVSS Base score",
      "CVSS Exploitability score",
      "CVSS Impact score"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How can hardware CVEs be mapped to CWEs to enable root-cause analysis and knowledge generation for IoT/hardware security?",
        "Can an ontology-driven storytelling framework automatically update and reveal patterns and the evolution of vulnerabilities over time?",
        "Can machine learning predict CVSS exploitability and impact scores from available CVE information?",
        "Can GPT LLMs provide useful mitigation suggestions when combined with CWE-sourced mitigations and user-provided descriptions?"
      ],
      "gaps_identified": [
        "Prior work mostly concentrates on software vulnerabilities rather than hardware/IoT vulnerabilities.",
        "Lack of a holistic strategy that incorporates mitigation approaches, score prediction, and knowledge generation from CWE/CVE.",
        "Absence of detailed insights into how vulnerabilities and impacts have developed over time within CVE/CWE.",
        "Limited or no use of LLMs for mitigation suggestion in prior frameworks."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Rapid growth of hardware attacks on IoT devices and surging NVD CVEs make it hard to extract trends from unstructured data; need a holistic, automated framework to map vulnerabilities to weaknesses, analyze patterns, predict impact/exploitability, and suggest mitigations.",
      "potential_research_ideas": [
        "Create a benchmark to quantitatively evaluate mitigation suggestion quality from LLMs against expert-curated ground truth.",
        "Integrate retrieval-augmented generation over CWE/NVD and vendor advisories for more grounded GPT mitigation outputs.",
        "Develop a labeled dataset for CVE→CWE mapping and train supervised neural models (e.g., cross-encoders) to compare versus rule/NLP-based mapping.",
        "Build temporal knowledge graphs of CVE–CWE–product (CPE) relations and perform forecasting of emerging hardware weakness trends.",
        "Assess and improve robustness of LLM mitigation suggestions under prompt attacks or adversarially crafted CVE text.",
        "Link code/firmware repositories (when available) to CVEs to enable multi-modal mapping from code to CWE and mitigation steps.",
        "Human-in-the-loop active learning to refine ontology updates and labeling for edge cases."
      ],
      "architectural_improvement_recommendations": [
        "Add retrieval-augmented generation (RAG) with a curated vulnerability corpus (CWE mitigations, vendor bulletins, CERT advisories) before GPT prompting.",
        "Replace generic n-grams/POS with modern NLP: domain-adapted BERT/Longformer for NER, relation extraction, and CVE→CWE linking.",
        "Introduce a temporal knowledge graph with GNN-based reasoning and time-aware embeddings for storytelling and trend prediction.",
        "Calibrate and validate CVSS score prediction with supervised models (e.g., gradient boosting, calibrated classifiers) and report quantitative metrics.",
        "Add an evaluator LLM or rule-based verifier to check GPT mitigation outputs for plausibility, coverage, and hallucination risk.",
        "Leverage product CPE parsing and normalization to improve context for mapping and scoring.",
        "Implement explainable components (feature attributions, path explanations in the ontology) in the GUI."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Conceptual modeling approach to a hardware-security ontology to analyze software-assisted hardware attack types, patterns, and evolution.",
      "Interactive framework that uses NLP (n-grams, POS tagging) to discover patterns/linkages between vulnerabilities and IoT impacts from CVE/CWE.",
      "GUI enabling identification of mitigation strategies, storytelling, and knowledge generation.",
      "ML-based determination of CVSS exploitability and impact scores from GUI-constructed threat vectors and cleaned data.",
      "GPT-assisted Mitigation Suggestion: web-crawls CWE mitigations and integrates user descriptions into prompts to generate mitigation advice."
    ]
  },
  {
    "arxiv_id": "2312.13704v1",
    "title": "A Forecasting-Based DLP Approach for Data Security",
    "authors": "Kishu Gupta; Ashwani Kush",
    "abstract": "Sensitive data leakage is the major growing problem being faced by enterprises in this technical era. Data leakage causes severe threats for organization of data safety which badly affects the reputation of organizations. Data leakage is the flow of sensitive data/information from any data holder to an unauthorized destination. Data leak prevention (DLP) is set of techniques that try to alleviate the threats which may hinder data security. DLP unveils guilty user responsible for data leakage and ensures that user without appropriate permission cannot access sensitive data and also provides protection to sensitive data if sensitive data is shared accidentally. In this paper, data leakage prevention (DLP) model is used to restrict/grant data access permission to user, based on the forecast of their access to data. This study provides a DLP solution using data statistical analysis to forecast the data access possibilities of any user in future based on the access to data in the past. The proposed approach makes use of renowned simple piecewise linear function for learning/training to model. The results show that the proposed DLP approach with high level of precision can correctly classify between users even in cases of extreme data access.",
    "published_date": "2023-12-21",
    "pdf_link": "https://arxiv.org/pdf/2312.13704v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Data Security",
      "subdomain": "Data Loss/Leakage Prevention (DLP)",
      "specific_problem": "Forecasting-based detection of abnormal user data access to identify potential guilty users and restrict/grant access",
      "attack_types": [
        "insider data leakage",
        "accidental data leakage"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Time-series forecasting",
        "specific": "Piecewise linear regression (simple piecewise linear model)",
        "novel_contribution": "Applies a simple piecewise linear forecasting model to user access logs to predict future access time and enforce DLP policy via thresholded bounds"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Time-series forecasting"
    ],
    "datasets": [
      {
        "name": "User accessibility dataset (2014–2018)",
        "type": "proprietary",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Absolute Error",
      "Percentage Error",
      "Forecast Accuracy"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Need for a mechanism that can identify data leakage with more precision for greater data security",
        "Existing DLP approaches rely on specification-based rules; learning-based adaptation to user behavior is underutilized for insider/unknown attacks"
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Data leakage poses severe threats to organizations; the authors aim to provide a practical DLP solution that forecasts user access behavior to preemptively restrict suspicious access and integrate protection with user identity and role.",
      "potential_research_ideas": [
        "Evaluate the forecasting-based DLP on larger and diverse enterprise datasets and compare against established anomaly detection baselines.",
        "Incorporate additional behavioral features (e.g., file type, data sensitivity, device/location, time-of-day) into multivariate forecasting/anomaly detection.",
        "Model seasonality and concept drift with advanced time-series models (e.g., Prophet, LSTM/GRU, Temporal Convolutional Networks) and online learning.",
        "Combine forecasting with probabilistic thresholding and risk scoring (e.g., Bayesian credible intervals, conformal prediction) to calibrate alerts.",
        "Integrate content/context-aware DLP signals (fingerprinting, regex, NLP classification) with behavioral forecasts in a fusion model.",
        "Assess robustness to adversarial insiders who mimic normal behavior; design adversarially-aware anomaly detection.",
        "Develop explainability for security analysts (attribution to features/time windows) to guide response and policy updates."
      ],
      "architectural_improvement_recommendations": [
        "Replace simple piecewise linear fit with multivariate, seasonality-aware models (SARIMA/Prophet) or sequence models (LSTM/Transformer) with attention over recent access windows.",
        "Use rolling/online training with drift detection (e.g., ADWIN) to adapt thresholds and bounds per user over time.",
        "Employ probabilistic forecasting to derive user-specific dynamic upper/lower bounds with uncertainty estimates.",
        "Add feature engineering pipeline that ingests access metadata (resource sensitivity, channel, endpoint, identity attributes) and role-based baselines per peer group.",
        "Introduce hierarchical modeling: per-user models with shrinkage to department/role-level priors for cold-start users.",
        "Implement alert prioritization via cost-sensitive learning to balance false positives/negatives and integrate feedback loops from analyst actions."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "Python",
        "Jupyter Notebook",
        "Anaconda"
      ],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes a learning-based DLP framework that forecasts user data access behavior using a simple piecewise linear model trained on historical access logs.",
      "Computes user-specific upper and lower bounds (based on mean and standard deviation of absolute error) to trigger alerts and restrict access when predicted access exceeds limits.",
      "Demonstrates half-yearly and yearly forecasts indicating detection of unusually long access in 2019 compared to 2014–2018, suggesting potential leakage.",
      "Claims the approach can \"with high level of precision\" correctly classify between users even in cases of extreme data access."
    ]
  },
  {
    "arxiv_id": "2312.05666v1",
    "title": "Towards a Graph Neural Network-Based Approach for Estimating Hidden States in Cyber Attack Simulations",
    "authors": "Pontus Johnson; Mathias Ekstedt",
    "abstract": "This work-in-progress paper introduces a prototype for a novel Graph Neural Network (GNN) based approach to estimate hidden states in cyber attack simulations. Utilizing the Meta Attack Language (MAL) in conjunction with Relational Dynamic Decision Language (RDDL) conformant simulations, our framework aims to map the intricate complexity of cyber attacks with a vast number of possible vectors in the simulations. While the prototype is yet to be completed and validated, we discuss its foundational concepts, the architecture, and the potential implications for the field of computer security.",
    "published_date": "2023-12-09",
    "pdf_link": "https://arxiv.org/pdf/2312.05666v1",
    "paper_types": [
      "position",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Attack Graphs and Simulation",
      "specific_problem": "Estimating hidden (partially observable) states in cyber attack simulations",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "GNN",
        "specific": "Basic GNN (unspecified architecture)",
        "novel_contribution": "Use of a Graph Neural Network trained on MAL-defined, RDDL-simulated attack graphs to infer hidden attack states from partial observations"
      }
    ],
    "learning_paradigm": [],
    "datasets": [
      {
        "name": "RDDL-based cyber attack simulations (via PyRDDLGym)",
        "type": "synthetic",
        "domain": "attack_graphs",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "MAL-defined attack scenarios (various)",
        "type": "synthetic",
        "domain": "attack_graphs",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can a GNN trained on MAL-structured, RDDL-simulated attack graphs estimate hidden states in cyber attack scenarios?",
        "How can observation logic combining structural (MAL) and dynamic/stochastic (RDDL) information improve inference of unobservable compromise states?"
      ],
      "gaps_identified": [
        "Traditional models often fall short in capturing the dynamic and stochastic nature of real-world attacks.",
        "Lack of approaches combining GNNs with MAL and RDDL for dynamic, realistic cyber attack simulation and hidden-state estimation."
      ],
      "limitations": [
        "Work-in-progress: prototype not yet completed and validated.",
        "Integration of MAL scenarios with dynamic RDDL simulations is challenging.",
        "Accuracy of GNN hidden-state predictions under varying and unforeseen attack conditions is unproven.",
        "No quantitative evaluation, baselines, or metrics reported."
      ],
      "future_work": [
        "Complete system integration across MAL, RDDL, and GNN components.",
        "Refine and optimize the GNN to improve accuracy of hidden-state estimation.",
        "Conduct comprehensive validation and testing.",
        "Extend framework to a wider range of attack scenarios and network configurations."
      ],
      "motivation": "To better capture the dynamic and stochastic nature of cyber attacks and to estimate hidden states in complex attack simulations using a GNN combined with MAL and RDDL.",
      "potential_research_ideas": [
        "Develop a benchmark suite of MAL/RDDL-based attack simulation datasets with standardized observation models for hidden-state inference.",
        "Compare GNN-based hidden-state estimation with probabilistic graphical models (e.g., HMMs, DBNs, factor graphs) and hybrid methods.",
        "Introduce uncertainty quantification (e.g., Bayesian GNNs) for calibrated hidden-state probabilities and confidence intervals.",
        "Incorporate active information gathering/active sensing to reduce uncertainty in hidden states (POMDP formulations).",
        "Domain adaptation and transfer learning across different network topologies and MAL models.",
        "Integrate reinforcement learning for adaptive defense policies informed by inferred hidden states.",
        "Evaluate adversarial robustness (evasion/poisoning) in simulated settings where observations are manipulated."
      ],
      "architectural_improvement_recommendations": [
        "Adopt temporal/dynamic GNNs (e.g., TGNN, TGN, DCRNN) to better capture time-evolving attack graphs.",
        "Use heterogeneous/relational GNNs with typed nodes/edges reflecting MAL asset/attack-step semantics (R-GCN, HGT).",
        "Integrate attention mechanisms for observation weighting and long-range dependency modeling (graph attention + temporal attention).",
        "Model partial observability explicitly with variational inference (e.g., amortized inference, VAE-style latent states) coupled to GNN encoders.",
        "Fuse GNN predictions with probabilistic state-space models (e.g., Kalman/particle filters) for sequential hidden-state tracking.",
        "Encode MAL constraints as differentiable logic or via constrained training to enforce consistency with attack semantics.",
        "Implement uncertainty estimation (MC dropout, deep ensembles) for calibrated outputs."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/pontusj101/gnn idstrainer",
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": "Not specified; prototype implemented in Python with PyRDDLGym-based simulations."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Integration across MAL, RDDL simulations, and GNN training/inference pipelines.",
        "Generalization and accuracy under varying and unforeseen attack conditions.",
        "Lack of validation and performance metrics to guide deployment readiness."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes a novel GNN-based framework to estimate hidden states in cyber attack simulations.",
      "Integrates Meta Attack Language (MAL) for structural modeling with RDDL (via PyRDDLGym) for dynamic, stochastic simulation.",
      "Defines observation logic combining structural, temporal, and probabilistic information for learning hidden states.",
      "Provides an early-stage prototype implementation with MAL models, RDDL specifications, and a basic GNN."
    ]
  },
  {
    "arxiv_id": "2312.11559v1",
    "title": "Android Malware Detection with Unbiased Confidence Guarantees",
    "authors": "Harris Papadopoulos; Nestoras Georgiou; Charalambos Eliades; Andreas Konstantinidis",
    "abstract": "The impressive growth of smartphone devices in combination with the rising ubiquity of using mobile platforms for sensitive applications such as Internet banking, have triggered a rapid increase in mobile malware. In recent literature, many studies examine Machine Learning techniques, as the most promising approach for mobile malware detection, without however quantifying the uncertainty involved in their detections. In this paper, we address this problem by proposing a machine learning dynamic analysis approach that provides provably valid confidence guarantees in each malware detection. Moreover the particular guarantees hold for both the malicious and benign classes independently and are unaffected by any bias in the data. The proposed approach is based on a novel machine learning framework, called Conformal Prediction, combined with a random forests classifier. We examine its performance on a large-scale dataset collected by installing 1866 malicious and 4816 benign applications on a real android device. We make this collection of dynamic analysis data available to the research community. The obtained experimental results demonstrate the empirical validity, usefulness and unbiased nature of the outputs produced by the proposed approach.",
    "published_date": "2023-12-17",
    "pdf_link": "https://arxiv.org/pdf/2312.11559v1",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Mobile Security",
      "subdomain": "Android Malware Detection",
      "specific_problem": "Dynamic analysis-based Android malware detection with per-class confidence guarantees using conformal prediction",
      "attack_types": [
        "Android malware",
        "polymorphic malware"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Conformal Prediction",
        "specific": "Label-conditional Mondrian Inductive Conformal Prediction (LCMICP)",
        "novel_contribution": "Provides provably valid, per-class (malicious/benign) confidence guarantees that are unbiased by class imbalance; enables different confidence levels per class"
      },
      {
        "type": "primary",
        "category": "Tree Ensemble",
        "specific": "Random Forest (100 trees; sqrt(d) features per split)",
        "novel_contribution": "Used as the underlying algorithm for LCMICP; RF posterior probabilities form the nonconformity score"
      },
      {
        "type": "baseline",
        "category": "Tree Ensemble",
        "specific": "Random Forest (standard RF without conformal prediction)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Android device state recordings (LG E400) for 6,682 apps (malware-data)",
        "type": "public",
        "domain": "dynamic_analysis_device_state",
        "link": "https://github.com/harrisp/malware-data",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "AndroZoo Android APK collection",
        "type": "public",
        "domain": "android_apks",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Random Forest (RF) without Conformal Prediction",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can machine learning-based Android malware detection provide provably valid confidence guarantees for each prediction that hold independently for malicious and benign classes?",
        "Does Label-conditional Mondrian Inductive Conformal Prediction (LCMICP) with Random Forests yield unbiased, within-class valid predictions on a large-scale dataset collected from a real Android device?"
      ],
      "gaps_identified": [
        "Most ML-based mobile malware detectors do not quantify prediction uncertainty.",
        "Standard conformal prediction provides validity over all instances but not per-class; class imbalance can bias guarantees toward the majority (benign) class.",
        "Many dynamic analysis studies use emulated environments rather than real devices, limiting realism."
      ],
      "limitations": [
        "Original CP/LCMCP are computationally demanding for mobile deployment; addressed by using the inductive version (ICP).",
        "Battery-related features were excluded because the device was continuously charging.",
        "Derived 'Diff' features were excluded as they can be computed from other features.",
        "Dynamic analysis may not trigger malicious payloads reliably; it is difficult to determine when the malicious code will execute."
      ],
      "future_work": [],
      "motivation": "Provide unbiased, per-class confidence guarantees for Android malware detection and evaluate on a realistic, large-scale dataset collected on a real device.",
      "potential_research_ideas": [
        "Extend LCMICP to multi-class settings for malware family classification with per-family validity guarantees.",
        "Combine static and dynamic features and apply LCMICP over a multimodal model to improve coverage when dynamic payloads are not triggered.",
        "Use active stimulation or reinforcement learning to drive app interactions that are more likely to trigger malicious behaviors during dynamic analysis.",
        "Investigate distribution shift and concept drift in Android ecosystems and apply adaptive or online conformal calibration to maintain validity over time.",
        "Integrate out-of-distribution detection via credibility thresholds to flag potential zero-day malware and route for deeper analysis.",
        "Evaluate and calibrate with deep ensembles or gradient boosting as underlying models to compare nonconformity quality vs Random Forests."
      ],
      "architectural_improvement_recommendations": [
        "Adopt cross-conformal or jackknife+ calibration to use more data for training while retaining validity guarantees.",
        "Condition Mondrian taxonomy beyond labels (e.g., by app category, API usage, or behavior clusters) to achieve subgroup validity.",
        "Optimize nonconformity measures (e.g., margin-based scores or calibrated probabilities via Platt/Isotonic scaling) to improve efficiency (singleton prediction rates).",
        "Introduce cost-sensitive calibration with different significance levels per class tuned to operational risk.",
        "Implement streaming/online RF or GBM with rolling calibration sets for continual learning with validity."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "MATLAB"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Random Forest with 100 trees trained once on the proper training set; inductive conformal prediction requires storing calibration nonconformity scores; low per-sample inference cost suitable for mobile constraints; exact hardware/GPU not specified."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Android smartphone (LG E400) with on-device dynamic execution and simulated user interactions",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Limited computation and power on mobile devices; transductive CP is too heavy, motivating inductive CP.",
        "Difficulty in reliably triggering malicious behavior during dynamic analysis.",
        "Class imbalance with far more benign than malicious apps requiring per-class validity.",
        "Need for calibration sets and maintenance when apps and behaviors evolve."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes a machine-learning dynamic analysis approach that provides provably valid confidence guarantees per detection using Label-conditional Mondrian Inductive Conformal Prediction (LCMICP) with Random Forests.",
      "Guarantees hold independently for malicious and benign classes and are unaffected by class imbalance: “the particular guarantees hold for both the malicious and benign classes independently and are unaffected by any bias in the data.”",
      "Introduces and releases a large-scale dynamic analysis dataset recorded on a real Android device (LG E400) for 6,682 apps (1,866 malicious, 4,816 benign): https://github.com/harrisp/malware-data.",
      "Demonstrates empirical validity and unbiased nature of outputs; shows superiority over conventional RF baseline on the collected dataset (exact numbers not provided in the excerpt)."
    ]
  },
  {
    "arxiv_id": "2312.04596v1",
    "title": "Feature Analysis of Encrypted Malicious Traffic",
    "authors": "Anish Singh Shekhawat; Fabio Di Troia; Mark Stamp",
    "abstract": "In recent years there has been a dramatic increase in the number of malware attacks that use encrypted HTTP traffic for self-propagation or communication. Antivirus software and firewalls typically will not have access to encryption keys, and therefore direct detection of malicious encrypted data is unlikely to succeed. However, previous work has shown that traffic analysis can provide indications of malicious intent, even in cases where the underlying data remains encrypted. In this paper, we apply three machine learning techniques to the problem of distinguishing malicious encrypted HTTP traffic from benign encrypted traffic and obtain results comparable to previous work. We then consider the problem of feature analysis in some detail. Previous work has often relied on human expertise to determine the most useful and informative features in this problem domain. We demonstrate that such feature-related information can be obtained directly from machine learning models themselves. We argue that such a machine learning based approach to feature analysis is preferable, as it is more reliable, and we can, for example, uncover relatively unintuitive interactions between features.",
    "published_date": "2023-12-06",
    "pdf_link": "https://arxiv.org/pdf/2312.04596v1",
    "paper_types": [
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Traffic Analysis / Intrusion Detection",
      "specific_problem": "Detecting and distinguishing malicious encrypted HTTP(S) traffic from benign encrypted traffic using flow-level metadata and model-driven feature analysis",
      "attack_types": [
        "Encrypted malicious traffic over HTTPS/TLS",
        "Malware command-and-control over TLS",
        "Malware self-propagation over HTTPS"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "SVM",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Random Forest",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Gradient Boosted Trees",
        "specific": "XGBoost",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "CTU-13",
        "type": "public",
        "domain": "network_traffic",
        "link": "https://www.stratosphereips.org/datasets-ctu13",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Malware Capture Facility Project (MCFP) datasets (Czech Technical University ATG Group)",
        "type": "public",
        "domain": "network_traffic",
        "link": "https://www.stratosphereips.org/datasets-malware",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can malicious encrypted HTTP(S) traffic be distinguished from benign encrypted traffic using only flow-level and TLS/certificate metadata when decryption keys are unavailable?",
        "Can feature importance and interactions be reliably obtained directly from trained machine learning models, reducing reliance on human expert feature engineering?",
        "How do the most informative features differ across different ML models (SVM, Random Forest, XGBoost)?"
      ],
      "gaps_identified": [
        "Prior work often relies heavily on human expertise to determine important features for encrypted traffic detection.",
        "Deep packet inspection is ineffective when encryption keys are unavailable and incurs high overhead even when keys are present.",
        "Some prior data collection settings (e.g., DMZ enterprise-only traffic) may not be representative of general network traffic."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Rapid growth in malware using encrypted HTTPS traffic, combined with the impracticality of decrypting traffic at scale, motivates learning from traffic metadata and automating feature analysis to reduce dependence on human expertise.",
      "potential_research_ideas": [
        "Sequence/temporal modeling of flows (e.g., RNN/Transformer over per-connection features) to capture time-dependent behavior of encrypted malware.",
        "Integrate TLS fingerprinting features (e.g., JA3/JA3S) and certificate graph features with current flow/certificate features.",
        "Domain adaptation and transfer learning to improve generalization across networks and organizations.",
        "Self-supervised pretraining on unlabeled network flows to reduce labeling dependence, followed by fine-tuning for malware traffic detection.",
        "Adversarial robustness studies: evaluate and harden against malware that mimics benign TLS/flow metadata.",
        "Causal or counterfactual feature analysis to identify spurious correlations versus robust indicators.",
        "Online/continual learning to adapt feature importance and models to evolving encrypted traffic patterns."
      ],
      "architectural_improvement_recommendations": [
        "Use permutation importance and SHAP values across RF/XGBoost to quantify global and local feature effects and interactions.",
        "Calibrate tree-ensemble outputs (e.g., Platt scaling/Isotonic) for threshold selection at desired false positive rates.",
        "Employ class-imbalance strategies (e.g., class-weighting, balanced bagging, focal loss in GBDT variants).",
        "Augment features with bidirectional flow statistics, burstiness measures, and inter-arrival time summaries.",
        "Build a streaming pipeline integrating Zeek/Bro feature extraction with incremental learning models for near-real-time detection.",
        "Evaluate and possibly adopt gradient-boosting variants with categorical handling and monotonic constraints for stability."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Applied SVM, Random Forest, and XGBoost to classify encrypted malicious HTTP traffic versus benign and achieved results comparable to prior work.",
      "Performed detailed, model-driven feature analysis to derive informative features and reveal unintuitive feature interactions directly from trained models.",
      "Constructed a comprehensive feature set by aggregating Zeek/Bro conn.log, ssl.log, and x509.log fields at the connection (4-tuple) level for HTTPS flows.",
      "Argued for and demonstrated a domain-free, automated approach to feature analysis that reduces reliance on human expert feature selection."
    ]
  },
  {
    "arxiv_id": "2311.15633v1",
    "title": "Toward a real-time TCP SYN Flood DDoS mitigation using Adaptive Neuro-Fuzzy classifier and SDN Assistance in Fog Computing",
    "authors": "Radjaa Bensaid; Nabila Labraoui; Ado Adamou Abba Ari; Leandros Maglaras; Hafida Saidi; Ahmed Mahmoud Abdu Lwahhab; Sihem Benfriha",
    "abstract": "The growth of the Internet of Things (IoT) has recently impacted our daily lives in many ways. As a result, a massive volume of data is generated and needs to be processed in a short period of time. Therefore, the combination of computing models such as cloud computing is necessary. The main disadvantage of the cloud platform is its high latency due to the centralized mainframe. Fortunately, a distributed paradigm known as fog computing has emerged to overcome this problem, offering cloud services with low latency and high-access bandwidth to support many IoT application scenarios. However, Attacks against fog servers can take many forms, such as Distributed Denial of Service (DDoS) attacks that severely affect the reliability and availability of fog services. To address these challenges, we propose mitigation of Fog computing-based SYN Flood DDoS attacks using an Adaptive Neuro-Fuzzy Inference System (ANFIS) and Software Defined Networking (SDN) Assistance (FASA). The simulation results show that FASA system outperforms other algorithms in terms of accuracy, precision, recall, and F1-score. This shows how crucial our system is for detecting and mitigating TCP SYN floods DDoS attacks.",
    "published_date": "2023-11-27",
    "pdf_link": "https://arxiv.org/pdf/2311.15633v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "DDoS Detection and Mitigation",
      "specific_problem": "Real-time detection and mitigation of TCP SYN Flood DDoS attacks in fog computing using SDN-assisted ANFIS",
      "attack_types": [
        "TCP SYN Flood"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Neuro-Fuzzy System",
        "specific": "ANFIS (Sugeno-type Adaptive Neuro-Fuzzy Inference System)",
        "novel_contribution": "Integrates ANFIS-based traffic classification with SDN-assisted real-time mitigation at fog servers; trained on CIC-DDoS2019 and an SDN-captured dataset for deployment in fog computing"
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "CIC-DDoS2019",
        "type": "public",
        "domain": "network_traffic",
        "link": "https://www.unb.ca/cic/datasets/ddos-2019.html",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "SDN dataset (captured from SDN environment)",
        "type": "private",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "KDD CUP 99",
        "type": "public",
        "domain": "network_traffic",
        "link": "http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "NSL-KDD",
        "type": "public",
        "domain": "network_traffic",
        "link": "https://www.unb.ca/cic/datasets/nsl.html",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "accuracy",
      "precision",
      "recall",
      "F1-score",
      "false positive rate"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can an ANFIS-based classifier, assisted by SDN, detect and mitigate TCP SYN Flood DDoS attacks in fog computing in real time?",
        "Does integrating SDN control with ANFIS-based classification improve detection performance (accuracy, precision, recall, F1) and enable timely mitigation at fog servers?"
      ],
      "gaps_identified": [
        "Many fog-based defenses miss integrated DDoS detection mechanisms and lack detailed computation/deployment discussion.",
        "Scheduling-based SDN IoT–fog defenses protect only during scheduled periods, leaving idle periods vulnerable.",
        "Fuzzy logic systems can become complex and hard to tune as variables/rules grow.",
        "KDD CUP99/NSL-KDD are outdated or unsuitable for modern DDoS requirements; need newer flow-based datasets like CIC-DDoS2019.",
        "Limited prior work uses ANFIS for SYN flood detection in fog computing with SDN."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Fog servers supporting IoT are highly susceptible to DDoS, especially TCP SYN floods; need low-latency detection and mitigation close to the edge with SDN programmability and a more suitable dataset.",
      "potential_research_ideas": [
        "Extend from single-attack (SYN Flood) to multi-attack and multi-stage DDoS detection with hierarchical classification at fog nodes.",
        "Online/streaming ANFIS with concept drift detection for non-stationary traffic in edge environments.",
        "Adversarial robustness evaluation and robust training against evasion (e.g., FGSM/PGD-style perturbations on flow features).",
        "Federated or collaborative learning across fog nodes/SDN domains to share models without sharing raw traffic.",
        "Lightweight feature extraction at data plane (eBPF/P4) and controller–switch co-design to offload parts of inference.",
        "Explainability overlays for ANFIS rules to support operator insights and policy debugging.",
        "Integration with automated SDN mitigation playbooks (rate limiting, ACL insertion, path re-routing) with control-loop stability analysis.",
        "Evaluation on real hardware testbeds and high-speed traffic (DPDK/SmartNIC) for throughput–latency trade-offs."
      ],
      "architectural_improvement_recommendations": [
        "Introduce incremental/online learning for ANFIS to adapt to evolving traffic and reduce retraining costs.",
        "Apply feature selection and rule simplification (e.g., L1 regularization or evolutionary search) to control ANFIS rule explosion.",
        "Co-design with programmable data planes (P4) to pre-filter SYN anomalies and reduce controller load.",
        "Add drift detection (e.g., ADWIN/EDDM) to trigger model updates and mitigate performance decay.",
        "Hybrid ensemble: ANFIS front-end with a lightweight secondary verifier (e.g., one-class SVM) to reduce false positives.",
        "Controller HA and scaling (clustering or sharding) to handle mitigation at scale under attack surges."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "SDN-controlled fog computing network (ANFIS model at SDN controller, deployed to fog servers)",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes FASA, an SDN-assisted ANFIS-based model to detect and mitigate TCP SYN Flood DDoS attacks in fog computing.",
      "Implements ANFIS to self-train fog servers for classifying normal vs. malicious packets.",
      "Deploys ANFIS at the SDN controller and fog servers using data captured from an SDN environment to allow benign traffic and block malicious traffic in real time.",
      "Evaluates using CIC-DDoS2019 and an SDN-captured dataset, reporting improved accuracy, precision, recall, F1-score, and low false positives."
    ]
  },
  {
    "arxiv_id": "2311.13454v1",
    "title": "Explaining high-dimensional text classifiers",
    "authors": "Odelia Melamed; Rich Caruana",
    "abstract": "Explainability has become a valuable tool in the last few years, helping humans better understand AI-guided decisions. However, the classic explainability tools are sometimes quite limited when considering high-dimensional inputs and neural network classifiers. We present a new explainability method using theoretically proven high-dimensional properties in neural network classifiers. We present two usages of it: 1) On the classical sentiment analysis task for the IMDB reviews dataset, and 2) our Malware-Detection task for our PowerShell scripts dataset.",
    "published_date": "2023-11-22",
    "pdf_link": "https://arxiv.org/pdf/2311.13454v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Malware Analysis",
      "subdomain": "Malware Detection",
      "specific_problem": "Explainability of neural text classifiers for malicious PowerShell script detection; identifying on-manifold token-level importance",
      "attack_types": [
        "Malware",
        "Evasion via code obfuscation / variable renaming",
        "Adversarial examples (conceptual)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Explainability/Attribution",
        "specific": "On-manifold gradient ensemble cosine-similarity method with norm filtering",
        "novel_contribution": "Uses theoretically-motivated properties of off-manifold gradients: filters tokens by small gradient norm and selects those with highest average cosine similarity across independently-initialized surrogate models to approximate on-manifold explanations for text"
      },
      {
        "type": "baseline",
        "category": "Gradient-based Saliency",
        "specific": "Gradient max-norm (top-norm words)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Perturbation-based Explanation",
        "specific": "LIME",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Shapley-value Explanation",
        "specific": "SHAP",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "DNN (unspecified)",
        "specific": "Neural network text classifier used for IMDB and PowerShell tasks",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "MLP (theoretical analysis)",
        "specific": "Two-layer fully-connected network (for gradient cosine similarity theory)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "IMDB Reviews (Sentiment Analysis)",
        "type": "public",
        "domain": "text_reviews",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "PowerShell Scripts (Industrial Malware Detection)",
        "type": "private",
        "domain": "code_scripts",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Gradient max-norm (saliency by word gradient norm)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "LIME",
        "paper_reference": "Ribeiro et al., 2016",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "SHAP",
        "paper_reference": "Lundberg and Lee, 2017",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can we generate on-manifold explanations for high-dimensional neural text classifiers using properties of off-manifold gradients?",
        "Do independently-initialized models produce uncorrelated off-manifold gradients that can be exploited to filter explanations?",
        "Can the proposed method yield more semantically correct token attributions than gradient norm, LIME, and SHAP on text tasks including security (PowerShell malware detection)?"
      ],
      "gaps_identified": [
        "Classic explainability tools often fail or produce off-manifold explanations for high-dimensional neural network inputs",
        "Existing on-manifold XAI approaches largely target continuous domains (e.g., images) and do not transfer easily to non-continuous text",
        "Post-hoc detection of off-manifold explanations in text is underexplored"
      ],
      "limitations": [
        "Assumes data lie approximately in a low-dimensional linear subspace; ℓ (off-manifold dimension) is not directly observable",
        "Manifold may not be strictly linear in real datasets; PCA may misclassify some dimensions",
        "Requires training multiple surrogate models on the same distribution (extra compute)",
        "Norm threshold selection is heuristic and data-dependent",
        "Evaluation is qualitative (no quantitative faithfulness metrics reported)",
        "Method improves explanations but does not address adversarial robustness of the classifier"
      ],
      "future_work": [
        "Study gradient behavior for architectures commonly used in text classification under implicit low-dimensional data assumptions",
        "Leverage manifold learning research to generate on-manifold post-hoc explanations in broader settings",
        "Explore how insights from explanations could enhance adversarial robustness"
      ],
      "motivation": "Improve trust and usability of neural text classifiers by producing on-manifold explanations in high-dimensional, non-continuous input spaces, with application to sentiment analysis and malware detection on PowerShell scripts",
      "potential_research_ideas": [
        "Develop quantitative evaluation metrics for on-manifold explanation faithfulness in text (e.g., counterfactual consistency, sufficiency/necessity tests constrained to grammar-preserving edits)",
        "Integrate a learned manifold projector (e.g., text autoencoder or masked language model) to more formally filter off-manifold gradients",
        "Cross-architecture surrogate ensembles (Transformer/CNN/RNN) to examine stability of on-manifold attributions across model families",
        "Adaptively learn the norm threshold using distributional calibration (per-layer/token embedding statistics)",
        "Extend to other security scripting domains (Bash, VBA, JavaScript) and mixed-code natural language logs",
        "Use checkpoint ensembles (SWA/EMA across epochs) to reduce retraining cost while preserving gradient diversity",
        "Combine with integrated gradients or SmoothGrad but apply the on-manifold filter to improve signal-to-noise",
        "Exploit ensemble agreement as a detector of off-manifold, potentially maliciously-crafted inputs (explanation-consistency monitoring)"
      ],
      "architectural_improvement_recommendations": [
        "Replace brute-force multiple retrainings with multi-checkpoint or multi-dropout ensembles to approximate surrogate diversity at lower cost",
        "Project token gradients onto an estimated subspace (PCA on embeddings or autoencoder latent space) before cosine-similarity aggregation",
        "Normalize gradients per-token by local curvature (Fisher/Jacobian norm) to avoid favoring tokens with inherently larger sensitivities",
        "Weight cosine similarity by attention scores in Transformer-based classifiers to align attributions with model focus",
        "Automate threshold T via percentile-based or Bayesian thresholding calibrated on validation gradient distributions"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Need to train multiple surrogate models on the same data distribution to compute ensemble gradient similarities",
        "Selecting and validating an appropriate gradient norm threshold per dataset or deployment",
        "Explaining long scripts with many tokens may require efficient gradient computation and memory management"
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes a simple post-hoc method to generate on-manifold explanations for text by filtering small-norm token gradients and selecting tokens with maximal average cosine similarity across independently-initialized surrogate models",
      "Provides theoretical grounding from high-dimensional analysis: off-manifold gradients have large norms and are largely uncorrelated across different initializations",
      "Demonstrates the method on IMDB sentiment analysis and an industrial PowerShell malware detection dataset, showing qualitative improvements over gradient norm, LIME, and SHAP",
      "Offers a practical procedure requiring no changes to training loss or special networks; only additional retrainings for surrogates and a dataset-specific norm threshold selection"
    ]
  },
  {
    "arxiv_id": "2312.05275v1",
    "title": "Exploring the Limits of ChatGPT in Software Security Applications",
    "authors": "Fangzhou Wu; Qingzhao Zhang; Ati Priya Bajaj; Tiffany Bao; Ning Zhang; Ruoyu \"Fish\" Wang; Chaowei Xiao",
    "abstract": "Large language models (LLMs) have undergone rapid evolution and achieved remarkable results in recent times. OpenAI's ChatGPT, backed by GPT-3.5 or GPT-4, has gained instant popularity due to its strong capability across a wide range of tasks, including natural language tasks, coding, mathematics, and engaging conversations. However, the impacts and limits of such LLMs in system security domain are less explored. In this paper, we delve into the limits of LLMs (i.e., ChatGPT) in seven software security applications including vulnerability detection/repair, debugging, debloating, decompilation, patching, root cause analysis, symbolic execution, and fuzzing. Our exploration reveals that ChatGPT not only excels at generating code, which is the conventional application of language models, but also demonstrates strong capability in understanding user-provided commands in natural languages, reasoning about control and data flows within programs, generating complex data structures, and even decompiling assembly code. Notably, GPT-4 showcases significant improvements over GPT-3.5 in most security tasks. Also, certain limitations of ChatGPT in security-related tasks are identified, such as its constrained ability to process long code contexts.",
    "published_date": "2023-12-08",
    "pdf_link": "https://arxiv.org/pdf/2312.05275v1",
    "paper_types": [
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Code Security and Vulnerability Management",
      "specific_problem": "Evaluating ChatGPT (GPT-3.5/GPT-4) on vulnerability detection and repair, bug fixing/debugging, debloating, decompilation, patching, root cause analysis, symbolic execution, and fuzzing",
      "attack_types": [
        "CWE-22 Path Traversal",
        "CWE-78 OS Command Injection",
        "CWE-79 Cross-site Scripting (XSS)",
        "CWE-89 SQL Injection",
        "CWE-119 Improper Restriction of Operations within the Bounds of a Memory Buffer",
        "CWE-125 Out-of-bounds Read",
        "CWE-190 Integer Overflow or Wraparound",
        "CWE-416 Use-After-Free",
        "CWE-476 NULL Pointer Dereference",
        "CWE-787 Out-of-bounds Write"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer LLM",
        "specific": "GPT-4 (ChatGPT)",
        "novel_contribution": "No new model; paper evaluates capabilities across multiple software security tasks"
      },
      {
        "type": "baseline",
        "category": "Transformer LLM",
        "specific": "GPT-3.5 (ChatGPT)",
        "novel_contribution": "Used as a comparative baseline to GPT-4 across tasks"
      }
    ],
    "learning_paradigm": [
      "Zero-shot prompting",
      "In-context learning"
    ],
    "datasets": [
      {
        "name": "SARD subset (100 synthetic test cases across 10 CWE types)",
        "type": "public",
        "domain": "source_code",
        "link": "https://samate.nist.gov/SARD/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Real-world CVE collection (34 vulnerabilities; vulnerable and fixed versions)",
        "type": "private",
        "domain": "source_code",
        "link": "https://cve.mitre.org/",
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "GPT-3.5 vs GPT-4 on synthetic vulnerability detection (overall precision)",
        "paper_reference": null,
        "metric": "Precision (synthetic dataset, total)",
        "their_result": "GPT-4: 87.50%",
        "baseline_result": "GPT-3.5: 67.69%"
      },
      {
        "method_name": "GPT-3.5 vs GPT-4 on synthetic vulnerability detection (overall recall)",
        "paper_reference": null,
        "metric": "Recall (synthetic dataset, total)",
        "their_result": "GPT-4: 98% (49/50)",
        "baseline_result": "GPT-3.5: 88% (44/50)"
      },
      {
        "method_name": "GPT-3.5 vs GPT-4 on real-world CVE detection (overall precision)",
        "paper_reference": null,
        "metric": "Precision (CVE dataset, total)",
        "their_result": "GPT-4: 73.91%",
        "baseline_result": "GPT-3.5: 41.67%"
      },
      {
        "method_name": "GPT-3.5 vs GPT-4 on real-world CVE detection (overall recall)",
        "paper_reference": null,
        "metric": "Recall (CVE dataset, total)",
        "their_result": "GPT-4: 50%",
        "baseline_result": "GPT-3.5: 17.24%"
      }
    ],
    "performance_metrics_used": [
      "precision",
      "recall",
      "success_rate"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can ChatGPT understand the goal of the task?",
        "How accurate the ChatGPT’s answers are?",
        "How different GPT-3.5 and GPT-4 performs?",
        "What is the limitation of ChatGPT if it cannot accomplish the task?"
      ],
      "gaps_identified": [
        "Existing studies focus mainly on detection and repair of vulnerabilities or bugs, leaving broader software security tasks underexplored.",
        "Many prior evaluations predate GPT-4 and thus do not reflect state-of-the-art LLM performance.",
        "ChatGPT has constrained ability to process long code contexts.",
        "ChatGPT cannot interpret binary or hexadecimal code effectively.",
        "Vulnerability detection precision suffers from false positives; numerical reasoning errors cause misses for overflow-type bugs.",
        "Performance on real-world vulnerabilities degrades substantially compared to synthetic cases."
      ],
      "limitations": [
        "Limited context window impedes analysis of large, real-world codebases; they resort to function-level inputs.",
        "Inability to process binaries/hex; evaluation restricted largely to source code.",
        "Notable false positives in vulnerability detection; precision only 67.69% (GPT-3.5) and 87.50% (GPT-4) on synthetic set.",
        "Numerical reasoning weaknesses (e.g., buffer size vs. read length) lead to false negatives in overflow cases.",
        "Performance drop on real-world CVEs (GPT-4 precision 73.91%, recall 50%).",
        "Prompt and task design sensitivity; manual curation needed for several tasks and test cases."
      ],
      "future_work": [
        "Address long-context limitations and integrate LLMs systematically into security-critical applications.",
        "Investigate techniques to enable LLMs to operate on binaries/IR safely and effectively.",
        "Reduce false positives and improve numerical/semantic reasoning for memory corruption vulnerabilities.",
        "Build broader, standardized benchmarks spanning the seven evaluated security tasks."
      ],
      "motivation": "To systematically evaluate the potential and limits of ChatGPT for software security tasks across a broad spectrum, using recent GPT-3.5 and GPT-4 models, and to identify strengths, weaknesses, and open research directions.",
      "potential_research_ideas": [
        "Design retrieval-augmented, hierarchical code understanding pipelines to overcome context limits (function-level summaries → module-level reasoning).",
        "Combine LLMs with static analysis, dataflow/taint analysis, and symbolic execution signals to guide vulnerability detection and patch validation.",
        "Develop LLM-driven decompilation assistants that leverage IR (e.g., LLVM) and CFG/DFG serialization for structured reasoning on binaries.",
        "Create an open benchmark suite covering vulnerability detection/repair, decompilation, patching, RCA, symbolic execution, and LLM-assisted fuzzing with standardized prompts and scoring.",
        "Implement feedback-driven repair loops with unit tests, fuzzing traces, and compiler diagnostics for iterative patch refinement (self-debugging).",
        "Explore program slicing to reduce prompt size while preserving vulnerability-relevant context; learn slice selection policies.",
        "Design multi-agent systems where specialized agents handle fuzz input generation, triage, minimization, and patch validation under LLM coordination.",
        "Investigate toolformer-style augmentation so the LLM can call external tools (compilers, sanitizers, fuzzers) during reasoning for grounded outputs."
      ],
      "architectural_improvement_recommendations": [
        "Introduce retrieval-augmented generation with code chunking, hierarchical summaries, and function-level embeddings for long repositories.",
        "Serialize program analysis artifacts (AST, CFG, DFG, taint traces) into compact, structured prompts; teach the LLM to attend to these signals.",
        "Use constrained decoding and templates for patch generation to reduce hallucinations and enforce compile/test success.",
        "Adopt self-consistency and multi-step chain-of-thought for numeric/memory-bound reasoning; add automated checks for buffer and index arithmetic.",
        "Integrate coverage-guided fuzzing loops where the LLM proposes seeds/mutations conditioned on runtime feedback.",
        "Leverage IR lifting for binary tasks and provide canonicalized, token-efficient representations to the LLM."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Black-box use of ChatGPT (GPT-3.5/GPT-4); no model training reported."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Context length limits hinder analysis of large codebases.",
        "Lack of binary/hex understanding limits applicability to source-only contexts.",
        "False positives and numerical reasoning errors reduce precision for memory-related CWEs.",
        "Prompt sensitivity and need for manual curation across diverse tasks.",
        "Dependence on proprietary APIs raises cost, latency, and data governance concerns."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Comprehensive multi-task empirical evaluation of ChatGPT (GPT-3.5/GPT-4) on seven software security applications.",
      "Quantitative benchmarking on vulnerability detection (synthetic and real-world CVEs) and reported success rates for repair and bug fixing.",
      "Identification of strengths (e.g., reasoning about control/data flow, command understanding, decompilation of short programs) and weaknesses (context length, binary/hex handling, numeric reasoning).",
      "Evidence that GPT-4 significantly improves over GPT-3.5 across most evaluated tasks."
    ]
  },
  {
    "arxiv_id": "2312.04346v2",
    "title": "Detection and Imputation based Two-Stage Denoising Diffusion Power System Measurement Recovery under Cyber-Physical Uncertainties",
    "authors": "Jianhua Pei; Jingyu Wang; Dongyuan Shi; Ping Wang",
    "abstract": "Power system cyber-physical uncertainties, including measurement ambiguities stemming from cyber attacks and data losses, along with system uncertainties introduced by massive renewables and complex dynamics, reduce the likelihood of enhancing the quality of measurements. Fortunately, denoising diffusion models exhibit powerful learning and generation abilities for the complex underlying physics of the real world. To this end, this paper proposes an improved detection and imputation based two-stage denoising diffusion model (TSDM) to identify and reconstruct the measurements with various cyber-physical uncertainties. The first stage of the model comprises a classifier-guided conditional anomaly detection component, while the second stage involves diffusion-based measurement imputation component. Moreover, the proposed TSDM adopts optimal variance to accelerate the diffusion generation process with subsequence sampling. Extensive numerical case studies demonstrate that the proposed TSDM can accurately recover power system measurements despite renewables-induced strong randomness and highly nonlinear dynamics. Additionally, the proposed TSDM has stronger robustness compared to existing reconstruction networks and exhibits lower computational complexity than general denoising diffusion models.",
    "published_date": "2023-12-07",
    "pdf_link": "https://arxiv.org/pdf/2312.04346v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Cyber-Physical Systems Security",
      "subdomain": "Power Grid/Smart Grid Security",
      "specific_problem": "Recovery of corrupted power system measurements (detection of anomalies/attacks and imputation of missing/corrupted SCADA/PMU data) under cyber-physical uncertainties",
      "attack_types": [
        "False Data Injection Attack (FDIA)",
        "PMU Data Manipulation Attack (PDMA)",
        "Replay attack",
        "Random noise injection",
        "Step attack",
        "Ramp attack",
        "Phase shift attack",
        "Amplitude scaling attack",
        "Data loss (random missing, non-random missing)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Diffusion Model",
        "specific": "Enhanced DDIM (classifier-guided conditional diffusion; two-stage detection-imputation)",
        "novel_contribution": "Two-stage denoising diffusion model (TSDM) with a classifier-guided anomaly detection stage and a diffusion-based imputation stage; Bayesian optimal variance estimation and subsequence sampling to accelerate generation"
      },
      {
        "type": "primary",
        "category": "Classifier Guidance",
        "specific": "Classifier-guided conditional diffusion",
        "novel_contribution": "Classifier-guided conditional anomaly detection to identify and rectify outliers before imputation"
      }
    ],
    "learning_paradigm": [
      "Unsupervised",
      "Conditional Generation"
    ],
    "datasets": [
      {
        "name": "IEEE 30-bus system (SCADA/PMU measurements simulated with DMAs and data loss)",
        "type": "synthetic",
        "domain": "power_system_measurements",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "NPCC 140-bus system (PMU measurements simulated with DMAs and data loss)",
        "type": "synthetic",
        "domain": "power_system_measurements",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "“all these existing recovery approaches may exhibit accuracy bottlenecks in the presence of cyber-physical uncertainties.”",
        "“the controllable data generation and efficient computing methods of diffusion models for deterministic data recovery trajectory still need further investigation.”",
        "Classic DSE (EKF/UKF/PF) “exhibit low estimation accuracy when encountering gross outliers induced by FDIA and PMU data manipulation attack (PDMA) … or PMU signal losses.”",
        "Time-series ML (LSTM/GRU) “may have overfitting and generalization issues when confronting strong randomness and complex nonlinearity.”",
        "Low-rank and ADMM-based recovery improve accuracy/convergence “only when the measurements are accurately observed.”"
      ],
      "limitations": [
        "“Assume that FDIA and data loss do not happen simultaneously” for the recovery formulation.",
        "Attack modeling is limited to six temporal forms (step, ramp, random noises, replay, phase shift, amplitude scaling)."
      ],
      "future_work": [],
      "motivation": "Cyber-physical uncertainties (cyber attacks and data losses, renewables-induced randomness, nonlinear dynamics) degrade measurement quality in SCADA/WAMS; need a robust, accurate, and efficient recovery method that can detect anomalies and impute missing/corrupted data.",
      "potential_research_ideas": [
        "Extend TSDM to jointly handle simultaneous FDIA and data loss in a unified detection-imputation framework.",
        "Integrate physics-informed constraints (AC power flow/DAE) into diffusion sampling via constrained or score-conditioned guidance to further improve physical consistency.",
        "Develop online/streaming TSDM with concept-drift adaptation for real-time WAMS, including uncertainty-aware step-size and adaptive subsequence length.",
        "Multi-modal fusion: graph-aware diffusion that jointly models SCADA and PMU on the grid topology; investigate GNN-conditioned diffusion.",
        "Adversarially robust diffusion: evaluate against adaptive attackers aware of TSDM and apply adversarial training or certified defenses.",
        "Privacy-preserving/federated training of diffusion priors across utilities without sharing raw measurements.",
        "Explainability for the detection stage (per-channel/time attributions) to support operator trust and forensics.",
        "Automatic schedule learning: learn the variance and subsequence sampling schedule to balance speed-accuracy under different operating conditions."
      ],
      "architectural_improvement_recommendations": [
        "Replace fixed optimal-variance schedule with a learned variance predictor conditioned on operating regime features (e.g., renewables level, event indicators).",
        "Introduce topology-aware conditioning (graph embeddings) and cross-attention between channels/time to better capture spatio-temporal structure.",
        "Use joint training with a physics-consistency loss (e.g., power flow residual penalties) during diffusion denoising.",
        "Incorporate a lightweight uncertainty estimator to adaptively choose subsequence length and number of reverse steps per instance.",
        "Unify the classifier guidance and imputation into a single multi-task diffusion model with shared backbone and task-specific heads."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": "The method accelerates diffusion generation using optimal variance and subsequence sampling; the authors claim lower computational complexity than general denoising diffusion models, but no hardware/training-time details are provided."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "A novel detection and imputation based two-stage architecture (TSDM): classifier-guided conditional anomaly detection followed by diffusion-based measurement imputation.",
      "Both stages are based on an enhanced DDIM with low-length subsequences, using Bayesian estimation of precise mean and optimal variance to accelerate generation.",
      "TSDM is built on established measurement and system uncertainty models; demonstrates accurate recovery for SCADA with renewables and WAMS with highly nonlinear dynamics, stronger robustness than existing reconstruction networks, and lower computational complexity than general diffusion models."
    ]
  },
  {
    "arxiv_id": "2311.16018v1",
    "title": "RIDE: Real-time Intrusion Detection via Explainable Machine Learning Implemented in a Memristor Hardware Architecture",
    "authors": "Jingdi Chen; Lei Zhang; Joseph Riem; Gina Adam; Nathaniel D. Bastian; Tian Lan",
    "abstract": "Deep Learning (DL) based methods have shown great promise in network intrusion detection by identifying malicious network traffic behavior patterns with high accuracy, but their applications to real-time, packet-level detections in high-speed communication networks are challenging due to the high computation time and resource requirements of Deep Neural Networks (DNNs), as well as lack of explainability. To this end, we propose a packet-level network intrusion detection solution that makes novel use of Recurrent Autoencoders to integrate an arbitrary-length sequence of packets into a more compact joint feature embedding, which is fed into a DNN-based classifier. To enable explainability and support real-time detections at micro-second speed, we further develop a Software-Hardware Co-Design approach to efficiently realize the proposed solution by converting the learned detection policies into decision trees and implementing them using an emerging architecture based on memristor devices. By jointly optimizing associated software and hardware constraints, we show that our approach leads to an extremely efficient, real-time solution with high detection accuracy at the packet level. Evaluation results on real-world datasets (e.g., UNSW and CIC-IDS datasets) demonstrate nearly three-nines detection accuracy with a substantial speedup of nearly four orders of magnitude.",
    "published_date": "2023-11-27",
    "pdf_link": "https://arxiv.org/pdf/2311.16018v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Real-time packet-level intrusion detection with explainable ML and memristor hardware co-design",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Autoencoder",
        "specific": "Autoencoder for payload compression",
        "novel_contribution": "Compress per-packet payload bytes into fixed-length embeddings to handle variable payload length."
      },
      {
        "type": "primary",
        "category": "RNN",
        "specific": "Recursive/Recurrent Autoencoder (RAE)",
        "novel_contribution": "Greedy hierarchical integration of an arbitrary-length sequence of packets into a compact joint embedding."
      },
      {
        "type": "primary",
        "category": "DNN",
        "specific": null,
        "novel_contribution": "DNN classifier on joint embeddings serves as the teacher policy prior to conversion to a tree."
      },
      {
        "type": "primary",
        "category": "Decision Tree",
        "specific": "CART with Cost-Complexity Pruning (CCP)",
        "novel_contribution": "Teacher–student conversion from DNN to an inherently explainable decision tree and pruning for compact, hardware-mappable policy."
      },
      {
        "type": "primary",
        "category": "Knowledge Distillation",
        "specific": "Teacher–student (DNN to Decision Tree)",
        "novel_contribution": "Use DNN-predicted labels to train the decision tree student for policy extraction."
      },
      {
        "type": "primary",
        "category": "Quantization",
        "specific": "β-bit threshold quantization",
        "novel_contribution": "Quantize node split thresholds to enable efficient analog memristor comparator implementation."
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "UNSW-NB15",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CIC-IDS-2017",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "accuracy",
      "inference_latency (microseconds)",
      "speedup (orders of magnitude)",
      "hardware resource usage (area, power/energy)"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "How to integrate an arbitrary-length sequence of packet payloads into a compact joint embedding for packet-level intrusion detection?",
        "How to achieve explainable packet-level detection without sacrificing DNN-level performance?",
        "How to map learned detection policies onto memristor-based hardware for microsecond, real-time inference under area/power constraints?",
        "How to jointly optimize software (tree size/pruning) and hardware (quantization/circuit parameters) to maximize accuracy under resource constraints?"
      ],
      "gaps_identified": [
        "Existing DL NIDS often focus on flow-level features and do not effectively coalesce features from sequences of packets.",
        "DNNs are computationally heavy and resource-intensive, making real-time packet-level detection on constrained edge devices difficult.",
        "Lack of explainability in DNN-based NIDS solutions.",
        "High-speed, low-latency networks require significant speedup over DNNs."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Enable an explainable, high-accuracy, real-time, packet-level NIDS suitable for resource-constrained edge environments via software–hardware co-design using memristor-based implementation.",
      "potential_research_ideas": [
        "End-to-end differentiable/soft decision tree training with hardware-aware constraints to remove teacher–student mismatch.",
        "Replace/augment RAEs with lightweight Transformer or 1D-CNN encoders for payload bytes while keeping hardware budgets.",
        "Online/continual learning to adapt embeddings and trees to traffic drift under hardware constraints.",
        "Evaluate and harden against adversarial packet manipulations targeting decision thresholds; develop robust training.",
        "Privacy-preserving (federated/split) training for cross-organization NIDS while retaining hardware mappability.",
        "Multi-resolution fusion of flow-level and packet-level embeddings to improve multiclass discrimination.",
        "Memristor fault-tolerance and aging-aware calibration for stability under device variability."
      ],
      "architectural_improvement_recommendations": [
        "Quantization-aware distillation: include β-bit quantization during DNN-to-tree training to minimize post-quantization accuracy loss.",
        "Use gradient-boosted trees distilled from DNNs and design ensemble-friendly memristor aggregation hardware.",
        "Attention-based or gated sequence aggregation instead of greedy RAEs to capture long-range dependencies with controlled model size.",
        "Introduce early-exit cascades of small trees to reduce average latency/energy on benign-dominant traffic.",
        "Hybrid analog–digital node design (analog comparison with digital control/correction) for reconfigurability and error mitigation.",
        "Apply hardware-constrained tree growth/pruning (limits on depth/fan-out) during training to minimize area and routing."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Memristor + 130nm CMOS hybrid analog front-end; microsecond-level per-packet inference; joint optimization over tree pruning and β-bit threshold quantization."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Resource-constrained edge sensors in high-speed, low-latency networks (e.g., IoBT)",
      "scalability_discussed": true,
      "inference_time": "microseconds per packet",
      "deployment_challenges": [
        "Keeping decision tree size tractable and reusable for hardware mapping",
        "Accuracy impact from β-bit threshold quantization",
        "Memristor device variability and fabrication constraints",
        "Designing flexible, reusable node blocks and chiplet interconnects"
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Joint-packet-feature method using autoencoders (including a recursive/recurrent autoencoder) to integrate variable-length packet sequences into compact embeddings.",
      "Teacher–student conversion of a DNN classifier to an explainable CART decision tree with cost complexity pruning for compactness.",
      "Software–hardware co-design and memristor-based analog implementation of the pruned decision tree enabling microsecond inference with area/energy efficiency.",
      "Evaluation on UNSW-NB15 and CIC-IDS-2017 showing nearly three-nines detection accuracy and nearly four orders of magnitude speedup."
    ]
  },
  {
    "arxiv_id": "2311.17603v2",
    "title": "sec-certs: Examining the security certification practice for better vulnerability mitigation",
    "authors": "Adam Janovsky; Jan Jancar; Petr Svenda; Łukasz Chmielewski; Jiri Michalik; Vashek Matyas",
    "abstract": "Products certified under security certification frameworks such as Common Criteria undergo significant scrutiny during the costly certification process. Yet, critical vulnerabilities, including private key recovery (ROCA, Minerva, TPM-Fail...), get discovered in certified products with high assurance levels. Furthermore, assessing which certified products are impacted by such vulnerabilities is complicated due to the large amount of unstructured certification-related data and unclear relationships between the certified products. To address these problems, we conducted a large-scale automated analysis of Common Criteria certificates. We trained unsupervised models to learn which vulnerabilities from NIST's National Vulnerability Database impact existing certified products and how certified products reference each other. Our tooling automates the analysis of tens of thousands of certification-related documents, extracting machine-readable features where manual analysis is unattainable. Further, we identify the security requirements that are associated with products being affected by fewer and less severe vulnerabilities. This indicates which aspects of certification correlate with higher security. We demonstrate how our tool can be used for better vulnerability mitigation on four case studies of known, high-profile vulnerabilities. All tools and continuously updated results are available at https://seccerts.org",
    "published_date": "2023-11-29",
    "pdf_link": "https://arxiv.org/pdf/2311.17603v2",
    "paper_types": [
      "empirical_analysis",
      "benchmark",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Security Certification and Compliance",
      "subdomain": "Vulnerability Management and Assurance",
      "specific_problem": "Automated mapping of vulnerabilities (CVE/CPE) to Common Criteria-certified products and learning inter-certificate reference relationships to assess vulnerability impact",
      "attack_types": [
        "Cryptographic key recovery",
        "Side-channel attacks",
        "Timing attacks",
        "Cryptographic implementation flaws"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Rule-based NLP",
        "specific": "Regular-expression-based feature extraction (472 regexes across 33 categories)",
        "novel_contribution": "Large-scale, automated extraction of machine-readable features from heterogeneous CC certification PDFs (certification reports, security targets, maintenance reports)"
      },
      {
        "type": "primary",
        "category": "Similarity-based matching",
        "specific": "Fuzzy string matching (RapidFuzz partial token sort ratio, partial token set ratio; normalized indel distance) with lemmatization",
        "novel_contribution": "Unsupervised classifier to map certified products to vulnerable CPEs/CVEs with thresholding tuned to achieve ~90% precision"
      },
      {
        "type": "primary",
        "category": "Graph inference / Entity resolution",
        "specific": null,
        "novel_contribution": "Unsupervised method to learn inter-certificate references and produce a reference graph for vulnerability propagation/notification"
      },
      {
        "type": "baseline",
        "category": "OCR/Text extraction",
        "specific": "pdftotext (--raw) and Tesseract OCR fallback",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "Common Criteria Portal certification artifacts (CSV/HTML plus Certification Reports, Security Targets, Maintenance Reports, Protection Profiles)",
        "type": "public",
        "domain": "certification_documents",
        "link": "https://www.commoncriteriaportal.org",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "NIST National Vulnerability Database (CVE records with CPE mappings)",
        "type": "public",
        "domain": "vulnerability_database",
        "link": "https://nvd.nist.gov",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "sec-certs weekly snapshots of CC and FIPS 140 certificates and extracted features",
        "type": "public",
        "domain": "certification_documents",
        "link": "https://seccerts.org",
        "is_new_contribution": true,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Precision (manual expert annotation of ≥100 samples per model)",
      "Normalized indel distance (RapidFuzz partial token sort/set ratios) for similarity scoring"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Which NVD-listed vulnerabilities (CVEs) impact existing Common Criteria-certified products?",
        "How do certified products reference each other, and how can these references be learned automatically?",
        "Which security requirements are associated with products being affected by fewer and less severe vulnerabilities?"
      ],
      "gaps_identified": [
        "Lack of systematic, large-scale analysis across the entire Common Criteria ecosystem",
        "Certification artifacts are unstructured PDFs with unclear inter-product relationships, hindering vulnerability impact assessment",
        "Prior quantitative work limited in scope (e.g., EAL4+ subset) and manual vulnerability mapping",
        "No prior research learning and leveraging inter-certificate references for vulnerability impact and notifications",
        "Limited transparency and complexity of certification schemes impede automated analysis"
      ],
      "limitations": [
        "Assumes the NVD CVE-to-CPE mapping is error-free: “We consider the mapping between CPEs and CVEs maintained by NIST as being error-free.”",
        "Precision–recall trade-off due to thresholding in similarity-based matching (threshold tuned for ~90% precision, potentially lowering recall)",
        "PDF-to-text conversion issues require OCR; some artifacts remain problematic even after Tesseract fallback",
        "Vendor naming/version normalization challenges can hinder accurate matching",
        "Focus centered on Common Criteria; generalization to other schemes is mentioned but not the primary empirical target"
      ],
      "future_work": [],
      "motivation": "Enable automated, scalable, and transparent analysis of security certifications to better assess vulnerability impact and identify certification aspects correlating with higher security.",
      "potential_research_ideas": [
        "Develop a supervised or weakly supervised entity resolution model using labeled pairs of (certificate title, CPE) to improve recall while maintaining high precision.",
        "Leverage modern language models to parse Security Targets and Certification Reports for richer semantic features (e.g., specific configurations, dependency chains).",
        "Construct a probabilistic vulnerability propagation model over the learned reference graph to quantify downstream impact and uncertainty.",
        "Extend the framework to other certification schemes (e.g., EMVCo, FIPS 140) with scheme-specific parsers and cross-scheme linkage.",
        "Active learning for human-in-the-loop labeling of ambiguous matches to iteratively improve the matcher and calibrate thresholds.",
        "Temporal analysis linking issuance/maintenance timelines to vulnerability disclosure and patch windows for risk forecasting."
      ],
      "architectural_improvement_recommendations": [
        "Introduce a two-stage matcher: rule-based pre-filtering followed by a learned re-ranker (e.g., Siamese encoder or character-level model) for CPE–certificate matching.",
        "Add robust vendor and product canonicalization via learned string normalization and alias dictionaries mined from historical artifacts.",
        "Use graph-based methods (e.g., probabilistic record linkage or GNNs) to refine inter-certificate references using multiple evidence signals.",
        "Integrate structured parsers for versioning (semantic versioning, ranges) to reduce false negatives in version matching.",
        "Implement confidence calibration and abstention mechanisms for automated alerts to stakeholders."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/crocs-muni/sec-certs",
      "frameworks": [
        "Python",
        "pdftotext",
        "Tesseract OCR",
        "RapidFuzz"
      ],
      "reproducibility_score": "high",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Public web portal (seccerts.org) analyzing real CC certification artifacts and NVD data; weekly updates",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Inconsistent and heterogeneous PDF artifacts across schemes/vendors",
        "PDF-to-text conversion failures requiring OCR",
        "Discrepancies between CSV and HTML metadata sources on the CC portal",
        "Ambiguities in vendor/product naming and version extraction",
        "Threshold selection balancing precision and recall for matching"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Open-source framework for automated collection and feature extraction from CC and FIPS 140 certification artifacts; weekly snapshots published.",
      "Unsupervised method for mapping NVD vulnerabilities (CVE/CPE) to affected Common Criteria-certified products.",
      "Quantitative analysis of vulnerabilities impacting CC-certified products based on automated mappings.",
      "Unsupervised method to learn inter-certificate references and build a reference graph for impact assessment and notifications.",
      "Identification of security requirements associated with fewer and less severe vulnerabilities, indicating aspects of certification correlating with higher security.",
      "Case studies on four high-profile vulnerabilities demonstrating improved vulnerability mitigation."
    ]
  },
  {
    "arxiv_id": "2311.08000v2",
    "title": "LiPar: A Lightweight Parallel Learning Model for Practical In-Vehicle Network Intrusion Detection",
    "authors": "Aiheng Zhang; Qiguang Jiang; Kai Wang; Ming Li",
    "abstract": "With the development of intelligent transportation systems, vehicles are exposed to a complex network environment. As the main network of in-vehicle networks, the controller area network (CAN) has many potential security hazards, resulting in higher generalization capability and lighter security requirements for intrusion detection systems to ensure safety. Among intrusion detection technologies, methods based on deep learning work best without prior expert knowledge. However, they all have a large model size and usually rely on large computing power such as cloud computing, and are therefore not suitable to be installed on the in-vehicle network. Therefore, we explore computational resource allocation schemes in in-vehicle network and propose a lightweight parallel neural network structure, LiPar, which achieve enhanced generalization capability for identifying normal and abnormal patterns of in-vehicle communication flows to achieve effective intrusion detection while improving the utilization of limited computing resources. In particular, LiPar adaptationally allocates task loads to in-vehicle computing devices, such as multiple electronic control units, domain controllers, computing gateways through evaluates whether a computing device is suitable to undertake the branch computing tasks according to its real-time resource occupancy. Through experiments, we prove that LiPar has great detection performance, running efficiency, and lightweight model size, which can be well adapted to the in-vehicle environment practically and protect the in-vehicle CAN bus security. Furthermore, with only the common multi-dimensional branch convolution networks for detection, LiPar can have a high potential for generalization in spatial and temporal feature fusion learning.",
    "published_date": "2023-11-14",
    "pdf_link": "https://arxiv.org/pdf/2311.08000v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "IoT Security",
      "subdomain": "Automotive/In-Vehicle Network Security",
      "specific_problem": "CAN bus intrusion detection with lightweight on-vehicle deployment and parallel resource allocation",
      "attack_types": [
        "DoS (low-ID flooding)",
        "Fuzzing (random ID/message injection)",
        "Impersonation/Spoofing (drive gear)",
        "Impersonation/Spoofing (RPM)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "CNN",
        "specific": "Multi-branch shallow 3x3 convolutions (DWParNet)",
        "novel_contribution": "Lightweight parallel multi-branch CNN for spatial feature extraction across different down-sampling dimensions; branches allocated across in-vehicle devices"
      },
      {
        "type": "primary",
        "category": "RNN",
        "specific": "LSTM (STParNet)",
        "novel_contribution": "Temporal feature extraction branch combined with CNN spatial features for spatiotemporal fusion to improve generalization"
      },
      {
        "type": "primary",
        "category": "Algorithm",
        "specific": "Resource adaptation scheduling",
        "novel_contribution": "Adaptively allocates branch computation to ECUs/domain controllers/gateways based on real-time resource occupancy to meet latency and resource constraints"
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Car-Hacking dataset",
        "type": "public",
        "domain": "vehicle_can_bus",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How to design an IDS for CAN that achieves high accuracy with low latency and low resource consumption suitable for ECUs?",
        "How to leverage parallel resources in in-vehicle networks to distribute deep learning computations across multiple devices?",
        "Can spatiotemporal feature fusion (CNN+LSTM) in a lightweight, shallow, multi-branch design improve generalization for CAN intrusion detection?"
      ],
      "gaps_identified": [
        "Deep learning IDS models for CAN are typically large, rely on cloud/edge resources, and are unsuitable for on-vehicle deployment.",
        "Existing approaches often use linear stacking of layers on a single device leading to large model size, high delay, and potential device overload.",
        "Limited methods exploit the natural parallelism of multiple ECUs and on-vehicle computing devices for IDS."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Vehicles require timely and reliable IDS on resource-constrained in-vehicle environments; large deep models with cloud reliance do not meet these constraints. The authors aim to create a lightweight, parallel, adaptable IDS suitable for deployment on the CAN gateway/OBD-II using available ECU resources.",
      "potential_research_ideas": [
        "Extend LiPar to CAN-FD and automotive Ethernet, evaluating cross-bus transfer learning and multi-bus fusion.",
        "Incorporate self-supervised or contrastive pretraining on unlabeled CAN streams to improve generalization and label efficiency.",
        "Study adversarial robustness for CAN IDS (bit-level perturbations, message re-timing) and develop robust training or certified defenses.",
        "Add explainability modules (e.g., attention attribution over bytes/IDs/timestamps) to support forensic diagnosis and safety certification.",
        "Federated or continual learning across fleets with privacy preservation to adapt to vehicle-specific distributions.",
        "Hardware-in-the-loop and real-car evaluations to measure latency, energy, and interference with ECU tasks under realistic loads.",
        "Online adaptive branch activation/early-exit mechanisms that meet strict deadlines under varying ECU loads.",
        "Anomaly-to-attack-type mapping (hierarchical classification) to support incident response on the bus."
      ],
      "architectural_improvement_recommendations": [
        "Replace or complement LSTM with dilated Temporal CNNs or lightweight attention (Performer/Linear attention) to reduce latency and improve long-range temporal modeling.",
        "Use depthwise separable and group convolutions, channel shuffle, and squeeze-and-excitation or light attention in CNN branches to further cut parameters.",
        "Apply model compression: post-training quantization (INT8), pruning, and knowledge distillation from a larger teacher to LiPar branches.",
        "Implement dynamic branch gating/early exit conditioned on confidence and resource state to save compute under low load.",
        "Optimize the resource adaptation scheduler using reinforcement learning or MPC with hard real-time constraints and ECU priority/risk weighting.",
        "Asynchronous pipeline across ECUs with micro-batching and zero-copy serialization to reduce inter-device transfer overhead.",
        "Neural architecture search constrained by ECU latency/memory budgets for per-branch architectures."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "In-vehicle gateway/OBD-II with branch computations on multiple ECUs, domain controllers, and computing gateways",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Severe memory and compute constraints on ECUs",
        "High timeliness requirements for on-vehicle IDS",
        "Risk of overloading a single ECU leading to detection delay",
        "Heterogeneous utilization/importance across ECU domains requiring adaptive scheduling",
        "Potential interference with original vehicle functions if compute is not carefully allocated"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Design of LiPar, a lightweight parallel IDS for IVN that distributes branch computations across multiple in-vehicle devices using a resource adaptation algorithm (claimed first parallel-based IDS for IVN).",
      "Multidimensional spatiotemporal feature extraction via combined CNN (spatial) and LSTM (temporal) branches, with feature fusion to enhance generalization.",
      "Resource adaptation algorithm that evaluates real-time device resource occupancy to allocate branch tasks to ECUs/domain controllers/gateways.",
      "Experimental evaluation indicating strong detection performance, efficiency, and lightweight model size suitable for practical in-vehicle deployment on CAN."
    ]
  },
  {
    "arxiv_id": "2312.00023v1",
    "title": "Hypergraph Topological Features for Autoencoder-Based Intrusion Detection for Cybersecurity Data",
    "authors": "Bill Kay; Sinan G. Aksoy; Molly Baird; Daniel M. Best; Helen Jenne; Cliff Joslyn; Christopher Potvin; Gregory Henselman-Petrusek; Garret Seppala; Stephen J. Young; Emilie Purvine",
    "abstract": "In this position paper, we argue that when hypergraphs are used to capture multi-way local relations of data, their resulting topological features describe global behaviour. Consequently, these features capture complex correlations that can then serve as high fidelity inputs to autoencoder-driven anomaly detection pipelines. We propose two such potential pipelines for cybersecurity data, one that uses an autoencoder directly to determine network intrusions, and one that de-noises input data for a persistent homology system, PHANTOM. We provide heuristic justification for the use of the methods described therein for an intrusion detection pipeline for cyber data. We conclude by showing a small example over synthetic cyber attack data.",
    "published_date": "2023-11-09",
    "pdf_link": "https://arxiv.org/pdf/2312.00023v1",
    "paper_types": [
      "position"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Anomaly-based intrusion detection in network flow/log data using hypergraph topological features with autoencoder pipelines and PHANTOM integration",
      "attack_types": [
        "Port scan",
        "General network intrusion/anomalous behavior"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Autoencoder",
        "specific": "Denoising autoencoder (unspecified architecture)",
        "novel_contribution": "Proposes using autoencoders directly on topological feature vectors derived from hypergraphs for anomaly detection, and as a denoising/preprocessing step before PHANTOM's persistent-homology-based anomaly scoring"
      }
    ],
    "learning_paradigm": [
      "Unsupervised",
      "One-class anomaly detection"
    ],
    "datasets": [
      {
        "name": "PNNL ARC deception network flow dataset (test range with red team activity)",
        "type": "proprietary",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Autoencoder reconstruction error thresholding (for anomaly detection)",
      "Wasserstein distance between persistence diagrams (PHANTOM anomaly score)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Do hypergraph-based topological features capture global behavior that is useful for anomaly detection in cybersecurity data?",
        "Can autoencoders detect network intrusions when trained on topological summaries of hypergraphs over time?",
        "Can autoencoders denoise inputs to persistent-homology-based systems (PHANTOM) to improve robustness to noisy/variable normal data?"
      ],
      "gaps_identified": [
        "Pairwise graph models are lossy for multi-way relations; hypergraphs better capture complex multi-entity interactions",
        "Distinguishing high-variability normal behavior from anomalies is challenging; noise can mask anomalies",
        "Access to both clean and dirty (attack) data for training/validation is difficult in practice",
        "Computation on hypergraphs/topological constructs can be expensive or intractable at scale",
        "Current PHANTOM inputs are user-defined; lack of principled incorporation of hypergraph topological features"
      ],
      "limitations": [
        "Position paper with heuristic justification and a small illustrative example only",
        "No quantitative evaluation or comparative baselines reported",
        "Autoencoder architectures and training details are unspecified",
        "End-to-end pipeline integration and deployment are not demonstrated",
        "Dataset is proprietary and not publicly available"
      ],
      "future_work": [
        "Integrate hypergraph summary statistics and topological features into PHANTOM",
        "Train autoencoders on raw logs or hypergraph-derived topological feature vectors to denoise and detect anomalies",
        "Develop advanced hypergraph analyses beyond simple statistics (e.g., Betti numbers, Hodge Laplacian spectra) for intrusion detection",
        "Evaluate on broader datasets and characterize thresholds for Wasserstein-based anomaly scoring",
        "Improve robustness to noisy normal behavior and address data availability challenges for clean/dirty labels"
      ],
      "motivation": "Capture complex multi-way relations in cyber data via hypergraphs so that topological features reflecting global structure can feed autoencoder-based pipelines for more effective anomaly/intrusion detection.",
      "potential_research_ideas": [
        "Build a dynamic hypergraph anomaly detection benchmark from public network-flow/log datasets with derived hypergraph structures and topological labels",
        "Explore hypergraph neural networks (HGNNs) and compare with TDA-feature-based autoencoders for intrusion detection",
        "Incorporate differentiable TDA layers (e.g., persistence-based neural layers) to learn topological summaries end-to-end with autoencoders",
        "Combine temporal models (sequence autoencoders, Transformers) over time-windowed topological features for dynamic anomaly detection",
        "Self-supervised pretraining on logs/hypergraphs (e.g., masked event prediction) prior to AE fine-tuning for anomalies",
        "Study adversarial robustness of topological-feature pipelines and develop defenses (e.g., topology-aware adversarial training)",
        "Develop explainability methods mapping topological anomalies back to concrete entities/flows and operations",
        "Online/streaming detection with concept drift adaptation for Wasserstein thresholds and AE reconstruction-error calibration"
      ],
      "architectural_improvement_recommendations": [
        "Use variational or denoising autoencoders with sparsity/contractive regularization to better handle noise and rare events",
        "Add temporal modeling (LSTM/GRU/Transformer encoder) over sequential windows of topological features",
        "Regularize with (hyper)graph/Laplacian penalties to preserve structural smoothness across related entities",
        "Integrate differentiable persistent homology layers or persistence-image embeddings before the AE",
        "Calibrate anomaly thresholds via extreme value theory or conformal prediction for both reconstruction error and Wasserstein distances",
        "Employ GPU-accelerated TDA and scalable hypergraph preprocessing to manage computational cost",
        "Use uncertainty estimation (MC dropout/ensembles) to improve anomaly confidence scoring"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Need access to clean and attack (dirty) data for training/validation",
        "High variability/noise in normal operations complicates anomaly discrimination",
        "Computational cost and potential intractability of hypergraph and topological computations",
        "Threshold selection and stability for Wasserstein-based anomaly scores",
        "Integration of new feature pipelines into existing SOC workflows and data pipelines"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Positions hypergraph-based topological features as high-fidelity inputs for autoencoder-driven intrusion detection",
      "Proposes two pipelines: (1) direct autoencoder on hypergraph-derived topological features; (2) autoencoder denoising before PHANTOM persistent-homology anomaly scoring",
      "Discusses topological constructs (e.g., Betti numbers, Hodge Laplacians, persistent homology) for capturing global structure from cyber data",
      "Provides a small illustrative example on a test-range network flow dataset where a hypergraph property (maximum degree) indicates a port scan",
      "Outlines planned integration of hypergraph summaries into PHANTOM and argues for robustness gains via autoencoder preprocessing"
    ]
  },
  {
    "arxiv_id": "2312.01941v1",
    "title": "Intrusion Detection System with Machine Learning and Multiple Datasets",
    "authors": "Haiyan Xuan; Mohith Manohar",
    "abstract": "As Artificial Intelligence (AI) technologies continue to gain traction in the modern-day world, they ultimately pose an immediate threat to current cybersecurity systems via exploitative methods. Prompt engineering is a relatively new field that explores various prompt designs that can hijack large language models (LLMs). If used by an unethical attacker, it can enable an AI system to offer malicious insights and code to them. In this paper, an enhanced intrusion detection system (IDS) that utilizes machine learning (ML) and hyperparameter tuning is explored, which can improve a model's performance in terms of accuracy and efficacy. Ultimately, this improved system can be used to combat the attacks made by unethical hackers. A standard IDS is solely configured with pre-configured rules and patterns; however, with the utilization of machine learning, implicit and different patterns can be generated through the models' hyperparameter settings and parameters. In addition, the IDS will be equipped with multiple datasets so that the accuracy of the models improves. We evaluate the performance of multiple ML models and their respective hyperparameter settings through various metrics to compare their results to other models and past research work. The results of the proposed multi-dataset integration method yielded an accuracy score of 99.9% when equipped with the XGBoost and random forest classifiers and RandomizedSearchCV hyperparameter technique.",
    "published_date": "2023-12-04",
    "pdf_link": "https://arxiv.org/pdf/2312.01941v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Network intrusion detection using supervised ML on combined datasets (benign vs malicious classification) with hyperparameter tuning",
      "attack_types": [
        "Fuzzers",
        "Analysis",
        "Backdoors",
        "DoS",
        "Exploits",
        "Generic",
        "Reconnaissance",
        "Shellcode",
        "Worms"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Gradient Boosting",
        "specific": "XGBoost",
        "novel_contribution": "Applied with RandomizedSearchCV and SMOTE on a combined UNSW-NB15 + KDD-CUP 1999 feature-aligned dataset; achieved 99.91% accuracy"
      },
      {
        "type": "primary",
        "category": "Ensemble Trees",
        "specific": "RandomForestClassifier (scikit-learn)",
        "novel_contribution": "Applied with RandomizedSearchCV and SMOTE on a combined UNSW-NB15 + KDD-CUP 1999 feature-aligned dataset; achieved 99.93% accuracy"
      },
      {
        "type": "baseline",
        "category": "Linear Models",
        "specific": "Logistic Regression (scikit-learn)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Hyperparameter Optimization",
        "specific": "RandomizedSearchCV (scikit-learn)",
        "novel_contribution": "Used to tune XGBoost, Random Forest, and Logistic Regression"
      },
      {
        "type": "primary",
        "category": "Imbalanced Learning / Sampling",
        "specific": "SMOTE (imbalanced-learn)",
        "novel_contribution": "Used to address class imbalance in both datasets prior to training"
      },
      {
        "type": "primary",
        "category": "Feature Selection",
        "specific": null,
        "novel_contribution": "Selected overlapping/relevant features to enable concatenation of UNSW-NB15 and KDD-CUP 1999; detailed mapping provided (Table 1)"
      },
      {
        "type": "primary",
        "category": "Preprocessing",
        "specific": "Categorical encoding to numeric",
        "novel_contribution": "Converted non-numeric fields (dashes, empty values, strings) to numeric before training"
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "UNSW-NB15 (first CSV of 4)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "KDD Cup 1999",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Logistic Regression",
        "paper_reference": null,
        "metric": "Accuracy, Precision, Recall, F1 (combined dataset, 10% training set)",
        "their_result": "XGBoost: Acc 0.9991, Prec 0.9991, Rec 0.9996, F1 0.9994",
        "baseline_result": "Logistic Regression: Acc 0.8064, Prec 0.9351, Rec 0.7796, F1 0.8503"
      },
      {
        "method_name": "Logistic Regression",
        "paper_reference": null,
        "metric": "Accuracy, Precision, Recall, F1 (combined dataset, 10% training set)",
        "their_result": "Random Forest: Acc 0.9993, Prec 0.9992, Rec 0.9998, F1 0.9995",
        "baseline_result": "Logistic Regression: Acc 0.8064, Prec 0.9351, Rec 0.7796, F1 0.8503"
      },
      {
        "method_name": "XGBoost on NSL-KDD (prior work)",
        "paper_reference": "Dhaliwal et al., 2018",
        "metric": "Accuracy",
        "their_result": "0.9991 (XGBoost on combined UNSW-NB15 + KDD-99 with tuning)",
        "baseline_result": "≈0.987 (XGBoost trained only on NSL-KDD as reported by prior work)"
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "Precision",
      "Recall",
      "F1 Score"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Does integrating multiple IDS datasets and applying hyperparameter tuning improve ML-based IDS performance compared to single-dataset training?"
      ],
      "gaps_identified": [
        "Limited research on prompt-engineering-based attacks against LLM-integrated systems and their manifestation in network traffic",
        "Unclear effectiveness of evaluated models against abnormal/AI-generated network attacks",
        "Comparative effectiveness of different hyperparameter tuning techniques for IDS remains underexplored"
      ],
      "limitations": [
        "Training set smaller than testing set (2–10% training used), which is acknowledged as atypical",
        "Only the 1st of 4 UNSW-NB15 CSV files was used",
        "Effectiveness against abnormal/AI-generated network traffic not evaluated",
        "Evaluation limited to three classical ML classifiers"
      ],
      "future_work": [
        "Evaluate whether the models are effective against abnormal/AI-generated network attacks",
        "Further investigate how LLMs can be exploited to inform IDS improvements",
        "Compare various hyperparameter tuning techniques beyond RandomizedSearchCV to identify the best per model"
      ],
      "motivation": "Improve IDS accuracy and efficacy to combat emerging AI-enabled threats by leveraging ML with hyperparameter tuning and training on multiple datasets.",
      "potential_research_ideas": [
        "Construct and evaluate IDS on synthetic AI-generated network traffic representing prompt-injection and LLM-agent misuse scenarios",
        "Domain adaptation and transfer learning to mitigate distribution shift between legacy datasets (KDD-99) and modern traffic (e.g., UNSW-NB15, CIC-based corpora)",
        "Develop a multiclass IDS on the combined datasets to detect specific attack categories rather than binary labels",
        "Self-supervised pretraining on large unlabeled flow data followed by supervised fine-tuning",
        "Federated or privacy-preserving IDS training across organizations without sharing raw traffic",
        "Robustness evaluation against adversarial evasion attacks and adversarial training for tree ensembles",
        "Uncertainty quantification and calibrated thresholds to reduce false positives in deployment",
        "Online/streaming inference architecture with concept drift detection and adaptive retraining"
      ],
      "architectural_improvement_recommendations": [
        "Use stratified k-fold cross-validation and repeated runs with fixed seeds to improve evaluation reliability",
        "Stacking/ensemble of XGBoost and Random Forest with meta-learner for potential gains",
        "Cost-sensitive learning or class-weighting to complement SMOTE and reduce overfitting to synthetic samples",
        "Feature engineering with interaction terms and automated selection (e.g., Boruta, SHAP-based pruning)",
        "Time-aware validation splits to reduce leakage and better reflect deployment",
        "Explainability via SHAP for feature importance auditing and model debugging",
        "Evaluate Bayesian optimization (e.g., Optuna) for potentially better hyperparameters than RandomizedSearchCV",
        "Harden against adversarial evasion (e.g., tree ensemble smoothing, randomized feature subsets at inference)"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "scikit-learn",
        "XGBoost",
        "imbalanced-learn",
        "Google Colab (environment)"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Google Colab Python 3 runtime, CPU accelerator, 51 GB RAM, 225.8 GB disk storage"
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes an IDS training approach that integrates multiple datasets (UNSW-NB15 and KDD Cup 1999) via feature alignment to improve accuracy",
      "Applies SMOTE for class imbalance handling and encodes categorical fields to numeric",
      "Tunes XGBoost, Random Forest, and Logistic Regression with RandomizedSearchCV; reports optimal hyperparameters",
      "Achieves high accuracy on the combined dataset: 99.91% (XGBoost) and 99.93% (Random Forest)",
      "Evaluates across varying training set sizes (2%–10%) and compares multiple metrics (Accuracy, Precision, Recall, F1)"
    ]
  },
  {
    "arxiv_id": "2311.15888v1",
    "title": "Towards Adaptive RF Fingerprint-based Authentication of IIoT devices",
    "authors": "Emmanuel Lomba; Ricardo Severino; Ana Fernández Vilas",
    "abstract": "As IoT technologies mature, they are increasingly finding their way into more sensitive domains, such as Medical and Industrial IoT, in which safety and cyber-security are of great importance. While the number of deployed IoT devices continues to increase exponentially, they still present severe cyber-security vulnerabilities. Effective authentication is paramount to support trustworthy IIoT communications, however, current solutions focus on upper-layer identity verification or key-based cryptography which are often inadequate to the heterogeneous IIoT environment. In this work, we present a first step towards achieving powerful and flexible IIoT device authentication, by leveraging AI adaptive Radio Frequency Fingerprinting technique selection and tuning, at the PHY layer for highly accurate device authentication over challenging RF environments.",
    "published_date": "2023-11-27",
    "pdf_link": "https://arxiv.org/pdf/2311.15888v1",
    "paper_types": [
      "position",
      "new_technique"
    ],
    "security_domain": {
      "primary": "IoT Security",
      "subdomain": "Device Authentication",
      "specific_problem": "PHY-layer authentication of IIoT devices via adaptive RF fingerprinting with AI-driven SDR parameter tuning and one-to-one radio ID verification",
      "attack_types": [
        "Device spoofing",
        "Sybil attack",
        "Replay attack",
        "Cloning detection",
        "Access control evasion"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "AutoML/Feature selection",
        "specific": null,
        "novel_contribution": "Closed-loop ML controller proposed to automatically select relevant RF features and tune SDR acquisition parameters (e.g., gains, filter bandwidths) to optimize feature quality under varying SNR/channel conditions"
      }
    ],
    "learning_paradigm": [],
    "datasets": [
      {
        "name": "Planned IIoT RF fingerprint dataset (SigMF-compliant)",
        "type": "private",
        "domain": "rf_signals",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can AI-driven adaptation of SDR acquisition parameters improve RF feature quality and thus authentication accuracy under challenging RF environments?",
        "How to realize robust, unforgeable, and portable PHY-layer fingerprints that remain stable across channel variations, device mobility, and environmental changes?",
        "Can a one-to-one radio ID verification approach improve security and reduce false acceptances compared to one-to-many classification in IIoT?",
        "How can SDR gateways and Edge/Cloud resources address IIoT heterogeneity and scalability for PHY-layer authentication?"
      ],
      "gaps_identified": [
        "Most RFF literature focuses on classification performance and not on automated, adaptive control of signal acquisition parameters in SDR-based receivers.",
        "Existing approaches typically rely on a limited, inflexible subset of signal features due to processing constraints, making accuracy highly SNR-dependent.",
        "Predominant use of one-to-many radio classification can grant access to rogue devices; one-to-one radio ID verification is underexplored.",
        "Solutions often target a single protocol and do not handle heterogeneous IIoT ecosystems.",
        "Closed-set assumptions prevail; open-set transmitter identification remains insufficiently addressed.",
        "Channel impairments and environmental variations are not handled in a self-adaptive manner.",
        "Lack of portability considerations across different SDR hardware."
      ],
      "limitations": [
        "Work-in-progress: current implementation focuses on signal acquisition automation and feature extraction modules; no end-to-end classification results are reported.",
        "AI controller for adaptive SDR tuning is not yet implemented/evaluated.",
        "Dataset under construction; not yet released for replication.",
        "No quantitative evaluation, metrics, or baseline comparisons are provided.",
        "Current testbed primarily uses 433 MHz transmitters; broader protocol coverage and higher-performance SDRs are pending.",
        "Robustness to channel impairments, device aging, and temperature variations is discussed but not experimentally validated."
      ],
      "future_work": [
        "Develop ML modules to adapt SDR elements (gains, filters) for improved feature quality.",
        "Build and publicly release a SigMF-compliant RF fingerprint dataset captured with multiple SDRs.",
        "Develop Edge/Cloud architecture for scalable classification and feature processing.",
        "Evaluate higher-performance SDR receivers such as USRPs.",
        "Address open-set transmitter recognition and fingerprint portability across hardware.",
        "Implement self-adaptive handling of channel impairments and environmental dynamics."
      ],
      "motivation": "Provide effective, scalable, and timely IIoT authentication by exploiting unique PHY-layer characteristics and leveraging AI to adapt SDR acquisition and feature extraction in heterogeneous and dynamic RF environments.",
      "potential_research_ideas": [
        "Formulate the SDR parameter tuning as a reinforcement learning problem with online reward signals derived from feature separability/verification accuracy.",
        "Investigate domain adaptation and invariant representation learning to mitigate channel and hardware-induced variability for fingerprint robustness and portability.",
        "Develop open-set verification with calibrated confidence (e.g., EVT-based or distance-based confidence) tailored to RF fingerprints.",
        "Design adversarially robust RFF verification by modeling and defending against over-the-air mimicry and generative spoofing attempts.",
        "Create multi-protocol, multi-band shared backbones that learn protocol-agnostic features while retaining protocol-specific heads for IIoT heterogeneity.",
        "Introduce active learning strategies to minimize labeling effort when onboarding new devices or after environment changes."
      ],
      "architectural_improvement_recommendations": [
        "Integrate a policy optimization module (e.g., Bayesian optimization or RL) to select SDR gains/filters and feature pipelines based on real-time SNR/channel estimates.",
        "Adopt a two-branch encoder: one branch for device-specific hardware impairments, another for channel compensation, with disentanglement losses.",
        "Use self-supervised pretraining (contrastive learning) on raw I/Q to learn robust embeddings before supervised verification fine-tuning.",
        "Employ open-set verification heads (e.g., one-class classifiers per device or metric learning with thresholding) rather than closed softmax.",
        "Implement online calibration and drift detection to handle device aging/temperature shifts and trigger adaptive retraining.",
        "Leverage edge-cloud split inference with quantized embeddings to reduce bandwidth while preserving accuracy."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "GNU Radio"
      ],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Industrial IoT networks with SDR gateways and Edge/Cloud backends",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Variable channel conditions (SNR, multipath, fading)",
        "Device aging and temperature effects on fingerprints",
        "Heterogeneity across protocols and hardware",
        "Open-set device onboarding and rejection",
        "Tuning SDR parameters in real time under resource constraints",
        "Ensuring fingerprint portability across different SDRs"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes an adaptive PHY-layer RFF authentication framework for IIoT with a closed-loop ML controller to tune SDR acquisition and feature extraction.",
      "Advocates one-to-one radio ID verification to mitigate misclassification risks of one-to-many approaches.",
      "Designs a dataset-building system with multi-transmitter choreography and SigMF-compliant storage; plans a public release.",
      "Leverages SDR gateways to support heterogeneous IIoT protocols and Edge/Cloud computing for scalability.",
      "Highlights challenges and design for fingerprint portability, robustness to channel variations, and open-set scenarios."
    ]
  },
  {
    "arxiv_id": "2312.07594v1",
    "title": "On the Prediction of Hardware Security Properties of HLS Designs Using Graph Neural Networks",
    "authors": "Amalia Artemis Koufopoulou; Athanasios Papadimitriou; Aggelos Pikrakis; Mihalis Psarakis; David Hely",
    "abstract": "High-level synthesis (HLS) tools have provided significant productivity enhancements to the design flow of digital systems in recent years, resulting in highly-optimized circuits, in terms of area and latency. Given the evolution of hardware attacks, which can render them vulnerable, it is essential to consider security as a significant aspect of the HLS design flow. Yet the need to evaluate a huge number of functionally equivalent de-signs of the HLS design space challenges hardware security evaluation methods (e.g., fault injection - FI campaigns). In this work, we propose an evaluation methodology of hardware security properties of HLS-produced designs using state-of-the-art Graph Neural Network (GNN) approaches that achieves significant speedup and better scalability than typical evaluation methods (such as FI). We demonstrate the proposed methodology on a Double Modular Redundancy (DMR) coun-termeasure applied on an AES SBox implementation, en-hanced by diversifying the redundant modules through HLS directives. The experimental results show that GNNs can be efficiently trained to predict important hardware security met-rics concerning fault attacks (e.g., critical and detection error rates), by using regression. The proposed method predicts the fault vulnerability metrics of the HLS-based designs with high R-squared scores and achieves huge speedup compared to fault injection once the training of the GNN is completed.",
    "published_date": "2023-12-11",
    "pdf_link": "https://arxiv.org/pdf/2312.07594v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Hardware Security",
      "subdomain": "Hardware Fault Injection Evaluation",
      "specific_problem": "Predicting hardware fault-vulnerability metrics (CER/DER/HER/SER) of HLS-generated diversified DMR designs directly from RTL netlist graphs using GNNs",
      "attack_types": [
        "fault injection",
        "double bit-flip (DBF)",
        "single bit-flip (SBF)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "GNN",
        "specific": "GCN",
        "novel_contribution": "Graph-oriented regression from RTL netlists to predict security (fault vulnerability) metrics of HLS-integrated countermeasures; three graph convolution layers + global max pooling + linear head; separate models per metric; log-transform for low-rate targets; k-fold CV and dynamic LR scheduling."
      },
      {
        "type": "primary",
        "category": "GNN",
        "specific": "GAT",
        "novel_contribution": "Attention-enabled graph convolution variant evaluated as alternative backbone within the same regression framework for predicting error-rate metrics."
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Regression"
    ],
    "datasets": [
      {
        "name": "Diversified DMR AES SBox HLS RTL netlist graphs with FI-derived labels (1022 designs)",
        "type": "synthetic",
        "domain": "hardware_netlists",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Fault Injection (FI) campaigns",
        "paper_reference": null,
        "metric": "Evaluation time / scalability; accuracy proxy (treats FI as ground truth)",
        "their_result": "\"speeds up the evaluation by several orders of magnitude (depending on the total number of designs, with a small loss of accuracy)\" and \"achieves huge speedup compared to fault injection once the training of the GNN is completed\"",
        "baseline_result": "Traditional FI \"require considerable time and computational efforts\" and are \"non-scalable, especially for complex circuits\"; statistical FI involves error margins."
      }
    ],
    "performance_metrics_used": [
      "R-squared (R2) on test set",
      "Mean Squared Error (training loss)",
      "Critical Error Rate (CER)",
      "Detected Error Rate (DER)",
      "Hang Error Rate (HER)",
      "Silent Error Rate (SER)",
      "Speedup vs FI"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can GNNs, trained on RTL netlist graphs, accurately predict hardware security (fault vulnerability) metrics of HLS-produced designs?",
        "Can this approach provide significant speedup and better scalability than FI-based evaluation to explore large HLS design spaces?",
        "Does HLS-level diversification of DMR replicas via directives impact predicted/pasured error detection metrics (e.g., under DBF)?"
      ],
      "gaps_identified": [
        "HLS tools and optimizations \"do not consider hardware security properties,\" necessitating post-generation evaluation and search of the HLS design space.",
        "Traditional FI campaigns are time-consuming and computationally expensive, making them non-scalable for complex circuits; statistical FI introduces uncertainty.",
        "Existing diversity quantification methods (D-metric, DIMP) are computationally intensive or limited (e.g., timing attacks focus) and often require RTL/gate-level exhaustive analysis.",
        "Prior GNN-based reliability works largely operate at gate level and/or perform node-level characterization; no prior GNN-based graph-oriented prediction of security/reliability metrics of HLS-integrated countermeasures was identified.",
        "Predictive models based only on high-level HLS report features can be misleading, potentially driving sub-optimal design choices."
      ],
      "limitations": [
        "Demonstrated on a single case study (AES SBox with DMR) as the demonstration vehicle.",
        "Some loss of accuracy relative to FI is acknowledged (\"small loss of accuracy\").",
        "Ground-truth labels rely on SBF campaigns post-processed to DBF; generalization to other fault models or larger designs not demonstrated in the text."
      ],
      "future_work": [],
      "motivation": "Enable fast, scalable evaluation of security properties across large HLS design spaces where FI-based methods are prohibitively expensive, while accounting for HLS effects on integrated countermeasures.",
      "potential_research_ideas": [
        "Extend the approach to additional countermeasures (e.g., TMR with voters, algorithmic countermeasures) and broader cryptographic or ML accelerator blocks.",
        "Active learning to minimize FI labeling cost by selecting most-informative designs/fault scenarios.",
        "Transfer learning/domain adaptation across different IPs or fault models to reduce retraining effort.",
        "Uncertainty-aware prediction (e.g., Bayesian GNNs or ensembles) to provide calibrated confidence for acceptance criteria in design flows.",
        "Integrate the predictor into an automated HLS directive search (closed-loop optimization) to co-optimize security metrics alongside area/latency.",
        "Evaluate and incorporate richer node/edge/hypergraph features (timing, fanout, toggle rates, hierarchy, resource mapping) and compare RTL vs gate-level graphs.",
        "Scale to larger, system-level designs and study compositional predictions (module-wise aggregation).",
        "Multi-task learning to jointly predict CER/DER/HER/SER with shared representations.",
        "Explore graph transformers or spectral GNNs and compare against GCN/GAT for this task.",
        "Counterfactual or sensitivity analyses to identify structural contributors to vulnerability and guide directive selection."
      ],
      "architectural_improvement_recommendations": [
        "Adopt a multi-task regression head to jointly predict all four error-rate metrics to exploit label correlations.",
        "Introduce edge features (signal direction, bus width, timing slack) and hierarchical pooling (e.g., DiffPool) to capture structure beyond node types/degrees.",
        "Use graph transformers or GATv2 for improved long-range dependency modeling; compare with deeper GCNs with residuals and normalization.",
        "Calibrated uncertainty via MC-Dropout or deep ensembles; add heteroscedastic loss if label noise varies across designs.",
        "Incorporate positional/structural encodings (e.g., Laplacian eigenvectors) and graph-level features (resource counts from HLS reports).",
        "Apply curriculum learning on label ranges (start with higher-rate designs) and balanced sampling to address skewed CER distributions.",
        "Employ active learning loop interfacing with FI to label only high-uncertainty regions.",
        "Evaluate a hierarchical two-stage model: fast classifier to detect likely-critical regimes, followed by fine-grained regressor."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/********",
      "frameworks": [
        "PyTorch",
        "PyTorch Geometric"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Requires initial FI-derived labels for training, which can still be costly for new design families.",
        "Generalization across significantly different IPs or countermeasures not yet demonstrated.",
        "Integration into existing HLS toolflows (e.g., avoiding unintended resource sharing, ensuring consistent netlist extraction).",
        "Trade-off between prediction accuracy and speed vs full FI remains to be quantified for larger, complex circuits."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes a GNN-based methodology to predict hardware security (fault vulnerability) metrics of HLS-produced designs directly from RTL netlist graphs, avoiding exhaustive FI in design-space exploration.",
      "Demonstrates the approach on an AES SBox protected with diversified DMR via HLS directives, generating 1022 design variants for training.",
      "Shows that GNNs achieve high R-squared on predicting CER/DER/HER/SER and provide \"huge speedup\" versus FI once trained, enabling scalable evaluation.",
      "Implements an automated flow including directive randomization, HLS synthesis, RTL netlist parsing via SPYDRNET, FI simulation, and GNN training with PyTorch Geometric.",
      "Commits to open-sourcing the methodology and tools."
    ]
  },
  {
    "arxiv_id": "2311.13800v1",
    "title": "Enhancing Intrusion Detection In Internet Of Vehicles Through Federated Learning",
    "authors": "Abhishek Sebastian; Pragna R; Sudhakaran G; Renjith P N; Leela Karthikeyan H",
    "abstract": "Federated learning is a technique of decentralized machine learning. that allows multiple parties to collaborate and learn a shared model without sharing their raw data. Our paper proposes a federated learning framework for intrusion detection in Internet of Vehicles (IOVs) using the CIC-IDS 2017 dataset. The proposed framework employs SMOTE for handling class imbalance, outlier detection for identifying and removing abnormal observations, and hyperparameter tuning to optimize the model's performance. The authors evaluated the proposed framework using various performance metrics and demonstrated its effectiveness in detecting intrusions with other datasets (KDD-Cup 99 and UNSW- NB-15) and conventional classifiers. Furthermore, the proposed framework can protect sensitive data while achieving high intrusion detection performance.",
    "published_date": "2023-11-23",
    "pdf_link": "https://arxiv.org/pdf/2311.13800v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "IoV Security",
      "subdomain": "Intrusion Detection Systems (IDS)",
      "specific_problem": "Federated learning-based multi-class intrusion detection on IoV network traffic while preserving data privacy",
      "attack_types": [
        "DoS",
        "Port Scan",
        "Brute Force",
        "Web Attack",
        "Bot",
        "Infiltration"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Federated Learning",
        "specific": "Custom FL with server-side bagging aggregation",
        "novel_contribution": "Two-edge-device FL where only models are shared; central server aggregates via Bagging to form a global model and returns updates"
      },
      {
        "type": "primary",
        "category": "Ensemble",
        "specific": "BaggingClassifier (server-side aggregation)",
        "novel_contribution": "Use of bagging to combine edge models into a super global model in FL setting"
      },
      {
        "type": "primary",
        "category": "Gradient Boosted Decision Trees",
        "specific": "CatBoost",
        "novel_contribution": "Edge-device classifiers with grid-search hyperparameter tuning (depth, iterations, learning_rate)"
      },
      {
        "type": "primary",
        "category": "Hyperparameter Optimization",
        "specific": "Grid Search",
        "novel_contribution": "Search over depth (3–7), iterations (50–200), learning_rate (0.1–1.0) for CatBoost at edges"
      },
      {
        "type": "primary",
        "category": "Data Resampling",
        "specific": "SMOTE",
        "novel_contribution": "Extensive oversampling to balance severe class imbalance (e.g., infiltration from 36 to 20,000 before outlier removal)"
      },
      {
        "type": "primary",
        "category": "Outlier Detection",
        "specific": "Isolation Forest/Isolation Trees",
        "novel_contribution": "Post-SMOTE outlier removal to reduce noise before training"
      },
      {
        "type": "baseline",
        "category": "Random Forest",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "K-Nearest Neighbors",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Gradient Boosting",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "AdaBoost",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Naive Bayes",
        "specific": "GaussianNB",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Naive Bayes",
        "specific": "MultinomialNB",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Federated Learning"
    ],
    "datasets": [
      {
        "name": "CIC-IDS 2017",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "KDD Cup 99",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "UNSW-NB15",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Random Forest",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "96.23%",
        "baseline_result": "94.99%"
      },
      {
        "method_name": "K-Nearest Neighbors (KNN)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "96.23%",
        "baseline_result": "86.21%"
      },
      {
        "method_name": "Gradient Boosting",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "96.23%",
        "baseline_result": "63.68%"
      },
      {
        "method_name": "AdaBoost",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "96.23%",
        "baseline_result": "59.47%"
      },
      {
        "method_name": "Gaussian Naive Bayes",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "96.23%",
        "baseline_result": "40.04%"
      },
      {
        "method_name": "Multinomial Naive Bayes",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "96.23%",
        "baseline_result": "31.18%"
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "Precision",
      "Recall",
      "Cohen's Kappa",
      "Confusion Matrix"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Centralized ML for IDS raises privacy risks; FL can preserve data privacy by keeping data local.",
        "CIC-IDS 2017 has missing/redundant data and severe class imbalance that degrade model performance without careful preprocessing.",
        "IoV IDS needs approaches that generalize across datasets while protecting sensitive data."
      ],
      "limitations": [
        "Central server model slightly less accurate than individual edge models, possibly due to noise from combining models and training on a smaller subset.",
        "Experiments simulate only two edge devices and one server; no real IoV deployment.",
        "Heavy SMOTE oversampling required due to extreme class imbalance; relies on outlier removal to mitigate noise."
      ],
      "future_work": [],
      "motivation": "Enhance IDS for Internet of Vehicles while preserving data privacy using a federated learning framework with robust preprocessing and tuning.",
      "potential_research_ideas": [
        "Adopt standard FL aggregators (FedAvg, FedProx, FedOpt) and compare to bagging-based aggregation for IDS.",
        "Incorporate secure aggregation and differential privacy to quantify privacy-utility trade-offs in IoV IDS.",
        "Design poisoning/backdoor-resilient FL IDS via robust aggregation (Krum, Trimmed Mean) and anomaly detection on model updates.",
        "Handle non-IID client data typical of IoV with personalization (pFedMe, FedPer) or meta-learning for IDS.",
        "Evaluate continual/online FL to handle concept drift in evolving network traffic and attacks.",
        "Leverage deep sequence models (e.g., Temporal CNNs, LSTMs, Transformers) on flow sequences within FL and compare to tree models.",
        "Multi-modal IoV IDS combining network flows with CAN-bus signals in privacy-preserving cross-silo/cross-device FL.",
        "Communication-efficient FL for IoV (quantization, sparsification) with bandwidth/latency constraints.",
        "Calibration and thresholding strategies to manage false positives in safety-critical IoV settings; cost-sensitive learning.",
        "Benchmark on additional, newer datasets and cross-dataset generalization (train on one, test on another) in FL."
      ],
      "architectural_improvement_recommendations": [
        "Replace server-side bagging with weighted federated averaging (FedAvg) proportional to client data size; evaluate FedProx for heterogeneity.",
        "Use class-weighted or focal loss instead of heavy SMOTE; or combine moderate SMOTE with cost-sensitive CatBoost.",
        "Adopt robust aggregation (Median, Trimmed Mean, Krum) to mitigate poisoned or low-quality client updates.",
        "Introduce secure aggregation and optional local differential privacy; measure privacy budgets.",
        "Add validation splits per client and early stopping; schedule local epochs and learning rates adaptively.",
        "Model stacking: combine CatBoost with neural models; or compare CatBoost to LightGBM/XGBoost for speed/accuracy.",
        "Non-IID partitioning in experiments to reflect real IoV distributions; increase number of clients; simulate stragglers.",
        "Add explainability (e.g., SHAP for CatBoost) to interpret feature importance for attack types.",
        "Evaluate calibration (ECE) and deploy thresholds tuned for different operating points (ROC/PR analysis).",
        "System-level: model compression/quantization for edge deployment; secure transport and integrity of model updates."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/abby1712/Federated_Learning_IDS_On_IOV",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Conceptual IoV setting with two internet-connected vehicles (edge devices) and a central server; IDS placed before CAN bus (simulated)",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": true,
      "fairness_bias": false
    },
    "contributions": [
      "Federated learning framework for IoV intrusion detection using CIC-IDS 2017.",
      "Preprocessing pipeline combining SMOTE oversampling and Isolation Trees for outlier removal to address extreme class imbalance.",
      "Edge-device CatBoost classifiers with grid-search hyperparameter tuning; server-side bagging aggregation into a global model.",
      "Demonstrated performance on CIC-IDS 2017 and cross-dataset tests on KDD Cup 99 and UNSW-NB15.",
      "Outperformed conventional classifiers (RF, KNN, Gradient Boost, AdaBoost, Naive Bayes) in accuracy on CIC-IDS 2017.",
      "Maintains data privacy by sharing models, not raw data."
    ]
  },
  {
    "arxiv_id": "2311.10197v2",
    "title": "You Cannot Escape Me: Detecting Evasions of SIEM Rules in Enterprise Networks",
    "authors": "Rafael Uetz; Marco Herzog; Louis Hackländer; Simon Schwarz; Martin Henze",
    "abstract": "Cyberattacks have grown into a major risk for organizations, with common consequences being data theft, sabotage, and extortion. Since preventive measures do not suffice to repel attacks, timely detection of successful intruders is crucial to stop them from reaching their final goals. For this purpose, many organizations utilize Security Information and Event Management (SIEM) systems to centrally collect security-related events and scan them for attack indicators using expert-written detection rules. However, as we show by analyzing a set of widespread SIEM detection rules, adversaries can evade almost half of them easily, allowing them to perform common malicious actions within an enterprise network without being detected. To remedy these critical detection blind spots, we propose the idea of adaptive misuse detection, which utilizes machine learning to compare incoming events to SIEM rules on the one hand and known-benign events on the other hand to discover successful evasions. Based on this idea, we present AMIDES, an open-source proof-of-concept adaptive misuse detection system. Using four weeks of SIEM events from a large enterprise network and more than 500 hand-crafted evasions, we show that AMIDES successfully detects a majority of these evasions without any false alerts. In addition, AMIDES eases alert analysis by assessing which rules were evaded. Its computational efficiency qualifies AMIDES for real-world operation and hence enables organizations to significantly reduce detection blind spots with moderate effort.",
    "published_date": "2023-11-16",
    "pdf_link": "https://arxiv.org/pdf/2311.10197v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Enterprise Security",
      "subdomain": "SIEM (Security Information and Event Management) / Intrusion Detection",
      "specific_problem": "Detecting evasions of SIEM rule-based (signature/misuse) detections, focusing on Windows process creation events",
      "attack_types": [
        "Evasion of signature-based detection",
        "Command/argument obfuscation",
        "Argument insertion/substitution/omission/reordering/recoding in process command lines"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Classifier (unspecified traditional ML)",
        "specific": null,
        "novel_contribution": "Adaptive misuse detection: supervised classifier trained to compare similarity of events to SIEM rules versus known-benign events; includes rule attribution to identify likely evaded rules"
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Enterprise SIEM events (4 weeks) from a large network (>50,000 users)",
        "type": "private",
        "domain": "log_files (Windows Event Log / Sysmon process creation events)",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Sigma rules corpus (Windows process creation subset; commit ID 12054544, 2021-02-04)",
        "type": "public",
        "domain": "siem_rules",
        "link": "https://github.com/SigmaHQ/sigma",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Hand-crafted evasion and match events (512 evasions, 461 matches) derived from Sigma process creation rules",
        "type": "synthetic",
        "domain": "log_files (Windows process creation events)",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Conventional supervised approach trained on attack events (rather than on SIEM rules vs benign)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Detection rate / recall (e.g., “70%” evasions detected at default sensitivity)",
      "False positives (reported “zero false alerts” at default sensitivity)",
      "Class imbalance characterization (~145,000:1 benign to malicious in validation)",
      "Computational efficiency / throughput (qualitative; suitable for large enterprise operation)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How prevalent and easy are evasions against widespread SIEM (Sigma) rules?",
        "Can machine learning detect SIEM rule evasions with few or no false alerts while preserving the benefits of misuse detection?",
        "Does training a model on SIEM rules vs. benign events outperform training on attack events for practical evasion detection?",
        "Is the approach computationally efficient enough for real-world deployment in large enterprises?"
      ],
      "gaps_identified": [
        "Lack of prior systematic analysis of SIEM rules (e.g., Sigma) for potential evasions in enterprise settings.",
        "Inherent blind spots of signature/misuse detection due to the impracticality of covering all variants and obfuscations.",
        "Anomaly detection in SIEM often yields prohibitive false alert volumes for practical operations."
      ],
      "limitations": [
        "Empirical rule-evasion analysis and primary evaluation focus on Windows process creation rules; other rule/log types receive limited evaluation.",
        "Manual effort restricted the rule analysis to a subset (process creation rules) of Sigma at a specific commit.",
        "Reliance on a single large enterprise’s SIEM data (private), which may limit generalizability.",
        "Assumption that successful evasions remain sufficiently similar to rules at the kernel/logging level; attackers might craft more divergent variants.",
        "Potential training data contamination by evasions is only partially assessed; broader poisoning scenarios remain open."
      ],
      "future_work": [],
      "motivation": "Reduce critical detection blind spots caused by easily evadable SIEM rules while retaining the operational benefits (low FP, interpretability) of misuse detection.",
      "potential_research_ideas": [
        "Contrastive or metric learning between rule representations and event representations to improve similarity-based evasion detection and attribution.",
        "Dual-encoder or cross-encoder architectures that ingest rule text/patterns and raw event strings to learn a joint semantic space.",
        "Robust tokenization and canonicalization pipelines for command-line logs to neutralize insertion/substitution/reordering/recoding evasions.",
        "Active learning with analyst feedback loops to refine the classifier and attribution thresholds under strict false-positive budgets.",
        "Online/continual learning to adapt to evolving rulesets, logging configurations, and attack TTPs while preventing catastrophic forgetting.",
        "Adversarial training and data sanitization techniques to defend against poisoning where attackers seed benign logs to bias models.",
        "Generalize to multi-log, multi-platform settings (e.g., Linux, macOS, cloud service logs) and correlation across hosts/users via graph modeling.",
        "Synthetic evasion generation (e.g., grammar-based or RL) to automatically stress-test and harden detectors.",
        "Calibrated uncertainty estimation and cost-sensitive thresholding to manage extreme class imbalance under strict FP constraints."
      ],
      "architectural_improvement_recommendations": [
        "Introduce a rule-event cross-attention model that aligns fields/signatures to event tokens for fine-grained attribution and improved recall.",
        "Adopt contrastive learning (InfoNCE) with positives from rule-matching events and hard negatives from benign near-misses.",
        "Augment features with canonicalized command lines (e.g., normalized switches, decoded IPs/encodings) plus raw character-level embeddings.",
        "Multi-task learning that jointly predicts evasion likelihood and the top-k evaded rules with calibrated confidence scores.",
        "Implement strict calibration (e.g., temperature scaling) and class-weighted loss or focal loss to handle extreme imbalance.",
        "Add drift detection and model retraining triggers tied to rule repository changes and log schema/version changes.",
        "Integrate rule-coverage analytics to identify weakly covered TTPs and guide data collection or rule hardening."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Described as computationally efficient for large enterprise operation; specific hardware or latency metrics not provided in the excerpt."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Enterprise SIEM environment with Windows endpoints (process creation logs via Windows Event Log/Sysmon)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Acquiring sufficient representative historical benign events and maintaining data quality.",
        "Handling extreme class imbalance with strict false-positive budgets.",
        "Model and data drift due to evolving rules, logging configurations, and attacker TTPs.",
        "Integration with existing SIEM pipelines and alert triage processes.",
        "Potential contamination/poisoning of training data by attacker-induced evasions."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Empirical analysis of 292 Sigma Windows process creation rules showing 38% fully evadable and 7% partially evadable; identification of five straightforward evasion types.",
      "Introduction of adaptive misuse detection that compares events to SIEM rules and historical benign events to detect evasions and attribute them to likely evaded rules.",
      "Design and open-source implementation of AMIDES, a proof-of-concept adaptive misuse detection system suitable for enterprise deployment.",
      "Evaluation on four weeks of enterprise SIEM data and 512 hand-crafted evasions, reporting: “AMIDES successfully detects a majority of these evasions without any false alerts” and specifically “70%” at default sensitivity on a highly imbalanced dataset (~145,000:1).",
      "Demonstration that training from SIEM rules vs. benign events is preferable in practice to conventional supervised training on attack events, and that the system operates efficiently enough for large networks."
    ]
  },
  {
    "arxiv_id": "2312.04749v1",
    "title": "Make out like a (Multi-Armed) Bandit: Improving the Odds of Fuzzer Seed Scheduling with T-Scheduler",
    "authors": "Simon Luo; Adrian Herrera; Paul Quirk; Michael Chase; Damith C. Ranasinghe; Salil S. Kanhere",
    "abstract": "Fuzzing is a highly-scalable software testing technique that uncovers bugs in a target program by executing it with mutated inputs. Over the life of a fuzzing campaign, the fuzzer accumulates inputs inducing new and interesting target behaviors, drawing from these inputs for further mutation. This rapidly results in a large number of inputs to select from, making it challenging to quickly and accurately select the \"most promising\" input for mutation. Reinforcement learning (RL) provides a natural solution to this \"seed scheduling\" problem: the fuzzer dynamically adapts its selection strategy by learning from past results. However, existing RL approaches are (a) computationally expensive (reducing fuzzer throughput) and/or (b) require hyperparameter tuning (reducing generality across targets and input types). To this end, we propose T-Scheduler, a seed scheduler built on multi-armed bandit theory that automatically adapts to the target without any hyperparameter tuning. We evaluate T-Scheduler over 35 CPU-yr of fuzzing, comparing it to 11 state-of-the-art schedulers. Our results show that T-Scheduler improves on these 11 schedulers on both bug-finding and coverage-expansion abilities.",
    "published_date": "2023-12-07",
    "pdf_link": "https://arxiv.org/pdf/2312.04749v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software and Application Security",
      "subdomain": "Software Testing and Analysis",
      "specific_problem": "Seed scheduling for coverage-guided greybox fuzzing",
      "attack_types": [
        "vulnerability discovery (general)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Reinforcement Learning / Multi-Armed Bandit",
        "specific": "Thompson Sampling with Beta-Bernoulli bandit",
        "novel_contribution": "Hyperparameter-free Beta-Bernoulli bandit formulation for fuzzing seed scheduling with a rareness-based correction factor; constant-time selection integrated into AFL++"
      },
      {
        "type": "baseline",
        "category": "Reinforcement Learning",
        "specific": "AFL-Hier (hierarchical RL-based scheduler)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Heuristic Scheduling",
        "specific": "AFL/AFL++ default heuristic scheduler with power schedule",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Reinforcement Learning / Heuristic-enhanced",
        "specific": "AFLFast",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Reinforcement Learning / Heuristic-enhanced",
        "specific": "EcoFuzz",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Reinforcement Learning / Heuristic-enhanced",
        "specific": "MobFuzz",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Reinforcement Learning / Scheduler",
        "specific": "K-Scheduler",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Reinforcement Learning",
      "Online Learning",
      "Bandit"
    ],
    "datasets": [
      {
        "name": "Magma",
        "type": "public",
        "domain": "program_benchmarks",
        "link": "https://github.com/HexHive/magma",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "FuzzBench",
        "type": "public",
        "domain": "program_benchmarks",
        "link": "https://github.com/google/fuzzbench",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "AFL++ default heuristic scheduler (with power schedule)",
        "paper_reference": null,
        "metric": "bug-finding; coverage expansion; throughput",
        "their_result": "“T-Scheduler improves on these 11 schedulers on both bug-finding and coverage-expansion abilities.”",
        "baseline_result": null
      },
      {
        "method_name": "AFL-Hier",
        "paper_reference": "[40]",
        "metric": "runtime overhead; bug-finding/coverage",
        "their_result": "Observed that AFL-Hier introduced >2× overhead over AFL++ without improvement; motivates lightweight bandit approach",
        "baseline_result": ">2× overhead vs AFL++ with no improvement in fuzzing outcomes (quote: “introduced >2× overhead over AFL++’s heuristic-based scheduler without any improvement in fuzzing outcomes.”)"
      },
      {
        "method_name": "AFLFast",
        "paper_reference": "[7]",
        "metric": "bug-finding; coverage",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "EcoFuzz",
        "paper_reference": "[46]",
        "metric": "bug-finding; coverage",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "MobFuzz",
        "paper_reference": "[48]",
        "metric": "bug-finding; coverage",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "K-Scheduler",
        "paper_reference": "[35]",
        "metric": "bug-finding; coverage",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "number of bugs found (bug-finding)",
      "code coverage (coverage expansion)",
      "iteration rate / throughput (executions per second)",
      "scheduler runtime overhead"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can seed scheduling in coverage-guided greybox fuzzing be formulated as a multi-armed bandit and solved efficiently with Thompson sampling?",
        "Can we eliminate hyperparameter tuning while maintaining or improving fuzzing outcomes across diverse targets and input types?",
        "What is the runtime overhead of existing RL-based and heuristic seed schedulers, and how does it affect fuzzing throughput?",
        "Does T-Scheduler improve bug-finding and coverage expansion across standard fuzzing benchmarks (Magma and FuzzBench)?"
      ],
      "gaps_identified": [
        "Existing RL approaches for fuzzing seed scheduling are computationally expensive, reducing fuzzer throughput.",
        "Many schedulers require hyperparameter tuning that is hard to generalize across targets and input formats.",
        "Heuristic-based schedulers rely on intuition rather than theoretically grounded learning with optimality guarantees.",
        "Balancing overhead costs with selection precision is under-analyzed in prior work."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Improve fuzzing effectiveness by intelligently and efficiently prioritizing inputs for mutation without hyperparameters, preserving throughput and generality while providing theoretical guarantees via bandit methods.",
      "potential_research_ideas": [
        "Extend T-Scheduler to contextual bandits by incorporating input and program features (e.g., execution time, depth, structural similarity) to inform arm selection.",
        "Adopt non-stationary bandit models (e.g., sliding-window Thompson sampling) to adapt to evolving corpora and program states during long campaigns.",
        "Multi-objective reward design that explicitly balances coverage growth, unique crash discovery, and execution throughput.",
        "Hierarchical or per-region bandits that allocate exploration across CFG regions or rare-path clusters.",
        "Integrate with adaptive mutation strategies (e.g., MOpt) to jointly optimize seed scheduling and mutation energy allocation."
      ],
      "architectural_improvement_recommendations": [
        "Augment the bandit with contextual features (contextual Thompson sampling) such as input size, execution time, and similarity to diversify exploration.",
        "Incorporate a non-stationary update scheme (decay or windowed counts) so α/β statistics emphasize recent feedback.",
        "Use a two-level scheduler: high-level bandit for feature/region selection; low-level selector for specific seeds within the region.",
        "Refine the rareness correction factor with learned or theoretically grounded scaling to better handle very sparse features.",
        "Couple seed scheduling with energy/power scheduling to co-optimize time budget per selected seed."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/asiaccs2024-t-scheduler",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Evaluation totaled >35 CPU-years across 35 programs; T-Scheduler adds constant-time scheduling overhead; observed AFL-Hier >2× overhead vs AFL++."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": "Constant-time per selection (bandit sampling).",
      "deployment_challenges": [
        "Maintaining high iteration rate while adding scheduling intelligence",
        "Large-scale evaluation is resource-intensive (CPU-years)",
        "Integration and compatibility with existing fuzzers and instrumentation"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Formulation of fuzzing seed scheduling as a multi-armed bandit problem.",
      "Design and implementation of T-Scheduler, a Thompson-sampling-based, hyperparameter-free scheduler with constant-time overhead and theoretical guarantees.",
      "Integration into AFL++ and large-scale evaluation on 35 programs from Magma and FuzzBench (>35 CPU-years).",
      "Empirical results showing improvements over 11 state-of-the-art schedulers in both bug-finding and coverage-expansion.",
      "Analysis of scheduler overhead costs and their impact on fuzzing outcomes.",
      "Release of implementation and results."
    ]
  },
  {
    "arxiv_id": "2311.12760v1",
    "title": "High-resolution Image-based Malware Classification using Multiple Instance Learning",
    "authors": "Tim Peters; Hikmat Farhat",
    "abstract": "This paper proposes a novel method of classifying malware into families using high-resolution greyscale images and multiple instance learning to overcome adversarial binary enlargement. Current methods of visualisation-based malware classification largely rely on lossy transformations of inputs such as resizing to handle the large, variable-sized images. Through empirical analysis and experimentation, it is shown that these approaches cause crucial information loss that can be exploited. The proposed solution divides the images into patches and uses embedding-based multiple instance learning with a convolutional neural network and an attention aggregation function for classification. The implementation is evaluated on the Microsoft Malware Classification dataset and achieves accuracies of up to $96.6\\%$ on adversarially enlarged samples compared to the baseline of $22.8\\%$. The Python code is available online at https://github.com/timppeters/MIL-Malware-Images .",
    "published_date": "2023-11-21",
    "pdf_link": "https://arxiv.org/pdf/2311.12760v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Malware Analysis",
      "subdomain": "Malware Family Classification",
      "specific_problem": "Robust image-based malware family classification without lossy resizing, resistant to adversarial binary enlargement/padding",
      "attack_types": [
        "Adversarial padding/enlargement (file size inflation with null bytes)",
        "Evasion via redundant data insertion"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Multiple Instance Learning (MIL) with Attention",
        "specific": "Attention-based (gated) embedding-level MIL aggregator (Ilse et al.-style) with top-K instance selection",
        "novel_contribution": "Applies attention-based MIL to high-resolution malware byteplots and introduces top-K attention aggregation to counter adversarial enlargement"
      },
      {
        "type": "primary",
        "category": "CNN",
        "specific": "Custom shallow CNN feature extractor with 5x5 convs, ReLU, max-pooling",
        "novel_contribution": "CNN used to embed 224x224 byteplot patches for MIL while preserving full-resolution information without global resizing"
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "Standard resized-image CNN (input resized to fixed resolution)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Weakly-supervised (MIL)"
    ],
    "datasets": [
      {
        "name": "Microsoft Malware Classification dataset",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Resizing-based CNN (fixed-size input via downsampling)",
        "paper_reference": null,
        "metric": "Accuracy on adversarially enlarged samples",
        "their_result": "96.6%",
        "baseline_result": "22.8%"
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "Mean Squared Error (MSE)",
      "Structural Similarity Index (SSIM)",
      "Signal-to-Noise Ratio (SNR)",
      "Mutual Information (MI)",
      "Image Entropy"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Does downsampling malware byteplot images cause significant, exploitable information loss?",
        "Can embedding-based MIL with attention on high-resolution patches robustly classify malware families under adversarial enlargement?",
        "How does adversarial padding (binary enlargement) impact information retained after resizing?"
      ],
      "gaps_identified": [
        "Prevailing reliance on resizing variable-sized malware images causes severe and measurable information loss.",
        "Resizing-based CNNs are vulnerable to adversarial binary enlargement that ‘dilutes’ discriminative content.",
        "Instance-based MIL or SPP-style divide-and-conquer approaches can fail because discriminative features span multiple patches.",
        "Limited acknowledgment and formal quantification of resizing-induced information loss in prior image-based malware work."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Enable robust, scalable, image-based malware family classification without lossy resizing and resistant to adversarial binary enlargement attacks.",
      "potential_research_ideas": [
        "Self-supervised or contrastive pretraining of patch embeddings on large unlabeled malware corpora to improve generalization and robustness.",
        "Hierarchical MIL or multi-scale MIL that aggregates information across multiple patch sizes and spatial hierarchies.",
        "Integrate PE-format priors (e.g., section-aware or positional encodings) into patch embeddings and attention to focus on .text/.data regions.",
        "Adversarial training with realistic enlargement and semantic NOP insertion strategies to harden against broader evasion tactics.",
        "Hybrid static-dynamic MIL where patch-level static embeddings are fused with lightweight dynamic traces or API n-grams via attention.",
        "Detection and mitigation pipeline that first flags anomalous enlargement/padding before MIL classification for defense-in-depth.",
        "Transformer-based MIL aggregator (set transformers or Perceiver-style) in place of MLP attention for improved capacity and permutation invariance.",
        "Uncertainty estimation and calibration on bag-level predictions to support triage and reduce overconfident errors under attack."
      ],
      "architectural_improvement_recommendations": [
        "Replace the custom CNN with a stronger backbone (e.g., ResNet/EfficientNet or lightweight ConvNeXt) adapted to 1-channel inputs.",
        "Use gated attention with soft top-k (e.g., top-k sparsemax/entmax) to retain differentiability and reduce hard selection brittleness.",
        "Add positional/section embeddings (relative offsets, PE section IDs) to instance features before attention aggregation.",
        "Adopt multi-scale patching (e.g., 112, 224, 448) and cross-scale attention to capture features spanning patch boundaries.",
        "Introduce contrastive MIL objectives (InfoNCE) between informative patches and negatives to sharpen instance-level representations.",
        "Incorporate test-time patch selection heuristics (e.g., entropy or gradient-based saliency) to prune uninformative enlarged regions efficiently."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/timppeters/MIL-Malware-Images",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Handling very large, variable-sized malware images without lossy resizing",
        "Memory and computational constraints of CNNs on high-resolution inputs"
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": true,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Formal empirical quantification of information loss due to resizing malware byteplots (MSE, SSIM, SNR, MI, entropy).",
      "A high-resolution malware image classification approach using embedding-based MIL with attention and top-K aggregation.",
      "Demonstrated robustness to adversarial binary enlargement, achieving up to 96.6% accuracy vs 22.8% for resizing-based CNN baseline.",
      "Released Python implementation for reproducibility."
    ]
  },
  {
    "arxiv_id": "2311.12372v2",
    "title": "PMANet: Malicious URL detection via post-trained language model guided multi-level feature attention network",
    "authors": "Ruitong Liu; Yanbin Wang; Haitao Xu; Zhan Qin; Fan Zhang; Yiwei Liu; Zheng Cao",
    "abstract": "The proliferation of malicious URLs has made their detection crucial for enhancing network security. While pre-trained language models offer promise, existing methods struggle with domain-specific adaptability, character-level information, and local-global encoding integration. To address these challenges, we propose PMANet, a pre-trained Language Model-Guided multi-level feature attention network. PMANet employs a post-training process with three self-supervised objectives: masked language modeling, noisy language modeling, and domain discrimination, effectively capturing subword and character-level information. It also includes a hierarchical representation module and a dynamic layer-wise attention mechanism for extracting features from low to high levels. Additionally, spatial pyramid pooling integrates local and global features. Experiments on diverse scenarios, including small-scale data, class imbalance, and adversarial attacks, demonstrate PMANet's superiority over state-of-the-art models, achieving a 0.9941 AUC and correctly detecting all 20 malicious URLs in a case study. Code and data are available at https://github.com/Alixyvtte/Malicious-URL-Detection-PMANet.",
    "published_date": "2023-11-21",
    "pdf_link": "https://arxiv.org/pdf/2311.12372v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Web Security",
      "subdomain": "Malicious URL Detection",
      "specific_problem": "Lexical-based detection of malicious vs benign URLs (binary and multi-class including phishing/defacement) with robustness to adversarial manipulations",
      "attack_types": [
        "phishing",
        "defacement",
        "malicious URL distribution",
        "adversarially perturbed URLs"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "CharBERT (12-layer encoder) with post-training",
        "novel_contribution": "Unsupervised post-training on URLs with three objectives (masked language modeling, noisy language modeling, domain discrimination) to adapt LM to URL domain and capture subword+character information"
      },
      {
        "type": "primary",
        "category": "Attention",
        "specific": "Dynamic layer-wise attention",
        "novel_contribution": "Layer-aware attention that dynamically assigns weights to multi-level encoder representations (low-to-high order features)"
      },
      {
        "type": "primary",
        "category": "Pooling",
        "specific": "Spatial Pyramid Pooling (SPP)",
        "novel_contribution": "SPP for multi-scale down-sampling to integrate local details and global context from URL representations"
      },
      {
        "type": "primary",
        "category": "RNN",
        "specific": "Bi-GRU (within CharBERT character embedding module)",
        "novel_contribution": "Character-aware embedding via Bi-GRU to capture character-level cues critical for malicious URLs"
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "URLNet",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "TException",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Hybrid (CNN+RNN/Attention)",
        "specific": "Grambeddings",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "BERT-based methods adapted from natural language",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Self-supervised",
      "Supervised",
      "Transfer Learning"
    ],
    "datasets": [
      {
        "name": "GramBeddings URL dataset",
        "type": "public",
        "domain": "url_strings",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Mendeley URL dataset (via MendeleyData)",
        "type": "public",
        "domain": "url_strings",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Kaggle 1 URL dataset (binary)",
        "type": "public",
        "domain": "url_strings",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Kaggle 2 URL dataset (multi-class: benign, defacement, phishing, malicious)",
        "type": "public",
        "domain": "url_strings",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "URLNet",
        "paper_reference": "URLNet [23]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "TException",
        "paper_reference": "TException [39]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Grambeddings",
        "paper_reference": "Grambeddings [4]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "BERT-based pre-trained models",
        "paper_reference": "[30], [5]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Transformer + expert network",
        "paper_reference": "[43]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Domain-adapted BERT trained from scratch for URLs",
        "paper_reference": "[44]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "AUC (ROC)"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Lack of domain-specific adaptability when transferring pre-trained LMs to URLs",
        "Insufficient modeling of character-level information crucial for malicious URL cues",
        "Neglect of multi-order (low-to-high level) encoding information across transformer layers",
        "Insufficient extraction of local information alongside global context in Transformers for URLs"
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Enhance malicious URL detection by adapting pre-trained language models to the URL domain and integrating character-aware, multi-level, and local-global feature extraction to overcome domain misalignment and information loss.",
      "potential_research_ideas": [
        "Incorporate side-information (WHOIS/DNS features, hosting age, SSL, and content snapshots) via multi-modal fusion with URL lexical features to further boost robustness.",
        "Develop continual/online post-training for evolving URL distributions and emerging attack patterns (incremental domain adaptation).",
        "Use contrastive learning across augmented URL views (e.g., token/character permutations, homograph variants) to improve representation robustness.",
        "Investigate certified or provable robustness for string-based adversarial manipulations of URLs.",
        "Design lightweight, distillable PMANet variants for on-device/edge deployment with limited compute.",
        "Extend to internationalized domain names (IDN) and homograph attacks with specialized tokenization and script-aware modeling.",
        "Graph-enhanced modeling that leverages URL linkage/host-IP graphs to complement lexical features.",
        "Active learning strategies to efficiently label rare/novel malicious patterns under class imbalance."
      ],
      "architectural_improvement_recommendations": [
        "Apply parameter-efficient fine-tuning (adapters, LoRA, prefix-tuning) during post-training/fine-tuning to reduce compute while preserving performance.",
        "Integrate character-CNN modules alongside Bi-GRU to better capture local n-gram patterns before fusion.",
        "Adopt mixture-of-experts or router over layers to learn dynamic layer utilization beyond scalar attention weights.",
        "Introduce contrastive multi-task objectives (e.g., SimCLR/InfoNCE) alongside MLM/NoisyLM/Domain Discrimination during post-training.",
        "Calibrate tokenization for URLs (custom BPE or unigram LM with URL-specific symbols) and evaluate subword vs byte-level tokenization.",
        "Add adversarial training with realistic URL perturbations (insertion, homoglyph, path/query shuffling) to harden robustness.",
        "Perform knowledge distillation from PMANet to compact student models for latency-sensitive applications."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/Alixyvtte/Malicious-URL-Detection-PMANet",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": true,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes PMANet, a pre-trained LM-guided multi-level feature attention network for malicious URL detection.",
      "Introduces an unsupervised post-training program with three objectives (MLM, NoisyLM, domain discrimination) to adapt CharBERT to URLs and capture subword/character information.",
      "Designs multi-order feature extraction, dynamic layer-wise attention, and spatial pyramid pooling to integrate low-to-high level and local-global features.",
      "Demonstrates superior performance across scenarios including small-scale data, class imbalance, cross-dataset generalization, adversarial attacks, and active case studies, reporting 0.9941 AUC under adversarial attacks and 100% detection of 20 malicious URLs in a case study."
    ]
  },
  {
    "arxiv_id": "2311.05462v2",
    "title": "ChatGPT and Other Large Language Models for Cybersecurity of Smart Grid Applications",
    "authors": "Aydin Zaboli; Seong Lok Choi; Tai-Jin Song; Junho Hong",
    "abstract": "Cybersecurity breaches targeting electrical substations constitute a significant threat to the integrity of the power grid, necessitating comprehensive defense and mitigation strategies. Any anomaly in information and communication technology (ICT) should be detected for secure communications between devices in digital substations. This paper proposes large language models (LLM), e.g., ChatGPT, for the cybersecurity of IEC 61850-based digital substation communications. Multicast messages such as generic object oriented system event (GOOSE) and sampled value (SV) are used for case studies. The proposed LLM-based cybersecurity framework includes, for the first time, data pre-processing of communication systems and human-in-the-loop (HITL) training (considering the cybersecurity guidelines recommended by humans). The results show a comparative analysis of detected anomaly data carried out based on the performance evaluation metrics for different LLMs. A hardware-in-the-loop (HIL) testbed is used to generate and extract dataset of IEC 61850 communications.",
    "published_date": "2023-11-09",
    "pdf_link": "https://arxiv.org/pdf/2311.05462v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Industrial Control Systems (ICS) Security",
      "subdomain": "Intrusion Detection for Smart Grid/Substation Communications",
      "specific_problem": "Anomaly detection (IDS) for IEC 61850 multicast messages (GOOSE and Sampled Values) using LLM-based human-in-the-loop framework",
      "attack_types": [
        "data injection",
        "denial-of-service (DoS)",
        "replay"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer (LLM)",
        "specific": "ChatGPT 4.0 (GPT-4)",
        "novel_contribution": "LLM-based HITL IDS for IEC 61850 communications; conversion of IDS rules and human cybersecurity guidelines into text instructions for LLM-driven anomaly detection"
      },
      {
        "type": "baseline",
        "category": "Transformer (LLM)",
        "specific": "Anthropic Claude 2",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer (LLM)",
        "specific": "Google Bard (PaLM 2)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Human-in-the-Loop",
      "Prompt-based (rule-guided)",
      "Zero-shot/Few-shot prompting"
    ],
    "datasets": [
      {
        "name": "HIL IEC 61850 GOOSE dataset (Wireshark PCAP-derived features)",
        "type": "private",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "HIL IEC 61850 Sampled Values (SV) dataset (Wireshark PCAP-derived features)",
        "type": "private",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Claude 2 (full training, GOOSE)",
        "paper_reference": null,
        "metric": "TPR/Recall",
        "their_result": "98.18% (ChatGPT 4.0)",
        "baseline_result": "89.09% (Claude 2)"
      },
      {
        "method_name": "Claude 2 (full training, GOOSE)",
        "paper_reference": null,
        "metric": "FPR",
        "their_result": "4% (ChatGPT 4.0)",
        "baseline_result": "32% (Claude 2)"
      },
      {
        "method_name": "Claude 2 (full training, GOOSE)",
        "paper_reference": null,
        "metric": "FNR",
        "their_result": "1.82% (ChatGPT 4.0)",
        "baseline_result": "10.91% (Claude 2)"
      },
      {
        "method_name": "Claude 2 (full training, GOOSE)",
        "paper_reference": null,
        "metric": "Precision",
        "their_result": "98.18% (ChatGPT 4.0)",
        "baseline_result": "85.96% (Claude 2)"
      },
      {
        "method_name": "Claude 2 (full training, GOOSE)",
        "paper_reference": null,
        "metric": "F1-score",
        "their_result": "98.18% (ChatGPT 4.0)",
        "baseline_result": "87.5% (Claude 2)"
      },
      {
        "method_name": "Claude 2 (full training, SV)",
        "paper_reference": null,
        "metric": "TPR/Recall",
        "their_result": "96.67% (ChatGPT 4.0)",
        "baseline_result": "88.3% (Claude 2)"
      },
      {
        "method_name": "Claude 2 (full training, SV)",
        "paper_reference": null,
        "metric": "FPR",
        "their_result": "0% (ChatGPT 4.0)",
        "baseline_result": "0% (Claude 2)"
      },
      {
        "method_name": "Claude 2 (full training, SV)",
        "paper_reference": null,
        "metric": "FNR",
        "their_result": "3.33% (ChatGPT 4.0)",
        "baseline_result": "11.67% (Claude 2)"
      },
      {
        "method_name": "Claude 2 (full training, SV)",
        "paper_reference": null,
        "metric": "Precision",
        "their_result": "100% (ChatGPT 4.0)",
        "baseline_result": "100% (Claude 2)"
      },
      {
        "method_name": "Claude 2 (full training, SV)",
        "paper_reference": null,
        "metric": "F1-score",
        "their_result": "98.3% (ChatGPT 4.0)",
        "baseline_result": "93.8% (Claude 2)"
      },
      {
        "method_name": "Google Bard/PaLM 2 (full training, GOOSE)",
        "paper_reference": null,
        "metric": "TPR/Recall",
        "their_result": "98.18% (ChatGPT 4.0)",
        "baseline_result": "89.1% (Bard/PaLM 2)"
      },
      {
        "method_name": "Google Bard/PaLM 2 (full training, GOOSE)",
        "paper_reference": null,
        "metric": "FPR",
        "their_result": "4% (ChatGPT 4.0)",
        "baseline_result": "20% (Bard/PaLM 2)"
      },
      {
        "method_name": "Google Bard/PaLM 2 (full training, GOOSE)",
        "paper_reference": null,
        "metric": "FNR",
        "their_result": "1.82% (ChatGPT 4.0)",
        "baseline_result": "10.9% (Bard/PaLM 2)"
      },
      {
        "method_name": "Google Bard/PaLM 2 (full training, GOOSE)",
        "paper_reference": null,
        "metric": "Precision",
        "their_result": "98.18% (ChatGPT 4.0)",
        "baseline_result": "90.7% (Bard/PaLM 2)"
      },
      {
        "method_name": "Google Bard/PaLM 2 (full training, GOOSE)",
        "paper_reference": null,
        "metric": "F1-score",
        "their_result": "98.18% (ChatGPT 4.0)",
        "baseline_result": "90.7% (Bard/PaLM 2)"
      },
      {
        "method_name": "Google Bard/PaLM 2 (full training, SV)",
        "paper_reference": null,
        "metric": "TPR/Recall",
        "their_result": "96.67% (ChatGPT 4.0)",
        "baseline_result": "81.6% (Bard/PaLM 2)"
      },
      {
        "method_name": "Google Bard/PaLM 2 (full training, SV)",
        "paper_reference": null,
        "metric": "FPR",
        "their_result": "0% (ChatGPT 4.0)",
        "baseline_result": "25% (Bard/PaLM 2)"
      },
      {
        "method_name": "Google Bard/PaLM 2 (full training, SV)",
        "paper_reference": null,
        "metric": "FNR",
        "their_result": "3.33% (ChatGPT 4.0)",
        "baseline_result": "18.34% (Bard/PaLM 2)"
      },
      {
        "method_name": "Google Bard/PaLM 2 (full training, SV)",
        "paper_reference": null,
        "metric": "Precision",
        "their_result": "100% (ChatGPT 4.0)",
        "baseline_result": "91.7% (Bard/PaLM 2)"
      },
      {
        "method_name": "Google Bard/PaLM 2 (full training, SV)",
        "paper_reference": null,
        "metric": "F1-score",
        "their_result": "98.3% (ChatGPT 4.0)",
        "baseline_result": "85.9% (Bard/PaLM 2)"
      }
    ],
    "performance_metrics_used": [
      "True Positive Rate (TPR)/Recall",
      "False Positive Rate (FPR)",
      "False Negative Rate (FNR)",
      "Precision",
      "F1-score"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can LLMs (e.g., ChatGPT, Claude, Bard) be used as IDS to detect anomalies in IEC 61850 GOOSE and SV communications?",
        "Does a human-in-the-loop process with rule-based recommendations improve LLM anomaly detection performance?",
        "How do different LLMs compare on anomaly detection metrics under varying levels of HITL training?"
      ],
      "gaps_identified": [
        "Traditional ML IDS require frequent re-training for new attack patterns, incurring latency and resource costs.",
        "Ensuring reliability and robustness in real-time power grids with evolving cyberattacks is challenging for ML-based IDS.",
        "Trade-off between model complexity and accuracy due to large datasets in substation environments.",
        "Adaptability of ML models to evolving cyberattacks and changing substation infrastructure is limited."
      ],
      "limitations": [
        "Focuses on online detection; not real-time intrusion detection due to LLM limitations and computational speed.",
        "Covers only IEC 61850 multicast messages (GOOSE and SV); other protocols left for future work.",
        "Use of closed-source LLMs with sensitive infrastructure data raises privacy and security concerns.",
        "Potential for LLMs to disclose confidential information without proper safeguards.",
        "Performance depends on quality of human recommendations and data; may still incur FPs/FNs.",
        "Cybersecurity of LLMs themselves is out of scope.",
        "Dataset not publicly available, limiting reproducibility."
      ],
      "future_work": [
        "Extend to other IEC 61850 and substation protocols beyond GOOSE and SV.",
        "Apply task-oriented dialogues (ToD) and fine-tuning to improve accuracy.",
        "Move from online to real-time IDS with improved computational efficiency.",
        "Strengthen privacy and access controls when applying LLMs to critical infrastructure data."
      ],
      "motivation": "Reduce the need for frequent re-training of traditional ML IDS and leverage LLMs' contextual understanding and HITL to detect novel anomalies in digital substation communications.",
      "potential_research_ideas": [
        "Develop an open, labeled benchmark dataset for IEC 61850 (GOOSE/SV) anomalies to standardize LLM and ML comparisons.",
        "Fine-tune an on-premise domain LLM on IEC 61850 logs and protocol semantics for privacy-preserving deployment.",
        "Create a hybrid rule-LLM system with constraint-aware decoding or neuro-symbolic integration for protocol compliance.",
        "Design a streaming, low-latency LLM pipeline for real-time IDS at substation edges with model distillation.",
        "Use synthetic data generation and adversarial example crafting to stress-test LLM-based IDS robustness.",
        "Build interpretable rationales from LLM outputs that map to specific IEC 61850 fields and violations.",
        "Compare LLM-based IDS against state-of-the-art ML (e.g., autoencoders+clustering) under identical conditions."
      ],
      "architectural_improvement_recommendations": [
        "Introduce task-oriented dialogues (ToD) and instruction-tuned prompts specific to IEC 61850 schemas.",
        "Fine-tune lightweight transformer classifiers on structured features while using LLMs for rule generation and adjudication.",
        "Implement constrained decoding guided by protocol rules (e.g., stNum/sqNum invariants, smpCnt cycles).",
        "Add a retrieval component to surface protocol documentation and past incidents during inference.",
        "Deploy an on-prem LLM or distilled small language model to meet real-time and privacy constraints.",
        "Incorporate streaming feature extractors and a sliding-window anomaly scorer for millisecond-level decisions."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Hardware-in-the-loop (HIL) digital substation testbed with SDN switches, IEDs, SCADA, real-time digital simulator",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Handling sensitive critical-infrastructure data with proper access control, encryption, and authentication",
        "Need for continuous human oversight (HITL) to validate outputs",
        "Risk of LLMs unintentionally disclosing confidential information",
        "Computational speed/latency limits preventing real-time IDS",
        "Integration with existing substation networks and SDN infrastructure",
        "Maintaining robustness to evolving attacks without frequent re-training"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes the use of different LLMs (ChatGPT 4.0, Claude 2, Bard/PaLM 2) for cybersecurity of IEC 61850-based digital substations with performance evaluation metrics.",
      "Introduces an LLM-based human-in-the-loop IDS to detect abnormal data in IEC 61850 GOOSE and SV communications.",
      "Converts IDS algorithm/rules to text-based instructions for LLM training and inference.",
      "Builds a hardware-in-the-loop (HIL) testbed and extracts real GOOSE and SV datasets for evaluation.",
      "Provides comparative analysis showing ChatGPT 4.0 outperforms Claude 2 and Bard/PaLM 2 on TPR, FPR/FNR, Precision, and F1-score at multiple training levels."
    ]
  },
  {
    "arxiv_id": "2312.10273v1",
    "title": "User Authentication and Identity Inconsistency Detection via Mouse-trajectory Similarity Measurement",
    "authors": "Rui Jin; Yong Liao; Pengyuan Zhou",
    "abstract": "Completely Automated Public Turing Test To Tell Computers and Humans Apart (CAPTCHA) is a type of challenge-response test widely used in authentication systems. A well-known challenge it faces is the CAPTCHA farm, where workers are hired to solve CAPTCHAs manually. In this work, we propose to tackle this challenge from a novel perspective, converting CAPTCHA farm detection to identity inconsistency detection, which essentially becomes an authentication process. Specifically, we develop a novel embedding model, which measures the similarity between mouse trajectories collected during the session and when registering/solving CAPTCHA, to authenticate and detect identity inconsistency. Moreover, unlike most existing works that employ a separate mouse movement classifier for each individual user, which brings in considerable costs when serving a large number of users, our model performs detection tasks using only one classifier for all users, significantly reducing the cost. Experiment results validate the superiority of our method over the state-of-the-art time series classification methods, achieving 94.3% and 97.7% of AUC in identity and authentication inconsistency detection, respectively.",
    "published_date": "2023-12-16",
    "pdf_link": "https://arxiv.org/pdf/2312.10273v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Authentication and Access Control",
      "subdomain": "Behavioral Biometrics",
      "specific_problem": "Mouse-dynamics-based user authentication and identity inconsistency detection for CAPTCHA farm detection",
      "attack_types": [
        "CAPTCHA farming",
        "Bot abuse via token misappropriation"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Hybrid CNN+RNN embedding (Siamese-style)",
        "specific": "Parallel 1D-CNN (3 layers with BN, GAP) + 2-layer LSTM embedding with shared weights on paired inputs; 3-layer FNN classifier",
        "novel_contribution": "Embedding-based similarity model for mouse trajectories using a single classifier for all users; pairwise training with shared encoders; concatenated embeddings for binary similarity prediction"
      },
      {
        "type": "primary",
        "category": "Data preprocessing / feature engineering",
        "specific": "Segmentation-trimming-concatenation of moving-state segments; features dx, dy, dx/dt, dy/dt; sample length 256",
        "novel_contribution": "Moving-state segmentation with gap/key-change based cuts, removal of short/oversized segments, and concatenation to densify information across guided and unguided environments"
      },
      {
        "type": "primary",
        "category": "Training strategy",
        "specific": "Base sample selection and dynamic authentication (multiple half-overlapping samples)",
        "novel_contribution": "Representative base-sample selection via BCE-based scoring against in-user and cross-user validation samples; dynamic multi-sample inference averaging to trade time for accuracy"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Metric learning",
      "Pairwise similarity learning"
    ],
    "datasets": [
      {
        "name": "SapiMouse",
        "type": "public",
        "domain": "mouse_trajectories",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Balabit Mouse Dynamics Challenge",
        "type": "public",
        "domain": "mouse_trajectories",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Hybrid (SapiMouse + Balabit combined)",
        "type": "public",
        "domain": "mouse_trajectories",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "AUC",
      "FAR",
      "FRR"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can mouse-trajectory similarity be used to detect identity inconsistency indicative of CAPTCHA farming?",
        "Can a single classifier with shared embeddings across users replace per-user classifiers for mouse-based authentication without sacrificing accuracy?",
        "Can preprocessing unifies guided (CAPTCHA-like) and unguided session data for robust similarity learning?"
      ],
      "gaps_identified": [
        "Most prior mouse-based authentication systems require training a separate classifier per user, leading to high cost and scalability issues.",
        "State-of-the-art systems often need long authentication times (10–30 minutes), impractical for many applications and risky for hijacking.",
        "Existing CAPTCHAs cannot defend against farm attacks because they are designed to be easy for humans and lack identity consistency checks.",
        "One-class SVMs and multi-output deep learning attempts to reduce per-user models have shown unencouraging results.",
        "Continuous authentication systems cannot detect unseen users in CAPTCHA farm scenarios."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Address the inability of CAPTCHAs to detect farm attacks and reduce the cost and latency of mouse-based authentication by reframing CAPTCHA farm detection as identity inconsistency detection via mouse-trajectory similarity.",
      "potential_research_ideas": [
        "Collect and evaluate on a real-world, paired dataset of CAPTCHA-solving and subsequent session mouse trajectories to directly measure farm detection performance.",
        "Adversarial evaluation against human-mimicking bots or GAN-generated trajectories; develop adversarially robust training for mouse embeddings.",
        "Multi-modal fusion (mouse + keystroke + page/viewport events) to improve robustness and reduce time-to-decision.",
        "Self-supervised pretraining on large unlabeled mouse-trajectory corpora, then fine-tune for similarity to improve generalization to unseen users.",
        "Domain adaptation techniques to better bridge guided (CAPTCHA-like) and unguided session behaviors and devices.",
        "Continual learning to track user behavior drift while preventing catastrophic forgetting; calibration of thresholds for low FAR at short sample lengths.",
        "Privacy-preserving training and deployment (on-device inference, federated learning, or differential privacy) for biometric data."
      ],
      "architectural_improvement_recommendations": [
        "Replace BCE pairwise classifier with metric-learning losses (contrastive/triplet) and prototypical or nearest-centroid inference for better open-set behavior.",
        "Explore Temporal CNNs/TCNs or lightweight Transformers with efficient attention for improved sequence modeling and latency.",
        "Add normalization/statistics layers invariant to device resolution/DPI and pointer acceleration; incorporate device/context embeddings.",
        "Use hard negative mining and curriculum over segment difficulty; implement balanced batching by user to avoid bias.",
        "Quantize/prune the model for web/edge deployment and real-time inference; consider distillation from a larger teacher model.",
        "Calibrate scores via Platt scaling/temperature scaling for stable thresholds across populations and environments."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Requires sufficient effective mouse movement per decision (on average 18.5 seconds after preprocessing).",
        "Cross-device variability (e.g., screen resolution) necessitates normalization to avoid optimistic estimates.",
        "Integration with web authentication flows and storage of trajectory embeddings securely (biometric data handling)."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "A novel framework to measure similarity between two mouse trajectories for both user authentication and CAPTCHA farm detection.",
      "An embedding-based approach (shared 1D-CNN + LSTM encoders) enabling a single classifier for all users, reducing training and maintenance cost.",
      "Base sample selection and dynamic authentication (multi-sample averaging) to leverage trajectory diversity and improve accuracy.",
      "Demonstration on a hybrid dataset combining guided (SapiMouse) and unguided (Balabit) settings with reported AUCs of 94.3% (identity inconsistency) and 97.7% (authentication)."
    ]
  },
  {
    "arxiv_id": "2312.08317v1",
    "title": "Prompt Engineering-assisted Malware Dynamic Analysis Using GPT-4",
    "authors": "Pei Yan; Shunquan Tan; Miaohui Wang; Jiwu Huang",
    "abstract": "Dynamic analysis methods effectively identify shelled, wrapped, or obfuscated malware, thereby preventing them from invading computers. As a significant representation of dynamic malware behavior, the API (Application Programming Interface) sequence, comprised of consecutive API calls, has progressively become the dominant feature of dynamic analysis methods. Though there have been numerous deep learning models for malware detection based on API sequences, the quality of API call representations produced by those models is limited. These models cannot generate representations for unknown API calls, which weakens both the detection performance and the generalization. Further, the concept drift phenomenon of API calls is prominent. To tackle these issues, we introduce a prompt engineering-assisted malware dynamic analysis using GPT-4. In this method, GPT-4 is employed to create explanatory text for each API call within the API sequence. Afterward, the pre-trained language model BERT is used to obtain the representation of the text, from which we derive the representation of the API sequence. Theoretically, this proposed method is capable of generating representations for all API calls, excluding the necessity for dataset training during the generation process. Utilizing the representation, a CNN-based detection model is designed to extract the feature. We adopt five benchmark datasets to validate the performance of the proposed model. The experimental results reveal that the proposed detection algorithm performs better than the state-of-the-art method (TextCNN). Specifically, in cross-database experiments and few-shot learning experiments, the proposed model achieves excellent detection performance and almost a 100% recall rate for malware, verifying its superior generalization performance. The code is available at: github.com/yan-scnu/Prompted_Dynamic_Detection.",
    "published_date": "2023-12-13",
    "pdf_link": "https://arxiv.org/pdf/2312.08317v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Malware Analysis",
      "subdomain": "Dynamic Analysis",
      "specific_problem": "API-sequence-based dynamic malware detection with robust generalization to unseen/updated API calls",
      "attack_types": [
        "obfuscated/shelled malware",
        "packed/wrapped malware",
        "general Windows malware"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "LLM Prompting",
        "specific": "GPT-4",
        "novel_contribution": "Prompt-engineered GPT-4 generates explanatory text for each API call; first application of prompt engineering to dynamic malware analysis."
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "BERT",
        "novel_contribution": "Use pre-trained BERT to embed GPT-4–generated API-call explanations; no dataset-specific training needed to obtain API-call representations."
      },
      {
        "type": "primary",
        "category": "CNN",
        "specific": "Custom multi-layer CNN",
        "novel_contribution": "CNN that ingests a 3D tensor of per-API-call text embeddings with depth-wise/per-layer convolution and multi-kernel 2D conv blocks for local feature extraction."
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "TextCNN",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "RNN/LSTM",
        "specific": "BiLSTM",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Transfer Learning",
      "Few-shot"
    ],
    "datasets": [
      {
        "name": "Aliyun",
        "type": "unknown",
        "domain": "api_call_sequences",
        "link": null,
        "is_new_contribution": false,
        "availability": "unknown"
      },
      {
        "name": "Other benchmark datasets (names not specified)",
        "type": "unknown",
        "domain": "api_call_sequences",
        "link": null,
        "is_new_contribution": false,
        "availability": "unknown"
      }
    ],
    "baselines": [
      {
        "method_name": "TextCNN",
        "paper_reference": null,
        "metric": "recall (and overall detection performance)",
        "their_result": "Outperforms TextCNN; cross-database and few-shot experiments achieve almost a 100% malware recall",
        "baseline_result": null
      },
      {
        "method_name": "BiLSTM",
        "paper_reference": null,
        "metric": "Representation quality (cosine similarity heatmaps) and detection performance",
        "their_result": "Proposed method yields denser, more informative similarity matrices and better generalization",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "recall",
      "accuracy"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can prompt engineering with GPT-4 produce higher-quality, generalizable representations for API-call sequences in dynamic malware analysis?",
        "Does using explanatory text embeddings alleviate issues with unknown API calls and concept drift?",
        "Can the approach improve cross-database generalization and few-shot detection performance compared to state-of-the-art baselines?"
      ],
      "gaps_identified": [
        "Limited representation quality of API-call features in prior models due to dependence on dataset-specific training.",
        "Weak generalization performance and overfitting to specific datasets.",
        "Sensitivity to API concept drift and inability to represent unseen API calls."
      ],
      "limitations": [
        "Generating explanatory text per API call is computationally expensive; mitigated by building a vocabulary and reusing explanations.",
        "Quality of generated explanations depends on prompt design and LLM behavior (authors compare direct vs designed prompts to illustrate potential issues)."
      ],
      "future_work": [],
      "motivation": "Improve robustness and generalization of API-sequence-based dynamic malware detection by leveraging LLM-generated semantic explanations to handle unseen API calls and concept drift.",
      "potential_research_ideas": [
        "Replace GPT-4 with open-source LLMs (e.g., Llama-family) fine-tuned on software/API documentation to reduce cost and improve reproducibility.",
        "Augment explanations with retrieval from official API documentation (MSDN) to reduce hallucinations and standardize semantics (RAG).",
        "Incorporate API arguments, return values, and temporal context into the explanations to enrich behavioral semantics.",
        "Adversarially evaluate and harden against manipulated or misleading API-call explanations (robust prompting / verification).",
        "Build a domain-specific API knowledge graph and fuse it with text embeddings via graph neural networks for richer relations.",
        "Online/continual learning to update the vocabulary and embeddings as APIs evolve (concept-drift handling pipeline)."
      ],
      "architectural_improvement_recommendations": [
        "Use lightweight sentence encoders (e.g., MiniLM, MPNet) with trainable adapters to reduce compute while preserving quality.",
        "Add attention or cross-attention over the 3D tensor to better fuse vertical (text) and horizontal (sequence) contexts.",
        "Introduce temporal convolution (TCN) or Transformer encoder layers after CNN blocks to capture longer-range dependencies.",
        "Calibrate and verify LLM outputs with retrieval-grounded checks and confidence scoring before embedding.",
        "Cache and hash explanation texts with a fast lookup layer; batch-embed new APIs asynchronously to minimize latency."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/yan-scnu/Prompted_Dynamic_Detection",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "High cost/latency of generating and maintaining LLM-based explanations at scale; mitigated by vocabulary reuse.",
        "Dependence on external GPT-4 service (availability, cost, data handling).",
        "Operational pipeline to sandbox, extract API sequences, map to vocabulary, and handle API updates (concept drift)."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "First to apply prompt engineering (GPT-4) to dynamic malware analysis by generating explanatory text per API call.",
      "Obtain API-call representations without dataset-specific training, improving representation quality and generalization.",
      "Theoretically able to represent unseen API calls, addressing concept drift; design a CNN classifier over the 3D text-embedding tensor that outperforms TextCNN and achieves near-100% malware recall in cross-database and few-shot settings."
    ]
  },
  {
    "arxiv_id": "2311.11605v1",
    "title": "Machine learning-based malware detection for IoT devices using control-flow data",
    "authors": "Gergely Hevesi",
    "abstract": "Embedded devices are specialised devices designed for one or only a few purposes. They are often part of a larger system, through wired or wireless connection. Those embedded devices that are connected to other computers or embedded systems through the Internet are called Internet of Things (IoT for short) devices.   With their widespread usage and their insufficient protection, these devices are increasingly becoming the target of malware attacks. Companies often cut corners to save manufacturing costs or misconfigure when producing these devices. This can be lack of software updates, ports left open or security defects by design. Although these devices may not be as powerful as a regular computer, their large number makes them suitable candidates for botnets. Other types of IoT devices can even cause health problems since there are even pacemakers connected to the Internet. This means, that without sufficient defence, even directed assaults are possible against people.   The goal of this thesis project is to provide better security for these devices with the help of machine learning algorithms and reverse engineering tools. Specifically, I study the applicability of control-flow related data of executables for malware detection. I present a malware detection method with two phases. The first phase extracts control-flow related data using static binary analysis. The second phase classifies binary executables as either malicious or benign using a neural network model. I train the model using a dataset of malicious and benign ARM applications.",
    "published_date": "2023-11-20",
    "pdf_link": "https://arxiv.org/pdf/2311.11605v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "IoT Security",
      "subdomain": "Malware Detection",
      "specific_problem": "Binary-level IoT malware detection on ARM using static control-flow data (CFG)",
      "attack_types": [
        "IoT malware",
        "Botnet malware (e.g., Mirai, Tsunami/Kaiten)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "GNN",
        "specific": "structure2vec",
        "novel_contribution": "Proof-of-concept application of structure2vec for classifying control-flow graphs of ARM IoT binaries; graph node tagging via byte-sequence–based identifiers; end-to-end pipeline from static CFG recovery (angr) to graph classification."
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "ARM malware binaries (private set)",
        "type": "private",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Benign ARM binaries from D-Link firmware images",
        "type": "public",
        "domain": "firmware_images",
        "link": "https://support.dlink.com/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Benign ARM binaries from Ubiquiti firmware images",
        "type": "public",
        "domain": "firmware_images",
        "link": "https://www.ui.com/download",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "SHA-256 hash list of benign ARM files (for selection)",
        "type": "private",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Is control-flow–related static data (CFG) of executables effective for IoT malware detection?",
        "Can a graph neural network (structure2vec) classify ARM binaries as malicious or benign from their control-flow graphs?",
        "Does linking style (static vs dynamic) confound benign/malicious classification, and can collecting libraries for benign binaries mitigate this?"
      ],
      "gaps_identified": [
        "Traditional hash/signature-based detection is brittle against evolving IoT malware families and polymorphism.",
        "Need for scalable IoT malware analysis beyond manual reverse engineering.",
        "Most prior graph learning works focus on completion/link prediction; limited applications to malware graph classification are identified in the thesis.",
        "Potential confounding due to benign samples being dynamically linked while malware may be statically linked."
      ],
      "limitations": [
        "ARM-only dataset; generalization to other architectures untested.",
        "Dataset imbalance (malware >> benign) requiring balancing.",
        "Static analysis only; susceptible to packing/obfuscation and missing runtime behaviors.",
        "Dependence on angr CFG recovery; required bug fixes indicate fragility in control-flow extraction.",
        "Potential spurious correlations due to linking style and presence of library functions."
      ],
      "future_work": [],
      "motivation": "Provide better security for IoT devices by leveraging machine learning on control-flow data extracted via static binary analysis to detect malware.",
      "potential_research_ideas": [
        "Evaluate cross-architecture generalization (x86, MIPS, RISC-V) and multi-arch models.",
        "Augment node/edge features with richer semantics (opcodes, API categories, edge types like conditional/unconditional calls).",
        "Integrate dynamic analysis traces or emulation-derived behaviors with CFGs for multimodal graph classification.",
        "Adopt self-supervised/contrastive pretraining on large unlabeled firmware graphs to reduce label needs.",
        "Benchmark against modern GNNs (GCN, GIN, GraphSAGE, GAT) and graph pooling methods; perform ablations on features and graph granularity (CFG vs call graph).",
        "Investigate robustness to common obfuscations/packing; develop deobfuscation-aware or invariant representations.",
        "Design explainability for CFG-based decisions (per-node/edge importance, subgraph rationales) to aid analysts.",
        "Construct and release a standardized, de-duplicated IoT firmware malware/benign CFG benchmark with clear splits."
      ],
      "architectural_improvement_recommendations": [
        "Replace/compare structure2vec with modern message-passing GNNs (GIN/GraphSAGE/GAT) and hierarchical pooling (e.g., DiffPool/TopKPool).",
        "Encode edge types and control-flow semantics (conditional vs unconditional, call/ret) as edge features; use edge-aware GNNs.",
        "Enhance node features beyond byte-sequence IDs: opcode histograms, API category tags, basic block length, entropy, imported library provenance.",
        "Use positional encodings on graphs (e.g., Laplacian PE) and virtual global nodes for better global context.",
        "Apply class-imbalance handling (focal loss, class weights), stratified splits, and k-fold cross-validation.",
        "Adopt data augmentation for graphs (node/edge dropout, subgraph sampling) and regularization (dropout, batch norm).",
        "Multi-task learning (malicious/benign + family classification) to improve representation quality."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "PyTorch",
        "angr",
        "selenium",
        "binwalk"
      ],
      "reproducibility_score": "low",
      "computational_requirements": "Static analysis on a VM; training/inference on desktop: Ryzen 5 2600 CPU, 16 GB RAM, GTX 1060 (6 GB VRAM)."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Resource constraints on IoT devices limit on-device analysis; need off-device/edge processing.",
        "Packed/obfuscated binaries hinder static CFG recovery.",
        "Linking style and library resolution can confound models; requires robust preprocessing and dependency extraction.",
        "Firmware unpacking at scale (binwalk) is storage- and time-intensive.",
        "Architecture heterogeneity across IoT fleets complicates model deployment and generalization.",
        "Model drift as malware evolves; requires continual learning and dataset updates."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "End-to-end malware detection pipeline for IoT binaries using static control-flow graphs and a graph neural network (structure2vec).",
      "Automated collection and processing of benign ARM binaries from D-Link and Ubiquiti firmware (web crawler + binwalk) to obtain needed libraries and mitigate linking-style confounds.",
      "Implementation of graph node tagging (byte-sequence–based IDs) and conversion into structure2vec’s required input format.",
      "Evaluation plan including runtime performance of CFG recovery and detection performance; comparison and bug fixes for angr CFGFast/CFGEmulated during control-flow recovery."
    ]
  },
  {
    "arxiv_id": "2312.01219v3",
    "title": "A Hierarchical Security Events Correlation Model for Real-time Cyber Threat Detection and Response",
    "authors": "Herbert Maosa; Karim Ouazzane; Mohamed Chahine Ghanem",
    "abstract": "Intrusion detection systems perform post-compromise detection of security breaches whenever preventive measures such as firewalls do not avert an attack. However, these systems raise a vast number of alerts that must be analysed and triaged by security analysts. This process is largely manual, tedious and time-consuming. Alert correlation is a technique that tries to reduce the number of intrusion alerts by aggregating those that are related in some way. However, the correlation is performed outside the IDS through third-party systems and tools, after the high volume of alerts has already been raised. These other third-party systems add to the complexity of security operations. In this paper, we build on the very researched area of correlation techniques by developing a novel hierarchical event correlation model that promises to reduce the number of alerts issued by an Intrusion Detection System. This is achieved by correlating the events before the IDS classifies them. The proposed model takes the best of features from similarity and graph-based correlation techniques to deliver an ensemble capability not possible by either approach separately. Further, we propose a correlation process for correlation of events rather than alerts as is the case in current art. We further develop our own correlation and clustering algorithm which is tailor-made to the correlation and clustering of network event data. The model is implemented as a proof of concept with experiments run on the DARPA 99 Intrusion detection set. The correlation achieved 87 percent data reduction through aggregation, producing nearly 21000 clusters in about 30 seconds.",
    "published_date": "2023-12-02",
    "pdf_link": "https://arxiv.org/pdf/2312.01219v3",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Real-time hierarchical event correlation within an IDS to reduce alert volume by aggregating related events prior to classification",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Clustering",
        "specific": null,
        "novel_contribution": "Custom correlation and clustering algorithm tailored to categorical, low-dimensional network event data; used to form clusters via similarity-based rules before graph correlation"
      },
      {
        "type": "primary",
        "category": "Graph-based model",
        "specific": null,
        "novel_contribution": "Graph correlation over event clusters to discover interconnections and temporal sequences indicating unusual traffic patterns"
      },
      {
        "type": "baseline",
        "category": "Association Rule Mining",
        "specific": "Apriori-like frequent itemset mining (discussed in related work, e.g., CLIQUE)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Unsupervised",
      "Rule-based"
    ],
    "datasets": [
      {
        "name": "DARPA 99 Intrusion Detection Evaluation Dataset",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "data_reduction_percentage",
      "number_of_clusters",
      "processing_time_seconds"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can correlating events (rather than alerts) before IDS classification reduce the number of alerts issued by an IDS?",
        "Can a hierarchical combination of similarity-based clustering and graph correlation enable real-time attack detection?",
        "Can an event-correlation process integrated inside the IDS reduce operational complexity compared to third-party alert correlation?"
      ],
      "gaps_identified": [
        "Most prior work focuses on alert correlation performed outside the IDS, after high alert volumes are generated.",
        "Hierarchical correlation works often apply the same technique at multiple levels, propagating inherent weaknesses through the pipeline.",
        "Some approaches rely on external SIEMs and custom tools, increasing operational complexity.",
        "Real-time handling typically begins at correlation/analytics, not end-to-end from event generation through detection.",
        "No standard end-to-end correlation process; existing models vary and target subsets of the pipeline."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Reduce alert overload and operational complexity by correlating events upstream (before IDS classification) using a hierarchical similarity+graph approach suitable for real-time detection.",
      "potential_research_ideas": [
        "Evaluate the model on modern, diverse IDS datasets and live enterprise traffic to assess generalization and robustness.",
        "Incorporate adaptive or learned similarity metrics (e.g., metric learning) to replace static thresholds for event clustering.",
        "Introduce incremental/online clustering to better handle high-volume streams and concept drift.",
        "Augment the graph correlation stage with graph neural networks to learn complex temporal-spatial patterns.",
        "Integrate automated root-cause and multi-step attack reconstruction on top of correlated graphs.",
        "Design an explainability layer that translates clusters and graph structures into analyst-friendly narratives.",
        "Assess adversarial robustness of the correlation pipeline against crafted event sequences."
      ],
      "architectural_improvement_recommendations": [
        "Replace fixed-distance similarity with learned embeddings plus approximate nearest neighbor search for scalable, low-latency clustering.",
        "Adopt a streaming architecture (e.g., event streaming and in-memory state) with backpressure control for end-to-end real-time guarantees.",
        "Use online/incremental clustering (e.g., micro-cluster maintenance) to avoid periodic re-clustering and handle drift.",
        "Enhance the graph stage with temporal edge weighting and decay to emphasize recent correlations.",
        "Add anomaly scoring over clusters/graph components to prioritize alerts and reduce false positives.",
        "Implement distributed correlation across IDS sensors with consensus to reduce single-point bottlenecks."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": "Reported ~21,000 clusters produced in about 30 seconds (hardware unspecified)"
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Proposed for distributed IDS architecture with real-time event streaming to a correlation unit",
      "scalability_discussed": true,
      "inference_time": "Produced ~21,000 clusters in ~30 seconds (proof-of-concept)",
      "deployment_challenges": [
        "Integration into existing IDS pipelines to correlate events pre-classification",
        "Sustaining low latency at high network event volumes",
        "Tuning similarity thresholds for diverse environments",
        "Managing concept drift and evolving traffic patterns",
        "Potential false positives from graph correlation if relationships are spurious"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "A novel hierarchical event correlation model that performs correlation before IDS classification to reduce alert volume.",
      "An ensemble approach combining similarity-based clustering with graph-based correlation to capture both aggregation and sequence relationships.",
      "A custom correlation and clustering algorithm tailored for categorical, low-dimensional network event data.",
      "A proposed correlation process operating on events rather than alerts to reduce reliance on third-party SIEMs.",
      "Proof-of-concept implementation evaluated on the DARPA 99 dataset, achieving approximately 87% data reduction and producing ~21,000 clusters in ~30 seconds."
    ]
  },
  {
    "arxiv_id": "2311.09449v2",
    "title": "HAL 9000: a Risk Manager for ITSs",
    "authors": "Tadeu Freitas; Carlos Novo; Joao Soares; Ines Dutra; Manuel E. Correia; Behnam Shariati; Rolando Martins",
    "abstract": "HAL 9000 is an Intrusion Tolerant Systems (ITSs) Risk Manager, which assesses configuration risks against potential intrusions. It utilizes gathered threat knowledge and remains operational, even in the absence of updated information. Based on its advice, the ITSs can dynamically and proactively adapt to recent threats to minimize and mitigate future intrusions from malicious adversaries. Our goal is to reduce the risk linked to the exploitation of recently uncovered vulnerabilities that have not been classified and/or do not have a script to reproduce the exploit, considering the potential that they may have already been exploited as zero-day exploits. Our experiments demonstrate that the proposed solution can effectively learn and replicate National Vulnerability Database's evaluation process with 99% accuracy.",
    "published_date": "2023-11-15",
    "pdf_link": "https://arxiv.org/pdf/2311.09449v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "System Security",
      "subdomain": "Intrusion Tolerant Systems",
      "specific_problem": "Automated risk assessment and configuration recommendation for ITSs by predicting CVSS scores for new/unrated CVEs and reassessing configurations",
      "attack_types": [
        "software vulnerabilities",
        "zero-day exploitation",
        "exploits"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "NLP text modeling",
        "specific": null,
        "novel_contribution": "Predicts CVSS scores directly from CVE free-text descriptions to remove dependency on delayed NVD scoring within an ITS risk manager"
      },
      {
        "type": "primary",
        "category": "Clustering",
        "specific": null,
        "novel_contribution": "Clusters vulnerability descriptions to identify similar/shared vulnerabilities across platforms as part of configuration risk assessment"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "NVD CVE data feeds (CVE descriptions with CVSS scores)",
        "type": "public",
        "domain": "vulnerability_text",
        "link": "https://nvd.nist.gov/vuln/data-feeds",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Diversity Policy for Intrusion Tolerant Systems (Heo et al.)",
        "paper_reference": "Heo et al. [9]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Lazarus ITS Risk Manager",
        "paper_reference": "Garcia et al. [6]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "accuracy"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can we automatically predict CVSS scores from CVE descriptions to enable timely risk assessment before NVD completes evaluation?",
        "Can an ITS autonomously reassess risk and recommend lower-risk configurations when new vulnerabilities are reported but unrated?",
        "Can clustering of CVE descriptions help detect shared or similar vulnerabilities across different OSs/applications to inform configuration choices?"
      ],
      "gaps_identified": [
        "Significant delay and backlog in NVD CVE evaluation leading to late scoring and delayed risk assessments",
        "Existing ITS risk managers depend on NVD base scores and cannot react promptly to new/unrated CVEs",
        "Manual intervention required in prior work (e.g., K-means cluster count selection) slows response and widens vulnerability window",
        "K-means susceptibility to outliers can degrade clustering quality for CVE descriptions",
        "Risk management components can be targeted via input injection and model/data poisoning if not isolated"
      ],
      "limitations": [
        "Assumes deployment in a secure controller plane to mitigate input injection and data poisoning threats",
        "Relies on OSINT sources; quality and completeness of external data may affect predictions and recommendations",
        "Details of model architecture and training are not disclosed in the provided text, limiting reproducibility",
        "No reported real-world deployment evaluation in production ITS environments"
      ],
      "future_work": [],
      "motivation": "Reduce risk from newly uncovered and potentially already exploited vulnerabilities by enabling ITSs to act proactively without waiting for NVD scoring, thereby minimizing exposure to zero-day-like threats.",
      "potential_research_ideas": [
        "Predict full CVSS vector metrics (not just base score) to improve explainability and enable rule-based reasoning over individual metrics",
        "Incorporate transformer-based CVE language models (e.g., fine-tuned BERT/DistilBERT) and compare against classical ML baselines on CVSS prediction",
        "Add uncertainty estimation and calibration for predicted scores to guide conservative configuration changes under ambiguity",
        "Adopt continual/online learning to rapidly incorporate newly labeled CVEs once NVD scores are published",
        "Explore semi-supervised learning using unlabeled recent CVEs and weak labels from auxiliary sources (e.g., KEV catalog, vendor advisories)",
        "Integrate robust text defenses (adversarial training, data sanitization) to mitigate input injection and poisoning risks",
        "Use clustering methods that avoid manual K (e.g., HDBSCAN) and evaluate stability and impact on configuration recommendations",
        "Build a graph linking CVEs, affected products, configurations, and exploits to perform graph-based risk propagation and diversification planning",
        "Conduct end-to-end studies quantifying impact of prediction errors on ITS availability, security, and BFT constraints (e.g., 3f+1 invariants)"
      ],
      "architectural_improvement_recommendations": [
        "Replace manual-parameter clustering with density-based or hierarchical clustering (e.g., HDBSCAN/bisecting K-means) and automate parameter selection",
        "Introduce an ensemble of CVSS predictors (classical ML + transformers) with stacking and uncertainty-aware decision logic",
        "Implement data validation and provenance tracking for OSINT ingestion with anomaly detection on the input stream",
        "Add a model governance layer: periodic drift detection, calibration checks, and scheduled retraining with latest NVD labels",
        "Decouple scoring and configuration selection via a policy optimization layer that explicitly balances security and BFT constraints"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Securing a controller plane to isolate the risk manager from adversarial inputs",
        "Potential input injection against OSINT ingestion",
        "Potential data poisoning attacks against the ML model",
        "Dependence on timeliness and quality of external OSINT sources"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduce HAL 9000, a novel ML-based Risk Manager for ITSs that automates risk assessment and configuration advice",
      "Predict CVSS scores for unrated CVEs from descriptions to operate without waiting for NVD scoring",
      "Achieve \"99% accuracy\" in learning and replicating NVD’s evaluation process (quote from abstract)",
      "Provide a modular design to facilitate interchangeability between ML/AI models",
      "Include a clustering component to reason about similar/shared vulnerabilities across platforms",
      "Automate end-to-end risk assessment and configuration recommendation without human intervention after initialization"
    ]
  },
  {
    "arxiv_id": "2401.00280v3",
    "title": "Advancing TTP Analysis: Harnessing the Power of Large Language Models with Retrieval Augmented Generation",
    "authors": "Reza Fayyazi; Rozhina Taghdimi; Shanchieh Jay Yang",
    "abstract": "Tactics, Techniques, and Procedures (TTPs) outline the methods attackers use to exploit vulnerabilities. The interpretation of TTPs in the MITRE ATT&CK framework can be challenging for cybersecurity practitioners due to presumed expertise and complex dependencies. Meanwhile, advancements with Large Language Models (LLMs) have led to recent surge in studies exploring its uses in cybersecurity operations. It is, however, unclear how LLMs can be used in an efficient and proper way to provide accurate responses for critical domains such as cybersecurity. This leads us to investigate how to better use two types of LLMs: small-scale encoder-only (e.g., RoBERTa) and larger decoder-only (e.g., GPT-3.5) LLMs to comprehend and summarize TTPs with the intended purposes (i.e., tactics) of a cyberattack procedure. This work studies and compares the uses of supervised fine-tuning (SFT) of encoder-only LLMs vs. Retrieval Augmented Generation (RAG) for decoder-only LLMs (without fine-tuning). Both SFT and RAG techniques presumably enhance the LLMs with relevant contexts for each cyberattack procedure. Our studies show decoder-only LLMs with RAG achieves better performance than encoder-only models with SFT, particularly when directly relevant context is extracted by RAG. The decoder-only results could suffer low `Precision' while achieving high `Recall'. Our findings further highlight a counter-intuitive observation that more generic prompts tend to yield better predictions of cyberattack tactics than those that are more specifically tailored.",
    "published_date": "2023-12-30",
    "pdf_link": "https://arxiv.org/pdf/2401.00280v3",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Threat Intelligence",
      "subdomain": "TTP Analysis / ATT&CK Mapping",
      "specific_problem": "Map cyberattack procedure descriptions to MITRE ATT&CK tactics (multi-label classification)",
      "attack_types": [
        "Collection",
        "C2",
        "Credential Access",
        "Defense Evasion",
        "Discovery",
        "Execution",
        "Exfiltration",
        "Impact",
        "Initial Access",
        "Lateral Movement",
        "Persistence",
        "Privilege Escalation",
        "Reconnaissance",
        "Resource Development"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Retrieval-Augmented Generation",
        "specific": "GPT-3.5-turbo-1106 with FAISS retrieval and OpenAI embeddings",
        "novel_contribution": "RAG pipeline over ATT&CK procedure URLs (top-3 similar procedures; 3 chunks of 8,000 chars with 500 overlap), analysis showing RAG improves decoding-only LLM performance on tactic prediction; evidence that generic prompts outperform specific prompts"
      },
      {
        "type": "baseline",
        "category": "Transformer (Decoder-only LLM)",
        "specific": "GPT-3.5-turbo-1106 (prompt-only, zero-shot)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer (Encoder-only)",
        "specific": "SecureBERT",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer (Encoder-only)",
        "specific": "RoBERTa-base",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised (multi-label classification) for encoder-only models",
      "Zero-shot prompting (decoder-only)",
      "Retrieval-Augmented Generation (decoder-only)"
    ],
    "datasets": [
      {
        "name": "MITRE ATT&CK v14.1 tactics/techniques/sub-techniques descriptions with tactic mappings (639 descriptions)",
        "type": "public",
        "domain": "cti_text",
        "link": "https://attack.mitre.org/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "MITRE ATT&CK enterprise procedure descriptions with URLs (9,532 procedures)",
        "type": "public",
        "domain": "cti_text",
        "link": "https://github.com/RezzFayyazi/TTP-LLM",
        "is_new_contribution": true,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "SecureBERT-SFT (encoder-only)",
        "paper_reference": "Aghaei et al., 2022",
        "metric": "Samples Average F1",
        "their_result": "0.68 (GPT-3.5 + RAG w/ relevant procedures)",
        "baseline_result": "0.54"
      },
      {
        "method_name": "RoBERTa-base-SFT (encoder-only)",
        "paper_reference": "Liu et al., 2019",
        "metric": "Samples Average F1",
        "their_result": "0.68 (GPT-3.5 + RAG w/ relevant procedures)",
        "baseline_result": "0.41"
      },
      {
        "method_name": "GPT-3.5 prompt-only (no RAG)",
        "paper_reference": null,
        "metric": "Samples Average F1",
        "their_result": "0.68 (GPT-3.5 + RAG w/ relevant procedures)",
        "baseline_result": "0.60"
      },
      {
        "method_name": "GPT-3.5 + RAG (exact URL upper bound)",
        "paper_reference": null,
        "metric": "Samples Average F1",
        "their_result": "0.68 (GPT-3.5 + RAG w/ relevant procedures)",
        "baseline_result": "0.95"
      }
    ],
    "performance_metrics_used": [
      "Precision",
      "Recall",
      "Samples Average F1-Score",
      "Per-tactic F1"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How do encoder-only LLMs with supervised fine-tuning compare to decoder-only LLMs with RAG in interpreting cyberattack procedure descriptions and predicting ATT&CK tactics?",
        "Can RAG mitigate hallucination and improve accuracy of decoder-only LLMs on TTP tactic prediction?",
        "Do more generic prompts outperform more specific, tailored prompts for tactic prediction?",
        "How do precision and recall trade off between e-LLMs (SFT) and d-LLMs (RAG) for this task?"
      ],
      "gaps_identified": [
        "No publicly available datasets mapping ATT&CK procedures to tactics for supervised training and evaluation.",
        "LLM hallucination issues reduce reliability for high-precision domains like cybersecurity.",
        "Unclear best practices for applying LLMs efficiently and properly to cybersecurity tasks like TTP interpretation.",
        "ATT&CK TTP mapping can be imperfect and ambiguous, leading to differing analyst conclusions."
      ],
      "limitations": [
        "Inherent imperfection and ambiguity in ATT&CK procedure-to-tactic mappings.",
        "Decoder-only LLMs achieved high recall but notably lower precision, indicating hallucination during decoding.",
        "Some ATT&CK technique/sub-technique URLs contain tactic names, which could leak answers in RAG.",
        "Idealized RAG condition (exact URL) inflates performance; realistic retrieval (top-3 similar procedures) performs lower.",
        "Evaluation extracted tactic names by keyword search only, ignoring analogous terms (estimated ~1% deviation).",
        "Limited labeled data for SFT (639 descriptions); procedure descriptions not used for training to preserve realism.",
        "d-LLMs were not fine-tuned due to computational resource constraints.",
        "Variation in URL text length may influence RAG performance.",
        "Non-determinism in LLMs acknowledged; mitigated via temperature=0 and a fixed seed."
      ],
      "future_work": [
        "Develop methods to increase precision of decoder-only LLMs without compromising recall.",
        "Explore prompt design effects further (generic vs. specific) for tactic prediction.",
        "Improve retrieval quality and negative control to avoid leakage from tactic names in URLs.",
        "Share curated datasets and artifacts to enable broader benchmarking."
      ],
      "motivation": "Interpretation of ATT&CK TTPs is complex and ambiguous, and best practices for using LLMs in high-stakes cybersecurity domains remain unclear; this work seeks practical approaches that practitioners can adopt.",
      "potential_research_ideas": [
        "Constrained decoding or set-based decoding that restricts outputs to ATT&CK tactic ontology to boost precision.",
        "Verifier or reranker model that cross-checks generated tactics against retrieved evidence spans (retrieval-verified generation).",
        "Dual-stage system: discriminative encoder-only classifier for candidate set + generative decoder-only model for justification and final selection.",
        "Self-consistency with majority voting across diverse retrieval contexts to reduce hallucinations.",
        "Hard-negative and counterfactual retrieval to stress-test RAG and prevent leakage from URL artifacts.",
        "Construct and release a human-labeled procedure-to-tactics corpus with multiple annotators to quantify ambiguity.",
        "Leverage knowledge graphs of ATT&CK techniques-tactics relations for multi-hop retrieval and reasoning.",
        "Calibrate decision thresholds per-tactic using validation data to balance precision/recall per label.",
        "Evaluate and adapt open-source decoder-only models (e.g., LLaMA-2/3) with domain-specific RAG for privacy-preserving deployments."
      ],
      "architectural_improvement_recommendations": [
        "Add an evidence-aware verifier head that scores each predicted tactic by alignment to retrieved spans and rejects unsupported ones.",
        "Use retrieval reranking (e.g., cross-encoder re-ranker) after FAISS to improve top-3 context quality.",
        "Adopt chain-of-thought or claim-evidence prompting with explicit citation of retrieved text for each predicted tactic.",
        "Employ ontology-constrained output formatting (function-calling or schema-constrained decoding) to reduce spurious labels.",
        "Introduce a small supervised calibration layer on top of LLM logits to convert free-form outputs into calibrated multi-label predictions.",
        "De-bias retrieval by stripping or masking tactic names from URLs and titles prior to chunking to avoid leakage."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/RezzFayyazi/TTP-LLM",
      "frameworks": [
        "FAISS",
        "OpenAI API"
      ],
      "reproducibility_score": "high",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Ambiguity in ATT&CK mappings can produce differing conclusions for the same procedure.",
        "Decoder-only LLM hallucinations reduce precision and reliability.",
        "RAG performance depends on retrieval quality; realistic retrieval without exact URLs yields lower accuracy.",
        "Limited labeled data for fine-tuning in real-world settings.",
        "Potential leakage when tactic names appear in URLs or metadata of retrieved documents."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Analyzes and compares SFT of encoder-only LLMs vs. RAG-enhanced decoder-only LLMs for interpreting TTPs.",
      "Demonstrates decoder-only LLMs with RAG outperform encoder-only SFT models, especially with directly relevant context.",
      "Finds decoder-only LLMs have high recall but low precision, motivating methods to increase precision without sacrificing recall.",
      "Shows preliminary evidence that more generic prompts can yield better tactic predictions than specific prompts.",
      "Provides curated datasets and code artifacts for reproducibility and further research."
    ]
  },
  {
    "arxiv_id": "2312.17677v2",
    "title": "Prompt Fuzzing for Fuzz Driver Generation",
    "authors": "Yunlong Lyu; Yuxuan Xie; Peng Chen; Hao Chen",
    "abstract": "Crafting high-quality fuzz drivers not only is time-consuming but also requires a deep understanding of the library. However, the state-of-the-art automatic fuzz driver generation techniques fall short of expectations. While fuzz drivers derived from consumer code can reach deep states, they have limited coverage. Conversely, interpretative fuzzing can explore most API calls but requires numerous attempts within a large search space. We propose PromptFuzz, a coverage-guided fuzzer for prompt fuzzing that iteratively generates fuzz drivers to explore undiscovered library code. To explore API usage in fuzz drivers during prompt fuzzing, we propose several key techniques: instructive program generation, erroneous program validation, coverage-guided prompt mutation, and constrained fuzzer scheduling. We implemented PromptFuzz and evaluated it on 14 real-world libraries. Compared with OSS-Fuzz and Hopper (the state-of-the-art fuzz driver generation tool), fuzz drivers generated by PromptFuzz achieved 1.61 and 1.63 times higher branch coverage than those by OSS-Fuzz and Hopper, respectively. Moreover, the fuzz drivers generated by PromptFuzz detected 33 genuine, new bugs out of a total of 49 crashes, out of which 30 bugs have been confirmed by their respective communities.",
    "published_date": "2023-12-29",
    "pdf_link": "https://arxiv.org/pdf/2312.17677v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Fuzzing",
      "specific_problem": "Automatic fuzz driver generation for C/C++ libraries via coverage-guided prompt fuzzing to explore undiscovered library code and find vulnerabilities",
      "attack_types": [
        "memory safety violations (e.g., heap/stack errors; detected via AddressSanitizer)",
        "undefined behavior (detected via UBSan)",
        "I/O misuse leading to errors (validated via a custom File-Sanitizer)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer LLM",
        "specific": "ChatGPT, GPT-4",
        "novel_contribution": "Coverage-guided prompt fuzzing: iteratively mutates prompts to steer LLM code generation toward API usages that increase unseen library code coverage; integrates erroneous program validation and constrained fuzzer scheduling"
      },
      {
        "type": "primary",
        "category": "Prompt engineering / zero-shot prompting",
        "specific": "Zero-shot program synthesis with library-specific context",
        "novel_contribution": "Instructive program generation using a structured prompt template that includes task specification, library context (API signatures, type definitions, headers), and library-specific specifications"
      }
    ],
    "learning_paradigm": [
      "Zero-shot prompting",
      "Inference with RLHF-pretrained LLMs"
    ],
    "datasets": [
      {
        "name": "14 real-world C/C++ libraries (targets for fuzzing)",
        "type": "public",
        "domain": "library_code (C/C++)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "libvpx (example target)",
        "type": "public",
        "domain": "library_code (multimedia codec, C/C++)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "OSS-Fuzz (manually written fuzz drivers integrated in OSS-Fuzz)",
        "paper_reference": null,
        "metric": "Branch coverage",
        "their_result": "1.61× higher branch coverage than OSS-Fuzz",
        "baseline_result": "Normalized to 1.0× (baseline)"
      },
      {
        "method_name": "Hopper (state-of-the-art automatic fuzz driver generation tool; interpretative fuzzing)",
        "paper_reference": null,
        "metric": "Branch coverage",
        "their_result": "1.63× higher branch coverage than Hopper",
        "baseline_result": "Normalized to 1.0× (baseline)"
      }
    ],
    "performance_metrics_used": [
      "branch coverage",
      "number of crashes",
      "number of genuine new bugs",
      "number of bugs confirmed by maintainers/communities"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can coverage-guided prompt mutation of LLMs generate higher-quality fuzz drivers that explore undiscovered library code compared to existing approaches?",
        "How effective are instructive program generation and runtime validation (sanitizer- and coverage-based) in filtering erroneous LLM-generated programs?",
        "Does constrained/power scheduling of fuzzers help LLM-generated drivers reach deep library states and increase coverage?"
      ],
      "gaps_identified": [
        "Manual fuzz driver development is time-consuming and requires deep library knowledge, often yielding limited coverage.",
        "Consumer-code-derived fuzz drivers reach deep states but have limited API coverage because they only reflect usage present in consumer traces.",
        "Interpretative fuzzing (e.g., Hopper) explores many APIs but needs numerous attempts in a vast search space to find useful invocation sequences.",
        "Prior LLM-based fuzz driver generation used narrow instructions, causing low API usage diversity and failure to cover infrequent code or deep states.",
        "LLM outputs are often erroneous; existing validation (compiler or simple rules) misses complex semantic errors (e.g., API misuse), leading to false positives."
      ],
      "limitations": [
        "Elimination policy may discard some generated programs that actually trigger true library bugs, since any sanitizer-reported violation results in removal regardless of whether the bug is in generated code or library code.",
        "Initial seed inputs may be unsuitable for many generated programs; requires iterative corpus evolution to enable deeper validation.",
        "Prompt context length and token cost constraints necessitate random subsampling of APIs and types, which may omit relevant context.",
        "Generated code may not strictly follow instructions and may contain semantic/API misuses, requiring extensive validation.",
        "Reliance on proprietary LLMs (ChatGPT, GPT-4) and their behavior; inference variability not fully controlled."
      ],
      "future_work": [],
      "motivation": "Automatic generation of high-quality fuzz drivers remains challenging; existing static/dynamic derivation methods and interpretative fuzzing either have limited coverage or large search costs. LLMs can generate code over diverse APIs but need coverage-guided prompt mutation and robust validation to be useful for fuzzing.",
      "potential_research_ideas": [
        "Integrate static analysis or API-usage mining to guide prompt construction toward under-tested API dependency chains and stateful protocols.",
        "Use reinforcement learning or bandit optimization to learn prompt mutation policies that directly optimize coverage or bug discovery rate.",
        "Incorporate constrained decoding/program repair to automatically fix common API misuses in LLM outputs before validation.",
        "Leverage symbolic execution or concolic testing to infer argument constraints and generate valid inputs, reducing dependence on long fuzzing warm-up.",
        "Adopt multi-agent LLM systems (planner, coder, verifier) where a verifier agent reasons over API requirements and patches generated code.",
        "Exploit library documentation/specs (e.g., doxygen, headers, examples) via retrieval-augmented generation to reduce hallucinations and improve semantic correctness.",
        "Generalize to multi-language libraries (Rust, Go) and cross-language bindings (Python/C++) with language-specific prompt templates and validators.",
        "Develop open-source, reproducible LLM backends (e.g., code-specialized open models) fine-tuned on fuzz driver corpora."
      ],
      "architectural_improvement_recommendations": [
        "Add a static API-misuse checker and simple dataflow checks before sanitizer-based runtime validation to filter obvious misuse earlier.",
        "Introduce retrieval-augmented prompts from library docs and examples to improve semantic adherence and reduce context-length pressure.",
        "Implement a repair loop: compiler/error logs and sanitizer traces feed back into automatic code edits (patch prompts) before discarding seeds.",
        "Use coverage-guided function-set selection with dependency-aware clustering to choose API combinations more strategically than random.",
        "Hybridize with lightweight concolic execution to produce satisfying inputs for hard-to-reach branches during validation.",
        "Parameterize prompt templates with common fuzzing idioms (RAII cleanup, bounds checks, error handling) as reusable code snippets.",
        "Cache and reuse successful prompt fragments and seed programs across libraries with similar APIs to accelerate convergence."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "Clang/LLVM sanitizers (ASan, UBSan)",
        "Custom File-Sanitizer",
        "Grey-box fuzzer (compatible with existing fuzzers; unspecified)"
      ],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Open-source C/C++ library fuzzing (OSS-Fuzz-style pipeline)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "LLM-generated code may contain semantic/API misuses requiring thorough validation.",
        "Prompt context length and token costs constrain how many APIs/types can be included per generation.",
        "Need to evolve suitable input corpora for diverse generated programs to enable deep validation.",
        "Large search space of API combinations and usage patterns; requires effective scheduling and prioritization.",
        "Potential loss of true positives due to aggressive erroneous-program elimination."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "PromptFuzz: a coverage-guided fuzzer for prompt fuzzing that iteratively mutates prompts to generate fuzz drivers exploring undiscovered library code.",
      "Instructive program generation using a structured prompt template with task, library context (APIs, types, headers), and library specifications.",
      "Erroneous program validation combining compiler checks with runtime sanitizers (ASan, UBSan, File-Sanitizer), fuzzing-based input evolution, and coverage-based validation.",
      "Coverage-guided prompt mutation and constrained/power fuzzer scheduling to steer generation toward deeper code paths.",
      "Automatic conversion of constant arguments to fuzzer-controlled variables with inferred constraints and fusion into a single fuzz driver compatible with existing fuzzers.",
      "Empirical evaluation on 14 real-world libraries showing 1.61× and 1.63× higher branch coverage than OSS-Fuzz and Hopper, respectively, and detection of 49 crashes including 33 genuine new bugs, 30 confirmed by communities."
    ]
  },
  {
    "arxiv_id": "2312.01200v1",
    "title": "FRAUDability: Estimating Users' Susceptibility to Financial Fraud Using Adversarial Machine Learning",
    "authors": "Chen Doytshman; Satoru Momiyama; Inderjeet Singh; Yuval Elovici; Asaf Shabtai",
    "abstract": "In recent years, financial fraud detection systems have become very efficient at detecting fraud, which is a major threat faced by e-commerce platforms. Such systems often include machine learning-based algorithms aimed at detecting and reporting fraudulent activity. In this paper, we examine the application of adversarial learning based ranking techniques in the fraud detection domain and propose FRAUDability, a method for the estimation of a financial fraud detection system's performance for every user. We are motivated by the assumption that \"not all users are created equal\" -- while some users are well protected by fraud detection algorithms, others tend to pose a challenge to such systems. The proposed method produces scores, namely \"fraudability scores,\" which are numerical estimations of a fraud detection system's ability to detect financial fraud for a specific user, given his/her unique activity in the financial system. Our fraudability scores enable those tasked with defending users in a financial platform to focus their attention and resources on users with high fraudability scores to better protect them. We validate our method using a real e-commerce platform's dataset and demonstrate the application of fraudability scores from the attacker's perspective, on the platform, and more specifically, on the fraud detection systems used by the e-commerce enterprise. We show that the scores can also help attackers increase their financial profit by 54%, by engaging solely with users with high fraudability scores, avoiding those users whose spending habits enable more accurate fraud detection.",
    "published_date": "2023-12-02",
    "pdf_link": "https://arxiv.org/pdf/2312.01200v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Financial Security",
      "subdomain": "Fraud Detection",
      "specific_problem": "Estimating per-user susceptibility ('fraudability') to e-commerce/credit card transaction fraud and crafting sequence-based adversarial transactions against fraud detectors",
      "attack_types": [
        "Adversarial evasion",
        "Transaction/credit card fraud",
        "Account takeover"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "RNN Autoencoder",
        "specific": "LSTM undercomplete autoencoder (UAE) anomaly detector",
        "novel_contribution": "Used as sequence-based surrogate/target fraud detector with detection-time modeling via reconstruction error thresholds"
      },
      {
        "type": "primary",
        "category": "Adversarial example generation",
        "specific": "FGSM/BIM-inspired sequence-based adversarial transaction injection",
        "novel_contribution": "Novel sequence-based attack that chooses both the location within a transaction sequence and mutable attributes of the injected transaction"
      },
      {
        "type": "primary",
        "category": "Regression",
        "specific": null,
        "novel_contribution": "Fraudability score predictor that learns to estimate per-user susceptibility from aggregated user profile features"
      },
      {
        "type": "baseline",
        "category": "Heuristic selection strategy",
        "specific": "Random user selection",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Heuristic injection strategy",
        "specific": "Random injection",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Unsupervised",
      "Supervised",
      "Adversarial"
    ],
    "datasets": [
      {
        "name": "Real e-commerce platform transactions (3M+ transactions, 500k+ users)",
        "type": "proprietary",
        "domain": "financial_transactions",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Alibaba platform dataset (as cited in Can et al.)",
        "type": "proprietary",
        "domain": "financial_transactions",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Taobao fraud detection dataset (as cited in Qingyu et al.)",
        "type": "proprietary",
        "domain": "financial_transactions",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Proprietary banking transaction datasets (as cited in Carminati et al.)",
        "type": "proprietary",
        "domain": "financial_transactions",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Random user selection (vs FRAUDability-guided selection)",
        "paper_reference": null,
        "metric": "Attack success (evasion) rate",
        "their_result": "“improve the success rate by 58% for the random injection strategy and by 19% for the proposed adversarial attack injection strategy.”",
        "baseline_result": "Random user selection; absolute rates not provided"
      },
      {
        "method_name": "Random user selection (vs FRAUDability-guided selection)",
        "paper_reference": null,
        "metric": "Money stolen (profit)",
        "their_result": "“166% increase in the money stolen with the random injection strategy and a 30% increase with the adversarial attack injection strategy.”",
        "baseline_result": "Random user selection; absolute amounts not provided"
      },
      {
        "method_name": "Random injection (vs proposed adversarial injection)",
        "paper_reference": null,
        "metric": "Undetected rate (evasion)",
        "their_result": "Proposed sequence-based adversarial attack achieves 92–98% undetected fraudulent transactions",
        "baseline_result": "Not explicitly reported for random injection"
      }
    ],
    "performance_metrics_used": [
      "Evasion rate (attack success rate)",
      "Money stolen (profit)",
      "Mean Absolute Error (MAE) for fraudability score prediction",
      "Injection rate",
      "Time-to-detection",
      "Detection rate"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can we estimate the ability of a fraud detection system to detect fraud for each user (a per-user 'fraudability score')?",
        "Do fraudability scores allow prioritizing targets (from attacker view) or protections (from defender view) to materially change evasion success and profit?",
        "Can a sequence-based adversarial injection strategy effectively evade sequence-based fraud detectors?"
      ],
      "gaps_identified": [
        "Prior AML works in fraud detection did not consider user-to-user variability in attack success or per-user detector performance.",
        "Existing AML in fraud largely targeted instance-based models; sequence-based fraud detectors remain relatively unexplored."
      ],
      "limitations": [
        "Evaluation relies on a proprietary single e-commerce dataset; generalization to other platforms is unverified.",
        "Scalability: deriving scores for every individual user is infeasible at enterprise scale; requires training on a subset of users.",
        "Assumes the majority of transactions in the surrogate dataset are benign.",
        "Attacker can only manipulate limited mutable transaction features (amount, item category, temporal fields).",
        "Sequence window fixed at n=10 based on preliminary results; sensitivity to window size not fully explored."
      ],
      "future_work": [],
      "motivation": "Fraud detectors do not perform equally well for all users due to behavioral variability; estimating per-user susceptibility enables targeted defense (or attack) allocation.",
      "potential_research_ideas": [
        "Extend fraudability estimation to multi-objective scoring that jointly optimizes injection rate, time-to-detection, and expected loss with configurable risk trade-offs.",
        "Develop defender-side adaptive thresholds and MFA policies driven by predicted fraudability with closed-loop feedback to reduce high-risk users’ scores.",
        "Incorporate graph-based signals (user–merchant–device networks) to improve fraudability prediction beyond per-user aggregates.",
        "Calibrate and quantify uncertainty in fraudability predictions (e.g., Bayesian or deep ensembles) to inform conservative decisions for high-uncertainty users.",
        "Investigate transferability across platforms: meta-learn fraudability predictors that adapt to new merchants with minimal data.",
        "Design robust training of sequence detectors via adversarial training using the proposed sequence-based attacks to reduce fraudability disparities.",
        "Assess ethical and fairness impacts: audit whether fraudability correlates with protected attributes and mitigate disparate impact."
      ],
      "architectural_improvement_recommendations": [
        "Replace/augment LSTM autoencoder with Transformer-based sequence models (e.g., Transformer AEs) to capture long-range dependencies.",
        "Use contrastive sequence representation learning to build stronger surrogate detectors and richer user profiles.",
        "Jointly train the fraudability predictor with a differentiable surrogate detector in a multi-task setup to better align scores with detector behavior.",
        "Introduce per-user adaptive anomaly thresholds via calibration layers conditioned on user profiles.",
        "Augment features with merchant/device/IP graph embeddings (GraphSAGE/GAT) for context-aware detection and scoring.",
        "Apply adversarial training using the proposed sequence-based perturbations to harden the detector.",
        "Model temporal point processes for transaction timing to better simulate and detect temporal anomalies."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "E-commerce platform (offline evaluation on real transaction logs)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Selecting and monitoring a representative subset of users due to large user bases (scalability).",
        "Time-driven data collection; sufficient monitoring duration needed for stable user profiles.",
        "Risk of dual-use: fraudability scores can aid attackers if disclosed."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introducing the concept of a financial fraudability score, showing detectors do not perform equally on all users.",
      "An ML-based method to estimate a fraud detection system’s per-user performance and assign a fraudability score.",
      "A sequence-based adversarial learning technique inspired by FGSM/BIM to craft adversarial transactions against sequence models.",
      "Empirical validation on a real e-commerce dataset: FRAUDability-guided targeting improved attack success by 58% (random injection) and 19% (adversarial injection), profit by 166% and 30%, respectively.",
      "Proposed adversarial injection achieves 92–98% undetected fraudulent transactions; fraudability predictor attains low MAE even with one week of data."
    ]
  },
  {
    "arxiv_id": "2312.01113v1",
    "title": "Malicious code detection in android: the role of sequence characteristics and disassembling methods",
    "authors": "Pinar G. Balikcioglu; Melih Sirlanci; Ozge A. Kucuk; Bulut Ulukapi; Ramazan K. Turkmen; Cengiz Acarturk",
    "abstract": "The acceptance and widespread use of the Android operating system drew the attention of both legitimate developers and malware authors, which resulted in a significant number of benign and malicious applications available on various online markets. Since the signature-based methods fall short for detecting malicious software effectively considering the vast number of applications, machine learning techniques in this field have also become widespread. In this context, stating the acquired accuracy values in the contingency tables in malware detection studies has become a popular and efficient method and enabled researchers to evaluate their methodologies comparatively. In this study, we wanted to investigate and emphasize the factors that may affect the accuracy values of the models managed by researchers, particularly the disassembly method and the input data characteristics. Firstly, we developed a model that tackles the malware detection problem from a Natural Language Processing (NLP) perspective using Long Short-Term Memory (LSTM). Then, we experimented with different base units (instruction, basic block, method, and class) and representations of source code obtained from three commonly used disassembling tools (JEB, IDA, and Apktool) and examined the results. Our findings exhibit that the disassembly method and different input representations affect the model results. More specifically, the datasets collected by the Apktool achieved better results compared to the other two disassemblers.",
    "published_date": "2023-12-02",
    "pdf_link": "https://arxiv.org/pdf/2312.01113v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Malware Analysis",
      "subdomain": "Android Malware Detection",
      "specific_problem": "Static analysis-based binary classification of Android apps (benign vs malware) using smali instruction sequences; assessing impact of disassembler choice and sequence unit granularity on detection performance",
      "attack_types": [
        "Adware",
        "Banking malware",
        "SMS malware",
        "Mobile riskware"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "RNN",
        "specific": "LSTM",
        "novel_contribution": "Applies LSTM from an NLP perspective on Dalvik smali instruction sequences including operands; systematically varies sequence unit (instruction/basic block/method/class) and disassembler (JEB, IDA, Apktool) to quantify their impact on accuracy"
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Dataset 1 (AndMal2017 benign + VirusShare malware)",
        "type": "public",
        "domain": "android_apk_smali",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "AndMal2017 (2017 pack) - benign subset",
        "type": "public",
        "domain": "android_apk_smali",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "VirusShare - malware subset",
        "type": "public",
        "domain": "android_apk_smali",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Dataset 2 (AndMal2020)",
        "type": "public",
        "domain": "android_apk_smali",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "accuracy"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How does the choice of disassembler (JEB, IDA Pro, Apktool) affect the performance of LSTM-based Android malware detection?",
        "How do different input sequence units (instruction, basic block, method, class) influence detection accuracy?",
        "Do representation differences introduced by disassemblers materially alter model outcomes?"
      ],
      "gaps_identified": [
        "Reported accuracy values in malware detection are often compared without accounting for critical factors such as disassembly method and input representation/sequence characteristics.",
        "The multi-parameter nature of deep learning models obscures the impact of upstream code representation choices, which are typically treated as operational assumptions rather than determinants of outcomes."
      ],
      "limitations": [
        "For Dataset 1, malware types from VirusShare are unspecified: \"Since there was no information about the types of malware we collected from the Virusshare, we cannot state the types of malware we used in Dataset 1.\"",
        "Different disassemblers could not produce Dalvik assembly for a few apps, leading to slightly different sample counts per tool.",
        "Each disassembler produces different-sized outputs; the authors limited sequence counts (e.g., ISM to 200 million instructions) to enable fair comparison.",
        "Commercial disassembler configurations (JEB demo, IDA with IDAPython) and preprocessing choices may affect reproducibility."
      ],
      "future_work": [],
      "motivation": "To investigate and emphasize factors beyond model hyperparameters—particularly disassembly method and input data characteristics—that affect reported accuracy in Android malware detection.",
      "potential_research_ideas": [
        "Develop a cross-disassembler normalization layer that maps smali/opcode tokens from different tools into a unified intermediate representation, reducing tool-induced variance.",
        "Train multi-view models that jointly ingest outputs from multiple disassemblers (late fusion/attention) to improve robustness.",
        "Pretrain code-language models (e.g., transformer-based) on large smali corpora and fine-tune for malware detection to test sensitivity vs. LSTM.",
        "Hierarchical sequence models that explicitly model instruction→basic-block→method→class structure with hierarchical attention.",
        "Evaluate adversarially robust tokenization and obfuscation-aware augmentation (e.g., random register renaming, string obfuscation) to improve resilience.",
        "Standardize a benchmark protocol that fixes disassembler, tokenization, and sequence limits to enable fair cross-paper comparison."
      ],
      "architectural_improvement_recommendations": [
        "Replace or complement LSTM with transformer encoders using bytecode/smali token embeddings and positional encodings.",
        "Introduce hierarchical encoders (per-basic-block and per-method) with pooling/attention to capture long-range dependencies beyond LSTM limits.",
        "Learn disassembler-invariant embeddings via domain-adversarial training to reduce representation shifts across tools.",
        "Use subtoken/BPE tokenization for smali to compact the vocabulary and handle rare operands while keeping operands (not only opcodes).",
        "Incorporate static program graphs (CFG/CG) as auxiliary inputs via GNN components alongside sequence encoders."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Very large sequence volumes; e.g., Instruction-as-Sequence Model limited to 200 million instructions per tool for comparability; no explicit GPU/CPU specs reported."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Variation across disassemblers yields different representations and sequence sizes, affecting model performance.",
        "Some apps fail to disassemble on certain tools, causing coverage gaps.",
        "High computational load due to extremely large sequence counts necessitates capping, which may limit fidelity."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "An NLP-oriented LSTM approach to Android malware detection using smali instruction sequences that include both opcodes and operands.",
      "Systematic empirical study varying input sequence unit (instruction, basic block, method, class) and disassembler (JEB, IDA Pro, Apktool).",
      "Evidence that disassembly method and input representation significantly affect model outcomes; specifically, datasets produced by Apktool achieved better results than JEB and IDA.",
      "Publicly documented preprocessing pipelines for three common disassemblers and large-scale sequence construction from Android apps."
    ]
  },
  {
    "arxiv_id": "2311.05733v1",
    "title": "LogShield: A Transformer-based APT Detection System Leveraging Self-Attention",
    "authors": "Sihat Afnan; Mushtari Sadia; Shahrear Iqbal; Anindya Iqbal",
    "abstract": "Cyber attacks are often identified using system and network logs. There have been significant prior works that utilize provenance graphs and ML techniques to detect attacks, specifically advanced persistent threats, which are very difficult to detect. Lately, there have been studies where transformer-based language models are being used to detect various types of attacks from system logs. However, no such attempts have been made in the case of APTs. In addition, existing state-of-the-art techniques that use system provenance graphs, lack a data processing framework generalized across datasets for optimal performance. For mitigating this limitation as well as exploring the effectiveness of transformer-based language models, this paper proposes LogShield, a framework designed to detect APT attack patterns leveraging the power of self-attention in transformers. We incorporate customized embedding layers to effectively capture the context of event sequences derived from provenance graphs. While acknowledging the computational overhead associated with training transformer networks, our framework surpasses existing LSTM and Language models regarding APT detection. We integrated the model parameters and training procedure from the RoBERTa model and conducted extensive experiments on well-known APT datasets (DARPA OpTC and DARPA TC E3). Our framework achieved superior F1 scores of 98% and 95% on the two datasets respectively, surpassing the F1 scores of 96% and 94% obtained by LSTM models. Our findings suggest that LogShield's performance benefits from larger datasets and demonstrates its potential for generalization across diverse domains. These findings contribute to the advancement of APT attack detection methods and underscore the significance of transformer-based architectures in addressing security challenges in computer systems.",
    "published_date": "2023-11-09",
    "pdf_link": "https://arxiv.org/pdf/2311.05733v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Endpoint/Host Security",
      "subdomain": "Advanced Persistent Threat (APT) Detection",
      "specific_problem": "Detecting APT attack patterns from host system provenance logs by modeling event sequences",
      "attack_types": [
        "Advanced Persistent Threat (APT)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "RoBERTa-like encoder (self-attention)",
        "novel_contribution": "Customized log and temporal embeddings for provenance-derived event sequences; supervised objective with log-key cross-entropy for benign/malicious sequence separation"
      },
      {
        "type": "baseline",
        "category": "RNN",
        "specific": "LSTM",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Self-supervised (Masked Language Modeling)"
    ],
    "datasets": [
      {
        "name": "DARPA OpTC",
        "type": "public",
        "domain": "log_files (system_provenance)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "DARPA Transparent Computing (TC) Engagement 3 (E3)",
        "type": "public",
        "domain": "log_files (system_provenance)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "LSTM (on DARPA OpTC)",
        "paper_reference": null,
        "metric": "F1-score",
        "their_result": "98%",
        "baseline_result": "96%"
      },
      {
        "method_name": "LSTM (on DARPA TC E3)",
        "paper_reference": null,
        "metric": "F1-score",
        "their_result": "95%",
        "baseline_result": "94%"
      }
    ],
    "performance_metrics_used": [
      "F1-score",
      "Accuracy"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can transformer-based language models leveraging self-attention detect APT patterns from system provenance logs?",
        "Does a generalized data processing framework for provenance graphs improve performance across different APT datasets?",
        "Do transformers outperform LSTM and prior language-model-based approaches as sequence length and dataset size increase?"
      ],
      "gaps_identified": [
        "No prior attempts applying transformer-based language models specifically to APT detection from provenance logs.",
        "State-of-the-art provenance-graph techniques lack a generalized, cross-dataset data processing framework optimized for performance.",
        "RNN-based methods struggle with long-range dependencies and degrade as event sequence length and data volume grow.",
        "Transformer-based models applied directly to raw logs fail to capture crucial indicators without tailored preprocessing/embeddings."
      ],
      "limitations": [
        "Acknowledged computational overhead of training transformer networks.",
        "Transformer-based models face challenges on raw log data without customized feature extraction (necessity of specialized embeddings)."
      ],
      "future_work": [],
      "motivation": "Explore transformer self-attention for APT detection and provide a generalized provenance-graph processing pipeline that works across datasets.",
      "potential_research_ideas": [
        "Pre-train a domain-specific log language model on massive multi-source provenance logs and fine-tune for APT detection.",
        "Online/streaming APT detection with incremental provenance graph construction and continual learning.",
        "Contrastive or metric-learning objectives to better separate benign/malicious sequences and support few-shot novel APTs.",
        "Cross-domain transfer and domain adaptation (e.g., Windows to Linux) using adapters or feature alignment.",
        "Multi-modal fusion of provenance logs with network telemetry, EDR alerts, and file metadata.",
        "Adversarially robust training against log manipulation, missing events, or timing obfuscation.",
        "Explainable attention analysis with causal attribution over provenance paths to aid analysts.",
        "Distillation or linear-attention variants for resource-constrained endpoints.",
        "Graph-aware transformers with relational/structural encodings that explicitly leverage provenance edges.",
        "Imbalance-aware learning (focal loss, reweighting, curriculum) tailored to extreme class imbalance in APT traces."
      ],
      "architectural_improvement_recommendations": [
        "Incorporate relational positional encodings or a graph transformer over provenance edges to complement sequence modeling.",
        "Adopt long-sequence transformers (Longformer/BigBird) to handle very long event traces without heavy padding.",
        "Use time2vec or continuous-time positional encodings to enhance temporal modeling beyond discretized windows.",
        "Two-stage training: self-supervised masked event/object/action/time prediction pretraining, followed by supervised APT classification with focal loss.",
        "Memory- and compute-efficient attention (Performer/FlashAttention) for scalability on billion-event corpora.",
        "Hard-negative mining and contrastive learning over near-benign sequences to improve decision boundaries.",
        "Model uncertainty estimation (Bayesian heads, MC dropout) to flag low-confidence detections for analyst review."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Transformer training incurs significant computational overhead; performance benefits observed with larger datasets (no specific GPU/runtime details provided)."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Host-based provenance logging on Windows endpoints (provenance graph/event sequence pipeline)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "High computational cost for training and potentially inference at scale.",
        "Need to construct and maintain provenance graphs from raw system logs.",
        "Extreme class imbalance in APT datasets (e.g., ~0.0016% malicious in OpTC).",
        "Generalization across diverse OS/process ecosystems and logging configurations.",
        "Data storage/throughput constraints for multi-billion-event corpora."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces LogShield, a transformer-based APT detection framework over provenance-derived event sequences with customized log and temporal embeddings.",
      "Provides a generalized data processing pipeline for provenance graphs applicable across datasets.",
      "Demonstrates superior performance over LSTM and other language-model baselines; reports F1=98% (DARPA OpTC) and 95% (DARPA TC E3).",
      "Shows that performance improves with larger datasets and suggests potential for cross-domain generalization."
    ]
  },
  {
    "arxiv_id": "2312.01241v3",
    "title": "Just-in-Time Detection of Silent Security Patches",
    "authors": "Xunzhu Tang; Zhenghan Chen; Kisub Kim; Haoye Tian; Saad Ezzini; Jacques Klein",
    "abstract": "Open-source code is pervasive. In this setting, embedded vulnerabilities are spreading to downstream software at an alarming rate. While such vulnerabilities are generally identified and addressed rapidly, inconsistent maintenance policies may lead security patches to go unnoticed. Indeed, security patches can be {\\em silent}, i.e., they do not always come with comprehensive advisories such as CVEs. This lack of transparency leaves users oblivious to available security updates, providing ample opportunity for attackers to exploit unpatched vulnerabilities. Consequently, identifying silent security patches just in time when they are released is essential for preventing n-day attacks, and for ensuring robust and secure maintenance practices. With LLMDA we propose to (1) leverage large language models (LLMs) to augment patch information with generated code change explanations, (2) design a representation learning approach that explores code-text alignment methodologies for feature combination, (3) implement a label-wise training with labelled instructions for guiding the embedding based on security relevance, and (4) rely on a probabilistic batch contrastive learning mechanism for building a high-precision identifier of security patches. We evaluate LLMDA on the PatchDB and SPI-DB literature datasets and show that our approach substantially improves over the state-of-the-art, notably GraphSPD by 20% in terms of F-Measure on the SPI-DB benchmark.",
    "published_date": "2023-12-02",
    "pdf_link": "https://arxiv.org/pdf/2312.01241v3",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Vulnerability and Patch Management",
      "specific_problem": "Just-in-time detection of silent security patches in open-source repositories",
      "attack_types": [
        "n-day attacks (vulnerability exploitation of unpatched software)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "PT-Former",
        "novel_contribution": "New cross-modal alignment and fusion module using self-attention and cross-attention to align CodeT5+ (code) and LLaMA-7b (text) embeddings into a single patch representation."
      },
      {
        "type": "primary",
        "category": "Contrastive Learning",
        "specific": "Stochastic Batch Contrastive Learning (SBCL)",
        "novel_contribution": "Batch-wise triplet mining with stochastic selection to minimize intra-class distance and maximize inter-class distance for high-precision security patch identification."
      },
      {
        "type": "primary",
        "category": "Instruction Tuning",
        "specific": null,
        "novel_contribution": "Label-wise training with a fixed natural language instruction to guide embeddings toward security relevance."
      },
      {
        "type": "primary",
        "category": "LLM-generated data augmentation",
        "specific": "ChatGPT-3.5",
        "novel_contribution": "Generates concise natural-language explanations of code diffs to augment inputs when commit messages are missing/insufficient."
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "CodeT5+",
        "novel_contribution": "Used as the code encoder to produce patch embeddings (no architectural novelty, serves as backbone)."
      },
      {
        "type": "primary",
        "category": "Transformer LLM",
        "specific": "LLaMA-7b",
        "novel_contribution": "Used as the text encoder for explanations, commit messages, and instruction (no architectural novelty, serves as backbone)."
      },
      {
        "type": "baseline",
        "category": "RNN",
        "specific": "TwinRNN",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Graph-based model",
        "specific": "GraphSPD",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Contrastive Learning",
      "Instruction-tuning"
    ],
    "datasets": [
      {
        "name": "PatchDB",
        "type": "public",
        "domain": "source_code_patches",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "SPI-DB",
        "type": "public",
        "domain": "source_code_patches",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "GraphSPD",
        "paper_reference": "[9] Wang et al.",
        "metric": "F-Measure (F1) on SPI-DB",
        "their_result": "“improves … notably GraphSPD by 20% in terms of F-Measure on the SPI-DB benchmark.”",
        "baseline_result": null
      },
      {
        "method_name": "GraphSPD",
        "paper_reference": "[9] Wang et al.",
        "metric": "F-Measure (F1) on PatchDB",
        "their_result": "“LLMDA achieves up to ∼42% … improvement over the incumbent state-of-the-art on [PatchDB].”",
        "baseline_result": null
      },
      {
        "method_name": "TwinRNN",
        "paper_reference": "[8]",
        "metric": "F-Measure (F1)",
        "their_result": "“consistently outperforms the baseline methods (i.e., TwinRNN and GraphSPD) on PatchDB and SPI-DB.”",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "F-Measure (F1)"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can silent security patches be detected just-in-time using multimodal representations of code diffs and text?",
        "Do LLM-generated explanations of code changes improve security patch detection when commit messages are missing or noisy?",
        "Does aligning code and text embeddings via the proposed PT-Former improve detection compared to unimodal or simple concatenation?",
        "Does stochastic batch contrastive learning (SBCL) improve the discriminative power and precision of security patch identifiers?",
        "Can instruction-guided, label-wise training steer embeddings toward security relevance effectively?"
      ],
      "gaps_identified": [
        "Prior work largely relies on syntactic features or sequential models that ignore program semantics, leading to high false positives.",
        "GraphSPD focuses on local code segments and may miss broader context of how functions or modules interact.",
        "Commit messages are often missing, insufficient, or misleading, reducing the effectiveness of methods relying on textual descriptions.",
        "Patches are often non-atomic, mixing security-relevant and cosmetic changes, complicating static semantics extraction."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Silent security patches in OSS lack advisories (e.g., CVEs), leaving users unaware of available security updates and enabling n-day attacks; there is a need to identify these patches just-in-time using richer representations.",
      "potential_research_ideas": [
        "Extend to whole-repository or project-level context modeling (e.g., call graph or module interaction graphs) to capture non-local security-relevant changes.",
        "Investigate fine-tuning or instruction-tuning of the text and code encoders jointly for the specific task to improve alignment beyond frozen backbones.",
        "Integrate static/dynamic program analysis signals (e.g., taint flows, API misuse) to complement LLM-derived explanations.",
        "Develop online/streaming detection for CI/CD pipelines with budgeted inference and update strategies.",
        "Create or release a larger, up-to-date benchmark with diverse languages and repositories to reduce dataset bias and improve generalization.",
        "Explore few-shot and cross-repo generalization, including domain adaptation to unseen projects/languages.",
        "Study robustness to noisy/incorrect LLM explanations and propose uncertainty-aware or explanation-quality-aware training.",
        "Incorporate explainable outputs that highlight code hunks responsible for a security classification to aid maintainers."
      ],
      "architectural_improvement_recommendations": [
        "Jointly train the code and text encoders with PT-Former using a multi-task objective (classification + contrastive alignment + explanation consistency).",
        "Replace or augment LLaMA-7b with a code-specialized text encoder (e.g., CodeLLaMA) for better code-change explanation embeddings.",
        "Introduce a graph-aware branch (e.g., code property graph encoder) fused with PT-Former to capture global program context.",
        "Use hard-negative mining across batches or memory bank for SBCL to further tighten class margins.",
        "Calibrate decision thresholds for high-precision operating points and incorporate focal or class-balanced losses.",
        "Adopt retrieval-augmented generation (RAG) to contextualize patches with project history or related issues/PRs before embedding."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces LLMDA, a language-centric framework that leverages LLM-generated explanations and multimodal representation learning to detect silent security patches without relying on explicit advisories.",
      "Proposes PT-Former to align and fuse code (CodeT5+) and text (LLaMA-7b) embeddings via self- and cross-attention into a single representation.",
      "Implements label-wise training with a fixed instruction to guide embeddings toward security relevance and a stochastic batch contrastive learning (SBCL) mechanism for high-precision identification.",
      "Achieves new state-of-the-art results on PatchDB and SPI-DB, including “∼42%” (PatchDB) and “∼20%” (SPI-DB) improvement over the incumbent GraphSPD in F-Measure, and consistently outperforms TwinRNN.",
      "Presents ablation studies showing contributions of each component and indicating robustness compared to prior SOTA."
    ]
  },
  {
    "arxiv_id": "2312.04135v3",
    "title": "A Novel Federated Learning-Based IDS for Enhancing UAVs Privacy and Security",
    "authors": "Ozlem Ceviz; Pinar Sadioglu; Sevil Sen; Vassilios G. Vassilakis",
    "abstract": "Unmanned aerial vehicles (UAVs) operating within Flying Ad-hoc Networks (FANETs) encounter security challenges due to the dynamic and distributed nature of these networks. Previous studies focused predominantly on centralized intrusion detection, assuming a central entity responsible for storing and analyzing data from all devices. However, these approaches face challenges including computation and storage costs, along with a single point of failure risk, threatening data privacy and availability. The widespread dispersion of data across interconnected devices underscores the need for decentralized approaches. This paper introduces the Federated Learning-based Intrusion Detection System (FL-IDS), addressing challenges encountered by centralized systems in FANETs. FL-IDS reduces computation and storage costs for both clients and the central server, which is crucial for resource-constrained UAVs. Operating in a decentralized manner, FL-IDS enables UAVs to collaboratively train a global intrusion detection model without sharing raw data, thus avoiding delay in decisions based on collected data, as is often the case with traditional methods. Experimental results demonstrate FL-IDS's competitive performance with Central IDS (C-IDS) while mitigating privacy concerns, with the Bias Towards Specific Clients (BTSC) method further enhancing FL-IDS performance even at lower attacker ratios. Comparative analysis with traditional intrusion detection methods, including Local IDS (L-IDS), sheds light on the strengths of FL-IDS. This study significantly contributes to UAV security by introducing a privacy-aware, decentralized intrusion detection approach tailored to UAV networks. Moreover, by introducing a realistic dataset for FANETs and federated learning, our approach differs from others lacking high dynamism and 3D node movements or accurate federated data federations.",
    "published_date": "2023-12-07",
    "pdf_link": "https://arxiv.org/pdf/2312.04135v3",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Federated learning-based intrusion detection for routing attacks in FANETs (AODV) without sharing raw data",
      "attack_types": [
        "sinkhole",
        "blackhole",
        "flooding"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Federated Learning",
        "specific": null,
        "novel_contribution": "FL-IDS with Bias Towards Specific Clients (BTSC) aggregation that prioritizes clients with superior detection capability"
      },
      {
        "type": "primary",
        "category": "Classifier",
        "specific": null,
        "novel_contribution": "Supervised attack classification trained in a federated manner on UAV-local data for routing attack detection in highly dynamic FANETs"
      },
      {
        "type": "baseline",
        "category": "Centralized classifier",
        "specific": "Central IDS (C-IDS)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "On-device classifier",
        "specific": "Local IDS (L-IDS)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Federated Learning"
    ],
    "datasets": [
      {
        "name": "Unnamed realistic FANET-FL dataset introduced by this paper (3D mobility; AODV; sinkhole/blackhole/flooding; 160 simulated networks; attacker ratios 5–25%)",
        "type": "synthetic",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "DJI Phantom 4 drone dataset",
        "type": "public",
        "domain": "uav_sensors",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "AWID2",
        "type": "public",
        "domain": "wireless_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "OMNeT++-based FANET Sybil attack dataset (RSS/TDoA) [18]",
        "type": "synthetic",
        "domain": "physical_layer_signals",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "ONE-simulator time delay attack dataset for FANETs [20]",
        "type": "synthetic",
        "domain": "network_latency",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "UAV-IDS-2020",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "KDDCup'99",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "NSL-KDD",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CICIDS2017",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CRAWDAD (modified for distributed VANET-like structure in [30])",
        "type": "public",
        "domain": "vehicular_network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "UAV Attack Dataset (GPS jamming/spoofing) [35]",
        "type": "public",
        "domain": "gps_signals",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "ToN_IoT",
        "type": "public",
        "domain": "iot_network_and_system_logs",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Ns-3 FANET jamming dataset (3,000 samples; PDR/throughput/RSSI) [30]",
        "type": "synthetic",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Central IDS (C-IDS)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Local IDS (L-IDS)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Centralized IDSs for FANETs suffer from computation/storage overheads, latency, and single point of failure; they also pose privacy risks.",
        "Most prior IDS works train/evaluate on datasets not suited for FANETs; lack of realistic FANET datasets capturing 3D high-speed mobility and frequent link disruptions.",
        "Existing FANET attack datasets often use small networks, 2D mobility, or low speeds inadequate for FANET realism.",
        "Few works apply federated learning to FANETs, and none target routing attacks (sinkhole/blackhole/flooding) in realistic 3D, highly dynamic scenarios."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Enable privacy-preserving, communication-efficient IDS for highly dynamic and resource-constrained FANETs by avoiding raw data sharing and addressing the lack of realistic datasets.",
      "potential_research_ideas": [
        "Add secure aggregation (e.g., SMPC) and differential privacy to FL-IDS; empirically evaluate privacy leakage (e.g., membership/gradient inversion attacks).",
        "Personalized FL for non-IID client data (e.g., FedPer, FedProx, local adaptation layers) to boost per-UAV detection performance.",
        "Asynchronous or partial client participation FL to handle intermittent connectivity and link breakages in FANETs.",
        "Communication-efficient FL via gradient sparsification, quantization, and update compression tailored to UAV bandwidth/energy constraints.",
        "Graph-based models (e.g., GNNs) leveraging network topology and routing paths to detect routing attacks more accurately.",
        "Online/continual learning to handle concept drift as UAV missions, environments, and attacker strategies change over time.",
        "Robust aggregation (e.g., coordinate-wise median, Krum) and byzantine-resilient methods to defend against poisoned/malicious clients.",
        "Multimodal fusion of routing, PHY/MAC telemetry (RSSI, TDoA), and GPS/IMU to improve detection under varied attack types.",
        "Real-world testbed deployment on embedded UAV hardware with energy, latency, and throughput profiling.",
        "Benchmarking across multiple realistic FANET simulators and flight logs to standardize evaluation protocols for FL-based IDS."
      ],
      "architectural_improvement_recommendations": [
        "Integrate robust and adaptive aggregation (FedProx/FedNova/robust-averaging) combined with BTSC to mitigate non-IID skew and malicious clients.",
        "Introduce a lightweight personalization head on each UAV (shared backbone + local head) to balance global generalization and local specialization.",
        "Employ model compression (pruning, 8-bit quantization) and periodicity control for uplink updates to reduce communication/energy costs.",
        "Adopt asynchronous FL with staleness-aware weighting to tolerate connectivity disruptions in FANETs.",
        "Augment features with temporal context (sequence models) or topology-aware encodings to better capture routing dynamics under attacks.",
        "Add secure aggregation and optional differential privacy noise calibrated to maintain utility while enhancing privacy guarantees."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "FANET with a central aggregator at a Ground Base Station or a stable UAV cluster head",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Frequent link breakages due to high UAV speeds",
        "Limited battery and compute resources on UAVs",
        "Communication bandwidth constraints",
        "Single point of failure risk in centralized IDS (mitigated by FL)",
        "Privacy concerns with raw data sharing (mitigated by FL)"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": true,
      "fairness_bias": false
    },
    "contributions": [
      "Novel federated learning-based IDS (FL-IDS) for FANETs that enables collaborative learning without raw data sharing and reduces communication costs.",
      "Introduction of a realistic FANET dataset reflecting 3D node movement, local data collection, and routing attacks (sinkhole, blackhole, flooding).",
      "BTSC (Bias Towards Specific Clients) method to enhance FL-IDS performance, particularly at lower attacker ratios.",
      "Comparative analysis against Central IDS (C-IDS) and Local IDS (L-IDS) demonstrating competitive performance with improved privacy and efficiency."
    ]
  },
  {
    "arxiv_id": "2312.14607v1",
    "title": "ChatGPT, Llama, can you write my report? An experiment on assisted digital forensics reports written using (Local) Large Language Models",
    "authors": "Gaëtan Michelet; Frank Breitinger",
    "abstract": "Generative AIs, especially Large Language Models (LLMs) such as ChatGPT or Llama, have advanced significantly, positioning them as valuable tools for digital forensics. While initial studies have explored the potential of ChatGPT in the context of investigations, the question of to what extent LLMs can assist the forensic report writing process remains unresolved. To answer the question, this article first examines forensic reports with the goal of generalization (e.g., finding the `average structure' of a report). We then evaluate the strengths and limitations of LLMs for generating the different parts of the forensic report using a case study. This work thus provides valuable insights into the automation of report writing, a critical facet of digital forensics investigations. We conclude that combined with thorough proofreading and corrections, LLMs may assist practitioners during the report writing process but at this point cannot replace them.",
    "published_date": "2023-12-22",
    "pdf_link": "https://arxiv.org/pdf/2312.14607v1",
    "paper_types": [
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Digital Forensics",
      "subdomain": "Forensic Reporting Automation",
      "specific_problem": "Assisting the generation of forensic investigation reports with Large Language Models (LLMs)",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "ChatGPT (proprietary OpenAI LLM; exact version not specified)",
        "novel_contribution": "Empirical case-study evaluation of using ChatGPT to draft different sections of digital forensics reports"
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "Llama (local Large Language Model; exact variant not specified)",
        "novel_contribution": "Empirical case-study evaluation of using a local LLaMA model to draft report sections; overview of challenges specific to Local LLMs"
      }
    ],
    "learning_paradigm": [
      "In-context prompting (zero-shot/few-shot not specified)"
    ],
    "datasets": [
      {
        "name": "Student-generated forensic reports (≈100)",
        "type": "private",
        "domain": "report_documents",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Tool reports generated by forensic suites (e.g., Autopsy, Cellebrite UFED)",
        "type": "proprietary",
        "domain": "tool_reports",
        "link": "https://www.autopsy.com/",
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Mandates (prosecutor instructions) for cases",
        "type": "private",
        "domain": "case_documents",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Examiner lab logs",
        "type": "private",
        "domain": "case_documents",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Qualitative assessment of text quality",
      "Presence of hallucinations/inaccuracies",
      "Section-level LLM-potential rating (High/Medium/Low)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "To what extent can (local) large language models assist forensic report writing?",
        "Which sections or elements in a forensic report are suitable for creation through the use of (local) large language models?"
      ],
      "gaps_identified": [
        "Limited assistance/automation for the reporting phase in digital forensics compared to collection and examination phases",
        "No common framework for evaluating and reporting scientific findings in digital forensics reports",
        "Prior LLM studies for digital forensics largely omit impact on report writing",
        "LLMs are not designed for high accuracy and suffer from hallucinations, a critical problem for forensic reporting"
      ],
      "limitations": [
        "Hallucinations and inaccuracies observed: “non-negligible amount of hallucinations and inaccuracies”",
        "Requires thorough human proofreading and corrections; LLMs cannot replace practitioners",
        "Complexity and variability of Results and Discussion sections limit full automation",
        "Reliance on private inputs (mandates, lab logs) and examiner expertise limits reproducibility/generalization",
        "Details of local LLM configurations and computational requirements not provided in the text excerpt"
      ],
      "future_work": [
        "“More research is required to assess the quality, accuracy, consistency, and risks of this technology for forensic report writing.”",
        "Develop more systematic evaluations of section-wise generation quality and safety",
        "Explore approaches to ground generations in tool reports and lab logs to reduce hallucinations",
        "Investigate standardized report structures/templates to improve automability"
      ],
      "motivation": "Mitigate forensic lab backlogs by assisting time-consuming report writing with LLMs while ensuring accuracy and reliability.",
      "potential_research_ideas": [
        "Create a benchmark and evaluation suite for LLM-assisted forensic report writing with gold-standard, legally sound references",
        "Develop retrieval-augmented generation (RAG) systems that ground LLM outputs in mandates, lab logs, and tool reports with inline citations",
        "Design verifiable generation methods (e.g., citation-required constrained decoding) to reduce hallucinations and improve evidential traceability",
        "Fine-tune or instruction-tune domain LLMs on anonymized forensic report corpora with strict privacy controls",
        "Build human-in-the-loop drafting tools with structured section templates and confidence/uncertainty indicators",
        "Develop methods to transform tool reports into structured knowledge graphs that LLMs can query for grounded writing",
        "Study legal admissibility and provenance tracking for LLM-generated text in forensic contexts, including audit trails and versioning",
        "Construct red-teaming/adversarial evaluation protocols specific to forensic reporting risks (misinterpretation, overclaiming)"
      ],
      "architectural_improvement_recommendations": [
        "Use RAG over structured extractions from tool reports and lab logs; require evidence-linked citations for each factual claim",
        "Adopt constrained decoding or template-guided generation for high-stakes sections (e.g., Items Received, Methodology)",
        "Implement function-calling/tool-use to fetch hashes, sizes, timestamps directly from parsed tool outputs",
        "Incorporate uncertainty estimation/calibration and highlight low-confidence statements for reviewer attention",
        "Enforce section schemas (JSON-first generation) and render to prose to maintain completeness and consistency",
        "Combine template/rule-based NLG for factual boilerplate with LLM rewriting for fluency; keep facts locked from alteration",
        "Local, air-gapped deployment with PII scrubbing and audit logging to meet privacy/legal requirements"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Hallucinations and inaccuracies can cause false allegations if unchecked",
        "Privacy/confidentiality constraints on sensitive case data, favoring local/offline models",
        "Lack of standardized report structure complicates automation",
        "Grounding outputs in evidence and ensuring provenance/auditability",
        "Legal admissibility concerns and need for human oversight",
        "High variability across cases and sections (especially Results/Discussion)"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "A thorough analysis of forensic reports to identify a consistent structure and content of sections",
      "Experimental results on the quality of LLM-generated text elements for the identified sections via a case study",
      "An overview of the challenges faced when using Local Large Language Models for report writing assistance",
      "Conclusion that LLMs can assist with proofreading and drafting but cannot replace practitioners at this time"
    ]
  },
  {
    "arxiv_id": "2311.06564v1",
    "title": "Seeing is Believing: A Federated Learning Based Prototype to Detect Wireless Injection Attacks",
    "authors": "Aadil Hussain; Nitheesh Gundapu; Sarang Drugkar; Suraj Kiran; J. Harshan; Ranjitha Prasad",
    "abstract": "Reactive injection attacks are a class of security threats in wireless networks wherein adversaries opportunistically inject spoofing packets in the frequency band of a client thereby forcing the base-station to deploy impersonation-detection methods. Towards circumventing such threats, we implement secret-key based physical-layer signalling methods at the clients which allow the base-stations to deploy machine learning (ML) models on their in-phase and quadrature samples at the baseband for attack detection. Using Adalm Pluto based software defined radios to implement the secret-key based signalling methods, we show that robust ML models can be designed at the base-stations. However, we also point out that, in practice, insufficient availability of training datasets at the base-stations can make these methods ineffective. Thus, we use a federated learning framework in the backhaul network, wherein a group of base-stations that need to protect their clients against reactive injection threats collaborate to refine their ML models by ensuring privacy on their datasets. Using a network of XBee devices to implement the backhaul network, experimental results on our federated learning setup shows significant enhancements in the detection accuracy, thus presenting wireless security as an excellent use-case for federated learning in 6G networks and beyond.",
    "published_date": "2023-11-11",
    "pdf_link": "https://arxiv.org/pdf/2311.06564v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Wireless Network Security",
      "subdomain": "Physical-layer Security and Authentication",
      "specific_problem": "Detection of reactive wireless injection (spoofing/impersonation) attacks at the base-station using PHY I/Q samples with secret-key assisted signaling and federated learning",
      "attack_types": [
        "reactive injection attacks",
        "impersonation",
        "spoofing"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "CNN",
        "specific": null,
        "novel_contribution": "Key-assisted preprocessing converts I/Q vectors to scatter-plot images that a lightweight CNN (3 conv layers with 256/128/64 filters, 3x3 kernels, ReLU, max-pooling, softmax) classifies as legitimate vs adversarial"
      },
      {
        "type": "primary",
        "category": "Federated Learning",
        "specific": "FedAvg",
        "novel_contribution": "Prototype FL over XBee backhaul to aggregate base-station CNN weights; demonstrates significant accuracy gains when local data is limited; claimed as first prototype applying FL to detect wireless injection attacks"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Federated"
    ],
    "datasets": [
      {
        "name": "Adalm Pluto SDR I/Q scatter-plot dataset (2-client setup)",
        "type": "proprietary",
        "domain": "radio_iq_samples",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Adalm Pluto SDR I/Q scatter-plot dataset (4-client setup)",
        "type": "proprietary",
        "domain": "radio_iq_samples",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Centralized union of local datasets (for reference centralized CNN)",
        "type": "proprietary",
        "domain": "radio_iq_samples",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Local CNN (no FL) vs FL (2-client setup)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "“after 10 FL rounds, base-station 1 … achieves roughly 93% accuracy, and base-station 2 … approaches 92%”",
        "baseline_result": "“initial average accuracy of 84.55% per client without FL”"
      },
      {
        "method_name": "Local CNN (no FL) vs FL (4-client setup)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "“After 15 rounds of FL, … ~91%, ~91.5%, ~92% and ~93.5%” (four clients)",
        "baseline_result": "“the average accuracy per base-station before implementing FL was 80.64%”"
      },
      {
        "method_name": "FL CNN vs Centralized CNN (2-client setup)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "~92–93% per client after 10 FL rounds",
        "baseline_result": "“centralized CNN yields 98% accuracy”"
      },
      {
        "method_name": "FL CNN vs Centralized CNN (4-client setup)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "~91–93.5% per client after 15 FL rounds",
        "baseline_result": "“testing with a centralized model … resulted in an accuracy rate of 98%”"
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "Precision",
      "Recall (Sensitivity)",
      "Specificity",
      "F1 Score"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can secret-key assisted PHY-layer signaling enable reliable ML classification of reactive injection attacks using I/Q samples at the base-station?",
        "Can federated learning across base-stations improve detection accuracy when local labeled data is insufficient while preserving data locality/privacy?",
        "What are the practical implementation aspects and latency/communication costs of FL over low-rate backhaul for wireless security use-cases?"
      ],
      "gaps_identified": [
        "Base-stations may lack sufficient labeled datasets locally, limiting standalone ML accuracy",
        "Centralized training raises privacy and latency concerns and may require cryptographic protections with overhead",
        "MAP detection is impractical due to unknown adversarial channel distribution"
      ],
      "limitations": [
        "High FL communication latency due to low-bandwidth XBee backhaul: “each round of FL consumed approximately 2.5 minutes … total 25 minutes over 10 rounds” (2 clients) and “approximately 5 minutes” per round (4 clients)",
        "Evaluation limited to lab SDR testbed (OOK signaling, specific dictionary, N=20, L=8) and scatter-plot image inputs",
        "No evaluation of robustness to adaptive/stronger attackers (e.g., key-guessing, mimicry) or varying channel conditions/mobility",
        "Privacy is assumed from FL but no formal privacy guarantees (e.g., differential privacy or secure aggregation) are implemented",
        "No code or dataset release; reproducibility depends on hardware setup details",
        "Potential information loss by converting I/Q vectors into scatter-plot images rather than learning directly from raw complex samples"
      ],
      "future_work": [
        "Use higher-rate backhaul to reduce FL latency to milliseconds in real-world settings",
        "Extend to more complex modulations, multi-antenna scenarios, and diverse environments",
        "Scale to more base-stations and study non-IID data effects in FL"
      ],
      "motivation": "Enhance detection of wireless reactive injection attacks at base-stations when local training data is limited, while preserving privacy and avoiding centralized latency via federated learning.",
      "potential_research_ideas": [
        "Learn directly from raw complex I/Q sequences using complex-valued CNNs or 1D temporal CNN/RNN/Transformer models to avoid scatter-plot conversion",
        "Incorporate secure aggregation and differential privacy into FL to provide formal privacy guarantees for wireless security data",
        "Communication-efficient FL for low-rate backhaul (sparsification, quantization, error-feedback, periodic averaging, sketching)",
        "Personalized FL (e.g., FedProx, pFedMe, FedPer) to handle non-IID channel/attack distributions across base-stations",
        "Adversarially robust training against adaptive attackers that attempt to emulate key-induced distributions or poison FL updates",
        "Semi/self-supervised pretraining on large unlabeled I/Q data to reduce labeled data dependency at base-stations",
        "Hardware-in-the-loop real-time implementation on SDR/FPGA with lightweight architectures (MobileNet/ResNet variants, pruning/distillation)",
        "Domain adaptation/meta-learning for cross-environment generalization across SNRs, channels, and mobility",
        "Broaden threat model to include jamming, replay, and protocol-level spoofing; build a public benchmark dataset"
      ],
      "architectural_improvement_recommendations": [
        "Replace scatter-plot image input with models operating on raw I/Q tensors (N x 2) using complex-valued convolutions or 1D CNNs with attention",
        "Adopt lightweight backbones (MobileNetV3, EfficientNet-Lite) with pruning/quantization for on-device inference",
        "Add data augmentation simulating channel impairments (fading, CFO, IQ imbalance, SNR variations) and label smoothing to improve generalization",
        "Use advanced FL optimizers (FedAvgM, FedProx, Scaffold) and model personalization to handle non-IID data",
        "Implement gradient/weight compression and secure aggregation to reduce backhaul load and improve privacy",
        "Explore early-exit networks for adaptive latency and energy saving at base-stations"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "TensorFlow 2.11.0",
        "Python",
        "MATLAB/Simulink",
        "digi-XBee v1.4.1"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Lightweight CNN with 7,253 parameters; trained on standard PCs (Windows 11 / Ubuntu 20.04). FL rounds took ~2.5 minutes (2-client) and ~5 minutes (4-client) per round over XBee backhaul due to low bandwidth."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Lab prototype: Adalm Pluto SDRs for fronthaul (Alice/Bob/Dave) and XBee S2C devices for backhaul FL (server as coordinator, base-stations as routers)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Low backhaul bandwidth causing large FL round times",
        "Need for higher-rate backhaul and potential TDMA scheduling",
        "Secret-key distribution and management between client and base-station",
        "Generalization across varying channel conditions and SNRs",
        "Synchronizing SDRs and reliable over-the-air parameter exchange"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": true,
      "fairness_bias": false
    },
    "contributions": [
      "Secret-key based PHY-layer signaling to induce discriminative I/Q patterns robust to reactive injection attacks",
      "Key-assisted preprocessing and CNN-based classifier on scatter-plot images for attack detection at the base-station",
      "Federated learning prototype over XBee backhaul (FedAvg) to improve accuracy while keeping data local",
      "SDR testbed implementation with Adalm Pluto radios and experimental validation showing significant FL-driven accuracy improvements",
      "Empirical comparison of local, federated, and centralized training, with centralized accuracy reported at 98%"
    ]
  },
  {
    "arxiv_id": "2311.16383v1",
    "title": "\"Do Users fall for Real Adversarial Phishing?\" Investigating the Human response to Evasive Webpages",
    "authors": "Ajka Draganovic; Savino Dambra; Javier Aldana Iuit; Kevin Roundy; Giovanni Apruzzese",
    "abstract": "Phishing websites are everywhere, and countermeasures based on static blocklists cannot cope with such a threat. To address this problem, state-of-the-art solutions entail the application of machine learning (ML) to detect phishing websites by checking if they visually resemble webpages of well-known brands. These techniques have achieved promising results in research and, consequently, some security companies began to deploy them also in their phishing detection systems (PDS). However, ML methods are not perfect and some samples are bound to bypass even production-grade PDS.   In this paper, we scrutinize whether 'genuine phishing websites' that evade 'commercial ML-based PDS' represent a problem \"in reality\". Although nobody likes landing on a phishing webpage, a false negative may not lead to serious consequences if the users (i.e., the actual target of phishing) can recognize that \"something is phishy\". Practically, we carry out the first user-study (N=126) wherein we assess whether unsuspecting users (having diverse backgrounds) are deceived by 'adversarial' phishing webpages that evaded a real PDS. We found that some well-crafted adversarial webpages can trick most participants (even IT experts), albeit others are easily recognized by most users. Our study is relevant for practitioners, since it allows prioritizing phishing webpages that simultaneously fool (i) machines and (ii) humans -- i.e., their intended targets.",
    "published_date": "2023-11-28",
    "pdf_link": "https://arxiv.org/pdf/2311.16383v1",
    "paper_types": [
      "new_dataset",
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Web Security",
      "subdomain": "Phishing Website Detection",
      "specific_problem": "Assessing human susceptibility to phishing webpages that evade ML/DL-based phishing detection systems (visual-similarity models)",
      "attack_types": [
        "Phishing",
        "Adversarial Evasion",
        "Social Engineering"
      ]
    },
    "ml_techniques": [
      {
        "type": "baseline",
        "category": "Deep Learning - visual similarity (image-based webpage matching)",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Active Learning (human-in-the-loop triage/queuing of uncertain cases)",
        "specific": null,
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Active Learning"
    ],
    "datasets": [
      {
        "name": "Adversarial Webpages (AW) screenshots",
        "type": "public",
        "domain": "web_page_screenshots",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "Sigma brand website dataset (websites associated to tracked brands)",
        "type": "proprietary",
        "domain": "brand_website_repository",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Proportion of users deceived (deception rate)",
      "Recognition/suspicion rate (users identifying pages as phishy)",
      "Statistical significance testing (§V-A)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How do users respond to phishing webpages that evaded a ML-based PDS?"
      ],
      "gaps_identified": [
        "Disconnection between human-centered and machine-centered anti-phishing research: \"papers that propose novel PDS tend to overlook how humans respond to those webpages that bypassed the proposed PDS; whereas papers that focus on the human perception of phishing websites do not consider webpages that evaded operational PDS.\"",
        "No prior user-studies used deployed ML misclassifications (Table I: Deployed ML misclassifications column shows no prior work meeting this criterion).",
        "Priming bias in prior user-studies: many studies prime participants about phishing, which can bias results (users expect phishing and are less likely to fall for it).",
        "Operational constraint: PDS must minimize false positives, leading to significant false negatives (e.g., \"a security company had over 9k 'evasions' in just one month [10]\").",
        "Limited understanding of which false negatives actually deceive end users versus those easily recognized as phishy."
      ],
      "limitations": [
        "Study uses screenshots of webpages (\"screenshots—in full HD resolution\"), not live, interactive webpages; interaction cues and browser UI signals may be absent.",
        "Data and PDS come from a single commercial provider (\"Sigma\"), potentially limiting generalizability across PDS designs.",
        "Focus tailored to Europe (brands and participant base), which may impact external validity to other regions.",
        "Adversarial webpages were selected from the PDS uncertain queue (mid-confidence band), not necessarily representing all false negatives.",
        "Details of the DL models and thresholds are proprietary (NDA), limiting replicability of the detection pipeline.",
        "Participants were not primed; while realistic, it may introduce variability in attention and context not controlled by the study design."
      ],
      "future_work": [
        "Use findings to prioritize and fix PDS errors for adversarial webpages that deceive most users: \"identify the AW that deceive most users, and then fixing PDS so that such AW are not misclassified.\"",
        "Develop and evaluate improvements to operational PDS informed by human deception patterns (§VII-B).",
        "Expand to other phishing vectors (e.g., emails) and other regions once data is available.",
        "Broaden to multiple PDS architectures (URL/HTML feature-based, hybrid) to assess generality."
      ],
      "motivation": "Bridge the gap between machine-centered detection and human-centered resilience by determining whether false negatives from real, ML-based PDS actually deceive users, thereby informing prioritization of defender resources.",
      "potential_research_ideas": [
        "Human-weighted training objectives: incorporate human deception likelihood into loss functions to prioritize reducing errors that fool users.",
        "Hard-negative mining from user-deceiving AW to improve visual-similarity models and calibrate thresholds per brand.",
        "Multimodal phishing detection combining visual similarity with DOM/HTML, URL, hosting, and brand-graph features for robustness.",
        "Browser-context-aware detection leveraging page interactivity cues, certificate data, and UI indicators absent in screenshots.",
        "Robust logo/brand verification via synthetic augmentation and adversarial training targeting imperceptible visual changes.",
        "Active-learning policies that sample uncertain cases most likely to deceive users (human-in-the-loop prioritization).",
        "Explainable phishing detectors that highlight visual/structural cues humans use, to aid analyst triage and user warnings.",
        "Cross-PDS comparative study to quantify generalization of human deception across different detection pipelines."
      ],
      "architectural_improvement_recommendations": [
        "Adopt a hybrid detector: fuse visual-similarity CNN outputs with URL/HTML/DOM features and brand-specific heuristics to reduce evasions.",
        "Calibrate per-brand decision thresholds using historical deception rates; apply cost-sensitive classification weighted by user deception risk.",
        "Integrate a deception-aware active learning loop that prioritizes samples similar to historically human-deceiving AW.",
        "Introduce robust training via adversarial augmentations (logo layout shifts, subtle color/spacing perturbations) and hard-negative replay.",
        "Deploy post-decision sanity checks (e.g., login workflow verification, form action domain checks, certificate issuer/SubjectAltName checks) for borderline cases.",
        "Improve uncertainty estimation (e.g., deep ensembles or temperature scaling) to better route mid-confidence cases to manual review.",
        "Leverage perceptual hashing and layout graph matching to capture template-level similarities beyond pixel-space features."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Commercial phishing detection system integrated with consumer endpoint/browser protection (Sigma)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Balancing false positives (user friction) vs. false negatives (missed phishing) in production.",
        "Thousands of evasions per month in real operations, requiring prioritization of analyst effort.",
        "High false positive rate concerns have slowed ML/DL integration into operational PDS.",
        "Easily crafted perturbations can evade visual-similarity models while remaining obvious to some users.",
        "Need for manual operator review queues and efficient active-learning pipelines."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": true,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "First user-study elucidating the response of humans to real phishing webpages that evaded a real phishing detection system based on deep learning.",
      "Quantitative validation via statistical tests (§V-A) and qualitative validation alongside practitioners (§VI-C), with recommendations for research (§VII-B).",
      "Practical insights on operational PDS behavior and guidance on improving them; sharing of phishing data [13].",
      "Bridges human- and machine-centered perspectives to inform prioritization of false negatives that both bypass machines and deceive users."
    ]
  },
  {
    "arxiv_id": "2312.13737v1",
    "title": "An Approach to Abstract Multi-stage Cyberattack Data Generation for ML-Based IDS in Smart Grids",
    "authors": "Ömer Sen; Philipp Malskorn; Simon Glomb; Immanuel Hacker; Martin Henze; Andreas Ulbig",
    "abstract": "Power grids are becoming more digitized, resulting in new opportunities for the grid operation but also new challenges, such as new threats from the cyber-domain. To address these challenges, cybersecurity solutions are being considered in the form of preventive, detective, and reactive measures. Machine learning-based intrusion detection systems are used as part of detection efforts to detect and defend against cyberattacks. However, training and testing data for these systems are often not available or suitable for use in machine learning models for detecting multi-stage cyberattacks in smart grids. In this paper, we propose a method to generate synthetic data using a graph-based approach for training machine learning models in smart grids. We use an abstract form of multi-stage cyberattacks defined via graph formulations and simulate the propagation behavior of attacks in the network. Within the selected scenarios, we observed promising results, but a larger number of scenarios need to be studied to draw a more informed conclusion about the suitability of synthesized data.",
    "published_date": "2023-12-21",
    "pdf_link": "https://arxiv.org/pdf/2312.13737v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "ICS/Smart Grid Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Synthetic generation of abstract IDS alert logs to train ML-based IDS for detecting multi-stage cyberattacks in smart grids",
      "attack_types": [
        "multi-stage cyberattacks",
        "reconnaissance",
        "initial access/exploitation",
        "lateral movement",
        "persistence",
        "collection/exfiltration"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Knowledge Graph / Graph-based Simulation",
        "specific": "RDF + SHACL-driven knowledge graph for network and attack modeling; SPARQL-based simulation",
        "novel_contribution": "Abstract multi-stage attack and network modeling with a knowledge graph to generate IDS alert logs aligned to MITRE ATT&CK ICS tactics/techniques, including controllable FP/FN rates"
      }
    ],
    "learning_paradigm": [],
    "datasets": [
      {
        "name": "Synthetic IDS alert logs generated by the proposed framework (smart grid multi-stage attacks)",
        "type": "synthetic",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "CICIDS2017",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "UNSW-NB15",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CIDDS-001",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "MITRE ATT&CK for ICS (knowledge base, used for tactics/techniques mapping)",
        "type": "public",
        "domain": "threat_intelligence",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How to replicate smart grids with all layers relevant for attack data synthesis while considering multi-stage patterns?",
        "How to execute multi-stage attacks that replicate consistent and flexible attack patterns to increase dataset diversity?",
        "How to generate adequate datasets using processable formats and consistent data structures based on use cases?"
      ],
      "gaps_identified": [
        "Lab-generated datasets are costly, inflexible, and hard to scale; they are not easily modifiable without re-running experiments.",
        "Attack data are often underrepresented relative to normal traffic in recorded datasets.",
        "Privacy and security restrictions hinder access to real attack data for ML training and validation.",
        "Limited diversity and availability of public datasets forces evaluation on a single dataset, risking poor generalization to real networks.",
        "Need for reproducible multi-stage attack sequences and integrated representations that can be automated."
      ],
      "limitations": [
        "The evaluation covers only selected scenarios; a larger number of scenarios are needed to conclude on the suitability of synthesized data.",
        "Quality of synthetic data can suffer from inaccurate modeling and simulation compared to laboratory data.",
        "No public release of code or datasets is indicated in the provided content."
      ],
      "future_work": [
        "Study a larger number and diversity of scenarios to assess suitability of synthesized data.",
        "Broaden comparisons with real datasets and operational logs to calibrate the generator.",
        "Evaluate training of ML-based IDS using the generated data and assess generalization."
      ],
      "motivation": "Training and testing data suitable for ML models to detect multi-stage cyberattacks in smart grids are often unavailable or unsuitable; synthetic, flexible, reproducible multi-stage attack data are needed.",
      "potential_research_ideas": [
        "Integrate physical-process (power system) simulators with the cyber generator to create cyber-physical multi-stage datasets linking ICS process anomalies to cyber alerts.",
        "Learn probabilistic attack progression models (e.g., HMMs or dynamic Bayesian networks) from real incident reports to parameterize the multi-stage generator.",
        "Add time-series traffic synthesis (flows/pcaps) conditioned on the abstract alerts to support sequence models (e.g., LSTMs, Transformers).",
        "Calibrate FP/FN generation using empirical IDS performance curves per sensor type/placement and per tactic/technique.",
        "Extend the knowledge graph with preconditions/observables for ATT&CK techniques and automated SAT/constraint solving to generate consistent multi-step campaigns.",
        "Create a benchmark suite of standardized smart grid topologies and attack playbooks for evaluating ML-based IDS trained on the synthetic alerts.",
        "Use generative models (e.g., diffusion/GANs) to refine alert attributes for realism while preserving MITRE-aligned labels."
      ],
      "architectural_improvement_recommendations": [
        "Introduce an explicit temporal model for attack stages (timestamps, dwell time distributions) to better support sequence learning.",
        "Model multiple heterogeneous sensors (NIDS/HIDS, different placements) and sensor fusion to produce multi-source alerts.",
        "Parameterize IDS detection probabilities by tactic/technique, sensor type, and network path characteristics; support ROC-based sampling of FP/FN.",
        "Add a traffic/flow synthesis back-end to emit NetFlow/pcap alongside alert logs for end-to-end IDS evaluation.",
        "Adopt STIX 2.1/CybOX for threat objects and MITRE ATT&CK technique objects to improve interoperability.",
        "Provide scenario templates and APIs to compose attacks using reusable technique blocks with SHACL validation of pre/post-conditions."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "Python",
        "rdflib",
        "pySHACL",
        "SPARQL",
        "RDF/Turtle"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "CPU-only; no additional hardware/virtualization required by design (N1/N2); generation completed in a reasonable time frame per authors."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Calibrating synthetic alerts to match real IDS behavior and base rates in operational smart grids",
        "Modeling realistic FP/FN characteristics across sensors and placements",
        "Ensuring fidelity of network/attack abstractions without access to sensitive operational data",
        "Interoperability with existing SOC/SIEM tooling and IDS alert schemas"
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "State-of-the-art overview and challenges of synthesizing multi-stage cyberattack data for smart grids.",
      "A knowledge-graph-based approach (RDF/SHACL, SPARQL) to model networks and simulate multi-stage attacks aligned with MITRE ATT&CK for ICS.",
      "An abstract IDS alert log schema mapping alerts to MITRE tactics/techniques with controllable FP/FN generation.",
      "Case studies indicating promising dataset generation capabilities (with note that more scenarios are required for firm conclusions)."
    ]
  },
  {
    "arxiv_id": "2312.13711v1",
    "title": "A Learning oriented DLP System based on Classification Model",
    "authors": "Kishu Gupta; Ashwani Kush",
    "abstract": "Data is the key asset for organizations and data sharing is lifeline for organization growth; which may lead to data loss. Data leakage is the most critical issue being faced by organizations. In order to mitigate the data leakage issues data leakage prevention systems (DLPSs) are deployed at various levels by the organizations. DLPSs are capable to protect all kind of data i.e. DAR, DIM/DIT, DIU. Statistical analysis, regular expression, data fingerprinting are common approaches exercised in DLP system. Out of these techniques; statistical analysis approach is most appropriate for proposed DLP model of data security. This paper defines a statistical DLP model for document classification. Model uses various statistical approaches like TF-IDF (Term Frequency- Inverse Document Frequency) a renowned term count/weighing function, Vectorization, Gradient boosting document classification etc. to classify the documents before allowing any access to it. Machine learning is used to test and train the model. Proposed model also introduces an extremely efficient and more accurate approach; IGBCA (Improvised Gradient Boosting Classification Algorithm); for document classification, to prevent them from possible data leakage. Results depicts that proposed model can classify documents with high accuracy and on basis of which data can be prevented from being loss.",
    "published_date": "2023-12-21",
    "pdf_link": "https://arxiv.org/pdf/2312.13711v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Data Security",
      "subdomain": "Data Loss Prevention (DLP)",
      "specific_problem": "Content-based document sensitivity classification for access control (unrestricted, internal, restricted)",
      "attack_types": [
        "data leakage/exfiltration",
        "insider data leakage"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Ensemble/Gradient Boosting",
        "specific": "Improvised Gradient Boosting Classification Algorithm (IGBCA)",
        "novel_contribution": "Improvised pipeline around Gradient Boosting using TF-IDF, SelectKBest(chi2), StratifiedKFold, and RandomizedSearchCV to improve accuracy over standard GBCA for document sensitivity classification"
      },
      {
        "type": "baseline",
        "category": "Ensemble/Gradient Boosting",
        "specific": "Gradient Boosting Classifier (GBCA)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Feature Extraction/Vectorizer",
        "specific": "TF-IDF (TfidfVectorizer)",
        "novel_contribution": "Used as core statistical representation of text for DLP classification"
      },
      {
        "type": "primary",
        "category": "Vectorizer",
        "specific": "Bag-of-Words (CountVectorizer)",
        "novel_contribution": "Tokenization to build sparse term-document matrix"
      },
      {
        "type": "primary",
        "category": "Feature Selection",
        "specific": "SelectKBest (chi2)",
        "novel_contribution": "Reduces dimensionality to mitigate overfitting and improve accuracy"
      },
      {
        "type": "primary",
        "category": "Model Selection/Cross-Validation",
        "specific": "StratifiedKFold",
        "novel_contribution": "Handles limited labeled data via stratified K-fold CV"
      },
      {
        "type": "primary",
        "category": "Hyperparameter Optimization",
        "specific": "RandomizedSearchCV",
        "novel_contribution": "Tuning within the pipeline to improve classification accuracy"
      },
      {
        "type": "primary",
        "category": "Text Preprocessing",
        "specific": "NLTK-based preprocessing (lowercasing, punctuation removal, stopword removal, stemming/lemmatization)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "In-house labeled document corpus (restricted/internal/unrestricted)",
        "type": "private",
        "domain": "documents_text",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Standard Gradient Boosting Classifier (GBCA)",
        "paper_reference": null,
        "metric": "accuracy",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Sensitivity (Recall for positive class)",
      "Specificity (True Negative Rate)",
      "Precision",
      "Recall",
      "F1-score",
      "Accuracy",
      "Error rate"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Conventional fingerprinting (common hashing) is brittle to small data modifications; advanced fingerprinting incurs high storage and computation due to extreme indexing (from related work).",
        "Prior ML work using SVM classified only sensitive vs. non-sensitive and not multi-level classes like secret/top secret/confidential (from related work).",
        "Plain TF-IDF similarity detects only very generic documents (from related work).",
        "Limited labeled data makes achieving high accuracy difficult; necessitates stratified K-fold CV and tuning."
      ],
      "limitations": [
        "Relies on a private, limited labeled dataset; no public data provided.",
        "Focuses on content-based statistical features (BoW/TF-IDF); contextual DLP signals (sender/receiver, channel, device) not integrated in the presented model.",
        "No adversarial robustness analysis against evasion through text obfuscation or paraphrasing.",
        "No runtime or scalability evaluation beyond noting sparse matrix complexity.",
        "No external validation or cross-organization generalization study."
      ],
      "future_work": [],
      "motivation": "Rising organizational data leaks and limitations of existing DLP techniques motivate a statistical ML approach that classifies documents by sensitivity to control access and prevent data loss.",
      "potential_research_ideas": [
        "Integrate contextual DLP features (sender/receiver, channel, device, time, file type) with content features for joint decision-making.",
        "Leverage pre-trained language models (e.g., BERT, RoBERTa) for semantic representation to reduce false positives/negatives versus BoW/TF-IDF.",
        "Semi-supervised and active learning to reduce labeled data needs in enterprise settings.",
        "Robustness against obfuscation: adversarial training and data augmentation with paraphrases, masking, and typos.",
        "Online/continual learning to handle concept drift in organizational vocabularies.",
        "Cost-sensitive and calibrated classification to align with business risk and minimize costly false positives.",
        "Explainable DLP decisions using token/phrase-level attributions to aid security analysts and reduce user friction.",
        "Federated learning or privacy-preserving training across subsidiaries without centralizing sensitive documents.",
        "Multi-modal DLP: combine OCR for images/PDFs and layout-aware models (e.g., LayoutLM) with text classification.",
        "Rule-ML hybrid: combine regex/fingerprints with ML confidence for high-precision enforcement."
      ],
      "architectural_improvement_recommendations": [
        "Replace BoW/TF-IDF with transformer embeddings (e.g., Sentence-BERT) and fine-tune a lightweight classifier.",
        "Adopt a two-stage architecture: fast high-recall filter (statistical) followed by precise semantic re-ranker (transformer).",
        "Incorporate metadata/context features via feature fusion (wide-and-deep) or late fusion ensembles.",
        "Use class-weighting/focal loss and threshold calibration to optimize for asymmetric costs.",
        "Apply nested cross-validation with grouped splits by document source to prevent leakage.",
        "Add adversarial preprocessing defenses (e.g., normalization, synonym canonicalization) and adversarial training.",
        "Implement incremental/streaming TF-IDF or hashing trick for scalable deployment; evaluate sparse vs. dense trade-offs.",
        "Provide human-in-the-loop feedback loop for continuous model improvement."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "scikit-learn",
        "NLTK"
      ],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Sparse matrix representations can be computationally expensive in space/time.",
        "Limited labeled data availability for training in enterprise settings."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes a learning-oriented DLP model that classifies documents into unrestricted, internal, and restricted classes using statistical text features.",
      "Introduces IGBCA (Improvised Gradient Boosting Classification Algorithm) as an improved training/tuning pipeline over standard GBCA.",
      "End-to-end architecture and workflow with preprocessing (NLTK), BoW, TF-IDF, feature selection, and gradient boosting classification.",
      "Validation framework detailing sensitivity, specificity, precision, recall, F1-score, accuracy, and error rate.",
      "Reports that the proposed model achieves high accuracy on the private dataset and improves over standard GBCA."
    ]
  },
  {
    "arxiv_id": "2311.06938v1",
    "title": "5G Networks and IoT Devices: Mitigating DDoS Attacks with Deep Learning Techniques",
    "authors": "Reem M. Alzhrani; Mohammed A. Alliheedi",
    "abstract": "The development and implementation of Internet of Things (IoT) devices have been accelerated dramatically in recent years. As a result, a super-network is required to handle the massive volumes of data collected and transmitted to these devices. Fifth generation (5G) technology is a new, comprehensive wireless technology that has the potential to be the primary enabling technology for the IoT. The rapid spread of IoT devices can encounter many security limits and concerns. As a result, new and serious security and privacy risks have emerged. Attackers use IoT devices to launch massive attacks; one of the most famous is the Distributed Denial of Service (DDoS) attack. Deep Learning techniques have proven their effectiveness in detecting and mitigating DDoS attacks. In this paper, we applied two Deep Learning algorithms Convolutional Neural Network (CNN) and Feed Forward Neural Network (FNN) in dataset was specifically designed for IoT devices within 5G networks. We constructed the 5G network infrastructure using OMNeT++ with the INET and Simu5G frameworks. The dataset encompasses both normal network traffic and DDoS attacks. The Deep Learning algorithms, CNN and FNN, showed impressive accuracy levels, both reaching 99%. These results underscore the potential of Deep Learning to enhance the security of IoT devices within 5G networks.",
    "published_date": "2023-11-12",
    "pdf_link": "https://arxiv.org/pdf/2311.06938v1",
    "paper_types": [
      "new_dataset",
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "IoT Security",
      "subdomain": "DDoS Detection",
      "specific_problem": "Detecting and mitigating DDoS attacks in 5G networks for IoT devices using deep learning on simulated network traffic",
      "attack_types": [
        "DDoS",
        "Bandwidth depletion (volume-based)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "CNN",
        "specific": "1D CNN with three Conv1D layers (filters: 64, 32, 16; kernel sizes: 8, 16, 3) + 1D MaxPooling layers, dropout, dense(ReLU) -> dense(sigmoid)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Feedforward Neural Network",
        "specific": "Three hidden fully connected layers (64, 32, 1 units) with sigmoid output",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "5G network dataset (OMNeT++/Simu5G) for IoT DDoS",
        "type": "synthetic",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Accuracy",
      "Precision",
      "Recall",
      "F1 Score",
      "Detection Rate",
      "False Alarm Rate",
      "Confusion Matrix"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Need for datasets tailored to IoT DDoS within 5G network contexts; authors \"present a novel 5G dataset\" generated via OMNeT++/Simu5G.",
        "Implementing security on IoT devices is difficult due to heterogeneous and constrained devices."
      ],
      "limitations": [
        "Evaluation only on a simulated 5G IoT network; no real-world deployment or traces.",
        "Single DDoS scenario based on high-rate, large-packet flood from three hosts; limited attack diversity and unspecified protocol mix.",
        "No cross-dataset validation or generalization study.",
        "Only two deep learning models (CNN, FNN) evaluated; no comparison against classical ML or state-of-the-art methods on the same data.",
        "Limited hyperparameter search; epochs fixed at 10 and learning rate 0.001.",
        "Significant preprocessing that drops many columns with missing values; potential loss of information.",
        "No public release/link for the dataset or code, limiting reproducibility."
      ],
      "future_work": [
        "In future work, we can try different models apart from those used in our study.",
        "We also recommend using the NETA framework within OMNeT++.",
        "Moreover, we suggest creating complex scenarios using the OMNeT++ frameworks. For example, a group of drones that cover various locations. These drones can be moved using the X-Plane software. These drones can potentially be exploited to generate DDoS attacks.",
        "To effectively detect and mitigate these attacks, we suggest using the TensorFlow framework integrated within OMNeT++."
      ],
      "motivation": "Leverage deep learning (CNN, FNN) to detect and mitigate DDoS attacks targeting IoT devices in 5G networks; construct a 5G network in OMNeT++ (INET, Simu5G) to generate a labeled dataset containing normal and attack traffic and evaluate DL models.",
      "potential_research_ideas": [
        "Develop and release a standardized, publicly available 5G-IoT DDoS benchmark with multiple attack families (protocol-based, application-layer, reflection/amplification) and varying intensities.",
        "Investigate zero-day and cross-network generalization using unsupervised/semisupervised methods (autoencoders, contrastive learning) and domain adaptation.",
        "Online/streaming detection at 5G edge (MEC) with concept drift handling and incremental learning.",
        "Adversarial robustness for DL-based DDoS detectors in 5G settings; evaluate and harden against evasion/poisoning.",
        "Explainable detection tailored to operators (feature attributions for flows, SHAP/Integrated Gradients) to support incident response.",
        "Energy- and resource-aware models deployable on constrained IoT gateways (TinyML, pruning, quantization).",
        "Graph-based detection using GNNs over communication graphs within 5G slices; combine with temporal models for sequence behavior.",
        "Cross-layer feature fusion (radio KPIs, core network telemetry, flow/session metadata) for improved detection fidelity.",
        "Data augmentation and simulation realism improvements (protocol mix, encrypted traffic, realistic background loads) to close sim-to-real gaps."
      ],
      "architectural_improvement_recommendations": [
        "Augment 1D CNN with temporal models (CNN+LSTM/TCN/Transformer) to capture bursty and long-range traffic patterns.",
        "Adopt multi-branch architectures separating per-flow features and aggregated statistics with attention-based fusion.",
        "Introduce unsupervised anomaly detection (autoencoders, Deep SVDD) in parallel to supervised detectors for zero-day coverage.",
        "Calibrate outputs and estimate uncertainty (temperature scaling, ensembling) to reduce false alarms in operations.",
        "Stratified cross-validation and robust evaluation (ROC/PR curves, calibration metrics) and ablations on preprocessing and feature subsets.",
        "Model compression (pruning, quantization) and knowledge distillation for edge deployment on IoT gateways/MEC.",
        "Richer feature engineering (flow-level aggregation windows, inter-arrival times, entropy of destinations) from packet/flow traces.",
        "Pipeline integration at MEC with batching and async inference; monitor latency/throughput and backpressure handling."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "OMNeT++",
        "Simu5G",
        "INET"
      ],
      "reproducibility_score": "low",
      "computational_requirements": "1D CNN chosen for faster training and no dedicated GPU requirement; training run with 10 epochs and learning rate 0.001."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Implementing security on heterogeneous and constrained IoT devices is difficult.",
        "Bridging simulation-to-real-world differences in 5G traffic and attack behavior."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "We use the OMNeT++ simulation tool with the Simu5G framework to generate a dataset for 5G networks involving IoT devices.",
      "We applied two Deep Learning algorithms, CNN and FNN, using the specifically released dataset for the 5G network, which includes DDoS attacks and normal data traffic.",
      "We evaluate the performance of models using a confusion matrix.",
      "Dataset details: \"We compiled a dataset that consists of 512,666 samples, including both benign data and DDoS attacks, with 16 features... The count of benign samples reached 256,354, while the count of DDoS attacks amounted to 256,312.\"",
      "Results (test set): CNN — Accuracy 99.74%, Precision 99.87%, Recall 99.61%, F1 99.74%; FNN — Accuracy 99.53%, Precision 99.53%, Recall 99.54%, F1 99.53%."
    ]
  },
  {
    "arxiv_id": "2312.13119v2",
    "title": "Graphene: Infrastructure Security Posture Analysis with AI-generated Attack Graphs",
    "authors": "Xin Jin; Charalampos Katsis; Fan Sang; Jiahao Sun; Elisa Bertino; Ramana Rao Kompella; Ashish Kundu",
    "abstract": "The rampant occurrence of cybersecurity breaches imposes substantial limitations on the progress of network infrastructures, leading to compromised data, financial losses, potential harm to individuals, and disruptions in essential services. The current security landscape demands the urgent development of a holistic security assessment solution that encompasses vulnerability analysis and investigates the potential exploitation of these vulnerabilities as attack paths. In this paper, we propose Graphene, an advanced system designed to provide a detailed analysis of the security posture of computing infrastructures. Using user-provided information, such as device details and software versions, Graphene performs a comprehensive security assessment. This assessment includes identifying associated vulnerabilities and constructing potential attack graphs that adversaries can exploit. Furthermore, Graphene evaluates the exploitability of these attack paths and quantifies the overall security posture through a scoring mechanism. The system takes a holistic approach by analyzing security layers encompassing hardware, system, network, and cryptography. Furthermore, Graphene delves into the interconnections between these layers, exploring how vulnerabilities in one layer can be leveraged to exploit vulnerabilities in others. In this paper, we present the end-to-end pipeline implemented in Graphene, showcasing the systematic approach adopted for conducting this thorough security analysis.",
    "published_date": "2023-12-20",
    "pdf_link": "https://arxiv.org/pdf/2312.13119v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Threat Modeling and Risk Assessment",
      "specific_problem": "Automated construction of multi-layer attack graphs from CVE text and infrastructure inventory for infrastructure security posture analysis and risk scoring",
      "attack_types": [
        "multi-step attack chains",
        "remote code execution",
        "privilege escalation",
        "denial of service/system crash",
        "clickjacking",
        "information disclosure/memory read",
        "cryptographic weaknesses",
        "network-layer exploits",
        "hardware/system vulnerabilities"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Named Entity Recognition (NER)",
        "specific": null,
        "novel_contribution": "Automatic extraction of vulnerability preconditions, postconditions, inputs, and outputs from natural language CVE descriptions to form attack graph nodes"
      },
      {
        "type": "primary",
        "category": "Word Embeddings / Semantic Similarity",
        "specific": null,
        "novel_contribution": "Semantic similarity matching of postconditions-to-preconditions to automatically link CVEs and generate attack graph edges across and within layers"
      },
      {
        "type": "primary",
        "category": "Rule-based/NLP Classification",
        "specific": "Keyword + CWE-guided layer classification",
        "novel_contribution": "Layering of vulnerabilities (ML, system, hardware, network, cryptography) using keyword matching and CWE info for layered and cumulative attack graphs"
      }
    ],
    "learning_paradigm": [],
    "datasets": [
      {
        "name": "CVE disclosures (National Vulnerability Database)",
        "type": "public",
        "domain": "vulnerability_reports",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Common Weakness Enumeration (CWE)",
        "type": "public",
        "domain": "security_taxonomy",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Common Vulnerability Scoring System (CVSS) scores",
        "type": "public",
        "domain": "vulnerability_scoring",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "User-provided infrastructure inventory and topology",
        "type": "proprietary",
        "domain": "network_topology_and_asset_inventory",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "CVSS severity",
      "Exploitability effort (qualitative within risk scoring)",
      "Impact based on asset criticality (infrastructure-aware risk score)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Prior approaches require manual input of infrastructure CVEs and are not continuously updated with new disclosures",
        "Reliance on hard-coded heuristics and pivot-word/keyword rules for pre/postcondition extraction fails to capture semantic information",
        "Some methods depend on proprietary formal specifications that are domain-specific and costly to maintain",
        "Limited scalability and adaptability of previous attack graph generation techniques",
        "Risk analysis often ignores the underlying infrastructure context and asset criticality"
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Provide a holistic, automated security posture assessment that identifies vulnerabilities and their exploit chains via attack graphs, evaluates exploitability and impact across layers, and quantifies risk tailored to the specific infrastructure.",
      "potential_research_ideas": [
        "Leverage transformer-based language models fine-tuned on CVE/CWE/CAPEC corpora to improve pre/postcondition extraction and reduce reliance on keyword rules",
        "Integrate knowledge graphs combining CVE, CWE, CPE, CAPEC, and vendor advisories to enhance condition linking and constraint checking in attack graph generation",
        "Develop probabilistic attack graphs that model uncertainty in extracted conditions and similarity matches, enabling confidence-aware risk scoring",
        "Incorporate dynamic signals (e.g., exploit availability, PoCs, telemetry) to adjust edge weights and path risks over time",
        "Learn to rank attack paths using feedback from red-team/blue-team exercises to prioritize mitigation",
        "Apply graph neural networks to reason over multi-layer graphs and predict likely unseen edges/steps",
        "Automate infrastructure discovery via asset inventory integrations (CMDB, cloud APIs) to minimize user input",
        "Evaluate adversarial robustness of the NLP pipeline against crafted CVE texts and implement defenses",
        "Add explainability modules that highlight text spans supporting each node/edge to assist analysts",
        "Explore privacy-preserving processing of proprietary infrastructure data (e.g., local processing, federated pipelines)"
      ],
      "architectural_improvement_recommendations": [
        "Replace or augment NER with domain-adapted transformer models (e.g., SciBERT/CyBERT-style) fine-tuned on CVE/CWE text",
        "Use contextual embeddings (e.g., SBERT) for similarity rather than static word2vec/GloVe, with hard constraints from CWE/CPE to reduce false links",
        "Introduce a hybrid symbolic-ML linker that enforces pre/postcondition type compatibility using a schema and knowledge graph",
        "Implement probabilistic edge weights with uncertainty estimates and propagate them during risk scoring",
        "Support incremental and streaming graph updates with change detection and re-scoring to handle continuous CVE feeds",
        "Incorporate asset criticality models learned from historical incident/impact data to calibrate risk scores"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Enterprise networks and computing infrastructures",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Parsing diverse, evolving natural-language CVE descriptions reliably (semantic extraction)",
        "Maintaining continuous updates with new vulnerabilities and changing taxonomies",
        "Accurate matching of pre/postconditions across heterogeneous layers without over-linking",
        "Incorporating asset criticality and infrastructure-specific context consistently",
        "Data completeness and correctness of user-provided inventory and topology"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "A fully automated security posture analyzer that generates attack graphs for computing infrastructures",
      "An NLP approach using NER and word embeddings to extract pre/postconditions and automate attack graph construction",
      "A multi-layer analysis framework (ML, system, hardware, network, cryptography) with both layered and cumulative attack graphs",
      "Infrastructure-tailored risk scoring methods combining exploitability and asset criticality"
    ]
  },
  {
    "arxiv_id": "2312.13041v1",
    "title": "Advancing SQL Injection Detection for High-Speed Data Centers: A Novel Approach Using Cascaded NLP",
    "authors": "Kasim Tasdemir; Rafiullah Khan; Fahad Siddiqui; Sakir Sezer; Fatih Kurugollu; Sena Busra Yengec-Tasdemir; Alperen Bolat",
    "abstract": "Detecting SQL Injection (SQLi) attacks is crucial for web-based data center security, but it is challenging to balance accuracy and computational efficiency, especially in high-speed networks. Traditional methods struggle with this balance, while NLP-based approaches, although accurate, are computationally intensive.   We introduce a novel cascade SQLi detection method, blending classical and transformer-based NLP models, achieving a 99.86% detection accuracy with significantly lower computational demands-20 times faster than using transformer-based models alone. Our approach is tested in a realistic setting and compared with 35 other methods, including Machine Learning-based and transformer models like BERT, on a dataset of over 30,000 SQL sentences.   Our results show that this hybrid method effectively detects SQLi in high-traffic environments, offering efficient and accurate protection against SQLi vulnerabilities with computational efficiency. The code is available at https://github.com/gdrlab/cascaded-sqli-detection .",
    "published_date": "2023-12-20",
    "pdf_link": "https://arxiv.org/pdf/2312.13041v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Web Application Security",
      "subdomain": "Web Attack Detection",
      "specific_problem": "SQL Injection (SQLi) detection with high accuracy and low latency for high-speed data center environments",
      "attack_types": [
        "SQL Injection"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Ensemble/Cascade",
        "specific": null,
        "novel_contribution": "Two-stage cascaded detector: Stage-1 fast classical NLP+ML filter; Stage-2 transformer-based re-analysis to reduce false alarms; 20x speedup vs transformer-only with F1=0.9981"
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "BERT (and variants)",
        "novel_contribution": "Used as the second-stage model within the cascade to re-evaluate suspicious SQL payloads and mitigate false positives"
      },
      {
        "type": "primary",
        "category": "SVM",
        "specific": null,
        "novel_contribution": "Used in Stage-1 and in ensemble configurations with TF-IDF/BoC/BoW features for high-throughput filtering"
      },
      {
        "type": "primary",
        "category": "Gradient Boosting",
        "specific": "XGBoost",
        "novel_contribution": "Used in Stage-1 and in ensemble configurations with TF-IDF/BoC/BoW features"
      },
      {
        "type": "primary",
        "category": "Naive Bayes",
        "specific": null,
        "novel_contribution": "Used in Stage-1 and in ensemble configurations with TF-IDF/BoC/BoW features"
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "BERT (standalone transformer-based classifiers)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "SVM",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Gradient Boosting",
        "specific": "XGBoost",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Naive Bayes",
        "specific": null,
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Unnamed publicly available SQLi dataset (~30,000 SQL sentences)",
        "type": "public",
        "domain": "sql_payloads",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "BERT (standalone transformer)",
        "paper_reference": null,
        "metric": "F1, FE (F1 Efficiency), inference latency",
        "their_result": "Proposed cascade: F1=0.9981; accuracy=99.86%; ~20x faster than transformer-only",
        "baseline_result": null
      },
      {
        "method_name": "Support Vector Machine (SVM) with TF-IDF/BoC/BoW",
        "paper_reference": null,
        "metric": "F1, FE, inference latency",
        "their_result": "Cascade outperforms SVM-only on FE when α=0.98; maintains higher accuracy with similar or better latency",
        "baseline_result": null
      },
      {
        "method_name": "XGBoost with TF-IDF/BoC/BoW",
        "paper_reference": null,
        "metric": "F1, FE, inference latency",
        "their_result": "Cascade achieves superior FE vs single-model XGBoost configurations",
        "baseline_result": null
      },
      {
        "method_name": "Naive Bayes with TF-IDF/BoC/BoW",
        "paper_reference": null,
        "metric": "F1, FE, inference latency",
        "their_result": "Cascade achieves superior FE vs single-model Naive Bayes configurations",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "F1 score",
      "Accuracy",
      "Recall",
      "Inference latency (ms)",
      "F1 Efficiency (FE) = α*F1 + (1-α)*l"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can SQL injection be detected with both high accuracy and low computational cost suitable for high-speed data centers and DPUs?",
        "Does a cascaded approach that blends fast classical NLP+ML with transformer-based models deliver comparable accuracy to transformers while significantly reducing latency?",
        "How can we objectively trade off speed and accuracy across methods for SQLi detection?"
      ],
      "gaps_identified": [
        "Traditional rule-based systems are static/inflexible, yield high false positives, and fail to capture novel SQLi patterns.",
        "Transformer-based NLP methods, while accurate, are computationally intensive and challenging for high-speed network environments.",
        "Most prior works focus on vulnerability testing rather than operational efficiency and practical feasibility for real-time detection.",
        "Limited number of transformer-based studies specifically targeting SQLi detection."
      ],
      "limitations": [
        "Evaluation is performed on a single publicly available SQLi dataset (~30k SQL sentences).",
        "Adversarial robustness, privacy, and fairness aspects are not evaluated.",
        "Hardware-specific deployment and end-to-end production DPU results are not detailed in the provided text (only 'realistic setting' is mentioned)."
      ],
      "future_work": [
        "Use the proposed FE metric for adaptive, dynamic model selection based on changing computational resources.",
        "Dynamically replace or tune the Stage-1 model according to FE to meet system load constraints.",
        "Integrate and optimize the cascade on specialized hardware such as Nvidia Bluefield 3 DPUs for real-time 400 Gbps traffic.",
        "Explore additional transformer variants as Stage-2 to further reduce false positives at minimal latency cost."
      ],
      "motivation": "Balance high detection accuracy with computational efficiency for SQLi detection in high-speed data centers, overcoming limitations of rule-based methods and the heavy cost of transformer-only approaches.",
      "potential_research_ideas": [
        "Adversarially robust cascaded SQLi detection with attack-aware data augmentation and adversarial training.",
        "AST (abstract syntax tree) or SQL-grammar-aware features in Stage-1 to improve precision while keeping latency low.",
        "Online/continual learning in Stage-1 to adapt to evolving SQLi patterns without full retraining.",
        "Hardware-aligned co-design: quantization, pruning, and operator fusion for Stage-2 transformers to maximize DPU throughput.",
        "Knowledge distillation from Stage-2 transformers to lightweight Stage-1 models for further speedups.",
        "Multi-task cascades that detect broader web attacks (XSS, command injection) with shared Stage-1 features."
      ],
      "architectural_improvement_recommendations": [
        "Replace or augment Stage-1 with character-level CNNs or lightweight transformers (e.g., DistilBERT/TinyBERT) optimized with quantization to reduce latency further.",
        "Introduce calibrated confidence thresholds for Stage-1 to tune the pass rate to Stage-2 under dynamic load.",
        "Add a cost-aware early-exit mechanism in Stage-2 to skip full transformer passes when early layers are confident.",
        "Use feature caching and approximate nearest neighbor search on TF-IDF vectors to accelerate Stage-1 decisions.",
        "Apply mixed-precision inference and operator fusion for Stage-2 on DPUs/GPUs."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/gdrlab/cascaded-sqli-detection",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Reported ~20x faster than transformer-only inference; designed for high-speed environments (e.g., consideration of 400 Gbps BlueField-3 DPU contexts)."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "High-speed data center setting / realistic environment; targeting DPUs (e.g., Nvidia BlueField 3) and high-traffic networks",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Transformer-based models impose high computation and memory overhead for real-time traffic.",
        "Integrating NLP-based SQLi detection into DPUs at 400 Gbps scale requires tight latency budgets.",
        "Balancing speed and accuracy under variable computational resources."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "A unique cascaded SQLi detection model blending classical ML classifiers with transformer-based NLP for high-speed, high-accuracy detection.",
      "A new performance metric, F1 Efficiency (FE), to compare methods under speed-accuracy trade-offs and enable dynamic model selection.",
      "An extensive comparison of 35 methods (ML classifiers and transformer models including BERT) evaluating accuracy and speed.",
      "Integration and evaluation of multiple ensemble models combining classic NLP features and ML models."
    ]
  },
  {
    "arxiv_id": "2312.08810v1",
    "title": "Deep Learning-Based Cyber-Attack Detection Model for Smart Grids",
    "authors": "Mojtaba Mohammadi; Arshia Aflaki; Abdollah Kavousifard; Mohsen Gitizadeh",
    "abstract": "In this paper, a novel artificial intelligence-based cyber-attack detection model for smart grids is developed to stop data integrity cyber-attacks (DIAs) on the received load data by supervisory control and data acquisition (SCADA). In the proposed model, first the load data is forecasted using a regression model and after processing stage, the processed data is clustered using the unsupervised learning method. In this work, in order to achieve the best performance, three load forecasting methods (i.e. extra tree regression (ETR), long short-term memory (LSTM) and bidirectional long short-term memory (BiLSTM)) are utilized as regression models and their performance is compared. For clustering and outlying detection, the covariance elliptic envelope (EE) is employed as an unsupervised learning method. To examine the proposed model, the hourly load data of the power company of the city of Johor in Malaysia is employed and Two common DIAs, which are DIAs targeting economic loss and DIAs targeting blackouts, are used to evaluate the accuracy of detection methods in several scenarios. The simulation results show that the proposed EE-BiLSTM method can perform more robust and accurate compared to the other two methods.",
    "published_date": "2023-12-14",
    "pdf_link": "https://arxiv.org/pdf/2312.08810v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Critical Infrastructure Security",
      "subdomain": "Smart Grid Security",
      "specific_problem": "Detection of data integrity attacks (DIAs) on AMI/SCADA load data using forecasting plus unsupervised outlier detection",
      "attack_types": [
        "Data Integrity Attack (DIA) targeting blackouts",
        "Data Integrity Attack (DIA) targeting economic loss"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Outlier Detection (Robust Covariance)",
        "specific": "Covariance Elliptic Envelope (EE)",
        "novel_contribution": "Combines forecast–actual residuals with EE for DIA detection in AMI; dynamically updates contamination hyperparameter each hour"
      },
      {
        "type": "primary",
        "category": "RNN",
        "specific": "Bidirectional LSTM (BiLSTM)",
        "novel_contribution": "Used for load forecasting within the detection pipeline; provided the strongest overall detection performance when paired with EE (EE-BiLSTM)"
      },
      {
        "type": "baseline",
        "category": "RNN",
        "specific": "LSTM",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Ensemble Trees",
        "specific": "Extra Trees Regressor (ETR)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "MLP",
        "specific": "Artificial Neural Network (ANN)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Ensemble Trees",
        "specific": "Random Forest Regressor (RFR)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Gradient Boosted Trees",
        "specific": "Gradient Tree Boosting (GTBR)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "Johor (Malaysia) Power Company Hourly Load Data (2009-2010)",
        "type": "proprietary",
        "domain": "smart_grid_load_time_series",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "EE-ETR",
        "paper_reference": null,
        "metric": "Accuracy/Specificity/F1 (Blackouts, Scenario 1)",
        "their_result": "EE-BiLSTM: Accuracy 96.17%, Specificity 97.86%, F1 81.41%",
        "baseline_result": "EE-ETR: Accuracy 95.48%, Specificity 97.48%, F1 78.06%"
      },
      {
        "method_name": "EE-LSTM",
        "paper_reference": null,
        "metric": "Accuracy/Specificity/F1 (Blackouts, Scenario 1)",
        "their_result": "EE-BiLSTM: Accuracy 96.17%, Specificity 97.86%, F1 81.41%",
        "baseline_result": "EE-LSTM: Accuracy 95.56%, Specificity 97.52%, F1 78.44%"
      },
      {
        "method_name": "EE-ETR",
        "paper_reference": null,
        "metric": "Accuracy/Specificity/F1 (Blackouts, Scenario 2)",
        "their_result": "EE-BiLSTM: Accuracy 99.30%, Specificity 99.61%, F1 96.34%",
        "baseline_result": "EE-ETR: Accuracy 99.23%, Specificity 99.57%, F1 95.93%"
      },
      {
        "method_name": "EE-LSTM",
        "paper_reference": null,
        "metric": "Accuracy/Specificity/F1 (Blackouts, Scenario 2)",
        "their_result": "EE-BiLSTM: Accuracy 99.30%, Specificity 99.61%, F1 96.34%",
        "baseline_result": "EE-LSTM: Accuracy 99.61%, Specificity 99.78%, F1 97.96%"
      },
      {
        "method_name": "EE-ETR",
        "paper_reference": null,
        "metric": "Accuracy/Specificity/F1 (Blackouts, Scenario 3)",
        "their_result": "EE-BiLSTM: Accuracy 99.84%, Specificity 99.91%, F1 99.31%",
        "baseline_result": "EE-ETR: Accuracy 99.77%, Specificity 99.87%, F1 98.96%"
      },
      {
        "method_name": "EE-LSTM",
        "paper_reference": null,
        "metric": "Accuracy/Specificity/F1 (Blackouts, Scenario 3)",
        "their_result": "EE-BiLSTM: Accuracy 99.84%, Specificity 99.91%, F1 99.31%",
        "baseline_result": "EE-LSTM: Accuracy 99.84%, Specificity 99.91%, F1 99.31%"
      },
      {
        "method_name": "Forecasting baselines",
        "paper_reference": null,
        "metric": "MAPE (test set)",
        "their_result": "\"BiLSTM and ETR are the most accurate forecasting models by about 2 percent MAPE error.\"",
        "baseline_result": "ANN worst; RFR/GTBR less accurate than LSTM/BiLSTM/ETR (exact numbers not provided in excerpt)"
      }
    ],
    "performance_metrics_used": [
      "MAPE",
      "RMSE",
      "Accuracy",
      "Precision",
      "Sensitivity (Recall)",
      "Specificity",
      "F1 Score"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "\"the research in this area is still in infancy.\"",
        "\"The research gap in cyber-attack detection within AMIs motivated us to write this paper.\"",
        "\"previous works mainly use traditional methods such as Kalman Filters or upper and lower bound detection to detect cyber-attacks.\""
      ],
      "limitations": [
        "\"Some exceptions, however, occurred in a small number of scenarios.\" (EE-BiLSTM not always best)",
        "Detection effectiveness depends on forecast accuracy: \"This highlights the vital role of accurate load forecasting in the detection algorithm.\"",
        "EE assumes inlier data are Gaussian-distributed and relies on robust covariance estimation.",
        "Contamination parameter requires online tuning; mis-specification can affect detection."
      ],
      "future_work": [],
      "motivation": "Address the gap of DIA detection within AMI using AI: \"The research gap in cyber-attack detection within AMIs motivated us to write this paper.\"",
      "potential_research_ideas": [
        "Integrate probabilistic load forecasting (with prediction intervals) and propagate uncertainty into EE via Mahalanobis distance thresholds or statistical tests.",
        "Evaluate and ensemble alternative unsupervised detectors (Isolation Forest, One-Class SVM, LOF, robust PCA, KDE, deep autoencoder/sequence models, normalizing flows) on the same residual space.",
        "Extend to multivariate AMI/SCADA features (voltage, current, power factor, weather, topology) and model dependencies via multivariate residuals or graph-based methods.",
        "Online/streaming adaptation with concept drift detection and incremental model updates for both forecaster and detector.",
        "Adversarial robustness: study adaptive attackers that shape residuals; design robust thresholds or certified detectors.",
        "Explainability: attribute anomalies to time-of-day/feature contributions (e.g., SHAP on residual features) to support operator triage.",
        "Benchmark against public smart grid datasets or release a synthetic benchmark with controlled DIA scenarios and labeled ground truth."
      ],
      "architectural_improvement_recommendations": [
        "Replace point forecasting with quantile or distributional forecasting (e.g., DeepAR/Transformer, Quantile LSTM) and use standardized residuals for EE.",
        "Hybrid detector: fuse EE with Isolation Forest or OC-SVM via stacking or Bayesian model averaging for better robustness across scenarios.",
        "Adaptive contamination estimation using robust change-point detection or time-varying quantile calibration tied to residual variance.",
        "Sequence-aware detection: apply temporal autoencoder or Temporal Convolutional Network on residual sequences to capture bursty attack patterns.",
        "Use graph neural networks to incorporate feeder topology and spatial correlations across meters for coordinated DIA detection."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "scikit-learn",
        "TensorFlow",
        "Keras"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Python 3.8; Intel Core i7-10750H @2.60GHz, 16GB RAM; lookback window = 14 hours; LSTM/BiLSTM each with two 128-cell layers + dropout 0.3 + dense output; 150 epochs; Adam optimizer; MSE loss; EE via scikit-learn; hyperparameters for ensembles tuned with TensorFlow Keras Optimizers."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Requires accurate real-time load forecasting; performance degrades with forecast error.",
        "Assumes Gaussian inlier distribution for EE; may not hold across all operating regimes.",
        "Online tuning of contamination parameter is needed and may be brittle under regime shifts.",
        "Access to high-quality, timely AMI/SCADA data and integration with SCADA pipelines."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes a novel AI-based cyber-attack detection model for AMIs that uses load forecasting followed by Elliptic Envelope outlier detection on processed residuals.",
      "Implements and compares three forecasting models (ETR, LSTM, BiLSTM) within the detection pipeline across multiple DIA scenarios of varying severity.",
      "Demonstrates that EE-BiLSTM generally outperforms EE-LSTM and EE-ETR, with noted exceptions in some scenarios.",
      "Provides a practical algorithm that updates the contamination parameter online and replaces detected attacked data with forecasted values for continuity."
    ]
  },
  {
    "arxiv_id": "2311.18545v2",
    "title": "Decentralized Deepfake Detection Blockchain Network using Dynamic Algorithm management",
    "authors": "Dipankar Sarkar",
    "abstract": "Deepfake technology is a major threat to the integrity of digital media. This paper presents a comprehensive framework for a blockchain-based decentralized system designed to tackle the escalating challenge of digital content integrity. The proposed system integrates advanced deep learning algorithms with the immutable and transparent nature of blockchain technology to create a trustless environment where authenticity can be verified without relying on a single centralized authority. Furthermore, the system utilizes smart contracts for dynamic algorithm management and token-based incentives further enhances the system's effectiveness and adaptability. The decentralized architecture of the system democratizes the process of verifying digital content and introduces a novel approach to combat deepfakes. The collaborative and adjustable nature of this system sets a new benchmark for digital media integrity, offering a more robust digital media environment.",
    "published_date": "2023-11-30",
    "pdf_link": "https://arxiv.org/pdf/2311.18545v2",
    "paper_types": [
      "position",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Digital Forensics",
      "subdomain": "Multimedia Forensics",
      "specific_problem": "Decentralized verification of digital media authenticity and deepfake detection with dynamic algorithm management on blockchain",
      "attack_types": [
        "Deepfakes (face modification, expression swaps)",
        "Audio impersonation/manipulation",
        "Re-recording attacks (screen re-capture)",
        "General content tampering (cropping, resizing)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "CNN",
        "specific": null,
        "novel_contribution": "Used as part of the proposed detection pipeline for images/videos; novelty is the on-chain-managed, off-chain-executed dynamic selection and lifecycle management via smart contracts."
      },
      {
        "type": "primary",
        "category": "GAN",
        "specific": null,
        "novel_contribution": "Mentioned for image/video analysis within the adaptable algorithm pool governed by smart contracts."
      },
      {
        "type": "primary",
        "category": "RNN/LSTM",
        "specific": "LSTM-RNN",
        "novel_contribution": "Applied to audio deepfake detection; integrated into dynamically managed model registry with incentive mechanisms."
      },
      {
        "type": "primary",
        "category": "Transfer Learning",
        "specific": null,
        "novel_contribution": "Proposed to accelerate adaptation to new deepfake techniques as part of dynamic model updates."
      },
      {
        "type": "primary",
        "category": "Embedding-based similarity/search",
        "specific": null,
        "novel_contribution": "Embeddings used for content matching against a trusted content registry; results anchored to blockchain; dynamically selected models generate embeddings."
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Transfer Learning"
    ],
    "datasets": [
      {
        "name": "FaceForensics++",
        "type": "public",
        "domain": "images_videos",
        "link": "https://github.com/ondyari/FaceForensics",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Trusted Content Registry (on-chain hashed originals)",
        "type": "proprietary",
        "domain": "media_hashes (audio/image/video)",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Detection methods struggle to keep pace with rapidly improving deepfake generation.",
        "Scalability and real-time application remain challenging for current systems.",
        "Difficulty detecting re-recorded (screen-captured) deepfakes.",
        "Balancing tolerance for legitimate edits (cropping, resizing) versus malicious tampering.",
        "Ensuring diverse data representation across cultures and languages."
      ],
      "limitations": [
        "Scalability concerns as content and node volume grows.",
        "Potential lack of real-time processing capability (latency).",
        "Handling diverse media and legitimate alterations is challenging.",
        "Re-recording attack vector not fully addressed; needs specialized algorithms."
      ],
      "future_work": [
        "Optimize network architecture and algorithm efficiency for scalability.",
        "Reduce latency to approach real-time analysis.",
        "Enhance models with diverse, representative datasets across modalities and locales.",
        "Develop specialized methods for re-recording attack detection.",
        "Improve discrimination between legitimate edits and malicious manipulation."
      ],
      "motivation": "Deepfakes pose a growing threat to information integrity; centralized verification cannot scale or adapt quickly enough, motivating a decentralized, adaptive, incentive-aligned framework.",
      "potential_research_ideas": [
        "zkML-based verifiable inference so nodes can prove model execution correctness on-chain without revealing model weights or inputs.",
        "Federated or cross-silo learning among validator nodes to update detectors without central data pooling, with on-chain aggregation and incentives.",
        "Adaptive on-chain governance for model selection using multi-armed bandits or Bayesian optimization to allocate traffic/rewards to the best-performing detectors over time.",
        "Robust perceptual hashing and learned robust hashing to better handle re-recordings and legitimate edits while resisting adversarial perturbations.",
        "Multimodal deepfake detection (audio-visual-text) with cross-modal consistency checks; extend to lip-sync and transcript alignment verification.",
        "Integration with content provenance standards (e.g., C2PA) and watermark detectors to fuse cryptographic provenance with forensic signals.",
        "Adversarial training and augmentation suites tailored to re-recording and compression pipelines common on social platforms.",
        "Reputation and slashing mechanisms for oracle/model hosts to mitigate gaming and Sybil attacks in incentive schemes.",
        "Streaming/online detection models for low-latency inference on long videos using segment-level scoring and temporal aggregation.",
        "Dataset generation pipelines with synthetic but realistic post-processing (transcoding, re-compression, camera capture) to close the train-test gap."
      ],
      "architectural_improvement_recommendations": [
        "Introduce zero-knowledge proofs (SNARKs/STARKs) or trusted execution environments (TEE) with remote attestation for verifiable off-chain inference.",
        "Use content-addressed storage (IPFS/Filecoin) for model artifacts and reference media with on-chain CIDs; maintain on-chain model registry and versioning.",
        "Adopt a Layer-2 (rollup) for higher throughput and lower fees; batch inference result commitments to reduce on-chain load.",
        "Design a robust oracle network with committee-based aggregation and stake-slashing; diversify oracle providers to reduce single-point trust.",
        "Implement hierarchical model routing: fast lightweight prefilter -> specialized heavyweight detector -> human-in-the-loop escalation.",
        "Standardize evaluation metrics and on-chain reporting schema (e.g., ROC-AUC, EER, t-DCF), with periodic audits using held-out challenge sets.",
        "Add fairness/robustness auditing pipelines with demographic metadata where appropriate; publish bias metrics and require for model onboarding.",
        "Employ robust perceptual hashing/learned hashing plus cryptographic hashing to balance edit tolerance and integrity.",
        "Containerize and pin model runtimes (e.g., OCI images) with deterministic seeds and pinned deps to aid reproducibility of off-chain nodes.",
        "Integrate watermark/provenance verification (C2PA, watermark detectors) as signals in a fusion model with trainable weights governed on-chain."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": "Off-chain deep learning inference (likely GPU-accelerated) managed by oracle services; exact hardware/training details not specified."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Proposed blockchain network (PoS) with off-chain DL inference nodes and oracle services; user-facing portal for submissions; trusted content providers register hashes on-chain.",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "On-chain throughput and gas costs for recording results and model management.",
        "Latency from off-chain inference/oracle round-trips hindering real-time use.",
        "Oracle trust and potential collusion/Sybil attacks; need for reputation/slashing.",
        "Model drift and rapid evolution of deepfakes requiring frequent updates.",
        "Robust hashing for edited/re-recorded media while preventing false positives.",
        "Ensuring privacy/compliance for analyzed content and consent management.",
        "Incentive gaming (reward manipulation) and metric spoofing without verifiable evaluation."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": true,
      "fairness_bias": true
    },
    "contributions": [
      "Proposes a decentralized blockchain-based architecture for deepfake detection and media authenticity verification.",
      "Introduces smart contract–driven dynamic algorithm management (registration, validation, lifecycle, performance tracking).",
      "Defines a token-based incentive mechanism rewarding high-performing models/nodes/content providers.",
      "Outlines a trusted content provider framework with on-chain hashing and notifications.",
      "Specifies off-chain model execution with oracle-mediated result anchoring to the blockchain.",
      "Discusses adaptability via continuous model updates, transfer learning, and community contributions."
    ]
  },
  {
    "arxiv_id": "2312.09651v1",
    "title": "What to Remember: Self-Adaptive Continual Learning for Audio Deepfake Detection",
    "authors": "Xiaohui Zhang; Jiangyan Yi; Chenglong Wang; Chuyuan Zhang; Siding Zeng; Jianhua Tao",
    "abstract": "The rapid evolution of speech synthesis and voice conversion has raised substantial concerns due to the potential misuse of such technology, prompting a pressing need for effective audio deepfake detection mechanisms. Existing detection models have shown remarkable success in discriminating known deepfake audio, but struggle when encountering new attack types. To address this challenge, one of the emergent effective approaches is continual learning. In this paper, we propose a continual learning approach called Radian Weight Modification (RWM) for audio deepfake detection. The fundamental concept underlying RWM involves categorizing all classes into two groups: those with compact feature distributions across tasks, such as genuine audio, and those with more spread-out distributions, like various types of fake audio. These distinctions are quantified by means of the in-class cosine distance, which subsequently serves as the basis for RWM to introduce a trainable gradient modification direction for distinct data types. Experimental evaluations against mainstream continual learning methods reveal the superiority of RWM in terms of knowledge acquisition and mitigating forgetting in audio deepfake detection. Furthermore, RWM's applicability extends beyond audio deepfake detection, demonstrating its potential significance in diverse machine learning domains such as image recognition.",
    "published_date": "2023-12-15",
    "pdf_link": "https://arxiv.org/pdf/2312.09651v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Multimedia Security",
      "subdomain": "Deepfake Detection",
      "specific_problem": "Continual learning for audio deepfake (synthetic speech) detection across unseen attack types",
      "attack_types": [
        "Text-to-Speech (TTS) deepfakes",
        "Voice Conversion (VC) deepfakes",
        "Synthetic speech attacks"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Continual Learning / Gradient Projection",
        "specific": "Radian Weight Modification (RWM)",
        "novel_contribution": "Self-adaptive gradient direction modification using class regrouping by in-class cosine distance, with learned rotated radians (LRR) from a self-attention module to rotate between orthogonal projectors P and Q; no replay memory required."
      },
      {
        "type": "primary",
        "category": "Self-Attention",
        "specific": "Batch-level self-attention before classifier",
        "novel_contribution": "Produces attention scores per sample that are mapped to learned rotation angles (LRR) to adapt gradient modification direction per batch."
      },
      {
        "type": "baseline",
        "category": "Transformer (self-supervised speech model)",
        "specific": "Wav2vec 2.0 XLSR-53",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "Self-Attention CNN (S-CNN) classifier",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Continual Learning",
        "specific": "Elastic Weight Consolidation (EWC)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Continual Learning",
        "specific": "Learning without Forgetting (LwF)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Continual Learning / Gradient Projection",
        "specific": "Orthogonal Weight Modification (OWM)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Continual Learning (audio deepfake)",
        "specific": "Detecting Fake Without Forgetting (DFWF)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Training Regime",
        "specific": "Fine-tuning",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Training Regime",
        "specific": "Replay-All (train on all datasets jointly)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Continual Learning",
      "Regularization/Projection-based CL"
    ],
    "datasets": [
      {
        "name": "ASVspoof2019 LA (S)",
        "type": "public",
        "domain": "audio_speech",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "ASVspoof2015 (T1)",
        "type": "public",
        "domain": "audio_speech",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "In-the-Wild (T2)",
        "type": "public",
        "domain": "audio_speech",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CLEAR (continual learning benchmark, image recognition)",
        "type": "public",
        "domain": "images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "EWC",
        "paper_reference": "Kirkpatrick et al., 2017",
        "metric": "EER (%) on T1 (S→T1 scenario)",
        "their_result": "0.212",
        "baseline_result": "0.570"
      },
      {
        "method_name": "LwF",
        "paper_reference": "Li and Hoiem, 2017",
        "metric": "EER (%) on T1 (S→T1 scenario)",
        "their_result": "0.212",
        "baseline_result": "0.343"
      },
      {
        "method_name": "OWM",
        "paper_reference": "Zeng et al., 2019",
        "metric": "EER (%) on T1 (S→T1 scenario)",
        "their_result": "0.212",
        "baseline_result": "0.540"
      },
      {
        "method_name": "DFWF",
        "paper_reference": "Ma et al., 2021",
        "metric": "EER (%) on T1 (S→T1 scenario)",
        "their_result": "0.212",
        "baseline_result": "0.689"
      },
      {
        "method_name": "Finetune",
        "paper_reference": null,
        "metric": "EER (%) on T1 (S→T1 scenario)",
        "their_result": "0.212",
        "baseline_result": "0.510"
      },
      {
        "method_name": "Replay-All",
        "paper_reference": "Parisi et al., 2019 (as lower bound reference)",
        "metric": "EER (%) on T1 (S→T1 scenario)",
        "their_result": "0.212",
        "baseline_result": "0.201"
      },
      {
        "method_name": "EWC",
        "paper_reference": "Kirkpatrick et al., 2017",
        "metric": "EER (%) on T2 (S→T2 scenario)",
        "their_result": "2.247",
        "baseline_result": "5.615"
      },
      {
        "method_name": "LwF",
        "paper_reference": "Li and Hoiem, 2017",
        "metric": "EER (%) on T2 (S→T2 scenario)",
        "their_result": "2.247",
        "baseline_result": "4.998"
      },
      {
        "method_name": "OWM",
        "paper_reference": "Zeng et al., 2019",
        "metric": "EER (%) on T2 (S→T2 scenario)",
        "their_result": "2.247",
        "baseline_result": "5.065"
      },
      {
        "method_name": "DFWF",
        "paper_reference": "Ma et al., 2021",
        "metric": "EER (%) on T2 (S→T2 scenario)",
        "their_result": "2.247",
        "baseline_result": "6.275"
      },
      {
        "method_name": "Finetune",
        "paper_reference": null,
        "metric": "EER (%) on T2 (S→T2 scenario)",
        "their_result": "2.247",
        "baseline_result": "4.978"
      },
      {
        "method_name": "Replay-All",
        "paper_reference": "Parisi et al., 2019 (as lower bound reference)",
        "metric": "EER (%) on T2 (S→T2 scenario)",
        "their_result": "2.247",
        "baseline_result": "2.160"
      },
      {
        "method_name": "EWC",
        "paper_reference": "Kirkpatrick et al., 2017",
        "metric": "EER (%) on T2 (S→T1→T2 scenario)",
        "their_result": "1.161",
        "baseline_result": "3.722"
      },
      {
        "method_name": "LwF",
        "paper_reference": "Li and Hoiem, 2017",
        "metric": "EER (%) on T2 (S→T1→T2 scenario)",
        "their_result": "1.161",
        "baseline_result": "1.540"
      },
      {
        "method_name": "OWM",
        "paper_reference": "Zeng et al., 2019",
        "metric": "EER (%) on T2 (S→T1→T2 scenario)",
        "their_result": "1.161",
        "baseline_result": "3.647"
      },
      {
        "method_name": "DFWF",
        "paper_reference": "Ma et al., 2021",
        "metric": "EER (%) on T2 (S→T1→T2 scenario)",
        "their_result": "1.161",
        "baseline_result": "6.478"
      },
      {
        "method_name": "EWC",
        "paper_reference": "Kirkpatrick et al., 2017",
        "metric": "EER (%) on T1 (S→T2→T1 scenario)",
        "their_result": "0.861",
        "baseline_result": "0.933"
      },
      {
        "method_name": "LwF",
        "paper_reference": "Li and Hoiem, 2017",
        "metric": "EER (%) on T1 (S→T2→T1 scenario)",
        "their_result": "0.861",
        "baseline_result": "0.897"
      },
      {
        "method_name": "OWM",
        "paper_reference": "Zeng et al., 2019",
        "metric": "EER (%) on T1 (S→T2→T1 scenario)",
        "their_result": "0.861",
        "baseline_result": "1.042"
      },
      {
        "method_name": "DFWF",
        "paper_reference": "Ma et al., 2021",
        "metric": "EER (%) on T1 (S→T2→T1 scenario)",
        "their_result": "0.861",
        "baseline_result": "1.332"
      }
    ],
    "performance_metrics_used": [
      "Equal Error Rate (EER)"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Existing audio deepfake detectors degrade on unseen attack types.",
        "Experience replay is often impractical due to difficulty acquiring/retaining old data in audio deepfake detection.",
        "Regularization-based audio-specific CL method DFWF mitigates forgetting but degrades learning on new attacks compared to finetuning."
      ],
      "limitations": [],
      "future_work": [
        "Make the code publicly available to facilitate adoption and further research.",
        "Generalize and evaluate RWM across diverse machine learning domains (e.g., beyond image recognition)."
      ],
      "motivation": "Deepfake speech poses real-world risks; detectors perform well on known attacks but struggle to adapt to new TTS/VC types without forgetting previously learned knowledge.",
      "potential_research_ideas": [
        "Adaptive, data-driven regrouping: learn the group split (rs) end-to-end per task or per batch via meta-learning rather than a fixed hyperparameter.",
        "Per-sample or prototype-based compactness estimation instead of class-level to better handle heterogeneous fake classes and long-tail attacks.",
        "Hybrid CL: combine RWM with small-memory replay or generative replay when available to further reduce forgetting while improving acquisition.",
        "Robust RWM under channel/noise shifts: integrate augmentation-invariant alignment or domain-adaptive projectors P/Q.",
        "Open-set and OOD-aware RWM: extend to detect and adapt to truly novel spoofing mechanisms without labels using self-supervised objectives.",
        "Uncertainty-aware rotation: incorporate Bayesian or ensemble estimates to modulate LRR by confidence/uncertainty."
      ],
      "architectural_improvement_recommendations": [
        "Replace or augment S-CNN with modern speech backbones (e.g., Conformer, ECAPA-TDNN) and multi-head attention for richer features.",
        "Meta-learn the mapping from attention scores to rotation angles (LRR) with a small controller network to improve stability across tasks.",
        "Condition the P/Q projector construction on class prototypes or Fisher Information for parameter-efficient rotation on large models.",
        "Introduce task-adaptive layer-wise rotation (different θ per layer) to better control plasticity-stability trade-offs.",
        "Integrate contrastive objectives between genuine/fake prototypes across tasks to stabilize cluster structure.",
        "Dynamic rs selection via validation-free criteria (e.g., distributional shift measures) to reduce hyperparameter sensitivity."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Accessing old data for replay can be impractical due to storage and privacy constraints (motivation for replay-free CL)."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes Radian Weight Modification (RWM), a continual learning approach that autonomously optimizes gradient direction across tasks based on feature-distribution similarity between classes (compact vs. dispersed).",
      "Introduces self-attention-derived Learned Rotated Radians (LRR) to rotate gradients between orthogonal projectors (P and Q), enabling knowledge acquisition while mitigating forgetting without accessing prior data.",
      "Demonstrates superior performance to mainstream CL methods (EWC, LwF, OWM, DFWF) on multiple audio deepfake datasets and shows applicability to image recognition (CLEAR).",
      "Provides implementation details and indicates code is included in supplemental material with plans for public release."
    ]
  },
  {
    "arxiv_id": "2312.02993v1",
    "title": "ZTCloudGuard: Zero Trust Context-Aware Access Management Framework to Avoid Misuse Cases in the Era of Generative AI and Cloud-based Health Information Ecosystem",
    "authors": "Khalid Al-hammuri; Fayez Gebali; Awos Kanan",
    "abstract": "Managing access between large numbers of distributed medical devices has become a crucial aspect of modern healthcare systems, enabling the establishment of smart hospitals and telehealth infrastructure. However, as telehealth technology continues to evolve and Internet of Things (IoT) devices become more widely used, they are also becoming increasingly exposed to various types of vulnerabilities and medical errors. In healthcare information systems, about 90\\% of vulnerabilities emerged from misuse cases and human errors. As a result, there is a need for additional research and development of security tools to prevent such attacks. This article proposes a zero-trust-based context-aware framework for managing access to the main components of the cloud ecosystem, including users, devices and output data. The main goal and benefit of the proposed framework is to build a scoring system to prevent or alleviate misuse cases while using distributed medical devices in cloud-based healthcare information systems. The framework has two main scoring schemas to maintain the chain of trust. First, it proposes a critical trust score based on cloud-native micro-services of authentication, encryption, logging, and authorizations. Second, creating a bond trust scoring to assess the real-time semantic and syntactic analysis of attributes stored in a healthcare information system. The analysis is based on a pre-trained machine learning model to generate the semantic and syntactic scores. The framework also takes into account regulatory compliance and user consent to create a scoring system. The advantage of this method is that it is applicable to any language and adapts to all attributes as it relies on a language model, not just a set of predefined and limited attributes. The results show a high F1 score of 93.5%, which proves that it is valid for detecting misuse cases.",
    "published_date": "2023-11-28",
    "pdf_link": "https://arxiv.org/pdf/2312.02993v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Healthcare Security",
      "subdomain": "Identity and Access Management (IAM) / Zero Trust for Medical IoT",
      "specific_problem": "Context-aware access management to prevent/detect misuse cases (human error/insider misuse) in cloud-based healthcare ecosystems across users, devices, and output data",
      "attack_types": [
        "misuse cases by authenticated users",
        "human error",
        "insider misuse",
        "AI-assisted erroneous/bias reports in telehealth",
        "improper device/data access in distributed IoT"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Word Embeddings",
        "specific": "word2vec (skip-gram)",
        "novel_contribution": "Attribute2Vec: applying pre-trained word2vec to healthcare attributes (user x, device y, output z) to compute semantic similarity for Bond Trust scoring"
      },
      {
        "type": "primary",
        "category": "Word Embeddings",
        "specific": "GloVe",
        "novel_contribution": "Use GloVe-derived co-occurrence weights to scale semantic/syntactic similarity contributions in Bond Trust"
      },
      {
        "type": "primary",
        "category": "Similarity / Metric Learning (non-parametric)",
        "specific": "Cosine similarity with thresholding and Softmax normalization",
        "novel_contribution": "Binary logical similarity via angle threshold, weighted aggregation across x–y, x–z, y–z pairs into a bond trust score"
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": null,
        "novel_contribution": "Mentioned as an AI tool for digital health data processing; not used in the proposed method"
      }
    ],
    "learning_paradigm": [
      "Unsupervised (pre-trained embeddings)",
      "Rule-based scoring"
    ],
    "datasets": [
      {
        "name": "Altibbi.com Arabic Medical Consultation Questions (1.5M)",
        "type": "proprietary",
        "domain": "health_consultation_text",
        "link": "https://www.altibbi.com/",
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "RBAC (Role-Based Access Control)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "ABAC (Attribute-Based Access Control)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "F1 score"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How can a zero-trust, context-aware framework manage access across distributed medical devices and cloud resources while preventing misuse cases caused by authenticated users?",
        "Can semantic and syntactic analysis of user, device, and output attributes using pre-trained language models effectively detect misuse cases in real time?"
      ],
      "gaps_identified": [
        "Traditional RBAC/ABAC struggle with dynamic, cloud-based, globally distributed healthcare environments and are labor-intensive to maintain.",
        "A large fraction of vulnerabilities (about 90%) stem from misuse cases and human errors, often by authorized users, making detection difficult.",
        "Unclear which contextual attributes should be incorporated into zero-trust policies without degrading service quality.",
        "Need to evaluate risk continuously and maintain a chain of trust among users, devices, and data."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Prevent or alleviate misuse cases and human errors in cloud-based healthcare systems using a zero-trust, context-aware access management framework with continuous trust scoring.",
      "potential_research_ideas": [
        "Replace general word2vec with domain-specific contextual embeddings (e.g., BioClinicalBERT, ClinicalLongformer) to better capture clinical semantics in Bond Trust.",
        "Incorporate a graph-based trust model (knowledge graph of users-devices-data-policy) with GNNs to reason over multi-hop relations and detect anomalous trust chains.",
        "Develop an adaptive thresholding mechanism for cosine similarity using calibrated uncertainty or conformal prediction to control false positives in different clinical contexts.",
        "Augment Bond Trust with temporal behavior modeling (sequence models) to detect deviations from a user/device’s historical patterns.",
        "Integrate differential privacy or federated learning to compute semantic similarity across institutions without sharing raw data.",
        "Evaluate adversarial robustness against prompt/attribute manipulation or vector-space poisoning attacks on embeddings.",
        "Extend framework to multimodal signals (DICOM imaging features, vitals) via cross-modal embeddings to validate output consistency."
      ],
      "architectural_improvement_recommendations": [
        "Swap pre-trained word2vec/GloVe with contextual encoders (e.g., SapBERT/BioClinicalBERT) and use approximate nearest neighbor indices (FAISS/Vertex AI Matching Engine) for scalable similarity search.",
        "Introduce a risk-aware policy engine that fuses Critical Trust (CT), Bond Trust (BT), and environmental risk (network, geo, workload) via a calibrated probabilistic model (e.g., logistic regression or gradient boosting) instead of simple aggregation.",
        "Adopt a hierarchical knowledge graph storing x–y–z entities and constraints; compute trust via path-based metrics and GNN message passing.",
        "Add online learning and drift detection to update attribute vocabularies and thresholds as terminology and device fleets evolve.",
        "Implement explainability hooks that surface top contributing attributes and similarity pairs for each access decision to aid auditors and clinicians."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Cloud-based healthcare ecosystem (telehealth, smart hospitals)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Complexity of managing access across distributed IoT devices and cloud services.",
        "Real-time monitoring and analysis of user, device, and output data streams.",
        "Ensuring regulatory compliance (HIPAA, GDPR, Canada Bill C-27/CPPA/PIDPTA/AIDA) within access policies.",
        "Authenticating heterogeneous IoT hardware (e.g., PUF-based methods) and maintaining device health posture.",
        "Operational burden of defining and maintaining attributes/policies in dynamic environments."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Zero-trust, context-aware access management framework over users (x), devices (y), and output data (z).",
      "Critical Trust (CT) scoring using cloud-native microservices: authentication, authorization, encryption, logging.",
      "Bond Trust (BT) scoring via semantic and syntactic analysis using pre-trained embeddings (Attribute2Vec with word2vec; GloVe-based weighting).",
      "Hierarchical encoding and decision-making that incorporates regulatory compliance, access constraints, levels, and operations.",
      "Empirical case study reporting a high F1 score of 93.5% for detecting misuse cases."
    ]
  },
  {
    "arxiv_id": "2312.12667v1",
    "title": "Discovering Malicious Signatures in Software from Structural Interactions",
    "authors": "Chenzhong Yin; Hantang Zhang; Mingxi Cheng; Xiongye Xiao; Xinghe Chen; Xin Ren; Paul Bogdan",
    "abstract": "Malware represents a significant security concern in today's digital landscape, as it can destroy or disable operating systems, steal sensitive user information, and occupy valuable disk space. However, current malware detection methods, such as static-based and dynamic-based approaches, struggle to identify newly developed (``zero-day\") malware and are limited by customized virtual machine (VM) environments. To overcome these limitations, we propose a novel malware detection approach that leverages deep learning, mathematical techniques, and network science. Our approach focuses on static and dynamic analysis and utilizes the Low-Level Virtual Machine (LLVM) to profile applications within a complex network. The generated network topologies are input into the GraphSAGE architecture to efficiently distinguish between benign and malicious software applications, with the operation names denoted as node features. Importantly, the GraphSAGE models analyze the network's topological geometry to make predictions, enabling them to detect state-of-the-art malware and prevent potential damage during execution in a VM. To evaluate our approach, we conduct a study on a dataset comprising source code from 24,376 applications, specifically written in C/C++, sourced directly from widely-recognized malware and various types of benign software. The results show a high detection performance with an Area Under the Receiver Operating Characteristic Curve (AUROC) of 99.85%. Our approach marks a substantial improvement in malware detection, providing a notably more accurate and efficient solution when compared to current state-of-the-art malware detection methods.",
    "published_date": "2023-12-19",
    "pdf_link": "https://arxiv.org/pdf/2312.12667v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Malware Analysis",
      "subdomain": "Malware Detection",
      "specific_problem": "Graph-based malware classification from LLVM IR-derived dependency graphs of C/C++ programs",
      "attack_types": [
        "Spyware",
        "Botnet",
        "Trojan",
        "Rootkit",
        "Trojan-Backdoor",
        "Worm",
        "Ransomware",
        "Injection",
        "Mixed"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "GNN",
        "specific": "GraphSAGE (mean aggregator)",
        "novel_contribution": "Malware Graph Network (MGN): LLVM IR-based dependency graphs with node features from operation names and edge weights (e.g., data sizes/latency), processed by 6-layer GraphSAGE with embedding, global pooling, and Leaky-ReLU activation to classify malware vs. benign."
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "Image-based CNN [12]",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "RNN",
        "specific": "ARI-LSTM with attention on N-grams [18]",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Hybrid CNN-RNN",
        "specific": "Bi-GRU-CNN on bytes [15]",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "DNN",
        "specific": "EE-DNN on bytes [19]",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "SA-CNN on opcodes [7]",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "H-CNN on opcodes [20]",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "C/C++ malware and benign source code dataset (24,376 applications)",
        "type": "private",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "CNN (image) [12]",
        "paper_reference": "Gibert et al., 2019",
        "metric": "Accuracy (ACC)",
        "their_result": "98.55%",
        "baseline_result": "93.09%"
      },
      {
        "method_name": "CNN (image) [12]",
        "paper_reference": "Gibert et al., 2019",
        "metric": "AUROC",
        "their_result": "99.85%",
        "baseline_result": "98.31%"
      },
      {
        "method_name": "ARI-LSTM (N-gram) [18]",
        "paper_reference": "Agrawal et al., 2019",
        "metric": "Accuracy (ACC)",
        "their_result": "98.55%",
        "baseline_result": "98.28%"
      },
      {
        "method_name": "ARI-LSTM (N-gram) [18]",
        "paper_reference": "Agrawal et al., 2019",
        "metric": "AUROC",
        "their_result": "99.85%",
        "baseline_result": "99.50%"
      },
      {
        "method_name": "Bi-GRU-CNN (bytes) [15]",
        "paper_reference": "Chaganti et al., 2022",
        "metric": "Accuracy (ACC)",
        "their_result": "98.55%",
        "baseline_result": "96.36%"
      },
      {
        "method_name": "Bi-GRU-CNN (bytes) [15]",
        "paper_reference": "Chaganti et al., 2022",
        "metric": "AUROC",
        "their_result": "99.85%",
        "baseline_result": "99.43%"
      },
      {
        "method_name": "EE-DNN (bytes) [19]",
        "paper_reference": "Gibert et al., 2018",
        "metric": "Accuracy (ACC)",
        "their_result": "98.55%",
        "baseline_result": "97.45%"
      },
      {
        "method_name": "EE-DNN (bytes) [19]",
        "paper_reference": "Gibert et al., 2018",
        "metric": "AUROC",
        "their_result": "99.85%",
        "baseline_result": "99.72%"
      },
      {
        "method_name": "SA-CNN (opcode) [7]",
        "paper_reference": "Zhang et al., 2020",
        "metric": "Accuracy (ACC)",
        "their_result": "98.55%",
        "baseline_result": "97.38%"
      },
      {
        "method_name": "SA-CNN (opcode) [7]",
        "paper_reference": "Zhang et al., 2020",
        "metric": "AUROC",
        "their_result": "99.85%",
        "baseline_result": "99.66%"
      },
      {
        "method_name": "H-CNN (opcode) [20]",
        "paper_reference": "Gibert et al., 2019 (IJCNN)",
        "metric": "Accuracy (ACC)",
        "their_result": "98.55%",
        "baseline_result": "98.18%"
      },
      {
        "method_name": "H-CNN (opcode) [20]",
        "paper_reference": "Gibert et al., 2019 (IJCNN)",
        "metric": "AUROC",
        "their_result": "99.85%",
        "baseline_result": "99.74%"
      },
      {
        "method_name": "ARI-LSTM (per-class)",
        "paper_reference": "Agrawal et al., 2019",
        "metric": "Accuracy (Spyware)",
        "their_result": "98.32%",
        "baseline_result": "98.11%"
      },
      {
        "method_name": "ARI-LSTM (per-class)",
        "paper_reference": "Agrawal et al., 2019",
        "metric": "Accuracy (Botnet)",
        "their_result": "98.67%",
        "baseline_result": "97.00%"
      },
      {
        "method_name": "ARI-LSTM (per-class)",
        "paper_reference": "Agrawal et al., 2019",
        "metric": "Accuracy (Trojan)",
        "their_result": "99.11%",
        "baseline_result": "98.89%"
      },
      {
        "method_name": "ARI-LSTM (per-class)",
        "paper_reference": "Agrawal et al., 2019",
        "metric": "Accuracy (Rootkit)",
        "their_result": "98.33%",
        "baseline_result": "97.67%"
      },
      {
        "method_name": "ARI-LSTM (per-class)",
        "paper_reference": "Agrawal et al., 2019",
        "metric": "Accuracy (Trojan-Backdoor)",
        "their_result": "98.50%",
        "baseline_result": "97.83%"
      },
      {
        "method_name": "ARI-LSTM (per-class)",
        "paper_reference": "Agrawal et al., 2019",
        "metric": "Accuracy (Worm)",
        "their_result": "100.00%",
        "baseline_result": "99.00%"
      },
      {
        "method_name": "ARI-LSTM (per-class)",
        "paper_reference": "Agrawal et al., 2019",
        "metric": "Accuracy (Ransomware)",
        "their_result": "100.00%",
        "baseline_result": "100.00%"
      },
      {
        "method_name": "ARI-LSTM (per-class)",
        "paper_reference": "Agrawal et al., 2019",
        "metric": "Accuracy (Injection)",
        "their_result": "98.89%",
        "baseline_result": "98.33%"
      },
      {
        "method_name": "ARI-LSTM (per-class)",
        "paper_reference": "Agrawal et al., 2019",
        "metric": "Accuracy (Mixed)",
        "their_result": "100.00%",
        "baseline_result": "99.60%"
      }
    ],
    "performance_metrics_used": [
      "AUROC",
      "Accuracy",
      "F1-score"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Static analysis methods struggle with zero-day malware where signatures change.",
        "Dynamic analysis approaches are limited by customized VM environments.",
        "Deep learning-based dynamic approaches have black-box nature that hinders interpretability."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Develop a robust, mathematically grounded, and interpretable malware detection method that overcomes limitations of static and dynamic analysis by modeling program structure and dependencies via LLVM into complex networks and classifying with GNNs.",
      "potential_research_ideas": [
        "Extend MGN to binaries/assembly without requiring source code by constructing graphs from disassembly or IR (e.g., LLVM from lifted binaries or VEX).",
        "Cross-language generalization: support Java, Python, and Android apps (DEX) with language-agnostic IR and evaluate domain adaptation.",
        "Incorporate heterogeneous graphs capturing API/library calls, system calls, and control- and data-flow as typed edges for richer semantics.",
        "Adversarial robustness: study evasion via code obfuscation and semantics-preserving transformations; develop adversarial training and graph sanitization defenses.",
        "Self-supervised pretraining on large unlabeled code graphs (contrastive learning or masked node/edge modeling) to improve zero-day detection.",
        "Hierarchical/pooled graph representations (e.g., DiffPool, SAGPool, TopKPool) to capture multi-scale program structure and reduce graph size.",
        "Temporal/dynamic graph modeling to capture execution phases using sequence of dependency graphs and temporal GNNs.",
        "Open-set and out-of-distribution malware detection calibrated with uncertainty estimation (e.g., deep ensembles, temperature scaling).",
        "Explainable GNNs for code (e.g., GNNExplainer, PGExplainer) to highlight critical operations/edges as human-interpretable signatures.",
        "Integrate edge weights and attributes (data sizes, latencies) directly into message passing with edge-aware aggregators or attention."
      ],
      "architectural_improvement_recommendations": [
        "Use edge-aware GNNs (e.g., Graph Attention Networks with edge features or EGNN) to leverage weighted dependencies.",
        "Adopt Graph Transformers with positional/structural encodings (Laplacian eigenvectors, RWSE) for long-range dependencies.",
        "Introduce hierarchical pooling (SAGPool/DiffPool) and virtual nodes to capture program-level context.",
        "Multi-view fusion of control-flow graphs (CFG), data-flow graphs (DFG), and call graphs via heterogeneous GNNs (R-GCN/HGT).",
        "Apply self-supervised pretraining (graph contrastive learning) followed by supervised fine-tuning on malware labels.",
        "Calibrate outputs and support open-set detection with energy-based scores or temperature scaling.",
        "Automate hyperparameter search and neighbor sampling strategies; consider layer-wise residual connections and normalization."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces Malware Graph Network (MGN): transforms C/C++ program LLVM IR into weighted dependency graphs and classifies with a GraphSAGE-based GNN.",
      "Uses operation names as node features and profiles data/control dependencies; edges weighted by data sizes/latency from dynamic traces.",
      "Demonstrates high performance on a 24,376-program dataset (12,815 malware, 11,561 benign), achieving AUROC of 99.85% and ACC of 98.55%.",
      "Outperforms state-of-the-art baselines across multiple feature modalities (images, n-grams, bytes, opcodes).",
      "Provides ablation study showing benefits of an embedding layer, Leaky-ReLU, and 6 GraphSAGE layers (hidden size 128).",
      "Presents interpretability via topology-based features and t-SNE visualization separating benign vs. malicious graph structures."
    ]
  },
  {
    "arxiv_id": "2312.13993v1",
    "title": "Open-Set: ID Card Presentation Attack Detection using Neural Transfer Style",
    "authors": "Reuben Markham; Juan M. Espin; Mario Nieto-Hidalgo; Juan E. Tapia",
    "abstract": "The accurate detection of ID card Presentation Attacks (PA) is becoming increasingly important due to the rising number of online/remote services that require the presentation of digital photographs of ID cards for digital onboarding or authentication. Furthermore, cybercriminals are continuously searching for innovative ways to fool authentication systems to gain unauthorized access to these services. Although advances in neural network design and training have pushed image classification to the state of the art, one of the main challenges faced by the development of fraud detection systems is the curation of representative datasets for training and evaluation. The handcrafted creation of representative presentation attack samples often requires expertise and is very time-consuming, thus an automatic process of obtaining high-quality data is highly desirable. This work explores ID card Presentation Attack Instruments (PAI) in order to improve the generation of samples with four Generative Adversarial Networks (GANs) based image translation models and analyses the effectiveness of the generated data for training fraud detection systems. Using open-source data, we show that synthetic attack presentations are an adequate complement for additional real attack presentations, where we obtain an EER performance increase of 0.63% points for print attacks and a loss of 0.29% for screen capture attacks.",
    "published_date": "2023-12-21",
    "pdf_link": "https://arxiv.org/pdf/2312.13993v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Identity and Access Management",
      "subdomain": "Document Authentication / Presentation Attack Detection",
      "specific_problem": "ID card presentation attack detection for print and screen recapture attacks using GAN-generated synthetic PA data to augment training",
      "attack_types": [
        "print (scan-print) presentation attacks",
        "screen recapture presentation attacks"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "GAN",
        "specific": "pix2pix",
        "novel_contribution": "Used to translate bona fide ID card images to print/screen PA domains with automatically generated paired data via ORB-based homography alignment"
      },
      {
        "type": "primary",
        "category": "GAN",
        "specific": "pix2pixHD",
        "novel_contribution": "High-resolution conditional image-to-image translation for generating full ID card PA images"
      },
      {
        "type": "primary",
        "category": "GAN",
        "specific": "CycleGAN",
        "novel_contribution": "Unpaired translation from bona fide to PA domains for both print and screen tasks"
      },
      {
        "type": "primary",
        "category": "GAN",
        "specific": "CUT (Contrastive Unpaired Translation)",
        "novel_contribution": "Patch-wise contrastive unpaired translation to preserve content while transferring PA textures"
      },
      {
        "type": "primary",
        "category": "CNN",
        "specific": "MobileNetV2 (ImageNet-pretrained, frozen backbone)",
        "novel_contribution": "Backbone for PAD classifier; trained to discriminate bona fide vs print/screen PA using synthetic and real samples"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "MIDV-2020",
        "type": "public",
        "domain": "document_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "DLC-2021 (Document Liveness Challenge 2021)",
        "type": "public",
        "domain": "document_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Generated synthetic PA (GAN-translated from bona fide)",
        "type": "synthetic",
        "domain": "document_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "MobileNetV2 PAD trained on real-only data (print task)",
        "paper_reference": null,
        "metric": "EER",
        "their_result": "“we obtain an EER performance increase of 0.63 % points for print attacks” compared to real-only baseline",
        "baseline_result": null
      },
      {
        "method_name": "MobileNetV2 PAD trained on real-only data (screen task)",
        "paper_reference": null,
        "metric": "EER",
        "their_result": "“a loss of 0.29 % for screen capture attacks” when using synthetic PA augmentation vs real-only baseline",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "EER"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Does augmenting training with GAN-generated synthetic presentation attack samples (instead of additional bona fide samples) yield comparable or improved PAD performance?",
        "Can supervised and unsupervised image-to-image translation models effectively transfer print and screen PA characteristics to bona fide ID card images?"
      ],
      "gaps_identified": [
        "Obtaining diverse, representative, and privacy-compliant ID document datasets is difficult due to legal constraints (e.g., GDPR) and data sensitivity.",
        "Handcrafted creation of presentation attack samples is time-consuming and requires expertise.",
        "Most prior PAD works rely on private datasets, hindering reproducibility and public benchmarking."
      ],
      "limitations": [
        "Public synthetic ID datasets have limited commercial applicability due to reduced number of subjects.",
        "Results are derived from open-source datasets with synthetic subjects; generalization to real-world production data is unvalidated.",
        "Only one PAD backbone (MobileNetV2, frozen backbone) is evaluated; impact of stronger architectures remains unknown."
      ],
      "future_work": [],
      "motivation": "Reduce dependence on scarce, private PA data by generating realistic synthetic presentation attacks and assess their utility for training open-access ID card PAD systems.",
      "potential_research_ideas": [
        "Evaluate cross-dataset and cross-device generalization, including real bank/government datasets via privacy-preserving protocols.",
        "Explore diffusion-based image-to-image models for higher-fidelity PA generation and compare against GANs.",
        "Incorporate self-supervised or contrastive pretraining for the PAD backbone using large unlabeled document video frames.",
        "Develop open-set/OOD PAD detection that outputs calibrated uncertainty for unseen PAI species.",
        "Joint training of generator and detector (adversarial data augmentation loop) to target failure modes adaptively.",
        "Physics- and photo-metric modeling of print and screen artifacts (Moiré, specular highlights, halftoning) to guide generation.",
        "Per-pixel PA segmentation to provide localized evidence and enable explainability."
      ],
      "architectural_improvement_recommendations": [
        "Unfreeze later stages of MobileNetV2 or adopt modern lightweight transformers (e.g., MobileViT, ConvNeXt) with fine-tuning.",
        "Use multi-scale inputs and high-frequency branches to better capture print/screen artifacts.",
        "Add uncertainty estimation (e.g., Monte Carlo Dropout, deep ensembles) for open-set PAD decisions.",
        "Train with domain adversarial alignment or style randomization to improve robustness across capture devices and lighting.",
        "Augment with patch-level auxiliary tasks (e.g., artifact classification for Moiré or halftone detection) to guide features."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Training ran for 200 epochs (GANs, batch size 1 on 224x224 crops) and 100 epochs (PAD, batch size 128). Hardware: 32 CPU cores, 236 GB RAM, 40 GB GPU."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Legal/privacy constraints (e.g., GDPR) limit access to real ID document data.",
        "Effort and expertise required to craft diverse PAIs (print/screen) for training and evaluation.",
        "Generalization to varied devices, materials, and environmental conditions remains uncertain."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Comprehensive analysis of SOTA related to ID card PAD and open-access databases.",
      "Exploration of GAN-based methods (pix2pix, pix2pixHD, CycleGAN, CUT) to generate synthetic images simulating print and screen PA.",
      "Development of supervised and unsupervised PA generation methods that retain original bona fide content for full ID cards.",
      "Training of an ID card PAD system using only open-access databases, reporting improvements/limitations achievable with such data.",
      "Demonstration that synthetic attack presentations can complement real attacks, with an EER improvement of 0.63 percentage points for print and a 0.29 percentage point loss for screen."
    ]
  },
  {
    "arxiv_id": "2311.17097v1",
    "title": "Anonymous Jamming Detection in 5G with Bayesian Network Model Based Inference Analysis",
    "authors": "Ying Wang; Shashank Jere; Soumya Banerjee; Lingjia Liu; Sachin Shetty; Shehadi Dayekh",
    "abstract": "Jamming and intrusion detection are critical in 5G research, aiming to maintain reliability, prevent user experience degradation, and avoid infrastructure failure. This paper introduces an anonymous jamming detection model for 5G based on signal parameters from the protocol stacks. The system uses supervised and unsupervised learning for real-time, high-accuracy detection of jamming, including unknown types. Supervised models reach an AUC of 0.964 to 1, compared to LSTM models with an AUC of 0.923 to 1. However, the need for data annotation limits the supervised approach. To address this, an unsupervised auto-encoder-based anomaly detection is presented with an AUC of 0.987. The approach is resistant to adversarial training samples. For transparency and domain knowledge injection, a Bayesian network-based causation analysis is introduced.",
    "published_date": "2023-11-28",
    "pdf_link": "https://arxiv.org/pdf/2311.17097v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Wireless/Cellular Security",
      "specific_problem": "Anonymous RF jamming detection in 5G NSA using cross-layer protocol statistics with causal inference",
      "attack_types": [
        "RF jamming",
        "WiFi-type interference at LTE/NR bands",
        "5G NR (N78) jamming",
        "LTE uplink jamming (B1 1.95 GHz)",
        "LTE downlink jamming (B1 2.14 GHz)",
        "Control channel jamming",
        "Data channel jamming"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Tree Ensemble",
        "specific": "Random Forest",
        "novel_contribution": "Used as a lightweight instantaneous discriminator on cross-layer 5G statistics; achieved 100% accuracy for binary jamming detection across listed types"
      },
      {
        "type": "primary",
        "category": "Decision Tree",
        "specific": null,
        "novel_contribution": "Instantaneous single-timestamp classifier for fast on-field detection; high accuracy and feasible for UE/IoT deployment"
      },
      {
        "type": "baseline",
        "category": "Linear Model",
        "specific": "Logistic Regression",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Probabilistic",
        "specific": "Gaussian Naive Bayes",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "KNN",
        "specific": "K-Neighbors Classifier",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "RNN",
        "specific": "LSTM (1 hidden layer, 100 LSTM units, FC-100, 2-class output)",
        "novel_contribution": "Temporal-based detector over k=2 time steps (LTE+NR samples) to exploit temporal/cross-cell correlation; augmented with BNM-guided inference for scenario-specific improvement"
      },
      {
        "type": "primary",
        "category": "Autoencoder",
        "specific": "Ensemble of autoencoders with hierarchical feature clustering and secondary AE (from [12])",
        "novel_contribution": "Unsupervised anomaly detector trained on no-jamming data; robust to adversarial/poisoned training during early phase and with graceful degradation thereafter"
      },
      {
        "type": "primary",
        "category": "Probabilistic Graphical Model",
        "specific": "Bayesian Network Model (BNM)",
        "novel_contribution": "Domain-knowledge-informed DAG for causal analysis (CQI→MCS→Throughput) to provide transparency and improve detection/diagnosis"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "5G NSA jamming/interference testbed traces (UE+BS cross-layer statistics)",
        "type": "proprietary",
        "domain": "wireless_signal_metrics",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "LSTM temporal-based detector",
        "paper_reference": null,
        "metric": "AUC",
        "their_result": "AUC 0.964–1.0 (instantaneous supervised tree-based classifiers)",
        "baseline_result": "AUC 0.923–1.0 (LSTM)"
      },
      {
        "method_name": "Random Forest vs other instantaneous classifiers",
        "paper_reference": null,
        "metric": "Accuracy (binary detection: jamming vs no jamming)",
        "their_result": "Random Forest: 100% accuracy for all listed jamming types",
        "baseline_result": "Other methods (e.g., logistic regression, Gaussian NB, KNN, decision tree) lower; exact values not reported"
      },
      {
        "method_name": "BNM-augmented LSTM vs raw LSTM (LTE DL -5 dBm scenario)",
        "paper_reference": null,
        "metric": "Precision / Recall",
        "their_result": "Precision 80.9%, Recall 86.4% (with BNM causal evidence: MCS variance increase observed)",
        "baseline_result": "Precision 46.7%, Recall 85.8% (raw LSTM)"
      },
      {
        "method_name": "Unsupervised ensemble autoencoder (unknown jamming detection)",
        "paper_reference": "[12]",
        "metric": "Per-class accuracy; AUC (robustness tests)",
        "their_result": "No interference: 96.7%; 5G NR -11 dBm: 100%; LTE UL (0/-5/-11 dBm): 100%; AUC 0.987 overall",
        "baseline_result": "N/A (no external baseline reported); fails to detect LTE DL interference due to overlapping feature distributions"
      }
    ],
    "performance_metrics_used": [
      "AUC (ROC)",
      "Accuracy",
      "Precision",
      "Recall"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can cross-layer 5G NSA protocol statistics enable fast, high-accuracy detection of jamming without relying on raw IQ data?",
        "How do instantaneous (single-timestamp) discriminative models compare to temporal LSTM models for jamming detection?",
        "Can an unsupervised anomaly detector trained only on non-interference data reliably detect unknown jamming types?",
        "Can a Bayesian Network provide transparent causal explanations (e.g., CQI→MCS→Throughput) and improve detection/diagnosis performance?",
        "How robust is the unsupervised approach to adversarial/poisoned training samples and at which training phases is it most vulnerable?"
      ],
      "gaps_identified": [
        "Preventive/APD measures are ineffective against unforeseen or zero-day attacks.",
        "Existing anomaly IDS lack a synthesized understanding of cross-layer responses and upper-layer domain knowledge, limiting accuracy/efficiency.",
        "Prior approaches often rely on IQ samples at lower layers, requiring large caches and heavy processing.",
        "Causal transparency is lacking in many ML-based detectors."
      ],
      "limitations": [
        "Unsupervised autoencoder approach fails to detect LTE downlink interference because its features overlap with no-interference distributions.",
        "Lower LSTM performance for LTE DL -5 dBm due to limited training data sample size.",
        "Model evaluation limited to WiFi-type interference waveforms and specific center frequencies/power levels in a lab RF enclosure.",
        "Proprietary dataset/testbed limits external reproducibility.",
        "BNM inference can suffer from computational complexity and cumulative error with larger DAGs (mitigated here by domain-informed topology)."
      ],
      "future_work": [],
      "motivation": "Reliable, transparent, and real-time detection of known and unknown jamming in 5G to protect mission-critical applications; bridge IDS/APD by combining ML detection and causal analysis.",
      "potential_research_ideas": [
        "Develop self-supervised or contrastive pretraining on cross-layer 5G telemetry to improve unsupervised anomaly detection, especially for LTE DL-like overlaps.",
        "Incorporate raw PHY features (e.g., limited IQ statistics, spectral moments) alongside protocol KPIs for multi-modal detection without heavy IQ storage.",
        "Domain adaptation and transfer learning across bands, deployments, and vendors to improve generalization in private/public 5G networks.",
        "Online learning with drift detection to adapt thresholds/models to evolving RF environments and jammer strategies.",
        "Federated or split learning across base stations/edges to learn robust detectors without sharing raw telemetry.",
        "Causal discovery augmentation to refine BNM structure from data while preserving domain constraints; use counterfactual reasoning for root cause analysis.",
        "Adversarially robust anomaly detection (e.g., certified defenses, robust clustering) tailored to poisoning in different training phases.",
        "Explainability dashboards combining BNM causal paths with feature attributions (e.g., SHAP) for operator insights."
      ],
      "architectural_improvement_recommendations": [
        "Replace or complement LSTM with Temporal Convolutional Networks or Transformer encoders for better temporal/context modeling.",
        "Use deep one-class objectives (Deep SVDD) or normalizing flows for density estimation to improve separation in LTE DL cases.",
        "Hybrid model: supervised tree-based detector cascaded with unsupervised AE and BNM-driven post-hoc verification to reduce false positives.",
        "Feature engineering: derive stability/variance metrics (e.g., temporal CQI/MCS variance at multiple horizons) and cross-cell consistency features.",
        "Calibration and thresholding: apply temperature scaling or isotonic regression to produce calibrated anomaly scores for actionable thresholds.",
        "Active learning to reduce annotation cost by querying the most uncertain/impactful samples.",
        "Data augmentation: simulate realistic interference/jamming profiles (beyond WiFi-like) including control/data-channel-specific patterns."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "MATLAB (Signal Generation Toolbox)"
      ],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "5G NSA testbed with UE, Base Station (O-RAN-compliant option), Core Network; RF enclosure with external signal generator (R&S SMW2000A)",
      "scalability_discussed": true,
      "inference_time": "Instantaneous detection within one sample period (180 ms between samples; can be reduced with higher-performance hardware)",
      "deployment_challenges": [
        "Supervised approach requires data annotation and knowledge of known jamming vocabulary.",
        "Model performance sensitive to limited training data for some scenarios (e.g., LTE DL -5 dBm).",
        "Unsupervised model struggles when feature distributions of interference and normal overlap (LTE DL).",
        "Potential vulnerability to training data poisoning outside the initial clustering phase; early/late training more vulnerable.",
        "Integration constraints for deployment at BS or as a separate node; compute trade-offs for UE/IoT migration.",
        "Coverage of jammer types and RF environments may limit generalization; need for broader waveform profiles."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": true,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces a cross-layer, anonymous jamming detection system for 5G NSA using both supervised and unsupervised learning.",
      "Design and implementation of a full 5G platform (UE, BS, core network) with a cyber attack generator and RF signal generator for data collection.",
      "Instantaneous discriminative models (tree-based) achieving AUC 0.964–1.0 and 100% binary detection accuracy in listed scenarios.",
      "Temporal LSTM-based detector (k=2) with detailed precision/recall per scenario; BNM augmentation improves LTE DL -5 dBm precision/recall.",
      "Unsupervised ensemble autoencoder trained on non-interference data achieving AUC 0.987 and 96.7–100% per-class accuracy for several unknown jamming types.",
      "Robustness analysis against adversarial/poisoned training showing immunity during initial clustering phase and graceful degradation otherwise.",
      "Bayesian Network Model for causal transparency and root/direct/indirect cause analysis (CQI→MCS→Throughput) with demonstrated performance gains."
    ]
  },
  {
    "arxiv_id": "2311.14594v1",
    "title": "MABFuzz: Multi-Armed Bandit Algorithms for Fuzzing Processors",
    "authors": "Vasudev Gohil; Rahul Kande; Chen Chen; Ahmad-Reza Sadeghi; Jeyavijayan Rajendran",
    "abstract": "As the complexities of processors keep increasing, the task of effectively verifying their integrity and security becomes ever more daunting. The intricate web of instructions, microarchitectural features, and interdependencies woven into modern processors pose a formidable challenge for even the most diligent verification and security engineers. To tackle this growing concern, recently, researchers have developed fuzzing techniques explicitly tailored for hardware processors. However, a prevailing issue with these hardware fuzzers is their heavy reliance on static strategies to make decisions in their algorithms. To address this problem, we develop a novel dynamic and adaptive decision-making framework, MABFuzz, that uses multi-armed bandit (MAB) algorithms to fuzz processors. MABFuzz is agnostic to, and hence, applicable to, any existing hardware fuzzer. In the process of designing MABFuzz, we encounter challenges related to the compatibility of MAB algorithms with fuzzers and maximizing their efficacy for fuzzing. We overcome these challenges by modifying the fuzzing process and tailoring MAB algorithms to accommodate special requirements for hardware fuzzing.   We integrate three widely used MAB algorithms in a state-of-the-art hardware fuzzer and evaluate them on three popular RISC-V-based processors. Experimental results demonstrate the ability of MABFuzz to cover a broader spectrum of processors' intricate landscapes and doing so with remarkable efficiency. In particular, MABFuzz achieves up to 308x speedup in detecting vulnerabilities and up to 5x speedup in achieving coverage compared to a state-of-the-art technique.",
    "published_date": "2023-11-24",
    "pdf_link": "https://arxiv.org/pdf/2311.14594v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Hardware Security",
      "subdomain": "Hardware Fuzzing and Verification",
      "specific_problem": "Dynamic seed selection to maximize coverage and accelerate vulnerability detection in processor fuzzing",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Multi-Armed Bandit (Reinforcement Learning)",
        "specific": "ε-greedy",
        "novel_contribution": "Adapted to hardware fuzzing via a γ-window coverage saturation monitor and a reset-arms mechanism; reward shaping combining local and global new coverage points."
      },
      {
        "type": "primary",
        "category": "Multi-Armed Bandit (Reinforcement Learning)",
        "specific": "UCB (Upper Confidence Bound)",
        "novel_contribution": "Modified to support reset arms (reinitialize N(a), Q(a)) under diminishing returns characteristic of fuzzing; uses γ-window saturation detection."
      },
      {
        "type": "primary",
        "category": "Multi-Armed Bandit (Reinforcement Learning)",
        "specific": "EXP3",
        "novel_contribution": "Modified to support reset arms by resetting weight to average of other arms and normalized rewards; tailored to non-stationary, diminishing-return fuzzing rewards."
      }
    ],
    "learning_paradigm": [
      "Reinforcement Learning",
      "Online Learning",
      "Multi-armed Bandits"
    ],
    "datasets": [
      {
        "name": "CVA6 (Ariane) RISC-V core",
        "type": "public",
        "domain": "hardware_processors",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Rocket Core (RISC-V)",
        "type": "public",
        "domain": "hardware_processors",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "BOOM (Berkeley Out-of-Order Machine)",
        "type": "public",
        "domain": "hardware_processors",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "TheHuzz (state-of-the-art simulation-based processor fuzzer)",
        "paper_reference": "[7]",
        "metric": "Vulnerability detection tests (lower is better) and relative speedup",
        "their_result": "“up to 308.89× speedup” on V7; per-vulnerability speedups: V1 ε-greedy 21.56× / UCB 13.04× / EXP3 4.5×; V2 0.73× / 2.26× / 8.33×; V3 59.75× / 7.71× / 11.95×; V4 2.43× / 1.48× / 1.22×; V5 0.35× / 0.13× / 0.63×; V6 2.33× / 2.11× / 2.36×; V7 308.89× / 185.34× / 73.16×",
        "baseline_result": "TheHuzz #tests: V1 6.00×10^2; V2 1.48×10^3; V3 2.39×10^2; V4 1.20×10^3; V5 2.50×10^1; V6 1.41×10^2; V7 9.27×10^2"
      },
      {
        "method_name": "TheHuzz (state-of-the-art simulation-based processor fuzzer)",
        "paper_reference": "[7]",
        "metric": "Coverage speedup (branch coverage)",
        "their_result": "“up to 5.38× speedup in achieving coverage compared to the state-of-the-art technique.”",
        "baseline_result": "Coverage achieved by TheHuzz (values not explicitly provided)"
      }
    ],
    "performance_metrics_used": [
      "branch coverage",
      "vulnerability detection speed (# of tests to trigger bug)",
      "relative speedup vs baseline (coverage and detection)"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can multi-armed bandit algorithms dynamically select fuzzing seeds to increase hardware coverage and detect processor vulnerabilities faster than static strategies?",
        "How to adapt MAB algorithms to the diminishing marginal returns characteristic of hardware fuzzing?",
        "How to monitor when a seed/arm is saturated and design MAB mechanisms to reset/replace ineffective arms?"
      ],
      "gaps_identified": [
        "Existing hardware fuzzers rely heavily on static decision strategies (e.g., static seed selection, FIFO queues) that ignore evolving design coverage and exploration needs.",
        "Prior work only applied dynamic strategies to mutation operators, not to other static decisions like seed selection.",
        "MAB algorithms had not been explored for hardware processor fuzzing.",
        "Classical MAB algorithms assume stationary or slowly varying rewards and do not directly handle diminishing returns common in fuzzing."
      ],
      "limitations": [
        "MABFuzz was slower than TheHuzz on an easy-to-find bug (V5) because the baseline detected it in only 25 tests.",
        "Evaluation limited to three open-source RISC-V processors; no results on closed-source/commercial designs mentioned.",
        "Only three MAB algorithms evaluated (ε-greedy, UCB, EXP3); no Thompson Sampling/discounted/sliding-window variants reported.",
        "Sensitivity analysis for hyperparameters (α, γ, η, number of arms) not reported in the provided text."
      ],
      "future_work": [],
      "motivation": "Static decision-making in hardware fuzzers leads to inefficient exploration; a dynamic, adaptive approach balancing exploration and exploitation is needed to maximize coverage and accelerate vulnerability detection.",
      "potential_research_ideas": [
        "Use contextual bandits leveraging seed/test features (e.g., instruction mix, control-flow depth, microarchitectural event stats) for arm selection.",
        "Adopt non-stationary bandit algorithms (discounted-UCB, sliding-window UCB, EXP3.S, SW-TS) to better track changing returns during fuzzing.",
        "Hierarchical/bilevel bandits: top-level selects seeds; lower-level selects mutation operators and parameters jointly.",
        "Multi-objective reward shaping combining branch coverage, state coverage (e.g., register/memory toggles), and differential testing bug likelihood.",
        "Batched/parallel bandit selection to utilize multi-core simulation farms efficiently with delayed feedback handling.",
        "Auto-tuning or meta-bandits to learn α, γ, η online; or Bayesian optimization over hyperparameters.",
        "Integrate FPGA/emulation-based fuzzing to accelerate feedback and scale to longer tests; combine with bandits for test budgeting.",
        "Transfer learning across processors: warm-start arm priors from prior designs or from ISA-level similarity.",
        "Incorporate Thompson Sampling and Bayesian bandits with priors over seed efficacy to improve exploration early.",
        "Credit assignment at finer granularity (per-test/per-mutation) with off-policy evaluation to improve reward estimates."
      ],
      "architectural_improvement_recommendations": [
        "Replace vanilla algorithms with non-stationary bandits (discounted/sliding-window UCB, EXP3.S) instead of ad-hoc resets; detect change points statistically.",
        "Add contextual features and deploy LinUCB/Linear Thompson Sampling; featurize seeds by instruction histograms, coverage signatures, or last-k coverage deltas.",
        "Implement batched arm pulls with delayed feedback corrections; use queueing-aware UCB to handle simulator latencies.",
        "Refine reward: weight coverage by rarity/novelty (e.g., inverse visitation counts) and proximity to differential mismatches.",
        "Adaptive γ-window per-arm based on variance of recent rewards; learn/reset thresholds online.",
        "Parallelize mutation and simulation pipelines; use prioritization queues informed by bandit confidence intervals.",
        "Combine seed-selection bandit with operator-selection bandit (multi-agent) for end-to-end adaptive fuzzing."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "Python",
        "Synopsys VCS (simulation)",
        "Chipyard (SoC environment)",
        "SPIKE (RISC-V ISA simulator)"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "CentOS on Intel Xeon (64 threads), 512 GB RAM, 2.6 GHz; 50,000 tests per benchmark per fuzzer; parameters: number of arms=10, α=0.25, γ=3, η=0.1; repeated each experiment ≥3 times."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "RTL simulation environment (Synopsys VCS, Chipyard, SPIKE differential testing)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "“the first technique that uses MAB algorithms, MABFuzz, to select test inputs in hardware fuzzers.”",
      "“We overcome challenges in adapting MAB to hardware fuzzers. In particular, we develop monitors to identify saturated inputs and modify MAB algorithms to handle such test inputs.”",
      "“We integrate three widely-used MAB algorithms in a hardware fuzzer, demonstrating the agnostic nature of our technique.”",
      "“We evaluate MABFuzz on three widely-used, open-sourced RISC-V processors and achieve up to 308.89× speedup in detecting vulnerabilities and up to 5.38× speedup in achieving coverage compared to the state-of-the-art simulation-based fuzzer.”"
    ]
  },
  {
    "arxiv_id": "2312.04940v1",
    "title": "Canaries and Whistles: Resilient Drone Communication Networks with (or without) Deep Reinforcement Learning",
    "authors": "Chris Hicks; Vasilios Mavroudis; Myles Foley; Thomas Davies; Kate Highnam; Tim Watson",
    "abstract": "Communication networks able to withstand hostile environments are critically important for disaster relief operations. In this paper, we consider a challenging scenario where drones have been compromised in the supply chain, during their manufacture, and harbour malicious software capable of wide-ranging and infectious disruption. We investigate multi-agent deep reinforcement learning as a tool for learning defensive strategies that maximise communications bandwidth despite continual adversarial interference. Using a public challenge for learning network resilience strategies, we propose a state-of-the-art expert technique and study its superiority over deep reinforcement learning agents. Correspondingly, we identify three specific methods for improving the performance of our learning-based agents: (1) ensuring each observation contains the necessary information, (2) using expert agents to provide a curriculum for learning, and (3) paying close attention to reward. We apply our methods and present a new mixed strategy enabling expert and learning-based agents to work together and improve on all prior results.",
    "published_date": "2023-12-08",
    "pdf_link": "https://arxiv.org/pdf/2312.04940v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Cyber-Physical Systems Security",
      "subdomain": "UAV/Drone Swarm Security",
      "specific_problem": "Maintaining resilient ad-hoc drone communications when drones are supply-chain compromised with persistent malware",
      "attack_types": [
        "Supply-chain compromise (firmware malware)",
        "Remote exploitation",
        "Privilege escalation",
        "Bandwidth flooding/consumption",
        "Traffic blocking (IP block list manipulation)",
        "Message interception/eavesdropping"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Rule-based Expert System",
        "specific": "Canary protocol (expert agent)",
        "novel_contribution": "Proposes a state-of-the-art expert technique (Canary) for defending drone communication networks; shown superior to standard DRL agents in the challenge."
      },
      {
        "type": "primary",
        "category": "Reinforcement Learning (Multi-agent Deep RL)",
        "specific": null,
        "novel_contribution": "Introduces a mixed strategy where expert and learning-based agents cooperate; identifies three improvements for MARL agents: (1) ensure observations contain necessary info, (2) use expert agents to provide a curriculum, (3) careful reward design."
      },
      {
        "type": "baseline",
        "category": "Reinforcement Learning (Multi-agent)",
        "specific": null,
        "novel_contribution": "Standard MARL agents in the default setting used as baselines; found unsatisfactory."
      },
      {
        "type": "baseline",
        "category": "Heuristic/No-op",
        "specific": "Sleep action only (no defense)",
        "novel_contribution": "Used to bound worst-case performance under unchecked malware."
      }
    ],
    "learning_paradigm": [
      "Reinforcement Learning",
      "Deep Reinforcement Learning",
      "Multi-agent Reinforcement Learning",
      "Curriculum Learning"
    ],
    "datasets": [
      {
        "name": "CybORG (Cyber Operations Research Gym)",
        "type": "public",
        "domain": "cyber_range_simulation",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CAGE Challenge (CybORG Challenge 3) drone-swarm scenario",
        "type": "public",
        "domain": "ad_hoc_drone_network_simulation",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Green agents only (malware disabled) — upper-bound noise estimate",
        "paper_reference": null,
        "metric": "Average episode score over 1000 episodes (max 0; min -9000)",
        "their_result": null,
        "baseline_result": "−355.3 with a standard deviation of 253.2"
      },
      {
        "method_name": "Sleep-only blue defense (no-op) — lower-bound baseline",
        "paper_reference": null,
        "metric": "Average episode score over 1000 episodes",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Standard DRL/MARL agents (unspecified algorithms)",
        "paper_reference": null,
        "metric": "Average episode score over 1000 episodes / leaderboard rank",
        "their_result": "Outperformed by proposed expert/mixed strategies",
        "baseline_result": null
      },
      {
        "method_name": "Prior CAGE challenge submissions (12 approaches from 8 teams)",
        "paper_reference": null,
        "metric": "Leaderboard average score",
        "their_result": "Authors: “improve on all prior results” and Canary “currently ranks as the top performing agent”",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Per-timestep reward (penalties) per Table 1",
      "Average episode score over 1000 episodes (episode length up to 500 steps)",
      "Leaderboard rank",
      "Standard deviation of episode scores"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "How difficult is the CAGE challenge?",
        "Can multi-agent deep reinforcement learning learn defensive strategies that maximise communications bandwidth under continual adversarial interference?",
        "What methods improve the performance of learning-based agents for this task (observation design, curriculum from experts, reward shaping)?"
      ],
      "gaps_identified": [
        "Few ACD simulators support MARL; only two environments support MARL and only the latest CybORG challenge is defense-focused.",
        "Standard DRL in this setting yields unsatisfactory results without careful observation design, curriculum, and reward tuning.",
        "Reward function structure (only negatives) and green-agent-induced noise make evaluation and learning harder."
      ],
      "limitations": [
        "Evaluation is conducted in a simulated environment (CybORG/CAGE drone-swarm scenario).",
        "Green agents cause unavoidable negative rewards due to unroutable destinations, raising the floor on achievable scores."
      ],
      "future_work": [],
      "motivation": "Enable resilient ad-hoc drone communication networks for disaster relief despite inevitable malware compromises from supply-chain attacks; evaluate MARL-based autonomous cyber defense where human expertise is scarce.",
      "potential_research_ideas": [
        "Develop centralized-training, decentralized-execution MARL (e.g., value factorization, MAPPO) tailored to this scenario and compare rigorously.",
        "Learn inter-drone communication protocols to utilize the 16-bit broadcast channel effectively (differentiable communication or discrete message learning).",
        "Employ graph neural networks over the dynamic network topology for state aggregation and decision-making.",
        "Imitation learning or DAgger from the Canary expert to accelerate MARL training and reduce exploration burden.",
        "Robust MARL with adversarial training or domain randomization against red-strategy switching and stochastic action failures.",
        "Hierarchical RL to separate local remediation actions (e.g., retake vs. block) from global coordination (routing/bandwidth management).",
        "Automated curriculum generation (teacher–student self-play) where expert difficulty and malware strategies are scheduled adaptively.",
        "Model-based or planning-augmented RL to handle delayed effects and partial observability in reclaim/block decisions.",
        "Counterfactual credit assignment methods (e.g., COMA-style) to improve multi-agent credit assignment under shared rewards.",
        "Multi-objective formulations balancing bandwidth, latency, and interception risk; investigate constrained RL for safety."
      ],
      "architectural_improvement_recommendations": [
        "Adopt CTDE with value factorization (e.g., QMIX/VDA) or MAPPO for stability and scalability in MARL.",
        "Integrate temporal memory (LSTM/GRU or transformer) to address the POMDP nature beyond the current observation vector.",
        "Use attention-based or GNN encoders to process sets of neighbors and dynamic network graphs.",
        "Design and learn discrete communication protocols over the 16-bit broadcast, including error-correcting codes and coordination messages.",
        "Leverage expert demonstrations from Canary for behavior cloning pretraining followed by RL fine-tuning.",
        "Refine reward shaping (dense intermediates for timely retake, penalties for over-blocking) and incorporate shaped credit assignment.",
        "Prioritized and stratified replay to handle rare but catastrophic events (e.g., complete compromise penalties)."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "CybORG",
        "PettingZoo",
        "Gym"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Simulated ad-hoc drone swarm network (CAGE challenge in CybORG)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Non-stationary multi-agent learning and strategy switching by the adversary",
        "Partial observability with limited-range sensing and a 16-bit broadcast channel",
        "Bandwidth constraints and costs of defensive remote actions",
        "Stochastic action failures and false positives/negatives in detection",
        "Unavoidable negative rewards from green-agent unroutable messages",
        "Dynamic swarm topology affecting routing and defense coordination"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes a state-of-the-art expert agent (Canary) for resilient drone communications in the CAGE challenge scenario.",
      "Demonstrates that standard DRL/MARL agents are unsatisfactory without careful design.",
      "Identifies three concrete improvements for learning-based agents: (1) ensure observations contain necessary information, (2) use expert agents to provide a curriculum, (3) pay close attention to reward.",
      "Introduces a mixed expert + learning strategy that improves on all prior results.",
      "Provides baseline analysis of task difficulty, including quantifying green-agent-induced noise: “green agents alone induce an average score of −355.3 with a standard deviation of 253.2.”",
      "Reports leaderboard context: 12 approaches from 8 teams; Canary “currently ranks as the top performing agent.”"
    ]
  },
  {
    "arxiv_id": "2311.05261v1",
    "title": "RAGLog: Log Anomaly Detection using Retrieval Augmented Generation",
    "authors": "Jonathan Pan; Swee Liang Wong; Yidi Yuan",
    "abstract": "The ability to detect log anomalies from system logs is a vital activity needed to ensure cyber resiliency of systems. It is applied for fault identification or facilitate cyber investigation and digital forensics. However, as logs belonging to different systems and components differ significantly, the challenge to perform such analysis is humanly challenging from the volume, variety and velocity of logs. This is further complicated by the lack or unavailability of anomalous log entries to develop trained machine learning or artificial intelligence models for such purposes. In this research work, we explore the use of a Retrieval Augmented Large Language Model that leverages a vector database to detect anomalies from logs. We used a Question and Answer configuration pipeline. To the best of our knowledge, our experiment which we called RAGLog is a novel one and the experimental results show much promise.",
    "published_date": "2023-11-09",
    "pdf_link": "https://arxiv.org/pdf/2311.05261v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Security Operations and Monitoring",
      "subdomain": "Log Analytics / Anomaly Detection",
      "specific_problem": "Zero-shot anomaly detection in system logs using Retrieval-Augmented Generation (RAG) without log parsing",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "LLM / Transformer",
        "specific": "OpenAI GPT-3.5 (via Q&A prompting)",
        "novel_contribution": "Uses a RAG pipeline to classify a single log line as normal/abnormal in zero-shot mode using only retrieved normal log entries as context; no log parsing required."
      },
      {
        "type": "primary",
        "category": "Retrieval-Augmented Generation (RAG)",
        "specific": "Dense-vector retrieval with vector database and LangChain retriever",
        "novel_contribution": "Vector DB stores only normal log samples; retrieved nearest normal logs are injected into a Q&A prompt for anomaly judgment."
      },
      {
        "type": "primary",
        "category": "Text Embeddings / Dense Retrieval",
        "specific": "OpenAI Embeddings (pre-trained)",
        "novel_contribution": "Embeddings used to index normal logs; similarity search conditions the LLM for semantic comparison."
      },
      {
        "type": "primary",
        "category": "Clustering",
        "specific": "k-means (elbow method for K selection)",
        "novel_contribution": "Unsupervised clustering used to select representative normal log samples for the vector DB; shown to outperform random sampling."
      },
      {
        "type": "primary",
        "category": "Prompting",
        "specific": "Question-and-Answer template with constrained output",
        "novel_contribution": "Prompt constrains outputs to 'normal' or 'abnormal' to reduce hallucination and ease evaluation; temperature=0.1."
      }
    ],
    "learning_paradigm": [
      "Unsupervised",
      "Zero-shot",
      "Retrieval-Augmented Generation"
    ],
    "datasets": [
      {
        "name": "BGL (BlueGene/L) system logs",
        "type": "public",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Thunderbird system logs (Sandia)",
        "type": "public",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "LogPrompt (zero-shot LLM prompting for logs)",
        "paper_reference": "[20] Y. Liu et al., 'LogPrompt: Prompt Engineering Towards Zero-... (2023)'",
        "metric": "Precision, Recall, F1",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Precision",
      "Recall",
      "F1-score"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can Retrieval Augmented Generation (RAG) with an LLM perform log anomaly detection in a zero-shot setting using only normal log entries for retrieval context?"
      ],
      "gaps_identified": [
        "Supervised log anomaly detection is constrained by scarcity of labeled anomalous log entries and class imbalance.",
        "Unsupervised methods struggle to robustly detect diverse and evolving anomaly patterns over time and require retraining when new anomalies emerge.",
        "Prevailing pipelines require multi-stage preprocessing (parsing, feature engineering, partitioning), which adds complexity and can introduce errors; few models ingest raw logs end-to-end.",
        "Log parsers can misinterpret semantics and handle OOV tokens poorly (cited prior work).",
        "LLMs have token context limits, memory limits, and hallucinations; limited evaluation exists on LLMs for log anomaly detection.",
        "Prior LLM-based approaches show prompt sensitivity, window-size limitations, high false positives, and low precision in zero-shot setups."
      ],
      "limitations": [
        "High resource consumption and execution latency due to invoking the LLM and analyzing one log entry at a time.",
        "Evaluation constrained by API cost; only a random 20% of the test split was used for inference.",
        "Relies on proprietary OpenAI embedding and LLM services; model/version details and prompts are not fully disclosed.",
        "Evaluation limited to two HPC log datasets; no production or real-time deployment study.",
        "Vector database contents limited to normal logs; approach effectiveness for rare/novel normal patterns or severe distribution drift not fully explored."
      ],
      "future_work": [
        "Further optimize the RAG model to analyze logs faster and at larger volumes."
      ],
      "motivation": "Reduce the human and data-engineering burden of log anomaly detection under volume/variety/velocity, scarcity of anomalous labels, and LLM limitations by leveraging RAG with only normal log retrieval.",
      "potential_research_ideas": [
        "Batch or micro-batch RAG inference over streams of logs with adaptive windowing/sessionization to amortize LLM calls.",
        "Integrate a cross-encoder or lightweight reranker after vector retrieval to improve contextual relevance before prompting the LLM.",
        "Train or fine-tune compact local LLMs on log corpora (self-supervised) and distill RAG behavior to reduce latency and cost.",
        "Augment retrieval with template mining (e.g., Drain) or contrastive embedding training on log templates to improve nearest-neighbor quality without full parsing.",
        "Develop self-consistency and calibration strategies (e.g., majority voting over paraphrased prompts) to reduce false positives/negatives.",
        "Incorporate temporal/contextual features (burstiness, sequence order) with a hybrid RAG+sequence model for session-level anomaly detection.",
        "Adversarial robustness study against prompt injection or crafted anomalous logs; add retrieval filters and prompt guards.",
        "Explainability add-on that highlights mismatches between the query and retrieved normals (token- or phrase-level) and produces human-readable rationales.",
        "Active learning loop that triages uncertain cases for analyst feedback to update retrieval index and prompt exemplars.",
        "Evaluate alternative open embeddings (e.g., bge, E5) and vector stores with ANN indexes optimized for log corpora and drift handling."
      ],
      "architectural_improvement_recommendations": [
        "Add a fast pre-filter (e.g., template similarity or lightweight classifier) to reduce LLM calls by routing obviously normal logs.",
        "Use hybrid retrieval (sparse+dense) and a cross-encoder reranker to improve retrieval precision before prompting.",
        "Implement streaming/online clustering with drift detection to keep the normal-log index representative over time.",
        "Adopt structured prompting with a schema and log-grammar constraints to further minimize hallucinations and enforce deterministic outputs.",
        "Batch multiple log lines per prompt with delimiting and per-line judgments to amortize context cost and reduce latency.",
        "Introduce confidence scoring and threshold calibration (e.g., temperature scaling) for operational deployment.",
        "Leverage few-shot synthetic anomalies (data augmentation) to probe decision boundaries and assess robustness.",
        "Explore local inference via quantized open LLMs and on-prem vector DBs for privacy and latency."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "LangChain",
        "OpenAI API"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "OpenAI GPT-3.5 with temperature 0.1; dense retrieval over a vector database; noted high resource use and latency; API cost limited testing to 20% of the test split."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "High resource consumption and latency of LLM inference.",
        "One-log-entry-at-a-time processing limits throughput.",
        "API cost constraints for large-scale evaluation.",
        "LLM context/token limits could affect retrieval-prompting for long or verbose logs."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces RAGLog, a RAG-based zero-shot log anomaly detection pipeline that requires no log parsing.",
      "Uses a vector database storing only normal log entries and retrieves closest normals as context for an LLM to judge anomalies via a Q&A prompt.",
      "Employs unsupervised k-means clustering to select representative normal samples for the index; clustering outperforms random selection in experiments.",
      "Evaluates on two public HPC log datasets (BGL and Thunderbird) using Precision/Recall/F1; reports promising F1 and constrained outputs ('normal'/'abnormal') with no observed hallucinations.",
      "Discusses limitations (latency, resource usage, per-line processing) and proposes optimizing for higher-throughput log analysis."
    ]
  },
  {
    "arxiv_id": "2311.11206v1",
    "title": "Robust Network Slicing: Multi-Agent Policies, Adversarial Attacks, and Defensive Strategies",
    "authors": "Feng Wang; M. Cenk Gursoy; Senem Velipasalar",
    "abstract": "In this paper, we present a multi-agent deep reinforcement learning (deep RL) framework for network slicing in a dynamic environment with multiple base stations and multiple users. In particular, we propose a novel deep RL framework with multiple actors and centralized critic (MACC) in which actors are implemented as pointer networks to fit the varying dimension of input. We evaluate the performance of the proposed deep RL algorithm via simulations to demonstrate its effectiveness. Subsequently, we develop a deep RL based jammer with limited prior information and limited power budget. The goal of the jammer is to minimize the transmission rates achieved with network slicing and thus degrade the network slicing agents' performance. We design a jammer with both listening and jamming phases and address jamming location optimization as well as jamming channel optimization via deep RL. We evaluate the jammer at the optimized location, generating interference attacks in the optimized set of channels by switching between the jamming phase and listening phase. We show that the proposed jammer can significantly reduce the victims' performance without direct feedback or prior knowledge on the network slicing policies. Finally, we devise a Nash-equilibrium-supervised policy ensemble mixed strategy profile for network slicing (as a defensive measure) and jamming. We evaluate the performance of the proposed policy ensemble algorithm by applying on the network slicing agents and the jammer agent in simulations to show its effectiveness.",
    "published_date": "2023-11-19",
    "pdf_link": "https://arxiv.org/pdf/2311.11206v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Wireless Security",
      "subdomain": "Anti-Jamming and Adversarial ML in Wireless",
      "specific_problem": "Robust network slicing and dynamic channel/resource allocation under adversarial jamming in multi-base-station 5G RAN",
      "attack_types": [
        "Jamming",
        "Adversarial perturbations to RL observations (wireless interference)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Reinforcement Learning",
        "specific": "Multi-Agent Actor-Critic with Centralized Critic (MACC)",
        "novel_contribution": "Centralized critic optimizing sum reward across base stations with decentralized pointer-network actors; parameter sharing and reward assignment over completed requests to speed convergence"
      },
      {
        "type": "primary",
        "category": "Sequence-to-Sequence / Attention RNN",
        "specific": "Pointer Network (LSTM encoder-decoder with attention)",
        "novel_contribution": "Actor implementation that handles varying input/output dimensionality (nr, N) and reduces action cardinality; uses trainable vectors W1, W2 and fixed v to mitigate vanishing gradients and speed training"
      },
      {
        "type": "primary",
        "category": "Reinforcement Learning",
        "specific": "Actor-Critic for Adversarial Jamming",
        "novel_contribution": "Deep RL-based jammer with listening and jamming phases that optimizes jamming location and channel set without direct feedback on victim policies"
      },
      {
        "type": "primary",
        "category": "Ensemble RL / Game-Theoretic Learning",
        "specific": "Nash-equilibrium-supervised Policy Ensemble (NesPE)",
        "novel_contribution": "Supervises policy ensemble training using an optimized mixed strategy profile approximating Nash equilibrium to improve robustness in a zero-sum incomplete information game"
      },
      {
        "type": "baseline",
        "category": "Reinforcement Learning",
        "specific": "Independent Actor-Critic (IAC)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Reinforcement Learning (Deep)",
      "Multi-agent RL",
      "Supervised (for Nash-equilibrium-supervised ensemble training signal)"
    ],
    "datasets": [
      {
        "name": "Simulated 5G RAN network slicing environment (multi-base-station, multi-user, dynamic interference with Jakes fading)",
        "type": "synthetic",
        "domain": "wireless_network_simulation",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Sum reward over requests/base stations (discounted return)",
      "Transmission rate rb,u_c per channel and sum rate rk per request",
      "Overall throughput/sum-rate",
      "Request success/failure (completion within lifetime and rate constraints)",
      "Queue denial/rejection events",
      "Performance degradation under jamming (rate/reward reduction)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How to learn coordinated slice/channel allocation across multiple base stations in a dynamic interference environment with user mobility to maximize overall performance?",
        "Can pointer-network actors effectively handle varying observation/action dimensionality in network slicing?",
        "How can a deep RL-based jammer with limited prior information and power budget minimize victim transmission rates by optimizing location and channels without direct feedback?",
        "Can a Nash-equilibrium-supervised policy ensemble improve robustness of network slicing against adaptive jamming in a zero-sum incomplete information game?"
      ],
      "gaps_identified": [
        "Most existing studies assume identical slice state across slices over time, reducing the problem to assigning numbers of slices rather than learning varying slice conditions.",
        "Limited consideration of multi-base-station cooperation with dynamic interference and user mobility in RL-based network slicing.",
        "Defense literature often focuses on performance against specific adversaries and does not fully consider zero-sum play against unknown, potentially adaptive opponents.",
        "Random exploration strategies common in games are unsafe in wireless networks where users may experience disconnection, motivating efficient exploration with fast convergence."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Enable robust, high-performance network slicing in dynamic multi-base-station 5G RAN while addressing adversarial vulnerabilities of deep RL to jamming; provide efficient learning and defensive strategies with fast convergence and coordination.",
      "potential_research_ideas": [
        "Incorporate graph neural networks to model and exploit the interference graph among base stations and channels for coordinated allocation.",
        "Apply risk-sensitive or distributional RL (e.g., CVaR optimization) to provide safety-aware slicing under worst-case jamming.",
        "Formulate a POMDP with belief tracking over jammer location/channel strategies and use Bayesian/meta-RL to adapt online.",
        "Integrate adversarial training/robust RL (e.g., RARL, PR-MDP) to harden policies against a spectrum of jamming strategies.",
        "Extend to mmWave/beamforming scenarios where the action space includes beams; study beam-jamming and defenses.",
        "Develop a real-world SDR testbed to validate MACC, jammer, and NesPE under realistic channel and mobility traces.",
        "Multi-objective RL for balancing QoS, energy, and robustness, including fairness among users and slices.",
        "Online change-point detection to trigger policy ensemble reweighting when jammer behavior shifts."
      ],
      "architectural_improvement_recommendations": [
        "Replace LSTM-based pointer network with a transformer-based pointer or set-to-set architecture for better scaling to large N and nr.",
        "Use GNN-based centralized critic (or CTDE with GNN critics) to aggregate cross-BS context efficiently.",
        "Adopt hierarchical RL separating request admission/queuing and channel assignment to reduce action complexity.",
        "Leverage CTDE algorithms like MADDPG/QMIX/HATRPO as comparative baselines and potential improvements.",
        "Employ communication-efficient/federated centralized training to reduce bandwidth between BS actors and the critic.",
        "Augment observations with uncertainty estimates and use ensemble critics for more stable TD targets under nonstationary jamming."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "5G RAN / MVNO multi-base-station environment (simulated)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Need for centralized-critic training coordination and communication between base stations and data center/server during training",
        "Safety-critical exploration in live networks where random exploration can cause service disruption",
        "Adapting to unknown, adaptive jammers with limited prior information and feedback",
        "Synchronization and latency constraints across distributed base stations",
        "Regulatory and coexistence constraints when emitting or responding to jamming in real deployments"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": true,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes a multi-agent deep RL framework with multiple actors and a centralized critic (MACC) for network slicing across multiple base stations.",
      "Introduces pointer-network actors to handle varying input/output dimensionality and reduce action cardinality in channel/request assignment.",
      "Develops a deep RL-based jammer with listening and jamming phases that optimizes location and channel selection without direct feedback, significantly reducing victim performance.",
      "Designs a Nash-equilibrium-supervised policy ensemble (NesPE) for both slicing agents and jammer, showing improved performance versus other ensemble methods in simulations.",
      "Provides simulation-based evaluation of slicing, jamming, and defensive strategies in a dynamic interference environment with user mobility."
    ]
  },
  {
    "arxiv_id": "2401.00468v1",
    "title": "Blockchain and Deep Learning-Based IDS for Securing SDN-Enabled Industrial IoT Environments",
    "authors": "Samira Kamali Poorazad; Chafika Benzaıd; Tarik Taleb",
    "abstract": "The industrial Internet of Things (IIoT) involves the integration of Internet of Things (IoT) technologies into industrial settings. However, given the high sensitivity of the industry to the security of industrial control system networks and IIoT, the use of software-defined networking (SDN) technology can provide improved security and automation of communication processes. Despite this, the architecture of SDN can give rise to various security threats. Therefore, it is of paramount importance to consider the impact of these threats on SDN-based IIoT environments. Unlike previous research, which focused on security in IIoT and SDN architectures separately, we propose an integrated method including two components that work together seamlessly for better detecting and preventing security threats associated with SDN-based IIoT architectures. The two components consist in a convolutional neural network-based Intrusion Detection System (IDS) implemented as an SDN application and a Blockchain-based system (BS) to empower application layer and network layer security, respectively. A significant advantage of the proposed method lies in jointly minimizing the impact of attacks such as command injection and rule injection on SDN-based IIoT architecture layers. The proposed IDS exhibits superior classification accuracy in both binary and multiclass categories.",
    "published_date": "2023-12-31",
    "pdf_link": "https://arxiv.org/pdf/2401.00468v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Industrial IoT Security",
      "subdomain": "Intrusion Detection in SDN-enabled IIoT",
      "specific_problem": "Integrated detection and prevention of IIoT/SCADA attacks and SDN flow-rule tampering using a CNN-based IDS and a blockchain-based verification system",
      "attack_types": [
        "Man-in-the-Middle (MITM) on SDN southbound interface",
        "Flow rule injection/tampering",
        "Command injection (state/parameter/function code)",
        "Malicious response injection (naive, complex)",
        "Denial of Service (false state injection)",
        "Reconnaissance"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "CNN",
        "specific": "1D-CNN (2 conv layers, max-pooling, average-pooling, 2 fully-connected)",
        "novel_contribution": "Deployed as an SDN application for IIoT/SCADA traffic classification and integrated with a blockchain-based verification component to jointly minimize command and rule injection impacts across SDN-based IIoT layers"
      },
      {
        "type": "baseline",
        "category": "Decision Tree",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "K-Nearest Neighbors",
        "specific": "Random Subspace Learning KNN (RSL-KNN)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Natural Gas Pipeline dataset (version 3)",
        "type": "public",
        "domain": "industrial_control_systems (SCADA) process/network data",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Decision Tree (DT) [18]",
        "paper_reference": "[18]",
        "metric": "Accuracy (binary classification)",
        "their_result": "94.75%",
        "baseline_result": "92.30%"
      },
      {
        "method_name": "Decision Tree (DT) [18]",
        "paper_reference": "[18]",
        "metric": "Accuracy (multi-class classification)",
        "their_result": "94.65%",
        "baseline_result": "92.30%"
      },
      {
        "method_name": "RSL-KNN [14]",
        "paper_reference": "[14]",
        "metric": "Accuracy (binary classification)",
        "their_result": "94.75%",
        "baseline_result": "best below 91.9%"
      },
      {
        "method_name": "RSL-KNN [14]",
        "paper_reference": "[14]",
        "metric": "Accuracy (multi-class classification)",
        "their_result": "94.65%",
        "baseline_result": "best below 91.9%"
      }
    ],
    "performance_metrics_used": [
      "accuracy",
      "precision",
      "recall",
      "F1-score"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Lack of integrated solutions that jointly address security across both IIoT and SDN layers in SDN-based IIoT architectures",
        "Existing works typically treat IIoT and SDN security separately rather than holistically"
      ],
      "limitations": [
        "Authors state: \"our objective is not to propose a method that is superior to previous research in terms of IDS accuracy or Blockchain overhead.\"",
        "Evaluation uses a single ICS dataset with class regrouping (reduced granularity): five injection attacks grouped into one 'Injection' class plus DoS and Recon for multi-class evaluation",
        "Hyperparameters chosen via trial-and-error; only one setting reported due to space constraints",
        "DoS considered is a false state injection (not flooding), limiting attack coverage"
      ],
      "future_work": [],
      "motivation": "Provide a holistic, integrated security approach for SDN-enabled IIoT by combining an SDN-application CNN-based IDS (payload anomaly/attack classification) with a blockchain-based system to verify and protect flow rules over the southbound interface against MITM/rule injection.",
      "potential_research_ideas": [
        "Evaluate the integrated IDS+blockchain architecture on multiple real IIoT/ICS datasets and live testbeds to assess generalization and operational robustness",
        "Extend to online/streaming detection with concept-drift/adaptation for evolving IIoT processes",
        "Incorporate sequence-aware models (e.g., Temporal CNNs, LSTM/GRU, Transformers) to capture temporal process dynamics and improve detection of stealthy attacks",
        "Investigate cryptographic alternatives or complements to blockchain (e.g., signed controller-switch channels with TEEs) and compare security/latency/overhead",
        "Design a fine-grained multi-task classifier preserving all original attack subclasses (NMRI, CMRI, MSCI, MPCI, MFCI, DoS, Recon) to avoid loss of granularity",
        "Add explanation modules (e.g., SHAP/Integrated Gradients) tailored to ICS operators for actionable incident response",
        "Assess and harden against adversarial ML attacks on the IDS (evasion/poisoning) in SDN-IIoT contexts",
        "Federated/edge learning across distributed controllers/detection nodes to preserve data locality and improve scalability",
        "Quantify and optimize blockchain overheads (latency, throughput, storage) under realistic SDN control-plane loads",
        "Combine graph-based flow modeling (e.g., GNNs over flow-rule graphs) with payload classifiers for joint header-payload reasoning"
      ],
      "architectural_improvement_recommendations": [
        "Adopt a temporal model (TCN/LSTM/Transformer) or hybrid CNN+RNN for improved time-series ICS signal and traffic characterization",
        "Preserve full attack taxonomy in training with class-imbalance handling (e.g., focal loss, class-weighting, augmentations) instead of grouping",
        "Introduce confidence calibration and abstention for controller decisions; integrate with SDN policy to quarantine uncertain flows",
        "Implement cryptographic attestation of flow-mod messages (e.g., MAC/signatures) in addition to blockchain logging for immediate tamper detection",
        "Use asynchronous, batched blockchain writes and lightweight consensus or permissioned BFT to limit control-plane latency",
        "Integrate runtime monitoring of blockchain-induced latency and fallback paths if verification exceeds thresholds",
        "Automate hyperparameter tuning (Bayesian/ASHA) and report full search spaces for reproducibility"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "NumPy",
        "Pandas",
        "Keras",
        "scikit-learn",
        "Savoir (for MultiChain)",
        "Mininet",
        "Ryu Controller",
        "OpenFlow"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Two Ubuntu 20.04 VMs (8 GB RAM with Mininet 2.3.0/Ryu 4.34; 4 GB RAM with MultiChain 2.2). CNN trained for 30 epochs with SGD (lr=0.01, momentum=0.8), batch size 100."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "SDN controller application (Ryu) with Mininet-simulated IIoT network; MultiChain private blockchain (controller node and Detection Node)",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduce an SDN-based IIoT system architecture that leverages SDN programmability for security while improving flexibility and scalability",
      "Propose an integrated method combining a CNN-based IDS (application-layer, payload analysis) and a blockchain-based system (network-layer, header/flow-rule verification) to jointly mitigate command and rule injection attacks",
      "Present a system model and attack model covering SCADA and SDN-specific attacks (e.g., MITM on southbound interface, flow-rule injection)",
      "Implement the method using Ryu, Mininet, MultiChain, and a CNN-based IDS; evaluate detection performance on a natural gas pipeline ICS dataset",
      "Demonstrate that the CNN-based IDS outperforms DT and RSL-KNN baselines on accuracy for both binary and multi-class classification",
      "Detail a blockchain-assisted detection node that cross-checks controller-issued flow rules with switch flow tables to flag tampering via MITM"
    ]
  },
  {
    "arxiv_id": "2311.12074v1",
    "title": "SecureBERT and LLAMA 2 Empowered Control Area Network Intrusion Detection and Classification",
    "authors": "Xuemei Li; Huirong Fu",
    "abstract": "Numerous studies have proved their effective strength in detecting Control Area Network (CAN) attacks. In the realm of understanding the human semantic space, transformer-based models have demonstrated remarkable effectiveness. Leveraging pre-trained transformers has become a common strategy in various language-related tasks, enabling these models to grasp human semantics more comprehensively. To delve into the adaptability evaluation on pre-trained models for CAN intrusion detection, we have developed two distinct models: CAN-SecureBERT and CAN-LLAMA2. Notably, our CAN-LLAMA2 model surpasses the state-of-the-art models by achieving an exceptional performance 0.999993 in terms of balanced accuracy, precision detection rate, F1 score, and a remarkably low false alarm rate of 3.10e-6. Impressively, the false alarm rate is 52 times smaller than that of the leading model, MTH-IDS (Multitiered Hybrid Intrusion Detection System). Our study underscores the promise of employing a Large Language Model as the foundational model, while incorporating adapters for other cybersecurity-related tasks and maintaining the model's inherent language-related capabilities.",
    "published_date": "2023-11-19",
    "pdf_link": "https://arxiv.org/pdf/2311.12074v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Automotive / Vehicle Security",
      "subdomain": "In-vehicle Network (IVN) Security / CAN Bus IDS",
      "specific_problem": "Control Area Network (CAN) intrusion detection and multi-class attack classification directly from CAN message logs",
      "attack_types": [
        "Denial of Service (DoS)",
        "Spoofing / Injection",
        "Fuzzy attacks",
        "Malfunction"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer Encoder",
        "specific": "BERT-base (fine-tuned for sequence classification)",
        "novel_contribution": "Directly fine-tunes BERT on tokenized CAN message logs without handcrafted feature engineering; adds a classification head on [CLS]"
      },
      {
        "type": "primary",
        "category": "Transformer Encoder",
        "specific": "SecureBERT (RoBERTa-base pre-trained on cybersecurity text, fine-tuned)",
        "novel_contribution": "Evaluates whether cybersecurity domain knowledge within SecureBERT benefits CAN IDS; adds classification head on [CLS]"
      },
      {
        "type": "primary",
        "category": "Transformer Decoder (LLM) with PEFT",
        "specific": "LLaMA 2-7B with LoRA adapters (fine-tuned; half-precision)",
        "novel_contribution": "Applies LoRA to adapt a 7B LLaMA 2 model for CAN IDS while modifying only 0.57% of parameters; uses [EOS]-based classification head so the base LLM retains general language capabilities"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Transfer Learning",
      "Fine-tuning",
      "Parameter-Efficient Fine-Tuning (LoRA)"
    ],
    "datasets": [
      {
        "name": "Pre-balanced CAN dataset (used for training/evaluation)",
        "type": "private",
        "domain": "vehicle_can_message_logs",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "BookCorpus",
        "type": "public",
        "domain": "text_corpus",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "English Wikipedia",
        "type": "public",
        "domain": "text_corpus",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "SecureBERT cybersecurity text corpus (98,411 elements; ~1B tokens)",
        "type": "public",
        "domain": "cybersecurity_text_corpus",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "LLaMA 2 pretraining mixture (CommonCrawl, GitHub code, Wikipedia (20 languages), public domain books, arXiv LaTeX, Stack Exchange)",
        "type": "public",
        "domain": "mixed_text_and_code_corpus",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "MTH-IDS (Multitiered Hybrid Intrusion Detection System)",
        "paper_reference": "[12] (as cited in the paper)",
        "metric": "False Alarm Rate (FAR)",
        "their_result": "\"a remarkably low false alarm rate of 3.10e-6\"",
        "baseline_result": "\"the false alarm rate is 52 times smaller than that of the leading model, MTH-IDS\""
      }
    ],
    "performance_metrics_used": [
      "balanced accuracy",
      "precision",
      "detection rate",
      "F1 score",
      "false alarm rate"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Existing CAN IDS methods depend on preprocessing/feature engineering and complex workflows, limiting detection of unknown/novel attacks.",
        "\"there is an absence of prior research endeavors that have previously employed SecureBERT and L LAMA 2 for the purpose of CAN intrusion detection and classification.\""
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Adapt pre-trained transformer models (including an LLM) to CAN intrusion detection to eliminate heavy preprocessing, leverage transfer learning, and evaluate whether cybersecurity domain knowledge helps.",
      "potential_research_ideas": [
        "Online/streaming CAN IDS with continual learning to handle concept drift and evolving attack patterns on real vehicles.",
        "Self-supervised pretraining directly on large unlabeled CAN logs (e.g., masked token/ID prediction) before supervised fine-tuning to improve robustness and reduce labeled data needs.",
        "Multi-modal fusion of CAN logs with physical/ECU-layer signals (timing/voltage fingerprints) using cross-attention to improve detection of spoofing/injection.",
        "Open-set/unknown attack detection on top of the classifier (e.g., energy-based or contrastive thresholds) to better handle novel attacks.",
        "Explainability for CAN IDS by attention rollout or probing token attributions to support VSOC forensics.",
        "Adversarial robustness assessment against log perturbations and data poisoning and defenses (e.g., adversarial training, randomized smoothing).",
        "Knowledge distillation or quantization-aware training to compress LLaMA2-based model for edge ECUs while retaining accuracy.",
        "Federated or split learning across fleets to protect data privacy while fine-tuning adapters."
      ],
      "architectural_improvement_recommendations": [
        "Adopt hierarchical tokenization (separate fields: CAN ID, DLC, payload bytes, timing deltas) with learned positional encodings to inject protocol structure.",
        "Use sequence-to-sequence forecasting head for next-message prediction combined with classification in a multi-task setup to capture temporal dependencies.",
        "Incorporate lightweight temporal modules (e.g., Transformer-XL memory or TCN) to model longer CAN sequences beyond a fixed window.",
        "Leverage prompt-tuning or prefix-tuning for the LLaMA2 model to further reduce trainable parameters vs. LoRA, and compare adapters.",
        "Calibrate outputs (temperature scaling) and add confidence estimation/OOD detection layers for safer deployment.",
        "Evaluate and integrate class-imbalance strategies (focal loss, reweighting) beyond dataset pre-balancing for realistic skewed traffic."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Vehicle Security Operations Center (VSOC) is proposed as a potential environment",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposed three IDS models for CAN logs: CAN-C-BERT, CAN-SecureBERT, and CAN-LLAMA2 (adapter-based).",
      "Detailed architectures and training procedures for the three models with a classification head.",
      "Demonstrated training on a pre-balanced CAN dataset; direct use of raw CAN message logs without traditional preprocessing/feature engineering.",
      "Empirical comparison with state-of-the-art (MTH-IDS), where CAN-LLAMA2 achieves top performance and CAN-SecureBERT is second-best.",
      "Reported CAN-LLAMA2 balanced accuracy/precision/detection rate/F1 of 0.999993 and FAR of 3.10e-6; FAR is 52x lower than MTH-IDS.",
      "Showed strong performance even when training with only 5% of the data; generalization improves with more data.",
      "Found that incorporating cybersecurity domain knowledge (SecureBERT) does not necessarily improve CAN attack detection vs general pretraining.",
      "Used LoRA for LLaMA2, modifying only 0.57% of parameters so the model retains general language capabilities for other tasks."
    ]
  },
  {
    "arxiv_id": "2312.14480v1",
    "title": "MetaAID 2.5: A Secure Framework for Developing Metaverse Applications via Large Language Models",
    "authors": "Hongyin Zhu",
    "abstract": "Large language models (LLMs) are increasingly being used in Metaverse environments to generate dynamic and realistic content and to control the behavior of non-player characters (NPCs). However, the cybersecurity concerns associated with LLMs have become increasingly prominent. Previous research has primarily focused on patching system vulnerabilities to enhance cybersecurity, but these approaches are not well-suited to the Metaverse, where the virtual space is more complex, LLMs are vulnerable, and ethical user interaction is critical. Moreover, the scope of cybersecurity in the Metaverse is expected to expand significantly. This paper proposes a method for enhancing cybersecurity through the simulation of user interaction with LLMs. Our goal is to educate users and strengthen their defense capabilities through exposure to a comprehensive simulation system. This system includes extensive Metaverse cybersecurity Q&A and attack simulation scenarios. By engaging with these, users will improve their ability to recognize and withstand risks. Additionally, to address the ethical implications of user input, we propose using LLMs as evaluators to assess user content across five dimensions. We further adapt the models through vocabulary expansion training to better understand personalized inputs and emoticons. We conduct experiments on multiple LLMs and find that our approach is effective.",
    "published_date": "2023-12-22",
    "pdf_link": "https://arxiv.org/pdf/2312.14480v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Metaverse Security",
      "subdomain": "User Interaction Safety and Content Moderation",
      "specific_problem": "Enhancing cybersecurity of user–LLM interactions in Metaverse applications via simulation-based training and automatic ethical input evaluation with vocabulary expansion",
      "attack_types": [
        "data leakage",
        "network snooping/surveillance",
        "malicious/jailbreak prompting",
        "code-based attacks (simulated)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer LLM",
        "specific": null,
        "novel_contribution": "Use LLM-as-evaluator to assess user inputs across five dimensions tailored to Metaverse social interactions"
      },
      {
        "type": "primary",
        "category": "Tokenizer/Vocabulary Learning",
        "specific": "Vocabulary Expansion Training (VET)",
        "novel_contribution": "Expand LLM vocabulary to better understand personalized inputs and emoticons while preserving prior knowledge"
      },
      {
        "type": "primary",
        "category": "Parameter-Efficient Fine-Tuning",
        "specific": "LoRA",
        "novel_contribution": "Combine VET with parameter-efficient fine-tuning on small LLMs to minimize compute for frequent input evaluation"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Prompt-based"
    ],
    "datasets": [
      {
        "name": "Metaverse cybersecurity Q&A corpus",
        "type": "synthetic",
        "domain": "security_QA",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "Metaverse attack simulation scenarios/code",
        "type": "synthetic",
        "domain": "attack_code",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "User input evaluation set (personalized inputs and emoticons)",
        "type": "synthetic",
        "domain": "user_inputs_text",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How can LLM-driven simulations (Q&A and attack scenarios) improve users' cybersecurity awareness and defense skills in the Metaverse?",
        "How can we automatically evaluate and enforce ethical user input in the Metaverse, especially with diverse personalized inputs and emoticons?"
      ],
      "gaps_identified": [
        "Existing cybersecurity methods focus on patching system vulnerabilities and are ill-suited for the complex, user-centric Metaverse environment.",
        "LLMs used in Metaverse applications are vulnerable, and ethical user interaction is critical but under-addressed.",
        "Current models are not equipped to handle diverse, personalized inputs and emoticons common in Metaverse interactions."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Build trusted, robust, and secure LLM-enabled Metaverse applications by educating users through simulations and ensuring ethical, safe user–LLM interactions.",
      "potential_research_ideas": [
        "Design a standardized benchmark for Metaverse user–LLM safety that includes multi-lingual, emoji-rich inputs and multi-modal (voice/gesture) interactions.",
        "Develop adversarial red-teaming specifically targeting emoji/pictographic tokens to stress-test the evaluator and VET.",
        "Integrate differential privacy or PII-aware filters into the evaluator to mitigate privacy leakage during simulations.",
        "Extend evaluator to a multi-agent oversight system (ensemble of small LLMs + rule-based guards) and study robustness vs. cost.",
        "Create curriculum-learning strategies for simulation training that adapt difficulty based on user performance signals.",
        "Investigate multilingual/low-resource token expansion policies for emojis, slang, and code-switching in Metaverse contexts.",
        "Explore multimodal VET (text+emoji+image stickers) for richer social signals in AR/VR chat environments."
      ],
      "architectural_improvement_recommendations": [
        "Use an ensemble of small LLM evaluators with disagreement-based escalation to a larger model for hard cases.",
        "Augment VET with tokenizer re-training plus adapter fusion (LoRA + prefix/prompt tuning) to reduce forgetting.",
        "Incorporate structured policy engines (regex/AST/code sandboxes and PII detectors) ahead of the LLM evaluator for defense-in-depth.",
        "Adopt active learning: uncertain or adversarial inputs are labeled by humans and fed back to continuously refine the evaluator.",
        "Add a safety reward model and lightweight RLHF on top of VET to align evaluator judgments with human policy.",
        "Introduce multi-modal input support (speech/gesture/emoji/sticker) with modality-specific adapters."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": "Designed to combine small LLMs with vocabulary expansion and parameter-efficient fine-tuning (LoRA) to reduce compute for frequent input evaluation; exact specs not provided."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Metaverse/XR application context (concept and simulations; no deployment details provided)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Handling diverse, personalized inputs and emoticons in real-time Metaverse interactions",
        "Mitigating LLM vulnerabilities (jailbreaks, prompt injection) during user-facing simulations",
        "Ensuring privacy/PII protection and ethical constraints amid rich social contexts",
        "Balancing safety evaluation quality with compute constraints for frequent moderation",
        "Integrating simulation and evaluation components into complex Metaverse application stacks"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes using LLMs to enhance the cybersecurity of user interactions within Metaverse applications.",
      "Introduces a simulation-based educational system with extensive cybersecurity Q&A and attack code scenarios to train users.",
      "Proposes LLM-as-evaluator to assess user content across five ethical/safety dimensions.",
      "Introduces a Vocabulary Expansion Training (VET) method combined with parameter-efficient fine-tuning to handle personalized inputs and emoticons with small LLMs.",
      "Presents experiments on multiple LLMs indicating the approach is effective (details not provided in the excerpt)."
    ]
  },
  {
    "arxiv_id": "2312.08818v2",
    "title": "A Cyber-Physical Architecture for Microgrids based on Deep learning and LORA Technology",
    "authors": "Mojtaba Mohammadi; Abdollah KavousiFard; Mortza Dabbaghjamanesh; Mostafa Shaaban; Hatem. H. Zeineldin; Ehab Fahmy El-Saadany",
    "abstract": "This paper proposes a cyber-physical architecture for the secured social operation of isolated hybrid microgrids (HMGs). On the physical side of the proposed architecture, an optimal scheduling scheme considering various renewable energy sources (RESs) and fossil fuel-based distributed generation units (DGs) is proposed. Regarding the cyber layer of MGs, a wireless architecture based on low range wide area (LORA) technology is introduced for advanced metering infrastructure (AMI) in smart electricity grids. In the proposed architecture, the LORA data frame is described in detail and designed for the application of smart meters considering DGs and ac-dc converters. Additionally, since the cyber layer of smart grids is highly vulnerable to cyber-attacks, t1his paper proposes a deep-learning-based cyber-attack detection model (CADM) based on bidirectional long short-term memory (BLSTM) and sequential hypothesis testing (SHT) to detect false data injection attacks (FDIA) on the smart meters within AMI. The performance of the proposed energy management architecture is evaluated using the IEEE 33-bus test system. In order to investigate the effect of FDIA on the isolated HMGs and highlight the interactions between the cyber layer and physical layer, an FDIA is launched against the test system. The results showed that a successful attack can highly damage the system and cause widespread load shedding. Also, the performance of the proposed CADM is examined using a real-world dataset. Results prove the effectiveness of the proposed CADM in detecting the attacks using only two samples.",
    "published_date": "2023-12-14",
    "pdf_link": "https://arxiv.org/pdf/2312.08818v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Smart Grid Security",
      "subdomain": "Advanced Metering Infrastructure (AMI) Security / Intrusion Detection",
      "specific_problem": "False Data Injection Attack (FDIA) detection on smart meters within AMI of isolated hybrid microgrids; end-to-end cyber-physical architecture integrating LoRa-based AMI and optimal scheduling",
      "attack_types": [
        "False Data Injection (FDIA)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "RNN",
        "specific": "Bidirectional LSTM (BLSTM)",
        "novel_contribution": "Used as a forecasting model to predict load and combined with sequential hypothesis testing (SHT) for multi-sample decision-making under uncertainty"
      },
      {
        "type": "primary",
        "category": "Statistical Testing",
        "specific": "Sequential Hypothesis Testing (SHT)",
        "novel_contribution": "Sequential decision-making on the residual (received vs predicted load) enabling detection with as few as two samples and addressing uncertainty better than single-sample detectors"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Time-series forecasting",
      "Statistical (sequential testing)"
    ],
    "datasets": [
      {
        "name": "IEEE 33-bus test system",
        "type": "public",
        "domain": "power_system_topology",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Real-world smart meter dataset (unspecified)",
        "type": "unknown",
        "domain": "smart_meter_load",
        "link": null,
        "is_new_contribution": false,
        "availability": "unspecified"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "false positive rate (alpha)",
      "false negative rate (beta)",
      "detection delay (number of samples)",
      "packet delivery ratio (PDR) (discussed for LoRa mesh vs star from prior work)",
      "operation cost (for optimal scheduling)",
      "load shedding amount (impact analysis under FDIA)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How to design a cyber-physical architecture for isolated hybrid microgrids that jointly considers optimal physical operation and a secure AMI communication layer?",
        "Can a BLSTM + SHT-based CADM detect FDIAs on smart meters using multiple sequential samples to handle uncertainty better than single-sample methods?",
        "What is the impact of FDIA on the physical operation of isolated hybrid microgrids (e.g., cost and load shedding)?"
      ],
      "gaps_identified": [
        "Prior work often treats physical and cyber layers separately without modeling their interactions in microgrids.",
        "Many AMI intrusion detection methods make decisions from a single observation, which is brittle under uncertainty.",
        "Existing AMI wireless options either consume high power (NB-IoT/cellular) or have low transmission range/reliability (Zigbee, WiFi), and star-topology LoRa has lower PDR than mesh."
      ],
      "limitations": [
        "Only theoretical details of the proposed LoRa-based AMI are provided; numerical simulations and practical implementation are left as future work.",
        "Evaluation of the LoRa mesh performance cites external work; the proposed LoRa AMI is not experimentally validated in this paper.",
        "The real-world dataset used to test CADM is not described (name, scope, access), hindering reproducibility.",
        "No quantitative comparison against competing CADMs is presented in the provided text."
      ],
      "future_work": [
        "Numerical simulations and practical implementation of the proposed LoRa framework (explicitly stated)."
      ],
      "motivation": "Microgrids are cyber-physical systems whose reliable and efficient operation depends on both optimal physical scheduling and secure, low-power, long-range AMI communications. AMI is vulnerable to cyber-attacks like FDIA, and existing detectors often decide on single samples. The paper aims to jointly address cyber and physical layers with a LoRa-based AMI and a BLSTM+SHT CADM that can decide over sequences.",
      "potential_research_ideas": [
        "End-to-end field deployment and measurement study of the LoRa mesh AMI in a live microgrid, including interference, duty-cycle, and latency impacts on detection.",
        "Evaluate and harden CADM against adaptive attackers (e.g., stealthy FDIAs that mimic forecast residual distributions) using robust statistics or adversarial training.",
        "Multi-meter, topology-aware detection that fuses residuals across meters using a graph neural network over the feeder topology.",
        "Incorporate uncertainty quantification (e.g., Bayesian LSTM/MC-dropout) and propagate predictive intervals into sequential testing for principled thresholds.",
        "Online learning and concept drift handling to maintain detection under changing load patterns and seasons.",
        "Joint optimization of communication scheduling (e.g., sampling/adaptive reporting over LoRa) and detection performance under bandwidth/energy constraints.",
        "Privacy-preserving collaborative training (e.g., federated learning across meters) to avoid centralizing raw consumption data.",
        "Comprehensive benchmarking against single-sample and change-point baselines on public AMI datasets with standardized protocols."
      ],
      "architectural_improvement_recommendations": [
        "Replace or augment BLSTM with Temporal Convolutional Networks or transformer-based time-series models (Informer/Temporal Fusion Transformer) for longer contexts and efficiency.",
        "Use sequential change-point detectors (e.g., CUSUM/GLR) and compare with Wald’s SPRT to optimize detection delay under false alarm constraints.",
        "Fuse multi-sensor features (voltage, reactive power, frequency) in the prediction residual to increase separability of FDIAs.",
        "Calibrate SHT thresholds per meter using validation residual distributions and cost-sensitive criteria (different alpha/beta by criticality).",
        "Implement edge inference on meter MCUs with model compression (quantization/pruning) and evaluate latency/energy on LoRa-class hardware.",
        "Introduce topology-aware joint testing (graphical SHT) that exploits power flow constraints to detect spatially coordinated FDIAs."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Isolated hybrid microgrids with LoRa-based AMI (mesh topology) and central control",
      "scalability_discussed": true,
      "inference_time": "Decision achievable in as few as two samples (sequential detection)",
      "deployment_challenges": [
        "AMI is highly vulnerable to cyber-attacks; requires robust detection.",
        "Star-topology LoRa exhibits lower PDR than mesh; reliability depends on topology and settings.",
        "Proposed LoRa framework not yet implemented or experimentally validated in this work."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Cyber-physical architecture for secured optimal operation of isolated hybrid microgrids.",
      "Wireless mesh network architecture for AMI based on LoRa technology with a detailed smart-meter data frame design.",
      "Deep-learning CADM combining BLSTM forecasting with SHT to detect FDIAs from sequences of samples.",
      "Simulation of FDIAs on the IEEE 33-bus HMG and analysis of physical impacts (e.g., load shedding).",
      "Demonstration that CADM can detect attacks using only two samples (on a real-world dataset, though unspecified)."
    ]
  },
  {
    "arxiv_id": "2312.13705v1",
    "title": "Benchmark Evaluation of Anomaly-Based Intrusion Detection Systems in the Context of Smart Grids",
    "authors": "Ömer Sen; Simon Glomb; Martin Henze; Andreas Ulbig",
    "abstract": "The increasing digitization of smart grids has made addressing cybersecurity issues crucial in order to secure the power supply. Anomaly detection has emerged as a key technology for cybersecurity in smart grids, enabling the detection of unknown threats. Many research efforts have proposed various machine-learning-based approaches for anomaly detection in grid operations. However, there is a need for a reproducible and comprehensive evaluation environment to investigate and compare different approaches to anomaly detection. The assessment process is highly dependent on the specific application and requires an evaluation that considers representative datasets from the use case as well as the specific characteristics of the use case. In this work, we present an evaluation environment for anomaly detection methods in smart grids that facilitates reproducible and comprehensive evaluation of different anomaly detection methods.",
    "published_date": "2023-12-21",
    "pdf_link": "https://arxiv.org/pdf/2312.13705v1",
    "paper_types": [
      "benchmark",
      "reproducibility",
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Smart Grid Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Reproducible benchmarking and evaluation of anomaly-based IDS for smart grids, including explainability and robustness assessment",
      "attack_types": [
        "network intrusions",
        "unknown/zero-day threats (anomaly detection focus)",
        "evasion attacks (adversarial robustness evaluation)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Ensemble Learning",
        "specific": "Stacking",
        "novel_contribution": "Framework supports analyzing arbitrary stacking configurations with customizable base classifiers and second-level models"
      },
      {
        "type": "primary",
        "category": "Explainability",
        "specific": "SHAP",
        "novel_contribution": "Explainability quality evaluated via accuracy, stability (SENS_MAX), and AUC-MoRF tailored to IDS evaluation"
      },
      {
        "type": "primary",
        "category": "Dimensionality Reduction",
        "specific": "PCA",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Preprocessing",
        "specific": "One-Hot Encoding",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Preprocessing",
        "specific": "Normalization",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Statistical Testing",
        "specific": "Wilcoxon signed-rank test; effect sizes (Cohen’s d)",
        "novel_contribution": "Integrated into benchmarking pipeline to quantify generalizability across datasets"
      },
      {
        "type": "primary",
        "category": "Robustness Analysis",
        "specific": "Adversarial robustness distance (Δadv), Lipschitz constant (lower bound)",
        "novel_contribution": "Provides classifier robustness assessment against evasion attacks within the benchmark"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "Bot-IoT",
        "type": "public",
        "domain": "network_traffic (IoT/ICS-relevant)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Power System dataset",
        "type": "public",
        "domain": "power_system_network (ICS/Smart Grid)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CICIDS-2017",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CIDDS-002",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "UNSW-NB15",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "NSL-KDD",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "False Positive Rate",
      "AUC Score",
      "Balanced Accuracy",
      "MMC (Matthews Correlation Coefficient, referred to as MMC in text)",
      "Training time",
      "Prediction time",
      "Explanation time",
      "Explainability accuracy (agreement between explanation-predicted output and model output)",
      "Stability of explanations (SENS_MAX)",
      "AUC-MoRF (Area Under Most Relevant First Perturbation Curve)",
      "Adversarial robustness distance (Δadv)",
      "Lipschitz constant (lower bound)",
      "Wilcoxon signed-rank test (statistical comparison across algorithms)",
      "Effect size (Cohen’s d) with confidence intervals"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How can we design a reproducible and comprehensive evaluation environment for anomaly-based IDS in smart grids?",
        "How do different anomaly detection approaches compare across multiple representative datasets from various Purdue model levels?",
        "How can we evaluate and compare the explainability of IDS models in a standardized, model-agnostic manner?",
        "How can robustness of learned classifiers against evasion attacks be quantified within a benchmark?",
        "How can statistical methods be integrated to assess generalizability and significance of results across datasets?"
      ],
      "gaps_identified": [
        "Existing studies often evaluate on a single dataset and lack comprehensive, in-depth metrics.",
        "Explainability of machine-learning-based IDS approaches is rarely evaluated in benchmarks.",
        "Statistical confidence and generalizability across datasets are often not considered.",
        "Smart-grid-specific characteristics and constraints are insufficiently addressed in prior benchmarks.",
        "Need for reproducible environments covering datasets, preprocessing, training, evaluation, and software/hardware configurations."
      ],
      "limitations": [
        "Scope limited to anomaly-based network IDSs specific to the smart grids domain.",
        "Binary classification only.",
        "Focus on supervised and unsupervised ML; no coverage of semi-supervised beyond mention.",
        "Robustness evaluation limited to evasion attacks; poisoning is not included.",
        "Feature construction and dataset creation are excluded due to protocol-specific dependencies.",
        "Explainability method centered on SHAP (model-agnostic), though evaluation metrics are method-agnostic.",
        "Assumes publicly available, labeled, network-based datasets."
      ],
      "future_work": [
        "Investigate and integrate new anomaly detection methods within the proposed environment."
      ],
      "motivation": "Provide a reproducible, comprehensive benchmark environment to evaluate machine-learning-based anomaly detection for smart grids with a focus on explainability, robustness, and statistical rigor.",
      "potential_research_ideas": [
        "Extend robustness evaluation to include data poisoning and backdoor attacks in addition to evasion.",
        "Incorporate domain adaptation and cross-dataset generalization studies for SG-specific datasets.",
        "Add concept drift detection and online/streaming evaluation for real-time SG operations.",
        "Human-in-the-loop assessment of explanations to correlate SHAP-based metrics with operator trust and response time.",
        "Multi-objective benchmarking that jointly optimizes detection, explanation quality, and computational cost.",
        "Benchmark temporal and graph-based models (e.g., RNNs/Transformers/GNNs) tailored to ICS/SG telemetry and network traffic.",
        "Integrate certified robustness methods and compare empirical vs. certified bounds within the framework."
      ],
      "architectural_improvement_recommendations": [
        "Add support for time-series deep models and sequence-aware features for ICS traffic (e.g., temporal CNNs, LSTMs, Transformers).",
        "Introduce AutoML pipelines to systematically explore preprocessing/model/hyperparameter spaces under the same statistical protocol.",
        "Implement drift-aware training and evaluation with sliding windows and online calibration.",
        "Provide plugin interfaces for additional explanation methods (e.g., LIME, Integrated Gradients) to broaden explainability comparisons.",
        "Incorporate adversarial training and randomized smoothing to improve and benchmark robustness.",
        "Enable distributed execution and dataset sharding to scale evaluations across larger SG datasets."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "SHAP",
        "MongoDB",
        "Containers (e.g., Docker)",
        "JSON-LD"
      ],
      "reproducibility_score": "high",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "High false positive rates common in ML-based IDS for SGs.",
        "Lack of explainability hampers trust and triage.",
        "Benchmarking is highly application- and dataset-dependent; representative SG datasets are required.",
        "Ensuring statistical confidence and reproducibility across diverse datasets and configurations.",
        "Balancing detection accuracy, runtime efficiency, and explainability."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": true,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes a reproducible, comprehensive benchmark environment for anomaly-based IDS in smart grids.",
      "Defines dataset selection strategy guided by the Purdue model to cover multiple ICS layers.",
      "Implements standardized preprocessing (normalization, one-hot encoding, PCA) configurable per evaluation.",
      "Integrates SHAP-based explainability with quantitative quality measures: accuracy, stability (SENS_MAX), and AUC-MoRF.",
      "Introduces robustness assessment within the benchmark via adversarial robustness distance and Lipschitz constant lower bound.",
      "Employs nonparametric statistical testing (Wilcoxon signed-rank) and effect sizes (Cohen’s d) to assess generalizability.",
      "Provides a containerized, versioned software environment and a document-oriented results store (MongoDB with BSON schema and JSON-LD) for reproducibility.",
      "Demonstrates capabilities through a case study (details beyond provided excerpt)."
    ]
  },
  {
    "arxiv_id": "2311.18525v1",
    "title": "Detecting Anomalous Network Communication Patterns Using Graph Convolutional Networks",
    "authors": "Yizhak Vaisman; Gilad Katz; Yuval Elovici; Asaf Shabtai",
    "abstract": "To protect an organizations' endpoints from sophisticated cyberattacks, advanced detection methods are required. In this research, we present GCNetOmaly: a graph convolutional network (GCN)-based variational autoencoder (VAE) anomaly detector trained on data that include connection events among internal and external machines. As input, the proposed GCN-based VAE model receives two matrices: (i) the normalized adjacency matrix, which represents the connections among the machines, and (ii) the feature matrix, which includes various features (demographic, statistical, process-related, and Node2vec structural features) that are used to profile the individual nodes/machines. After training the model on data collected for a predefined time window, the model is applied on the same data; the reconstruction score obtained by the model for a given machine then serves as the machine's anomaly score. GCNetOmaly was evaluated on real, large-scale data logged by Carbon Black EDR from a large financial organization's automated teller machines (ATMs) as well as communication with Active Directory (AD) servers in two setups: unsupervised and supervised. The results of our evaluation demonstrate GCNetOmaly's effectiveness in detecting anomalous behavior of machines on unsupervised data.",
    "published_date": "2023-11-30",
    "pdf_link": "https://arxiv.org/pdf/2311.18525v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Anomaly/Intrusion Detection",
      "specific_problem": "Unsupervised detection of anomalous endpoint/machine communication behavior using enterprise EDR-derived communication graphs (ATMs and machines communicating with AD)",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "GNN / Autoencoder",
        "specific": "GCN-based Variational Autoencoder (GCN-VAE) with inner-product decoder and dual reconstruction (adjacency + node features)",
        "novel_contribution": "Custom GCN-VAE architecture that reconstructs both normalized adjacency and feature matrices with a composite loss across five feature groups; applied to endpoint communication graphs for anomaly scoring; first use of GCN-based VAE for detecting anomalies/security incidents in communication networks as claimed by authors."
      },
      {
        "type": "primary",
        "category": "Graph Embedding",
        "specific": "Node2vec",
        "novel_contribution": "Used to derive 10-dimensional structural node embeddings from the communication graph as part of node feature matrix."
      },
      {
        "type": "primary",
        "category": "Feature Engineering",
        "specific": "TF-IDF with temporal decay",
        "novel_contribution": "Process features built via TF-IDF over processes-per-machine with a 7-day decay factor (0.9^d) to downweight common processes; inclusion of significant process features (count of PIDs, max/avg PID duration, process source directory one-hot)."
      }
    ],
    "learning_paradigm": [
      "Unsupervised",
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Carbon Black EDR netconn logs – ATMs (Large financial organization)",
        "type": "proprietary",
        "domain": "edr_logs (network connection events)",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Carbon Black EDR netconn logs – AD communications (Large financial organization)",
        "type": "proprietary",
        "domain": "edr_logs (network connection events)",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "#Anomalies (alerts/day)",
      "%Anomalies (alerted machines / total)",
      "% Good Alerts (analyst-labeled actionable anomalies)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "\"The availability of datasets that include both normal and abnormal attack instances is limited; therefore, in this research we took an unsupervised approach for anomaly detection.\"",
        "High variability across machine types leads a single model to be less accurate and prone to false alarms; necessitates profiling homogeneous subsets (e.g., ATMs, AD-related machines).",
        "Processing all enterprise machines is challenging due to data volume and computational cost for deep graph models."
      ],
      "limitations": [
        "Evaluated on a single organization’s data (financial org) over 16 days (2021-10-14 to 2021-10-29).",
        "Relies on training and evaluating on the same time window (reconstruction-based anomaly scoring on training data).",
        "Focus on homogeneous subsets (ATMs, AD-communicating machines); method may require per-group models.",
        "No comparative baselines reported against alternative anomaly detectors.",
        "Dataset is proprietary; external reproducibility is limited."
      ],
      "future_work": [],
      "motivation": "Signature/rule-based tools miss novel attacks; lack of labeled attack data motivates unsupervised methods that leverage graph structure and node attributes to detect anomalous communication patterns.",
      "potential_research_ideas": [
        "Benchmark GCNetOmaly against state-of-the-art graph anomaly detection and time-series baselines on public enterprise-like datasets or generated simulations.",
        "Extend to dynamic/temporal graph models (e.g., TGAT, TGN) to capture evolving communication patterns and reduce false positives.",
        "Domain adaptation or federated learning to generalize across organizations without sharing raw EDR logs.",
        "Incorporate edge features (protocol/port/process context) and attention mechanisms to improve attribution and reduce noise from common services.",
        "Active/semi-supervised loop leveraging limited analyst feedback to improve thresholding and calibration (e.g., PU learning, conformal prediction).",
        "Robustness studies against stealthy/adversarial traffic manipulations; design defenses via adversarial training or robust loss functions.",
        "Automated threshold calibration via Extreme Value Theory (EVT) or Bayesian decision theory to meet analyst workload constraints.",
        "Explainability enhancements with per-edge/feature SHAP on graph reconstructions and causal anomalies (counterfactual edits)."
      ],
      "architectural_improvement_recommendations": [
        "Replace/augment GCN with Graph Attention Networks (GAT) or Heterogeneous GNNs to model different node/edge types (internal/external, service categories).",
        "Model dynamics via temporal GNNs (TGAT/TGN) or sequence-aware decoders combining RNN/Transformer with graph context.",
        "Use decoders that explicitly reconstruct both adjacency and features with separate heads and calibrated losses; add edge-feature reconstruction.",
        "Adopt contrastive self-supervised pretraining (DGI, GRACE) before VAE fine-tuning to improve embeddings with limited labels.",
        "Incorporate edge weights and directions directly (e.g., message passing with weighted directed edges) and protocol-aware embeddings.",
        "Calibrate anomaly scores with EVT and per-segment normalization to control alert volume uniformly across machine cohorts.",
        "Introduce sparsity-aware losses and focal reconstruction terms for highly sparse process features to reduce bias toward common processes."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Enterprise environment (large financial organization) with Carbon Black EDR on endpoints; cohorts: ATMs and machines communicating with AD servers.",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Handling very large graphs and high event volumes (millions of events/day).",
        "Need to segment by homogeneous machine cohorts to reduce variability and false positives.",
        "Threshold selection to cap daily alerts (target <10) for analyst workload.",
        "Training per time-window and applying on same window; operationalizing rolling/online training.",
        "Proprietary data constraints and privacy/security compliance."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "A generic GCN-based method for representing organizational machine communication patterns using adjacency and rich node features (demographic, statistical, process-related, and Node2vec structural features).",
      "An unsupervised GCN-VAE framework (GCNetOmaly) with composite reconstruction loss to identify anomalous machines from EDR-derived communication graphs; evaluated on real enterprise data (ATMs and AD communications).",
      "Practical evaluation showing manageable alert volumes (≤5 machines/day) and high proportion of analyst-validated ‘good’ anomalies (e.g., 11/17 for AD; majority of ATM alerts anomalous)."
    ]
  },
  {
    "arxiv_id": "2312.14958v1",
    "title": "Graph Neural Network-Based Bandwidth Allocation for Secure Wireless Communications",
    "authors": "Xin Hao; Phee Lep Yeoh; Yuhong Liu; Changyang She; Branka Vucetic; Yonghui Li",
    "abstract": "This paper designs a graph neural network (GNN) to improve bandwidth allocations for multiple legitimate wireless users transmitting to a base station in the presence of an eavesdropper. To improve the privacy and prevent eavesdropping attacks, we propose a user scheduling algorithm to schedule users satisfying an instantaneous minimum secrecy rate constraint. Based on this, we optimize the bandwidth allocations with three algorithms namely iterative search (IvS), GNN-based supervised learning (GNN-SL), and GNN-based unsupervised learning (GNN-USL). We present a computational complexity analysis which shows that GNN-SL and GNN-USL can be more efficient compared to IvS which is limited by the bandwidth block size. Numerical simulation results highlight that our proposed GNN-based resource allocations can achieve a comparable sum secrecy rate compared to IvS with significantly lower computational complexity. Furthermore, we observe that the GNN approach is more robust to uncertainties in the eavesdropper's channel state information, especially compared with the best channel allocation scheme.",
    "published_date": "2023-12-13",
    "pdf_link": "https://arxiv.org/pdf/2312.14958v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Wireless and Mobile Security",
      "subdomain": "Physical Layer Security",
      "specific_problem": "Bandwidth allocation with instantaneous secrecy-rate constraints to mitigate eavesdropping in uplink to a base station",
      "attack_types": [
        "Eavesdropping"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "GNN",
        "specific": "Custom GNN with shared per-node FNN encoder, concatenation aggregation, softmax normalization, and readout for bandwidth allocation",
        "novel_contribution": "GNN supports a dynamic number of users (neurons) and uses unsupervised learning with the negative secrecy-rate objective; inputs include normalized per-user minimum bandwidth and surplus bandwidth."
      },
      {
        "type": "primary",
        "category": "Feedforward Neural Network",
        "specific": "Per-node FNN (2-16-8-1) shared across users",
        "novel_contribution": "Node-wise encoder to extract user features from normalized minimum bandwidth and surplus bandwidth; shared weights ensure scalability across varying user counts."
      },
      {
        "type": "baseline",
        "category": "Search/Heuristic Optimization",
        "specific": "Iterative Search (IvS) with resource block ∆W",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "Synthetic wireless channel samples for user-BS and user-eavesdropper links",
        "type": "synthetic",
        "domain": "wireless_channel",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Iterative Search (IvS)",
        "paper_reference": "[14]",
        "metric": "Sum secrecy rate (normalized), computational complexity",
        "their_result": "“the average sum secrecy rate achieved by GNN-USL is approximately 98.7% of IvS.” GNN methods have significantly lower computational complexity, especially for small ∆W.",
        "baseline_result": "IvS achieves the best sum secrecy rate but at high complexity scaling with 1/∆W and number of users."
      },
      {
        "method_name": "Best Channel Allocation (BeC)",
        "paper_reference": null,
        "metric": "Sum secrecy rate (normalized), robustness under eavesdropper CSI uncertainty",
        "their_result": "GNN-USL and GNN-SL outperform BeC across CSI uncertainties; GNN is more robust to eavesdropper CSI uncertainty.",
        "baseline_result": "BeC has the worst average sum secrecy rate and performance degrades with CSI uncertainty; complexity O(Ω)."
      },
      {
        "method_name": "GNN-SL (Supervised variant)",
        "paper_reference": null,
        "metric": "Normalized average sum secrecy rate; convergence speed; complexity",
        "their_result": "Both USL and SL converge; USL converges faster due to loss being the negative objective; inference complexity same as USL but SL requires label collection.",
        "baseline_result": "Comparable to IvS in sum secrecy rate with lower complexity; slower convergence than USL."
      }
    ],
    "performance_metrics_used": [
      "Sum secrecy rate (Mbps)",
      "Normalized sum secrecy rate",
      "Computational complexity (Big-O and dependence on K and ∆W)",
      "Convergence speed (epochs)",
      "Robustness under eavesdropper CSI uncertainty (%)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can a GNN allocate bandwidth for a variable number of users to maximize sum secrecy rate under instantaneous secrecy constraints?",
        "Can unsupervised training (without labels) achieve comparable secrecy-rate performance to iterative search with significantly lower complexity?",
        "How robust are GNN-based allocations to uncertainties in the eavesdropper’s CSI?"
      ],
      "gaps_identified": [
        "DNN-based approaches are limited to a fixed number of users and statistical (not instantaneous) constraints.",
        "Label collection for supervised learning is computationally expensive.",
        "Iterative search methods have high complexity and are sensitive to the bandwidth block size ∆W.",
        "Need for methods robust to dynamic eavesdropper locations and CSI uncertainties."
      ],
      "limitations": [
        "Evaluations are simulation-based; no real-world deployment.",
        "Single eavesdropper scenario; multi-eavesdropper or multi-cell cases not studied.",
        "Assumes accurate legitimate CSI; eavesdropper CSI uncertainty only modeled as additive noise in evaluation, not during training.",
        "Focuses on bandwidth allocation; power control and other resources not jointly optimized.",
        "User scheduling relies on binary search to determine Wk,min per user each slot, adding overhead."
      ],
      "future_work": [
        "“extend our current GNN to be a more robust model targeting the issues of wireless channel mismatches in the training and testing stage”",
        "“extend the current basic GNN model adapting to scalable wireless networks.”"
      ],
      "motivation": "Efficiently maximize physical-layer secrecy (sum secrecy rate) in dynamic wireless networks with changing users and eavesdropper locations, while reducing computational cost versus iterative search and avoiding label collection via unsupervised learning.",
      "potential_research_ideas": [
        "Distributionally robust or uncertainty-aware training that explicitly models eavesdropper CSI uncertainty during training (e.g., adversarial or robust loss).",
        "Joint optimization of bandwidth and power (and possibly user selection) within the GNN using differentiable constraints or Lagrangian methods.",
        "Adopt permutation-invariant message passing (e.g., GCN/GAT with sum/mean pooling) instead of concatenation to better handle large, variable K and improve generalization.",
        "Extend to multi-eavesdropper and multi-cell interference scenarios with cooperative BSs; explore graph structures over users and cells.",
        "Online adaptation via meta-learning or lightweight fine-tuning to address train-test channel mismatch.",
        "Integrate fairness or QoS differentiation (e.g., max-min secrecy rate or weighted objectives) into the loss.",
        "Robust policy distillation and model compression for edge deployment with latency/compute constraints.",
        "Compare with end-to-end DRL policies and hybrid model-based/model-free methods for dynamic environments."
      ],
      "architectural_improvement_recommendations": [
        "Replace concatenation with permutation-invariant aggregation (sum/mean/attention) and message-passing layers to scale with user count and improve inductive bias.",
        "Use graph attention networks (GAT) to weigh user interactions and capture competition for bandwidth.",
        "Incorporate constraint handling via differentiable Lagrangian or primal-dual layers to enforce secrecy-rate and bandwidth constraints during training.",
        "Model CSI uncertainty in the architecture/loss (e.g., expectation over noise samples, CVaR risk) for robustness.",
        "Multi-task heads to predict both allocations and expected secrecy improvements; uncertainty estimation to guide allocation under noisy CSI.",
        "Joint bandwidth–power allocation by extending the readout to multi-dimensional outputs per user."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Training shown for up to ~5000 epochs (Fig. 4) with batch size 64 and learning rate 1e-3; per-node FNN architecture 2-16-8-1; resource block ∆W typically 0.1 MHz for IvS label generation; complexity comparisons provided analytically."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Cellular/BS uplink with multiple users and a mobile eavesdropper",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Reliable estimation or bounding of eavesdropper CSI; performance degrades with CSI uncertainty.",
        "Computing per-user Wk,min via binary search per time slot (scheduling overhead).",
        "Generalization to channel mismatch between training and deployment.",
        "Handling variable numbers of users in real-time with strict secrecy constraints.",
        "Integration with existing RAN schedulers and joint resource control (e.g., power)."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "User scheduling algorithm enforcing an instantaneous minimum secrecy-rate constraint per user to prevent eavesdropping; includes dynamic eavesdropper locations.",
      "GNN-based bandwidth allocation with both supervised and unsupervised training; supports a dynamic number of users via shared per-node FNN and normalized inputs.",
      "Computational complexity analysis showing GNN methods can be significantly more efficient than iterative search (IvS), and robustness evaluation under eavesdropper CSI uncertainty."
    ]
  },
  {
    "arxiv_id": "2311.04194v2",
    "title": "Quantization-aware Neural Architectural Search for Intrusion Detection",
    "authors": "Rabin Yu Acharya; Laurens Le Jeune; Nele Mentens; Fatemeh Ganji; Domenic Forte",
    "abstract": "Deploying machine learning-based intrusion detection systems (IDSs) on hardware devices is challenging due to their limited computational resources, power consumption, and network connectivity. Hence, there is a significant need for robust, deep learning models specifically designed with such constraints in mind. In this paper, we present a design methodology that automatically trains and evolves quantized neural network (NN) models that are a thousand times smaller than state-of-the-art NNs but can efficiently analyze network data for intrusion at high accuracy. In this regard, the number of LUTs utilized by this network when deployed to an FPGA is between 2.3x and 8.5x smaller with performance comparable to prior work.",
    "published_date": "2023-11-07",
    "pdf_link": "https://arxiv.org/pdf/2311.04194v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Hardware-efficient network intrusion detection using quantization-aware neuroevolution for binary classification (attack vs normal) on network flows",
      "attack_types": [
        "fuzzers",
        "analysis",
        "backdoors",
        "exploits",
        "reconnaissance",
        "DoS",
        "generic",
        "shellcode",
        "worms"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Neuroevolution",
        "specific": "InfoNEAT (information theoretic NEAT)",
        "novel_contribution": "Adapts InfoNEAT for IDS as a single-model binary classifier (q-InfoNEAT) and integrates quantization-aware training into the evolutionary loop"
      },
      {
        "type": "primary",
        "category": "Quantization-aware training",
        "specific": "LQ-Nets (Learned Quantization)",
        "novel_contribution": "Learns weight/activation quantizers during training; adapted from CNNs to evolve and train quantized MLP-like NNs within InfoNEAT; 2-bit quantization of weights/activations compatible with bitwise ops"
      },
      {
        "type": "primary",
        "category": "MLP/DNN",
        "specific": "Evolved irregular MLP-like topologies via NEAT",
        "novel_contribution": "Evolution discovers compact irregular topologies; later reshaped with dummy nodes to a regular MLP for FPGA toolchain compatibility"
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "UNSW-NB15",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "BNN-based FPGA IDS",
        "paper_reference": "[5]-[7]",
        "metric": "Accuracy, LUTs",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "CNN-based FPGA IDS with feature reduction",
        "paper_reference": "[8]",
        "metric": "Accuracy, LUTs",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "accuracy",
      "F1-score",
      "LUTs",
      "model_size"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Deploying ML-based IDS on edge/FPGAs is constrained by compute, memory, power, and connectivity",
        "Traditional NEAT requires manual selection/stopping and may overfit; needs principled selection and stopping criteria",
        "Post-training quantization can hurt accuracy; integrating quantization into training is preferable for compact hardware-friendly models"
      ],
      "limitations": [
        "Deployed model must be modified (dummy nodes) to fit FINN's dense systolic array assumption, which is inefficient for irregular NEAT topologies",
        "Evaluation uses a binarized version of UNSW-NB15 (attack vs normal), not multi-class attack identification",
        "Observed sensitivity to class imbalance and cross-day testing; accuracy dropped from 0.86 to 0.79 when testing on an imbalanced dataset",
        "Results shown on a single dataset; generalization to other datasets not reported",
        "No explicit adversarial robustness, explainability, or privacy analysis"
      ],
      "future_work": [
        "Design an efficient accelerator that natively supports irregular NEAT-style topologies instead of reshaping to MLP for FINN"
      ],
      "motivation": "Enable accurate intrusion detection on resource-constrained hardware by automatically evolving quantized, compact neural networks suitable for FPGA/edge deployment.",
      "potential_research_ideas": [
        "Develop a hardware accelerator architecture that directly supports irregular NEAT graphs (sparse/irregular MLPs) to avoid dummy-node inflation",
        "Extend q-InfoNEAT to multi-class IDS to categorize attack types while maintaining compactness",
        "Introduce mixed-precision or layer-wise learned bitwidths within the neuroevolution loop to optimize accuracy-latency-resource trade-offs",
        "Combine q-InfoNEAT with knowledge distillation from larger teachers to improve accuracy under extreme quantization",
        "Add domain adaptation/continual learning mechanisms for cross-day and cross-network generalization without full retraining",
        "Integrate adversarial training or robust quantization objectives to improve resilience to evasion attacks",
        "Incorporate explainability constraints (e.g., feature attributions regularization) into the fitness function for operator trust",
        "Co-search feature extraction (e.g., 1D CNN over packet headers) and classifier under hardware constraints for improved discriminative power"
      ],
      "architectural_improvement_recommendations": [
        "Expand the search space to include structured sparsity and block-sparse connections; target sparse-matrix accelerators to reduce LUT/DSP usage",
        "Adopt mixed-precision QAT (per-layer/per-channel bitwidth search) driven by hardware cost models within the evolutionary fitness",
        "Introduce residual/skip patterns as primitives in the topology mutation operators to improve trainability while keeping hardware-friendly structure",
        "Add early-exit branches to reduce average inference cost under benign-heavy traffic",
        "Use knowledge distillation during evolution to stabilize training of 2-bit quantized candidates",
        "Evolve only FINN-compatible topologies (regularized dense layers) to avoid dummy-node insertion, or target alternate HLS frameworks that support irregular graphs"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "PyTorch",
        "TensorFlow",
        "FINN (Xilinx HLS)"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "FPGA accelerator (via FINN-generated HLS), targeting edge/hardware devices",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Irregular NEAT topologies are incompatible with FINN's dense systolic arrays, requiring dummy nodes and connections (inefficiency)",
        "Edge constraints: limited compute, memory, power, and connectivity",
        "Need for careful hardware-specific optimizations (pipelining, parallelism, memory hierarchy)",
        "Model updates/retraining on-device can be challenging",
        "Dataset imbalance can degrade performance in deployment"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Designs a neural architecture search methodology (q-InfoNEAT) to evolve compact neural networks for IDS on UNSW-NB15",
      "Integrates quantization-aware training (LQ-Nets) into InfoNEAT to train 2-bit quantized networks for model compression and latency/resource reductions",
      "Demonstrates comparable detection performance to prior work with models >1000× smaller and FPGA LUT usage 2.3×–8.5× smaller, and outlines FPGA deployment via FINN"
    ]
  },
  {
    "arxiv_id": "2311.04372v1",
    "title": "Enhancing Malware Detection by Integrating Machine Learning with Cuckoo Sandbox",
    "authors": "Amaal F. Alshmarni; Mohammed A. Alliheedi",
    "abstract": "In the modern era, malware is experiencing a significant increase in both its variety and quantity, aligning with the widespread adoption of the digital world. This surge in malware has emerged as a critical challenge in the realm of cybersecurity, prompting numerous research endeavors and contributions to address the issue. Machine learning algorithms have been leveraged for malware detection due to their ability to uncover concealed patterns within vast datasets. However, deep learning algorithms, characterized by their multi-layered structure, surpass the limitations of traditional machine learning approaches. By employing deep learning techniques such as CNN (Convolutional Neural Network) and RNN (Recurrent Neural Network), this study aims to classify and identify malware extracted from a dataset containing API call sequences. The performance of these algorithms is compared with that of conventional machine learning methods, including SVM (Support Vector Machine), RF (Random Forest), KNN (K-Nearest Neighbors), XGB (Extreme Gradient Boosting), and GBC (Gradient Boosting Classifier), all using the same dataset. The outcomes of this research demonstrate that both deep learning and machine learning algorithms achieve remarkably high levels of accuracy, reaching up to 99% in certain cases.",
    "published_date": "2023-11-07",
    "pdf_link": "https://arxiv.org/pdf/2311.04372v1",
    "paper_types": [
      "new_dataset",
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Malware Analysis",
      "subdomain": "Malware Detection",
      "specific_problem": "Binary classification of malware vs benign software using dynamic API call sequences extracted via Cuckoo Sandbox",
      "attack_types": [
        "general malware"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "RNN",
        "specific": null,
        "novel_contribution": "Applied to API call sequence data dynamically extracted by Cuckoo Sandbox from both malware and benign samples in a newly constructed dataset"
      },
      {
        "type": "primary",
        "category": "CNN",
        "specific": null,
        "novel_contribution": "Applied to API call sequences for malware detection and compared against RNN and classical ML"
      },
      {
        "type": "baseline",
        "category": "SVM",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Random Forest",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "KNN",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Gradient Boosting",
        "specific": "XGBoost",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Gradient Boosting",
        "specific": "Gradient Boosting Classifier",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Cuckoo API Call Sequences Dataset (this paper)",
        "type": "synthetic",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "API call sequence dataset from [23] (first 100 non-repeated parent-process API calls)",
        "type": "public",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "SVM",
        "paper_reference": null,
        "metric": "Accuracy (Experiment 1)",
        "their_result": "RNN 99%",
        "baseline_result": "SVM 91%"
      },
      {
        "method_name": "KNN",
        "paper_reference": null,
        "metric": "Accuracy (Experiment 1)",
        "their_result": "RNN 99%",
        "baseline_result": "KNN 92%"
      },
      {
        "method_name": "XGBoost",
        "paper_reference": null,
        "metric": "Accuracy (Experiment 1)",
        "their_result": "RNN 99%",
        "baseline_result": "XGB 93%"
      },
      {
        "method_name": "Random Forest",
        "paper_reference": null,
        "metric": "Accuracy (Experiment 1)",
        "their_result": "RNN 99%",
        "baseline_result": "RF 95%"
      },
      {
        "method_name": "Gradient Boosting Classifier",
        "paper_reference": null,
        "metric": "Accuracy (Experiment 1)",
        "their_result": "RNN 99%",
        "baseline_result": "GBC 95%"
      },
      {
        "method_name": "SVM",
        "paper_reference": null,
        "metric": "Accuracy (Experiment 2)",
        "their_result": "RNN 99%",
        "baseline_result": "SVM 95%"
      },
      {
        "method_name": "KNN",
        "paper_reference": null,
        "metric": "Accuracy (Experiment 2)",
        "their_result": "RNN 99%",
        "baseline_result": "KNN 99%"
      },
      {
        "method_name": "XGBoost",
        "paper_reference": null,
        "metric": "Accuracy (Experiment 2)",
        "their_result": "RNN 99%",
        "baseline_result": "XGB 96%"
      },
      {
        "method_name": "Random Forest",
        "paper_reference": null,
        "metric": "Accuracy (Experiment 2)",
        "their_result": "RNN 99%",
        "baseline_result": "RF 98%"
      },
      {
        "method_name": "Gradient Boosting Classifier",
        "paper_reference": null,
        "metric": "Accuracy (Experiment 2)",
        "their_result": "RNN 99%",
        "baseline_result": "GBC 99%"
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "Precision",
      "Recall",
      "F1-Score",
      "ROC-AUC"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Conventional malware datasets often fall short in providing the breadth and depth required to effectively combat emerging malware threats.",
        "Need for comprehensive dynamic-analysis-based behavioral datasets capturing nuanced tactics and evasion strategies."
      ],
      "limitations": [
        "Model performance is sensitive to hyperparameters (e.g., batch size) and requires careful tuning.",
        "Current dataset scope limited; authors note the need to add more malware varieties and support multi-class classification."
      ],
      "future_work": [
        "Grow the dataset to include more varieties of malware.",
        "Extend from binary to multi-class classification problems."
      ],
      "motivation": "Leverage deep learning on rich behavioral data from dynamic analysis (Cuckoo Sandbox) to improve malware detection beyond traditional ML and limited existing datasets.",
      "potential_research_ideas": [
        "Develop sequence models (LSTM/GRU/Transformer) with attention mechanisms for API-call-based malware detection.",
        "Investigate multi-class and multi-label malware family classification using the same dynamic features.",
        "Combine static and dynamic features in a hybrid model to reduce false positives and improve zero-day detection.",
        "Model longer and variable-length API call sequences (beyond first 100 calls) with hierarchical or dilated architectures.",
        "Incorporate temporal features such as inter-call timing and system state changes for richer behavioral modeling.",
        "Evaluate cross-sandbox and cross-environment generalization to mitigate environment-specific overfitting.",
        "Adversarial robustness: study resilience against API-call sequence manipulation and sandbox-evasion tactics.",
        "Explainability: apply sequence attribution (e.g., Integrated Gradients) to identify influential API calls for analyst insight."
      ],
      "architectural_improvement_recommendations": [
        "Replace vanilla RNN with bidirectional LSTM/GRU and add attention for better long-range dependency capture.",
        "Use Transformer encoders with positional encodings tailored for event sequences.",
        "Leverage pretraining on large unlabeled API call logs (self-supervised objectives) followed by fine-tuning.",
        "Implement class-imbalance handling via focal loss or cost-sensitive learning rather than only resampling.",
        "Calibrate probabilities (e.g., temperature scaling) and evaluate ROC-AUC and PR-AUC under class imbalance.",
        "Employ model ensembling (e.g., RF/XGB with deep models) to boost robustness."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "scikit-learn"
      ],
      "reproducibility_score": "low",
      "computational_requirements": "Not specified; DL trained with Adam optimizer; 80/20 train-test split; hyperparameter tuning via GridSearchCV for ML baselines."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Creation of a novel behavioral dataset comprising API call sequences extracted from benign and malware samples via Cuckoo Sandbox.",
      "Comprehensive evaluation of deep learning (CNN, RNN) versus classical ML (SVM, RF, KNN, XGB, GBC) on the same dataset.",
      "Demonstration of high detection performance, with accuracies up to 99% (RNN) and strong precision/recall/F1 and ROC-AUC.",
      "Secondary evaluation on a larger external API-call dataset (1,079 benign and 42,797 malware sequences) showing improved ML baseline performance with more samples."
    ]
  },
  {
    "arxiv_id": "2312.00040v1",
    "title": "Presentation Attack detection using Wavelet Transform and Deep Residual Neural Net",
    "authors": "Prosenjit Chatterjee; Alex Yalchin; Joseph Shelton; Kaushik Roy; Xiaohong Yuan; Kossi D. Edoh",
    "abstract": "Biometric authentication is becoming more prevalent for secured authentication systems. However, the biometric substances can be deceived by the imposters in several ways. Among other imposter attacks, print attacks, mask attacks, and replay attacks fall under the presentation attack category. The bio-metric images, especially the iris and face, are vulnerable to different presentation attacks. This research applies deep learning approaches to mitigate presentation attacks in a biometric access control system. Our contribution in this paper is two-fold: First, we applied the wavelet transform to extract the features from the biometric images. Second, we modified the deep residual neural net and applied it to the spoof datasets in an attempt to detect the presentation attacks. This research applied the proposed approach to biometric spoof datasets, namely ATVS, CASIA two class, and CASIA cropped image sets. The datasets used in this research contain images that are captured in both a controlled and uncontrolled environment along with different resolutions and sizes. We obtained the best accuracy of 93% on the ATVS Iris datasets. For CASIA two class and CASIA cropped datasets, we achieved test accuracies of 91% and 82%, respectively.",
    "published_date": "2023-11-23",
    "pdf_link": "https://arxiv.org/pdf/2312.00040v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Identity and Access Management",
      "subdomain": "Biometric Authentication",
      "specific_problem": "Presentation Attack Detection (PAD) / anti-spoofing for iris and face",
      "attack_types": [
        "print attack (printed photo)",
        "cut-photo attack (eyeholes cut out)",
        "wrap/moving photo attack",
        "video replay attack",
        "mask attack (3D/silicone mask) [from related work]",
        "textured contact lens (iris spoof) [from related work]"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Signal Processing / Feature Extraction",
        "specific": "Discrete Wavelet Transform (DWT) + Inverse DWT (IDWT)",
        "novel_contribution": "Use of DWT feature extraction combined with a modified ResNet for PAD; restricted to level-1 decomposition to avoid edge data loss"
      },
      {
        "type": "primary",
        "category": "CNN / Residual Network",
        "specific": "Modified ResNet (32 Conv2D layers, 2 MaxPool2D, 1 FC; single/double skip connections)",
        "novel_contribution": "Lighter custom ResNet architecture emphasizing reduced computational time while maintaining accuracy for PAD"
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "Modified VGGNet (19 layers) [ref.16]",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN / Residual Network",
        "specific": "Modified ResNet without DWT",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "Modified VGGNet with DWT",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "ATVS (BioSec/ATVS Iris liveness subset: periocular/iris, real vs print-scan fake)",
        "type": "public",
        "domain": "iris_periocular_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CASIA Face Anti-Spoofing (FASD) - Two Class split",
        "type": "public",
        "domain": "face_images_and_videos",
        "link": "http://biometrics.idealtest.org/dbDetailForUser.do?id=3",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CASIA Face Anti-Spoofing (FASD) - Agreement",
        "type": "public",
        "domain": "face_images_and_videos",
        "link": "http://www.cbsr.ia.ac.cn/english/FASDB_Agreement/Agreement.pdf",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CASIA Cropped (custom crops from CASIA using Haar cascades; includes custom resized sets)",
        "type": "synthetic",
        "domain": "face_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Modified ResNet (no DWT) vs Proposed DWT+Modified ResNet on ATVS Iris",
        "paper_reference": null,
        "metric": "Test accuracy (%)",
        "their_result": "92.57",
        "baseline_result": "94.40"
      },
      {
        "method_name": "Modified ResNet (no DWT) vs Proposed DWT+Modified ResNet on CASIA Two Class",
        "paper_reference": null,
        "metric": "Test accuracy (%)",
        "their_result": "90.80",
        "baseline_result": "91.00"
      },
      {
        "method_name": "Modified ResNet (no DWT) vs Proposed DWT+Modified ResNet on CASIA Cropped",
        "paper_reference": null,
        "metric": "Test accuracy (%)",
        "their_result": "82.40",
        "baseline_result": "85.70"
      },
      {
        "method_name": "Modified VGGNet (19 layers) vs Proposed DWT+Modified ResNet on ATVS Iris",
        "paper_reference": "ref. 16",
        "metric": "Test accuracy (%)",
        "their_result": "92.57",
        "baseline_result": "97.00"
      },
      {
        "method_name": "Modified VGGNet (19 layers) vs Proposed DWT+Modified ResNet on CASIA Two Class",
        "paper_reference": "ref. 16",
        "metric": "Test accuracy (%)",
        "their_result": "90.80",
        "baseline_result": "96.00"
      },
      {
        "method_name": "Modified VGGNet (19 layers) vs Proposed DWT+Modified ResNet on CASIA Cropped",
        "paper_reference": "ref. 16",
        "metric": "Test accuracy (%)",
        "their_result": "82.40",
        "baseline_result": "95.00"
      },
      {
        "method_name": "DWT+Modified VGGNet vs Proposed DWT+Modified ResNet on ATVS Iris",
        "paper_reference": "ref. 16",
        "metric": "Test accuracy (%)",
        "their_result": "92.57",
        "baseline_result": "89.00"
      },
      {
        "method_name": "DWT+Modified VGGNet vs Proposed DWT+Modified ResNet on CASIA Two Class",
        "paper_reference": "ref. 16",
        "metric": "Test accuracy (%)",
        "their_result": "90.80",
        "baseline_result": "86.00"
      },
      {
        "method_name": "DWT+Modified VGGNet vs Proposed DWT+Modified ResNet on CASIA Cropped",
        "paper_reference": "ref. 16",
        "metric": "Test accuracy (%)",
        "their_result": "82.40",
        "baseline_result": "78.00"
      },
      {
        "method_name": "Average execution time on ATVS Iris: Proposed DWT+Modified ResNet vs other methods",
        "paper_reference": null,
        "metric": "Average execution time (s)",
        "their_result": "1311",
        "baseline_result": "Modified ResNet: 1962; Modified VGGNet: 1459; DWT+Modified VGGNet: 3045"
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "ROC curve",
      "True Positive Rate (TPR)",
      "False Positive Rate (FPR)",
      "Area Under the Curve (AUC)",
      "Average execution time (seconds)",
      "Error rate (%)",
      "Cumulative Match Characteristic (CMC) rank-N accuracy"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can combining discrete wavelet transform feature extraction with a lighter modified ResNet detect biometric presentation attacks with reduced computational time while maintaining acceptable accuracy?",
        "How does the proposed approach perform across controlled (grayscale iris) and less controlled (color face) datasets?"
      ],
      "gaps_identified": [
        "Deep CNN-based PAD often requires high-resolution images, increasing computational complexity and training time.",
        "Potential data loss at edges in wavelet-based feature extraction when using deeper decomposition levels.",
        "Need for robust classifiers to detect small deviations between real and fake biometric samples.",
        "Performance differences between controlled grayscale iris datasets and less controlled color face datasets."
      ],
      "limitations": [
        "Performance degrades on less controlled CASIA color/cropped datasets (e.g., 82.4% test accuracy on CASIA cropped).",
        "Evaluation limited to three datasets; cross-dataset generalization is not reported.",
        "Only level-1 wavelet decomposition used to avoid edge losses; potential underutilization of multi-scale information.",
        "No comparison against external state-of-the-art PAD methods beyond internal baselines.",
        "No details on hardware configuration or hyperparameters for full reproducibility."
      ],
      "future_work": [
        "Test the framework with other popular biometric datasets and observe stability and performance.",
        "Focus on improving test accuracy and overall performance of the proposed architecture."
      ],
      "motivation": "Mitigate biometric presentation attacks (print, mask, replay) on iris and face while reducing computational cost of deep learning approaches, enabling quicker training/execution without sacrificing accuracy.",
      "potential_research_ideas": [
        "Cross-dataset and cross-protocol PAD evaluation with domain adaptation to improve generalization from controlled to uncontrolled environments.",
        "Multi-branch frequency-spatial architectures that learn from both raw RGB/IR images and multiple wavelet scales (including wavelet packet/dual-tree complex wavelets).",
        "Lightweight mobile PAD models (e.g., MobileNet/EfficientNet + wavelet features) optimized for edge devices at access points.",
        "Temporal PAD for video-based attacks using 3D CNNs or CNN+LSTM/Temporal Conv, incorporating micro-motion and rPPG signals.",
        "Self-supervised or contrastive pretraining on large unlabeled biometric corpora to improve robustness to unseen spoof types.",
        "Data augmentation and synthetic spoof generation (GAN-based) to cover diverse attack instruments (masks, contact lenses, print qualities).",
        "Quality-aware and artifact-guided PAD that jointly estimates image quality and liveness to reduce false accepts on low-quality inputs.",
        "Explainable PAD with saliency/attribution and frequency heatmaps to highlight spoof artifacts for operator trust.",
        "Adversarial robustness evaluation and adversarial training tailored to PAD to defend against adaptive attackers."
      ],
      "architectural_improvement_recommendations": [
        "Replace some standard convs with depthwise-separable convolutions and squeeze-and-excitation to reduce FLOPs while preserving accuracy.",
        "Incorporate multi-scale wavelet inputs (LL, LH, HL, HH at multiple levels) via parallel branches with feature fusion/attention.",
        "Use learnable wavelet/scattering transforms or DWT as a fixed first layer followed by trainable adapters.",
        "Adopt focal loss or AUC-optimizing losses to handle class imbalance and optimize detection thresholds.",
        "Apply domain-adversarial training (DANN) or CORAL to close the gap between controlled (ATVS) and uncontrolled (CASIA) distributions.",
        "Leverage pretrained backbones (e.g., ResNet50/EfficientNet) with fine-tuning and freeze lower layers to accelerate convergence.",
        "Integrate temporal modules (1D temporal convs) for video-based CASIA sequences to exploit motion/liveness cues.",
        "Add calibration (Platt scaling/temperature scaling) for better decision thresholds in deployment."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "OpenCV (for Haar cascades/cropping)"
      ],
      "reproducibility_score": "low",
      "computational_requirements": "Architecture: 32 Conv2D layers, 2 MaxPool2D, 1 FC. Reported average execution time on ATVS Iris: 1311s (DWT+Modified ResNet), 1962s (Modified ResNet), 1459s (Modified VGGNet), 3045s (DWT+Modified VGGNet). Hardware not specified."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": "Reported average execution time (ATVS) for end-to-end runs; per-sample inference time not provided.",
      "deployment_challenges": [
        "High-resolution inputs increase computational complexity.",
        "Performance sensitivity to capture conditions (controlled vs uncontrolled) and color/grayscale differences.",
        "Diversity of spoof instruments (print, video, masks, contact lenses) complicates generalization.",
        "Lack of calibrated thresholds and explainability for operator acceptance."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Applied discrete wavelet transform (and inverse) to extract features from iris and face images for PAD.",
      "Designed and implemented a lighter modified ResNet with single/double skip connections (32 Conv2D, 2 MaxPool2D, 1 FC) for PAD.",
      "Combined DWT feature extraction with the modified ResNet and evaluated on ATVS, CASIA two-class, and CASIA-cropped datasets.",
      "Achieved reported test accuracies: ATVS Iris up to ~93% (table: 92.57% with DWT+ResNet), CASIA two-class ~91%, CASIA cropped ~82%.",
      "Demonstrated reduced execution time with DWT+Modified ResNet compared to other tested configurations.",
      "Provided ROC analysis with TPR up to 98% on ATVS and AUC in range 0.96–1.00; CMC rank accuracies (Rank-1: 48.3%, Rank-2: 73.3%, Rank-3: 90.1%)."
    ]
  },
  {
    "arxiv_id": "2311.07056v2",
    "title": "STATGRAPH: Effective In-vehicle Intrusion Detection via Multi-view Statistical Graph Learning",
    "authors": "Kai Wang; Qiguang Jiang; Bailing Wang; Yulei Wu; Hongke Zhang",
    "abstract": "In-vehicle network (IVN) is facing complex external cyber-attacks, especially the emerging masquerade attacks with extremely high difficulty of detection while serious damaging effects. In this paper, we propose the STATGRAPH, which is an effective and fine-grained intrusion detection methodology for IVN security services via multi-view statistical graph learning on in-vehicle controller area network (CAN) messages with insight into their variations in periodicity, payload and signal combinations. Specifically, STATGRAPH generates two statistical graphs, timing correlation graph (TCG) and coupling relationship graph (CRG), in every CAN message detection window, where edge attributes in TCGs represent temporal correlation between different message IDs while edge attributes in CRGs denote the neighbour relationship and contextual similarity. Besides, a lightweight shallow layered graph convolution network is trained based on graph property of TCGs and CRGs, which learns the universal laws of various patterns more effectively and further enhance the performance of detection. To address the problem of insufficient attack types in previous intrusion detection, we select two real in-vehicle CAN datasets covering five new instances of sophisticated and stealthy masquerade attacks that are never investigated before. Experimental result shows STATGRAPH improves both detection granularity and detection performance over state-of-the-art intrusion detection methods. Code is available at https://github.com/wangkai-tech23/StatGraph.",
    "published_date": "2023-11-13",
    "pdf_link": "https://arxiv.org/pdf/2311.07056v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Automotive/Vehicle Security",
      "subdomain": "In-vehicle Network (IVN) Intrusion Detection",
      "specific_problem": "Fine-grained per-message detection of sophisticated masquerade attacks on CAN bus using multi-view graph learning",
      "attack_types": [
        "Fabrication (e.g., DoS, fuzzy, targeted ID injection)",
        "Suspension",
        "Masquerade"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "GNN",
        "specific": "Graph Convolutional Network (GCN)",
        "novel_contribution": "Multi-view statistical graph learning via Timing Correlation Graph (TCG) and Coupling Relationship Graph (CRG); custom node features fusing global TCG statistics with local payload; adjacency derived from contextual proximity and same-ID similarity"
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Car Hacking Dataset",
        "type": "public",
        "domain": "can_bus_messages",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "ROAD Dataset",
        "type": "public",
        "domain": "can_bus_messages",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Identification Granularity (IG)"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can multi-view statistical graph learning (TCG + CRG) improve detection of stealthy masquerade attacks that preserve timing behavior but alter payload?",
        "Can fine-grained, per-message intrusion identification be achieved on CAN traffic rather than coarse, window-level detection?"
      ],
      "gaps_identified": [
        "Existing methods often provide only coarse-grained, window-level detection and cannot locate the malicious message.",
        "Graph-based IVN IDSs largely focus on simpler attack types and under-explore sophisticated masquerade attacks.",
        "Single-view graph approaches relying mainly on IDs/frequencies fail to capture payload-only changes and coupling structures of data flow.",
        "Insufficient attack type coverage in prior evaluations.",
        "Limited modeling of short-term contextual dependence and same-ID similarity in graph construction."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Improve IVN security against increasingly sophisticated masquerade attacks by enabling fine-grained, per-message detection through multi-view statistical graph learning that captures both timing correlations and payload/context coupling.",
      "potential_research_ideas": [
        "Self-supervised or contrastive pretraining on unlabeled CAN streams using multi-view agreement between TCG and CRG to reduce labeling needs.",
        "Online/streaming adaptation to concept drift and evolving attack patterns with incremental graph learning.",
        "Hybrid multi-modal IDS combining CAN graph features with physical/kinematic sensors for cross-consistency checks against payload manipulations.",
        "Adversarial robustness evaluation and defense mechanisms specific to graph-based IDS on CAN (e.g., adversarial message perturbations).",
        "Explainability methods for per-message anomaly attributions (e.g., subgraph or edge-level importance) to aid traceability.",
        "Cross-vehicle transfer learning/domain adaptation to generalize models across different makes/models and CAN layouts."
      ],
      "architectural_improvement_recommendations": [
        "Incorporate attention-based GNNs or Graph Transformers to better weight informative neighbors and edge attributes.",
        "Model temporal dynamics explicitly with temporal GNNs or time-encoding on edges/nodes to capture rhythm changes across windows.",
        "Use edge-feature-aware GNN layers (e.g., EGNN/SE(3)-inspired or edge-conditioned conv) to exploit TCG/CRG edge weights.",
        "Multi-view co-training with consistency regularization between TCG and CRG embeddings; late fusion or gating for view reliability.",
        "Lightweight deployment optimization: pruning, quantization, and knowledge distillation for embedded ECUs.",
        "Integrate uncertainty estimation (e.g., MC Dropout, deep ensembles) to flag low-confidence detections for human review."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/wangkai-tech23/StatGraph",
      "frameworks": [],
      "reproducibility_score": "high",
      "computational_requirements": "Evaluated on an in-vehicle computing platform (NVIDIA Jetson Nano T206, ARM) and a PC (LENOVO 90VA000JCP). Lightweight shallow GCN aimed at small graphs and streaming efficiency."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "In-vehicle CAN bus (IVN); evaluated on in-vehicle computing platform",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Real-time detection requirements in IVN streaming settings",
        "Resource constraints on in-vehicle embedded platforms",
        "Bypass monitoring without modifying existing CAN protocol/ECUs",
        "Stealthy masquerade attacks preserve timing and interaction patterns while altering payload"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "STATGRAPH: multi-view statistical graph learning (TCG + CRG) with a lightweight shallow GCN for effective IVN intrusion detection and fine-grained per-message classification.",
      "Introduced Identification Granularity (IG) to quantify per-message detection granularity instead of coarse window-level recognition.",
      "Comprehensive experiments on real-vehicle CAN datasets (Car Hacking and ROAD) and on embedded (Jetson Nano) and PC platforms; first to investigate five new instances of stealthy masquerade attacks in ROAD."
    ]
  },
  {
    "arxiv_id": "2311.03825v1",
    "title": "IC-SECURE: Intelligent System for Assisting Security Experts in Generating Playbooks for Automated Incident Response",
    "authors": "Ryuta Kremer; Prasanna N. Wudali; Satoru Momiyama; Toshinori Araki; Jun Furukawa; Yuval Elovici; Asaf Shabtai",
    "abstract": "Security orchestration, automation, and response (SOAR) systems ingest alerts from security information and event management (SIEM) system, and then trigger relevant playbooks that automate and orchestrate the execution of a sequence of security activities. SOAR systems have two major limitations: (i) security analysts need to define, create and change playbooks manually, and (ii) the choice between multiple playbooks that could be triggered is based on rules defined by security analysts. To address these limitations, recent studies in the field of artificial intelligence for cybersecurity suggested the task of interactive playbook creation. In this paper, we propose IC-SECURE, an interactive playbook creation solution based on a novel deep learning-based approach that provides recommendations to security analysts during the playbook creation process. IC-SECURE captures the context in the form of alert data and current status of incomplete playbook, required to make reasonable recommendation for next module that should be included in the new playbook being created. We created three evaluation datasets, each of which involved a combination of a set of alert rules and a set of playbooks from a SOAR platform. We evaluated IC-SECURE under various settings, and compared our results with two state-of-the-art recommender system methods. In our evaluation IC-SECURE demonstrated superior performance compared to other methods by consistently recommending the correct security module, achieving precision@1 > 0.8 and recall@3 > 0.92",
    "published_date": "2023-11-07",
    "pdf_link": "https://arxiv.org/pdf/2311.03825v1",
    "paper_types": [
      "new_dataset",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Security Operations",
      "subdomain": "Incident Response Automation (SOAR)",
      "specific_problem": "Interactive playbook creation: next security module recommendation given alert context and partial playbook",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Neural Collaborative Filtering (NCF) recommender (neural network)",
        "specific": "NCF-style MLP with alert and module embeddings plus playbook graph embedding",
        "novel_contribution": "Context-aware recommender that jointly encodes SIEM alert features, current playbook subgraph, and current node to recommend the next SOAR module, including an End-of-Playbook (EOP) option"
      },
      {
        "type": "primary",
        "category": "Embedding/Representation Learning",
        "specific": null,
        "novel_contribution": "Custom one-hot alert encoding, alert embeddings, module embeddings, and playbook graph embeddings used as inputs to the NCF model"
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "BOTS",
        "type": "public",
        "domain": "log_files + SOAR_playbooks",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "AD",
        "type": "public",
        "domain": "log_files + SOAR_playbooks",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "SSC",
        "type": "public",
        "domain": "log_files + SOAR_playbooks",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "precision@k",
      "recall@k"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can a context-aware deep learning recommender accurately suggest the next SOAR module for playbook creation given an alert and the current partial playbook?",
        "Can learned relationships between alerts and playbooks replace rigid rule-based mappings to support novel alerts?"
      ],
      "gaps_identified": [
        "Manual, static, rule-based playbook creation and selection in SOAR is slow and brittle for novel alerts",
        "Heuristic response-selection methods are inflexible, opaque, and lack alert/playbook context",
        "Prior works focus on automated playbook selection or API mapping, not full interactive playbook creation with step-by-step module recommendation",
        "Ontology and rule-heavy approaches require extensive mappings and do not generalize to novel alerts",
        "No full-scale recommender guiding analysts through incident handling steps was identified in prior surveys"
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Reduce analyst workload and improve responsiveness by assisting analysts with data-driven, context-aware module recommendations during playbook creation for novel alerts.",
      "potential_research_ideas": [
        "Online/continual learning from analyst feedback to adapt recommendations to evolving environments and tools",
        "Cold-start handling for unseen modules/tools via metadata and description-driven embeddings (e.g., API docs, tool capabilities)",
        "Uncertainty-aware recommendations with calibrated confidence and abstention/EOP logic to improve safety",
        "LLM-assisted mapping from IRP text to initial playbook skeleton combined with IC-SECURE for stepwise refinement",
        "Cross-SIEM generalization: domain adaptation from one log schema (e.g., Splunk CIM) to others",
        "Human-in-the-loop counterfactual explanations: show why a module was recommended and what alert/playbook changes would alter the ranking",
        "Integrate cost/impact and policy constraints (business/risk) into a constrained recommendation objective"
      ],
      "architectural_improvement_recommendations": [
        "Introduce a graph neural network (GNN) over the playbook subgraph to better capture topology and conditional flows before fusion with alert embeddings",
        "Use pretrained textual encoders over alert fields and module/tool metadata (e.g., API descriptions) to enrich embeddings and improve cold-start",
        "Adopt listwise ranking losses (e.g., softmax cross-entropy over candidates or LambdaLoss) and nDCG optimization for top-k quality",
        "Incorporate uncertainty estimation (e.g., MC dropout or deep ensembles) to drive EOP decisions and human review thresholds",
        "Implement a two-stage retrieval-rerank pipeline: fast candidate generation (approximate nearest neighbors) then neural reranking",
        "Multi-objective training to balance effectiveness with operational cost/latency and policy constraints"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "SOAR platform in a SOC workflow (alert ingestion from SIEM, playbook authoring)",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Integration with diverse SOAR tools/modules and evolving APIs",
        "Cold-start for unseen alerts/modules and drift in alert schemas",
        "Ensuring safe recommendations and correct EOP detection in high-stakes environments"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposed unique representations of alerts and playbooks to provide context for recommendations",
      "Introduced a novel deep learning recommender (NCF-based) for next-module recommendation during playbook design, including EOP handling",
      "Collected three labeled datasets of alerts and mapped playbooks and made them available to the research community",
      "Demonstrated superior performance over two state-of-the-art recommender baselines, achieving “precision@1 > 0.8 and recall@3 > 0.92”"
    ]
  },
  {
    "arxiv_id": "2312.12161v1",
    "title": "Towards an in-depth detection of malware using distributed QCNN",
    "authors": "Tony Quertier; Grégoire Barrué",
    "abstract": "Malware detection is an important topic of current cybersecurity, and Machine Learning appears to be one of the main considered solutions even if certain problems to generalize to new malware remain. In the aim of exploring the potential of quantum machine learning on this domain, our previous work showed that quantum neural networks do not perform well on image-based malware detection when using a few qubits. In order to enhance the performances of our quantum algorithms for malware detection using images, without increasing the resources needed in terms of qubits, we implement a new preprocessing of our dataset using Grayscale method, and we couple it with a model composed of five distributed quantum convolutional networks and a scoring function. We get an increase of around 20 \\% of our results, both on the accuracy of the test and its F1-score.",
    "published_date": "2023-12-19",
    "pdf_link": "https://arxiv.org/pdf/2312.12161v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Malware Analysis",
      "subdomain": "Static Malware Detection",
      "specific_problem": "Binary classification of Windows PE malware vs benign using image-based section features and quantum neural networks",
      "attack_types": [
        "malware (general)",
        "Windows PE malware"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Quantum Convolutional Neural Network (QCNN)",
        "specific": "Distributed multi-QCNN (five QCNNs, one per PE section: .text, .data, .rdata, .rsrc, .reloc)",
        "novel_contribution": "Distributed QCNN architecture trained per PE section with 8-qubit QCNNs and a learned scoring function that fuses five section-level scores"
      },
      {
        "type": "primary",
        "category": "Gradient Boosted Trees",
        "specific": "XGBoost (scoring function)",
        "novel_contribution": "Trained as a meta-classifier to combine the five QCNN section scores into a final decision; outperformed RF and LightGBM on held-out test"
      },
      {
        "type": "baseline",
        "category": "Ensemble/Rule",
        "specific": "Majority vote over section QCNN outputs",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Gradient Boosted Trees",
        "specific": "LightGBM (alternative scoring function)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Random Forest",
        "specific": "RF (alternative scoring function)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Quantum Convolutional Neural Network (QCNN)",
        "specific": "Single QCNN on the complete binary image (PCA to 8 features)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "BODMAS",
        "type": "public",
        "domain": "malware_binaries (Windows PE files)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "PE Malware Machine Learning Dataset (Practical Security Analytics)",
        "type": "public",
        "domain": "malware_binaries (Windows PE files)",
        "link": "https://practicalsecurityanalytics.com/pe-malware-machine-learning-dataset/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Single QCNN on complete binary image",
        "paper_reference": null,
        "metric": "Accuracy (Test)",
        "their_result": "0.83 (Distributed QCNN + XGBoost)",
        "baseline_result": "0.60"
      },
      {
        "method_name": "Single QCNN on complete binary image",
        "paper_reference": null,
        "metric": "F1-score (Test)",
        "their_result": "0.83 (Distributed QCNN + XGBoost)",
        "baseline_result": "0.66"
      },
      {
        "method_name": "Distributed QCNN + RF scoring",
        "paper_reference": null,
        "metric": "Accuracy (Test)",
        "their_result": "0.83 (XGBoost scoring)",
        "baseline_result": "0.82"
      },
      {
        "method_name": "Distributed QCNN + RF scoring",
        "paper_reference": null,
        "metric": "F1-score (Test)",
        "their_result": "0.83 (XGBoost scoring)",
        "baseline_result": "0.80"
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "F1-score"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can quantum neural networks with few qubits be made effective for image-based malware detection?",
        "Does distributing QCNNs across PE file sections and learning a scoring function improve detection without increasing qubit count?"
      ],
      "gaps_identified": [
        "Quantum neural networks with few qubits perform poorly on image-based malware detection when trained on full images.",
        "PCA on full-file images may mix information across sections and degrade learning quality.",
        "Generalization to new malware remains a challenge in ML-based malware detection.",
        "Limited qubit availability constrains model capacity on current hardware."
      ],
      "limitations": [
        "Per-section QCNN results are modest and comparable to full-image QCNN when considered individually.",
        "Only five sections were used; other potentially informative sections were not explored.",
        "Only one QCNN architecture (specific convolution/pooling design) and limited training epochs (5) were evaluated.",
        "No comparison against strong classical baselines on the same data (e.g., feature-based LightGBM, MalConv).",
        "No evaluation on real hardware or noise effects; simulations only."
      ],
      "future_work": [
        "Increase the number of sections considered (e.g., .idata, .edata, .bss) and assess their impact.",
        "Explore different scoring functions and feature importance to identify most useful sections.",
        "Implement a weighted average scoring function to produce a probability/percentage of maliciousness.",
        "Improve QCNN architectures, including alternative convolution and pooling layer designs."
      ],
      "motivation": "Improve quantum machine learning performance for malware detection under strict qubit limitations by leveraging PE section structure and a learned fusion mechanism.",
      "potential_research_ideas": [
        "Hybrid section-wise architecture: classical CNN encoders per section feeding a small QCNN head, compared to purely quantum QCNNs.",
        "Attention-based or learned gating mechanisms to weight section contributions dynamically instead of post-hoc scoring.",
        "Time-split or family-split evaluations to assess temporal generalization and robustness to novel malware families.",
        "Adversarial robustness studies for section-manipulation attacks (e.g., padding or removing sections) and corresponding defenses.",
        "Noise-aware and hardware-efficient ansatz design with error mitigation for running on NISQ devices.",
        "Data augmentation tailored to PE sections (e.g., benign-preserving transformations) to improve generalization.",
        "Domain adaptation/transfer learning across datasets (BODMAS to other PE corpora) to test cross-corpus robustness.",
        "Quantum kernel methods over section embeddings as an alternative to QCNNs for few-qubit constraints.",
        "Automated architecture search (NAS) for QCNN layer patterns under qubit/depth budgets."
      ],
      "architectural_improvement_recommendations": [
        "Introduce data re-uploading and parameter-sharing within QCNN layers to increase expressivity without increasing qubit count.",
        "Replace fixed pooling with trainable pooling or quantum attention to preserve more informative amplitudes before tracing out qubits.",
        "Use calibrated, weighted stacking for the meta-classifier (e.g., XGBoost with Platt or isotonic calibration) to output well-calibrated probabilities.",
        "Incorporate section presence/absence as explicit binary features alongside QCNN scores, rather than encoding absence as -1.",
        "Expand the section set and adopt per-section feature selection to reduce noise for rarely informative sections.",
        "Evaluate alternative encoders (e.g., Ry/Rz rotation encodings with data re-uploading) beyond the product-state cos/sin mapping.",
        "Increase training epochs with early stopping and hyperparameter search for SPSB and the boosting models."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "XGBoost",
        "LightGBM",
        "Random Forest (scikit-learn likely; not specified)"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Five QCNNs with 8 qubits each (3 layers), trained for 5 epochs using SPSB optimizer; PCA reduces each 8x8 section image to 8 features; meta-classifier trained with XGBoost; exact hardware not specified."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Current quantum hardware constraints (limited qubits) necessitate small QCNNs.",
        "Mapping PE file sections into small fixed-size images (8x8) may limit fidelity of content representation.",
        "Combining multiple per-section quantum models introduces system complexity for deployment.",
        "Lack of evaluation on live enterprise streams or on-device constraints."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces a distributed QCNN approach with five section-specific QCNNs for Windows PE malware detection.",
      "Develops a learned scoring function (XGBoost) to fuse per-section QCNN outputs, improving accuracy and F1.",
      "PE-aware preprocessing: per-section grayscale images (8x8) extracted using LIEF; PCA to 8 features per section; encoding via product-state mapping.",
      "Demonstrates approximately 20% improvement over a single QCNN trained on full-file images (Accuracy: 0.83 vs 0.60; F1: 0.83 vs 0.66 on test).",
      "Provides interpretability at section level by exposing per-section QCNN scores.",
      "Empirically compares multiple scoring functions (XGBoost, LightGBM, Random Forest, Majority Vote) and selects XGBoost as best performer."
    ]
  },
  {
    "arxiv_id": "2311.04148v1",
    "title": "Contactless Fingerprint Biometric Anti-Spoofing: An Unsupervised Deep Learning Approach",
    "authors": "Banafsheh Adami; Nima Karimian",
    "abstract": "Contactless fingerprint recognition offers a higher level of user comfort and addresses hygiene concerns more effectively. However, it is also more vulnerable to presentation attacks such as photo paper, paper-printout, and various display attacks, which makes it more challenging to implement in biometric systems compared to contact-based modalities. Limited research has been conducted on presentation attacks in contactless fingerprint systems, and these studies have encountered challenges in terms of generalization and scalability since both bonafide samples and presentation attacks are utilized during training model. Although this approach appears promising, it lacks the ability to handle unseen attacks, which is a crucial factor for developing PAD methods that can generalize effectively. We introduced an innovative anti-spoofing approach that combines an unsupervised autoencoder with a convolutional block attention module to address the limitations of existing methods. Our model is exclusively trained on bonafide images without exposure to any spoofed samples during the training phase. It is then evaluated against various types of presentation attack images in the testing phase. The scheme we proposed has achieved an average BPCER of 0.96\\% with an APCER of 1.6\\% for presentation attacks involving various types of spoofed samples.",
    "published_date": "2023-11-07",
    "pdf_link": "https://arxiv.org/pdf/2311.04148v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Biometric Security",
      "subdomain": "Presentation Attack Detection (PAD)",
      "specific_problem": "Contactless fingerprint anti-spoofing that generalizes to unseen presentation attacks using unsupervised learning",
      "attack_types": [
        "photo paper (photo attack)",
        "paper printout (print attack)",
        "ecoflex",
        "playdoh",
        "wood glue",
        "latex",
        "dragonskin",
        "gelafix",
        "gelatin",
        "glue",
        "knetosil",
        "mouldable-clay",
        "mouldable-glue",
        "silly-putty"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Autoencoder (CNN) + Attention",
        "specific": "Convolutional Autoencoder with Convolutional Block Attention Module (CBAM)",
        "novel_contribution": "Integrates CBAM (channel and spatial attention) after each convolutional layer in both encoder and decoder; trained exclusively on bonafide images; uses reconstruction error as decision threshold for PAD to handle unseen attacks."
      },
      {
        "type": "baseline",
        "category": "Autoencoder (CNN)",
        "specific": "Convolutional Autoencoder (without CBAM)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "Swin-Transformer",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "AlexNet",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "ResNet (e.g., ResNet18/34/50)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "DenseNet (e.g., DenseNet-121/201)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "MobileNet-V2",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "NASNetMobile",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "Vision Transformer",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Classical ML + Handcrafted",
        "specific": "LBP, BSIF, HOG with SVM",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Unsupervised (one-class/anomaly detection)"
    ],
    "datasets": [
      {
        "name": "CLARKSON contactless fingerprint PAD dataset",
        "type": "public",
        "domain": "fingerprint_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "COLFISPOOF",
        "type": "public",
        "domain": "fingerprint_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "IIITD Spoofed Fingerphoto Database",
        "type": "public",
        "domain": "fingerprint_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "AlexNet (Fujio et al. 2018)",
        "paper_reference": "[16]",
        "metric": "HTER",
        "their_result": "APCER 1.6%, BPCER 0.96% (this paper)",
        "baseline_result": "HTER = 0.04%"
      },
      {
        "method_name": "AlexNet (IIITD) and ResNet (IIITD) (Marasco et al. 2022)",
        "paper_reference": "[17], [18]",
        "metric": "D-EER",
        "their_result": "APCER 1.6%, BPCER 0.96%",
        "baseline_result": "D-EER AlexNet = 2.14%, D-EER ResNet = 0.96%"
      },
      {
        "method_name": "DenseNet-121, NASNet (Purnapatra et al. 2023)",
        "paper_reference": "[19]",
        "metric": "APCER/BPCER",
        "their_result": "APCER 1.6%, BPCER 0.96%",
        "baseline_result": "APCER = 0.14%, BPCER = 0.18%"
      },
      {
        "method_name": "Multiple CNNs and Vision Transformer (Hailin Li et al. 2023)",
        "paper_reference": "[20]",
        "metric": "EER (among others)",
        "their_result": "APCER 1.6%, BPCER 0.96%",
        "baseline_result": "ResNet50 EER = 8.6% (four training/testing PAI cases studied)"
      },
      {
        "method_name": "Combination of two CNNs (Puranpatra et al. 2023 competition)",
        "paper_reference": "[21]",
        "metric": "APCER/BPCER/ACER",
        "their_result": "APCER 1.6%, BPCER 0.96%",
        "baseline_result": "BPCER = 0.62, APCER = 11.35, ACER = 6"
      },
      {
        "method_name": "ResNet-18/LeakyReLU with combined loss (Adami et al. 2023)",
        "paper_reference": "[22]",
        "metric": "APCER/BPCER/ACER",
        "their_result": "APCER 1.6%, BPCER 0.96%",
        "baseline_result": "BPCER = 0.12, APCER = 0.63, ACER = 0.68"
      },
      {
        "method_name": "Handcrafted features (Taneja et al. 2016)",
        "paper_reference": "[14]",
        "metric": "EER",
        "their_result": "APCER 1.6%, BPCER 0.96%",
        "baseline_result": "EER = 3.71%"
      },
      {
        "method_name": "Handcrafted (LBP/BSIF/HOG + SVM) (Wasnik et al. 2018)",
        "paper_reference": "[15]",
        "metric": "APCER/BPCER",
        "their_result": "APCER 1.6%, BPCER 0.96%",
        "baseline_result": "BPCER = 1.8, 0, 0.66; APCER = 10"
      }
    ],
    "performance_metrics_used": [
      "BPCER",
      "APCER",
      "ACER",
      "ROC curve"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can an unsupervised model trained only on bonafide contactless fingerprint images reliably detect diverse and unseen presentation attacks?",
        "Does integrating CBAM into a convolutional autoencoder improve feature representation and PAD performance under distribution shifts?"
      ],
      "gaps_identified": [
        "Existing PAD approaches assume similar training/testing distributions, limiting generalization to real-world unseen attacks.",
        "Contactless fingerprint PAIs are diverse, making it impractical to curate labeled training data covering all possible attacks for each deployment.",
        "Prior works often train on both live and spoof samples, hindering scalability and generalization."
      ],
      "limitations": [
        "Synthetic spoof type from CLARKSON was not investigated in this study.",
        "The CLARKSON subset used contained fewer samples than the original competition release.",
        "No deployment or on-device evaluation is reported; hardware, latency, and energy costs are unspecified."
      ],
      "future_work": [],
      "motivation": "Develop a hygienic, touchless fingerprint PAD that robustly detects unseen presentation attacks, addressing generalization and scalability challenges in smartphone-based contactless biometrics.",
      "potential_research_ideas": [
        "Open-set PAD with calibrated uncertainty estimates to better handle unseen PAIs and threshold selection.",
        "Domain generalization/domain adaptation across devices, sensors, and environmental conditions (illumination/background).",
        "Self-supervised or contrastive pretraining on large unlabeled contactless fingerprint corpora to improve representations.",
        "Multi-spectral or NIR/RGB fusion for contactless PAD to exploit material reflectance differences.",
        "Few-shot continual learning to rapidly incorporate novel PAIs without catastrophic forgetting.",
        "Generative modeling of spoof variants to augment training for robustness (diffusion or GAN-based spoof simulation).",
        "Joint segmentation-plus-PAD pipeline to focus on finger ROIs and reduce background spurious cues.",
        "Lightweight on-device architecture search and quantization/pruning for mobile deployment."
      ],
      "architectural_improvement_recommendations": [
        "Hybrid CAE+Transformer (e.g., CBAM-CAE encoder with lightweight Swin blocks) to capture longer-range dependencies.",
        "Feature-level one-class objectives (e.g., Deep SVDD or hypersphere loss) alongside reconstruction error for stronger anomaly discrimination.",
        "Adaptive, distribution-aware thresholding using extreme value theory or conformal prediction.",
        "Multi-scale CBAM placement and gated skip connections to preserve fine ridge details.",
        "Add frequency-domain branches (DCT/wavelet) and texture descriptors fused with deep features.",
        "Train with synthetic photometric/geometry augmentations to reduce overfitting to background and lighting.",
        "Knowledge distillation to a mobile-sized student with attention map consistency regularization."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Distribution shift across devices/environments causing generalization issues.",
        "Wide diversity of PAIs makes exhaustive supervised training impractical.",
        "Smartphone capture constraints (single camera type, limited compute).",
        "Sensitivity to illumination/background variations in contactless capture."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes an unsupervised CBAM-augmented convolutional autoencoder trained only on bonafide images for contactless fingerprint PAD.",
      "Designs attention placements after each convolution in encoder and decoder (channel then spatial) to enhance feature representation.",
      "Uses reconstruction error as a thresholded score to detect unseen PAIs.",
      "Evaluates on multiple public datasets (CLARKSON, COLFISPOOF, IIITD) and reports average BPCER of 0.96% with APCER of 1.6%.",
      "Compares against supervised approaches and literature that require spoof data in training and struggle with unseen attacks."
    ]
  },
  {
    "arxiv_id": "2312.13697v1",
    "title": "Investigation of Multi-stage Attack and Defense Simulation for Data Synthesis",
    "authors": "Ömer Sen; Bozhidar Ivanov; Martin Henze; Andreas Ulbig",
    "abstract": "The power grid is a critical infrastructure that plays a vital role in modern society. Its availability is of utmost importance, as a loss can endanger human lives. However, with the increasing digitalization of the power grid, it also becomes vulnerable to new cyberattacks that can compromise its availability. To counter these threats, intrusion detection systems are developed and deployed to detect cyberattacks targeting the power grid. Among intrusion detection systems, anomaly detection models based on machine learning have shown potential in detecting unknown attack vectors. However, the scarcity of data for training these models remains a challenge due to confidentiality concerns. To overcome this challenge, this study proposes a model for generating synthetic data of multi-stage cyber attacks in the power grid, using attack trees to model the attacker's sequence of steps and a game-theoretic approach to incorporate the defender's actions. This model aims to create diverse attack data on which machine learning algorithms can be trained.",
    "published_date": "2023-12-21",
    "pdf_link": "https://arxiv.org/pdf/2312.13697v1",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Critical Infrastructure Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Synthetic multi-stage cyberattack data generation for ML-based IDS in power grid (ICS/SCADA) using attack-graph and game-theoretic attacker–defender simulation; evaluation of classifiers on generated IDS alert logs",
      "attack_types": [
        "Multi-stage intrusion/kill-chain",
        "Network intrusion in ICS/SCADA",
        "SCADA server compromise",
        "Lateral movement",
        "Reconnaissance/scanning (MITRE ATT&CK techniques referenced)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Simulation/Data Generation",
        "specific": "Game-theoretic attacker–defender simulator with attack trees/graphs (MulVAL) and Dijkstra path optimization; learning-rate updates for both players; centrality-based IDS sensor placement",
        "novel_contribution": "Proposed procedural multi-stage attack–defense data generator for power grid, integrating attack trees with game-theoretic dynamics and defender sensor placement via current-flow betweenness centrality"
      },
      {
        "type": "baseline",
        "category": "Ensemble/Gradient Boosting",
        "specific": "XGBoost",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Random Forest",
        "specific": "Random Forest",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Decision Tree",
        "specific": "Decision Tree",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "SVM",
        "specific": "Support Vector Machine",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Naive Bayes",
        "specific": "Complement Naive Bayes",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Clustering",
        "specific": "K-Means",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "Synthetic multi-stage cyberattack dataset for power grid (this paper)",
        "type": "synthetic",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "CICIDS2017",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "ID2T-generated datasets",
        "type": "synthetic",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "UNSW-NB15",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Melody (smart grid attack data)",
        "type": "synthetic",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CVE/NVD vulnerability database (for TTC/risk modeling)",
        "type": "public",
        "domain": "vulnerability_database",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "XGBoost",
        "paper_reference": null,
        "metric": "Accuracy, F1, AUC, MCC (Table III)",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Random Forest",
        "paper_reference": null,
        "metric": "Accuracy, F1, AUC, MCC",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Decision Tree",
        "paper_reference": null,
        "metric": "Accuracy, F1, AUC, MCC",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "SVM",
        "paper_reference": null,
        "metric": "Accuracy, F1, AUC, MCC; fitting time",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Complement Naive Bayes",
        "paper_reference": null,
        "metric": "Accuracy, F1, AUC, MCC",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "K-Means",
        "paper_reference": null,
        "metric": "Accuracy, F1, AUC, MCC",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "F1-score",
      "AUC (ROC)",
      "Matthews Correlation Coefficient (MCC)",
      "Training/fitting time (qualitative)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How do complex, multi-stage attacker–defender dynamics affect the quality and characteristics of attack data for IDS training?",
        "What is the impact of different levels of attack complexity and defender configurations (sensor coverage, funds) on generated data and IDS detection performance?",
        "Which ML models perform best on IDS alerts derived from the simulated smart grid attack data?"
      ],
      "gaps_identified": [
        "Scarcity of high-quality, comprehensive cyberattack datasets for power grids due to confidentiality and privacy concerns",
        "Public datasets may be limited for ICS/SCADA contexts; sharing data can increase cyber risk, underscoring need for realistic synthetic datasets",
        "Challenges in building realistic lab environments and scenarios while ensuring data integrity and privacy"
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Enable ML-based IDS for power grids despite data scarcity by generating diverse, realistic synthetic data reflecting multi-stage attacks and defender responses.",
      "potential_research_ideas": [
        "Develop a conditional generator to synthesize IDS alerts conditioned on MITRE techniques, attacker skill, and defender posture; compare to the game-theoretic simulator",
        "Use graph neural networks over attack graphs and network topology to detect multi-stage intrusions directly from paths and alerts",
        "Incorporate sequence/time-series models (e.g., Transformers) for streaming alert classification and early-stage attack detection",
        "Domain adaptation/transfer learning from synthetic to limited real logs; evaluate few-shot fine-tuning on real ICS alerts",
        "Co-evolutionary RL for attacker and defender policies to generate harder scenarios and optimize sensor placement simultaneously",
        "Integrate differential privacy or privacy-preserving simulation parameters to safely share synthetic datasets",
        "Benchmark robustness by injecting label noise, sensor outages, and varying IDS rule-sets; study model stability and drift",
        "Calibrate the TTC and outage-cost models with operator-provided priors; perform sensitivity analyses to improve realism"
      ],
      "architectural_improvement_recommendations": [
        "Model IDS alerts as sequences and apply temporal models (e.g., Transformer encoder) with positional time gaps and signature semantics",
        "Apply heterogeneous GNNs on a joint graph (network topology + MulVAL attack graph + sensor placement) to score nodes/paths and alerts",
        "Adopt semi-supervised anomaly detection (e.g., Isolation Forest, One-Class SVM, Deep SVDD) to better handle label scarcity and class imbalance",
        "Leverage cost-sensitive learning and focal loss to reflect asymmetric outage costs and attack prevalence",
        "Implement online/continual learning to adapt to evolving attacker strategies and defender configurations",
        "Add explainability via SHAP/feature attribution on alert fields and graph-level explanations on attack paths",
        "Automate defender sensor placement via RL optimizing detection utility under budget constraints"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "scikit-learn",
        "XGBoost"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "PC with multicore CPU, >=16GB RAM, dedicated GPU with >=8GB VRAM, SSD; Python-based; MulVAL via Docker; grid search for hyperparameters; simulation includes attack graph generation and O(N^3) complexity steps."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Bridging synthetic-to-real domain gap for ICS alerts",
        "Selecting/placing sufficient IDS sensors under budget constraints while minimizing false positives",
        "Operational constraints in power grids (availability-critical) may limit aggressive defenses",
        "Evolving attacker strategies may induce concept drift in IDS models",
        "Data confidentiality concerns hinder sharing and validation against real incidents"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Developed a model to generate synthetic cyberattack data for ML-based IDS in power grids",
      "Integrated attack tree/graph modeling (MulVAL) with game-theoretic attacker–defender mechanics, including learning effects and centrality-based sensor placement",
      "Evaluated multiple ML models (RF, DT, SVM, CNB, XGB, K-Means) on the generated IDS alert data; analyzed how attacker–defender dynamics and complexity affect detection",
      "Provided simulation methodology with TTC-based success modeling (using CVE/NVD) and outage-cost-based risk for the power grid context"
    ]
  },
  {
    "arxiv_id": "2311.14514v1",
    "title": "FRAD: Front-Running Attacks Detection on Ethereum using Ternary Classification Model",
    "authors": "Yuheng Zhang; Pin Liu; Guojun Wang; Peiqiang Li; Wanyi Gu; Houji Chen; Xuelei Liu; Jinyao Zhu",
    "abstract": "With the evolution of blockchain technology, the issue of transaction security, particularly on platforms like Ethereum, has become increasingly critical. Front-running attacks, a unique form of security threat, pose significant challenges to the integrity of blockchain transactions. In these attack scenarios, malicious actors monitor other users' transaction activities, then strategically submit their own transactions with higher fees. This ensures their transactions are executed before the monitored transactions are included in the block. The primary objective of this paper is to delve into a comprehensive classification of transactions associated with front-running attacks, which aims to equip developers with specific strategies to counter each type of attack. To achieve this, we introduce a novel detection method named FRAD (Front-Running Attacks Detection on Ethereum using Ternary Classification Model). This method is specifically tailored for transactions within decentralized applications (DApps) on Ethereum, enabling accurate classification of front-running attacks involving transaction displacement, insertion, and suppression. Our experimental validation reveals that the Multilayer Perceptron (MLP) classifier offers the best performance in detecting front-running attacks, achieving an impressive accuracy rate of 84.59% and F1-score of 84.60%.",
    "published_date": "2023-11-24",
    "pdf_link": "https://arxiv.org/pdf/2311.14514v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Blockchain Security",
      "subdomain": "Front-running/MEV Detection",
      "specific_problem": "Ternary classification of Ethereum DApp front-running attacks into displacement, insertion, and suppression",
      "attack_types": [
        "displacement",
        "insertion",
        "suppression"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "MLP (Feedforward Neural Network)",
        "specific": "Multilayer Perceptron",
        "novel_contribution": "Used as the best-performing classifier within FRAD for ternary classification; optimized via Bayesian hyperparameter optimization"
      },
      {
        "type": "baseline",
        "category": "Gradient Boosted Trees",
        "specific": "XGBoost",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Gradient Boosting (Tree-based)",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Random Forest",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Hyperparameter Optimization",
        "specific": "Bayesian optimization",
        "novel_contribution": "Applied to tune model hyperparameters and improve detection performance"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Multi-class classification"
    ],
    "datasets": [
      {
        "name": "Frontrunner Jones Ethereum front-running dataset/tools [14]",
        "type": "public",
        "domain": "blockchain_transactions",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "XGBoost (XGB)",
        "paper_reference": null,
        "metric": "Per-class accuracy (displacement/insertion/suppression)",
        "their_result": "MLP: 85.97% / 86.35% / 81.47%",
        "baseline_result": "XGB: 83.75% / 84.92% / 81.01%"
      },
      {
        "method_name": "Random Forest (RF)",
        "paper_reference": null,
        "metric": "Per-class accuracy highlights",
        "their_result": "MLP: insertion 86.35%, suppression 81.47%",
        "baseline_result": "RF: insertion 87.30% (highest among models), suppression 78.71%; 13.78% of suppression misclassified as displacement"
      },
      {
        "method_name": "Gradient Boosting (GB)",
        "paper_reference": null,
        "metric": "Per-class accuracy (displacement/insertion/suppression)",
        "their_result": "MLP: 85.97% / 86.35% / 81.47%",
        "baseline_result": "GB: 85.38% / 86.51% / 80.55%"
      },
      {
        "method_name": "All baselines vs FRAD-MLP",
        "paper_reference": null,
        "metric": "Overall Accuracy; F1-score",
        "their_result": "Accuracy 84.59%; F1-score 84.60%",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "accuracy",
      "precision",
      "recall",
      "F1-score",
      "confusion matrix (TP, FP, TN, FN)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can front-running attacks in Ethereum DApps be accurately classified into displacement, insertion, and suppression using supervised learning?",
        "Which model among XGB, GB, RF, and MLP performs best for ternary front-running attack detection?",
        "Does Bayesian hyperparameter optimization improve detection performance for this task?"
      ],
      "gaps_identified": [
        "Insufficient methods for detailed front-running attack categorization.",
        "Most existing methods only perform binary classification (normal vs. front-running), lacking effective approaches to classify displacement, insertion, and suppression.",
        "Lack of diversity and comparison in training models."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Front-running attacks threaten Ethereum DApp transaction integrity; detailed classification into displacement, insertion, and suppression can equip developers with targeted countermeasures.",
      "potential_research_ideas": [
        "Model front-running detection using graph neural networks over transaction/mempool interaction graphs to capture structural patterns across addresses, pools, and blocks.",
        "Incorporate temporal and mempool dynamics (e.g., gas price time-series, mempool congestion, replacement patterns) via sequence models (Transformers or TCNs) for real-time detection.",
        "Cross-chain generalization: adapt and evaluate on other EVM chains and non-EVM chains to assess portability and domain shift.",
        "Semi-supervised or weakly supervised learning leveraging large unlabeled mempool data to reduce reliance on labeled attack instances.",
        "Adversarially robust training to counter strategic evasion by attackers (e.g., fee camouflage, transaction splitting).",
        "Explainable detection with SHAP/IG to identify contributing features per class (displacement/insertion/suppression) aiding operator trust and response playbooks.",
        "Active learning pipeline for continuous labeling of ambiguous mempool patterns to keep pace with evolving MEV strategies.",
        "Benchmark creation: standardized public benchmark with clear splits and per-class protocols for front-running detection."
      ],
      "architectural_improvement_recommendations": [
        "Replace the MLP with a hybrid architecture: GNN over transaction/address graphs feeding a Transformer/MLP head for ternary classification.",
        "Use class-balanced/focal losses and calibrated probability outputs to improve minority class (e.g., suppression) performance.",
        "Construct an ensemble (e.g., soft-vote of XGB + RF + MLP) with stacking to leverage complementary strengths observed across attack types.",
        "Augment features with mempool-level signals (gas price trajectories, replacement/nonce patterns, pending time, priority fee deltas) and block-level congestion indicators.",
        "Perform rigorous cross-validation and temporal splits to mitigate leakage and better estimate real-world performance.",
        "Automate hyperparameter tuning with Optuna/SMBO and early stopping; log and publish tuned configs for reproducibility."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Propose FRAD, a detection model to classify three types of Ethereum front-running attacks (displacement, insertion, suppression).",
      "Utilize Bayesian hyperparameter optimization to enhance detection performance.",
      "Comprehensive evaluation on 9,798 real-world transactions using XGB, GB, RF, and MLP; MLP achieved 84.59% accuracy and 84.60% F1-score."
    ]
  },
  {
    "arxiv_id": "2311.08910v2",
    "title": "Progressive Feedback-Enhanced Transformer for Image Forgery Localization",
    "authors": "Haochen Zhu; Gang Cao; Xianglin Huang",
    "abstract": "Blind detection of the forged regions in digital images is an effective authentication means to counter the malicious use of local image editing techniques. Existing encoder-decoder forensic networks overlook the fact that detecting complex and subtle tampered regions typically requires more feedback information. In this paper, we propose a Progressive FeedbACk-enhanced Transformer (ProFact) network to achieve coarse-to-fine image forgery localization. Specifically, the coarse localization map generated by an initial branch network is adaptively fed back to the early transformer encoder layers, which can enhance the representation of positive features while suppressing interference factors. The cascaded transformer network, combined with a contextual spatial pyramid module, is designed to refine discriminative forensic features for improving the forgery localization accuracy and reliability. Furthermore, we present an effective strategy to automatically generate large-scale forged image samples close to real-world forensic scenarios, especially in realistic and coherent processing. Leveraging on such samples, a progressive and cost-effective two-stage training protocol is applied to the ProFact network. The extensive experimental results on nine public forensic datasets show that our proposed localizer greatly outperforms the state-of-the-art on the generalization ability and robustness of image forgery localization. Code will be publicly available at https://github.com/multimediaFor/ProFact.",
    "published_date": "2023-11-15",
    "pdf_link": "https://arxiv.org/pdf/2311.08910v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Multimedia Forensics",
      "subdomain": "Digital Image Forensics",
      "specific_problem": "Pixel-level image forgery localization (coarse-to-fine detection of manipulated regions)",
      "attack_types": [
        "splicing",
        "copy-move",
        "inpainting",
        "blurring (post-processing)",
        "compression (post-processing)",
        "contrast adjustment (post-processing)",
        "online social network processing noise (context)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "SegFormer (Mix Transformer, MiT) backbone with MLP decoder",
        "novel_contribution": "Cascaded dual-branch, coarse-to-fine transformer with adaptive feedback from coarse map into early encoder layers"
      },
      {
        "type": "primary",
        "category": "Attention",
        "specific": "Holistic Attention Module (HAM) based feedback injection",
        "novel_contribution": "Uses blurred/enlarged coarse localization map to modulate intermediate encoder features, enhancing positive signals and suppressing interference"
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "Contextual Transformer (CoT) block within CSPM",
        "novel_contribution": "Leverages contextual keys/queries to guide self-attention learning for forensic feature enhancement"
      },
      {
        "type": "primary",
        "category": "CNN",
        "specific": "Dilated convolutions pyramid within CSPM",
        "novel_contribution": "Multi-rate dilated convolutions to capture subtle tampering traces at multiple scales without pooling"
      },
      {
        "type": "primary",
        "category": "Training Strategy",
        "specific": "Progressive two-stage training",
        "novel_contribution": "Train on realistic MBH synthetic samples then hard samples at different scales to improve generalization and robustness"
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "MBH realistic synthetic images (55K)",
        "type": "synthetic",
        "domain": "manipulated_images",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "CASIAv1",
        "type": "public",
        "domain": "manipulated_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CASIAv2",
        "type": "public",
        "domain": "manipulated_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Coverage",
        "type": "public",
        "domain": "manipulated_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "NIST16",
        "type": "public",
        "domain": "manipulated_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "IMD (Image Manipulation Dataset)",
        "type": "public",
        "domain": "manipulated_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CoMoFoD",
        "type": "public",
        "domain": "manipulated_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "AutoSplice",
        "type": "public",
        "domain": "manipulated_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "COCO (for synthetic composition)",
        "type": "public",
        "domain": "natural_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "MFCN (2018)",
        "paper_reference": "Shi et al., 2018",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "RGB-N (2018)",
        "paper_reference": "Bappy et al., 2018",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "EXIF-SC (2018)",
        "paper_reference": "Huh et al., 2018",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "ForSim (2019)",
        "paper_reference": "Bondi et al., 2019",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Noiseprint (2019)",
        "paper_reference": "Cozzolino et al., 2019",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "ManTra (2019)",
        "paper_reference": "Bammey et al., 2019",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "H-LSTM (2019)",
        "paper_reference": "Bi et al., 2019",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "SPAN (2020)",
        "paper_reference": "Hu et al., 2020",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "DFCN (2021)",
        "paper_reference": "Zhuang et al., 2021",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "TransForensics (2021)",
        "paper_reference": "Wang et al., 2021",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "MVSS (2022)",
        "paper_reference": "Dong et al., 2022",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "OSN (2022)",
        "paper_reference": "Moran et al., 2022",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "PSCC (2022)",
        "paper_reference": "Liu et al., 2022",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "ObjectFormer (2022)",
        "paper_reference": "Wu et al., 2022",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "CAT (2022)",
        "paper_reference": "Zhou et al., 2022",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "MSMG (2022)",
        "paper_reference": "Zhang et al., 2022",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "EMT (2023)",
        "paper_reference": "Chen et al., 2023",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "ERMPC (2023)",
        "paper_reference": "Jiang et al., 2023",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "HiFi (2023)",
        "paper_reference": "Zhang et al., 2023",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "TruFor (2023)",
        "paper_reference": "Baldacci et al., 2023",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "TBFormer (2023)",
        "paper_reference": "Zhou et al., 2023",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "IML-ViT (2023)",
        "paper_reference": "Cun et al., 2023",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can adaptive feedback from a coarse localization map to early transformer encoder layers improve detection of complex and subtle tampered regions?",
        "Does a cascaded, coarse-to-fine transformer with contextual spatial pyramid refinement enhance localization accuracy and reliability?",
        "How to automatically generate large-scale forged image samples that are realistic and coherent with real-world forensic scenarios?",
        "Can a progressive, cost-effective two-stage training protocol improve generalization and robustness for image forgery localization?"
      ],
      "gaps_identified": [
        "Existing encoder-decoder forensic networks lack feedback and refinement of intermediate results, relying only on final-output supervision.",
        "Detecting complex and subtle tampered regions requires more feedback information than what straight networks provide.",
        "Current large-scale synthetic training samples are often unrealistic due to naive splicing, leading to a simulation gap and degraded testing performance.",
        "Block-wise inconsistency approaches incur high computational cost at test time.",
        "Pooling-based pyramids may suppress subtle signals relevant to forensics."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Improve the localization of subtle and complex tampered regions via a feedback-enhanced, coarse-to-fine transformer and bridge the training data realism gap with an automatic, realistic synthetic data generation strategy.",
      "potential_research_ideas": [
        "Iterative multi-stage feedback with uncertainty estimation to progressively refine masks and calibrate confidence.",
        "Domain adaptation from synthetic (MBH) to real through self-training or adversarial feature alignment to reduce distribution shift.",
        "Augment feedback by integrating camera-model fingerprints (e.g., noiseprint) or EXIF consistency cues as auxiliary channels.",
        "Use generative diffusion or harmonization models to synthesize even more realistic and diverse manipulations and post-processings.",
        "Joint multi-task learning to predict manipulation types, boundaries, and post-processing attributes alongside masks.",
        "Extend to video forgery localization leveraging temporal consistency and feedback across frames.",
        "Knowledge distillation to lightweight student models for deployment on resource-constrained devices.",
        "Robustness-oriented training against social network recompression and common editing pipelines; evaluate cross-platform robustness."
      ],
      "architectural_improvement_recommendations": [
        "Make the feedback gate learnable and uncertainty-aware to weight HAM modulation based on confidence of coarse maps.",
        "Adopt deformable attention or cross-scale attention within the transformer to better capture boundary artifacts.",
        "Incorporate boundary-aware losses (e.g., boundary F1, Lovasz) or explicit edge decoders to sharpen mask contours.",
        "Replace/augment the MLP decoder with a lightweight convolutional decoder with skip connections for finer spatial detail.",
        "Introduce multi-branch decoders specialized for different manipulation/post-processing types with a routing mechanism.",
        "Pre-train encoder on self-supervised camera-trace tasks (e.g., demosaicing artifacts) to improve forensic signal sensitivity.",
        "Add a CRF-like or transformer-based refinement head at inference to enforce spatial consistency.",
        "Generalize HAM to multi-scale feedback (feed coarse maps to multiple early encoder stages)."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/multimediaFor/ProFact",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Availability of large-scale, realistic, pixel-level annotated training data is limited; synthetic-to-real generalization is challenging.",
        "Robustness to diverse post-processing pipelines (compression, blurring, contrast adjustment) and platform-induced noise.",
        "Potential performance degradation under resolution changes and diverse manipulation scales if not adequately trained."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposed ProFact: a progressive feedback-enhanced transformer network with a coarse-to-fine dual-branch architecture for image forgery localization.",
      "Introduced a realistic synthetic training data generation strategy (MBH: matting, blending, harmonization) for large-scale, coherent manipulations.",
      "Applied a progressive two-stage training protocol and demonstrated superior generalization and robustness on nine public forensic datasets."
    ]
  },
  {
    "arxiv_id": "2312.00508v3",
    "title": "TransURL: Improving malicious URL detection with multi-layer Transformer encoding and multi-scale pyramid features",
    "authors": "Ruitong Liu; Yanbin Wang; Zhenhao Guo; Haitao Xu; Zhan Qin; Wenrui Ma; Fan Zhang",
    "abstract": "Machine learning progress is advancing the detection of malicious URLs. However, advanced Transformers applied to URLs face difficulties in extracting local information, character-level details, and structural relationships. To address these challenges, we propose a novel approach for malicious URL detection, named TransURL. This method is implemented by co-training the character-aware Transformer with three feature modules: Multi-Layer Encoding, Multi-Scale Feature Learning, and Spatial Pyramid Attention. This specialized Transformer enables TransURL to extract embeddings with character-level information from URL token sequences, with the three modules aiding the fusion of multi-layer Transformer encodings and the capture of multi-scale local details and structural relationships. The proposed method is evaluated across several challenging scenarios, including class imbalance learning, multi-classification, cross-dataset testing, and adversarial sample attacks. Experimental results demonstrate a significant improvement compared to previous methods. For instance, it achieved a peak F1-score improvement of 40% in class-imbalanced scenarios and surpassed the best baseline by 14.13% in accuracy for adversarial attack scenarios. Additionally, a case study demonstrated that our method accurately identified all 30 active malicious web pages, whereas two previous state-of-the-art methods missed 4 and 7 malicious web pages, respectively. The codes and data are available at: https://github.com/Vul-det/TransURL/.",
    "published_date": "2023-12-01",
    "pdf_link": "https://arxiv.org/pdf/2312.00508v3",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Web Security",
      "subdomain": "URL/Phishing Detection",
      "specific_problem": "Malicious URL detection (binary and multi-class) with robustness to class imbalance, cross-dataset generalization, and adversarial manipulation",
      "attack_types": [
        "phishing",
        "malware distribution",
        "spam",
        "defacement",
        "adversarial perturbations on URL strings"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "CharBERT-based specialized Transformer",
        "novel_contribution": "Character-aware Transformer backbone co-trained with three feature modules to fuse multi-layer encodings and capture character-level, local, and structural URL patterns"
      },
      {
        "type": "primary",
        "category": "Attention",
        "specific": "Spatial Pyramid Attention",
        "novel_contribution": "Differentially weights regions to emphasize local spatial correlations in URL features beyond standard multi-head attention"
      },
      {
        "type": "primary",
        "category": "CNN",
        "specific": "Multi-Scale Feature Learning (incl. depthwise separable conv per figure DSConv3x3)",
        "novel_contribution": "Multi-scale local feature extraction across fused encoder layers to capture granular URL substructures"
      },
      {
        "type": "primary",
        "category": "RNN",
        "specific": "BiGRU (for character embeddings)",
        "novel_contribution": "Generates contextual character-level embeddings per token within the CharBERT dual-channel architecture"
      },
      {
        "type": "primary",
        "category": "Fusion",
        "specific": "Heterogeneous Interaction Module",
        "novel_contribution": "Fuses and re-separates token and character channels after each Transformer layer; dynamic fusion of multi-layer encoder outputs"
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "URLNet",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Hybrid CNN+RNN+Attention",
        "specific": "GramBeddings",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "BERT fine-tuning",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "URLTran",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Transfer Learning"
    ],
    "datasets": [
      {
        "name": "GramBeddings Dataset",
        "type": "public",
        "domain": "web_urls",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Mendeley Data URL dataset",
        "type": "public",
        "domain": "web_urls",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Kaggle 1 (binary URL classification)",
        "type": "public",
        "domain": "web_urls",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Kaggle 2 (multi-class URL classification: benign/defacement/phishing/malicious)",
        "type": "public",
        "domain": "web_urls",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "URLNet",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "GramBeddings",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "BERT fine-tuning on URLs",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "URLTran (Transformer for phishing URL detection)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Best baseline (unspecified) in adversarial attack scenario",
        "paper_reference": null,
        "metric": "Accuracy under adversarial attack",
        "their_result": "14.13% higher than best baseline",
        "baseline_result": null
      },
      {
        "method_name": "Best baseline (unspecified) in class-imbalanced scenario",
        "paper_reference": null,
        "metric": "F1-score (class-imbalanced setting)",
        "their_result": "Peak F1-score improvement of 40%",
        "baseline_result": null
      },
      {
        "method_name": "Prior SOTA method A (unspecified)",
        "paper_reference": null,
        "metric": "Case study recall on 30 active malicious webpages",
        "their_result": "Detected all 30/30",
        "baseline_result": "Missed 4 (detected 26/30)"
      },
      {
        "method_name": "Prior SOTA method B (unspecified)",
        "paper_reference": null,
        "metric": "Case study recall on 30 active malicious webpages",
        "their_result": "Detected all 30/30",
        "baseline_result": "Missed 7 (detected 23/30)"
      }
    ],
    "performance_metrics_used": [
      "F1-score",
      "Accuracy"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Transformers on URLs struggle to capture character-level information due to token-only inputs",
        "Transformers are less effective than CNNs at extracting local patterns crucial for malicious substructures",
        "Transformers lack mechanisms to model hierarchical URL structure directly",
        "CNN-based methods have plateaued and often rely on manual dual-channel feature engineering",
        "Pretrained language models exhibit domain bias when transferred from text to URLs without architectural adaptation"
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Improve malicious URL detection by enabling Transformer models to capture character-level details, local multi-scale patterns, and structural relationships, and to perform robustly under class imbalance, cross-dataset shifts, and adversarial attacks.",
      "potential_research_ideas": [
        "Pretrain a domain-specific character-aware Transformer on large unlabeled URL corpora using self-supervised objectives tailored to URL structure",
        "Adversarial training on URL strings (e.g., gradient-based or rule-based perturbations) to further boost robustness beyond evaluation-only adversarial tests",
        "Multimodal fusion of lexical URL features with DNS/WHOIS/host-based and certificate metadata for improved generalization",
        "Graph-based modeling of URL components (protocol, host, path, query) with GNNs fused with the Transformer encoder",
        "Contrastive learning across URL augmentations (e.g., TLD swaps, subdomain permutations) to learn invariant representations",
        "Certified robustness methods (randomized smoothing over character edits) for URL classifiers",
        "Federated or privacy-preserving training across organizations to leverage diverse URL distributions without data sharing",
        "Explainability methods to attribute predictions to URL substrings or hierarchical parts for analyst trust and debugging"
      ],
      "architectural_improvement_recommendations": [
        "Replace or augment BiGRU character encoder with lightweight convolutional character encoders to reduce latency",
        "Incorporate efficient Transformer variants (e.g., Performer/Longformer) to scale to very long URLs without quadratic cost",
        "Dynamic routing between token and char channels via gating mechanisms conditioned on URL structure",
        "Integrate learnable hierarchical parsing (protocol/host/path/query) with segment-wise attention and cross-segment relations",
        "Add contrastive pretraining heads on multi-layer fused features to stabilize training and improve transfer",
        "Use mixture-of-experts for different TLDs/languages to handle distribution shifts across datasets"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/Vul-det/TransURL/",
      "frameworks": [],
      "reproducibility_score": "high",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Severe class imbalance in real-world URL streams",
        "Cross-dataset distribution shift across sources and TLD distributions",
        "Adversarial manipulation of URL strings to evade detectors"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": true,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces TransURL, a character-aware Transformer with Multi-Layer Encoding fusion, Multi-Scale Feature Learning, and Spatial Pyramid Attention for malicious URL detection",
      "Achieves SOTA across challenging scenarios: class imbalance, small sample learning, multi-classification, cross-dataset validation, and adversarial sample attacks",
      "First to dynamically fuse multiple encoding layers of a deep Transformer for URL sequences with empirical evidence of gains",
      "Joint training framework combining Transformer with multi-scale convolution and spatial pyramid attention to capture long-range and local structural information",
      "Case study shows perfect detection on 30 active malicious webpages while two prior SOTA methods missed 4 and 7 respectively; reports a peak 40% F1 improvement in class-imbalanced settings and +14.13% accuracy over the best baseline under adversarial attacks"
    ]
  },
  {
    "arxiv_id": "2312.11575v2",
    "title": "Blind-Touch: Homomorphic Encryption-Based Distributed Neural Network Inference for Privacy-Preserving Fingerprint Authentication",
    "authors": "Hyunmin Choi; Simon Woo; Hyoungshick Kim",
    "abstract": "Fingerprint authentication is a popular security mechanism for smartphones and laptops. However, its adoption in web and cloud environments has been limited due to privacy concerns over storing and processing biometric data on servers. This paper introduces Blind-Touch, a novel machine learning-based fingerprint authentication system leveraging homomorphic encryption to address these privacy concerns. Homomorphic encryption allows computations on encrypted data without decrypting. Thus, Blind-Touch can keep fingerprint data encrypted on the server while performing machine learning operations. Blind-Touch combines three strategies to efficiently utilize homomorphic encryption in machine learning: (1) It optimizes the feature vector for a distributed architecture, processing the first fully connected layer (FC-16) in plaintext on the client side and the subsequent layer (FC-1) post-encryption on the server, thereby minimizing encrypted computations; (2) It employs a homomorphic encryption compatible data compression technique capable of handling 8,192 authentication results concurrently; and (3) It utilizes a clustered server architecture to simultaneously process authentication results, thereby enhancing scalability with increasing user numbers. Blind-Touch achieves high accuracy on two benchmark fingerprint datasets, with a 93.6% F1- score for the PolyU dataset and a 98.2% F1-score for the SOKOTO dataset. Moreover, Blind-Touch can match a fingerprint among 5,000 in about 0.65 seconds. With its privacy focused design, high accuracy, and efficiency, Blind-Touch is a promising alternative to conventional fingerprint authentication for web and cloud applications.",
    "published_date": "2023-12-18",
    "pdf_link": "https://arxiv.org/pdf/2312.11575v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Authentication and Access Control",
      "subdomain": "Biometric Authentication",
      "specific_problem": "Privacy-preserving fingerprint authentication in web/cloud using homomorphic encryption with distributed neural network inference",
      "attack_types": [
        "Server-side biometric data leakage",
        "Chosen-plaintext attack (IND-CPA) adversary model"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Siamese CNN",
        "specific": null,
        "novel_contribution": "Distributed inference split: client-side CNN + FC-16 in plaintext; server-side FC-1 and square activation under HE with encrypted feature vectors"
      },
      {
        "type": "primary",
        "category": "Homomorphic Encryption Inference",
        "specific": "CKKS scheme (Microsoft SEAL/SEAL-Python)",
        "novel_contribution": "HE-compatible pipeline including one-hot based ciphertext compression to pack up to 8,192 authentication results in one ciphertext; minimizes encrypted multiplications to two for server-side inference"
      },
      {
        "type": "baseline",
        "category": "CNN-based fingerprint representation",
        "specific": "DeepPrint",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "PolyU fingerprint dataset (Lin and Kumar 2018)",
        "type": "public",
        "domain": "fingerprint_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Sokoto fingerprint dataset (Shehu et al. 2018)",
        "type": "public",
        "domain": "fingerprint_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "DeepPrint",
        "paper_reference": "Engelsma, Cao, and Jain (2019)",
        "metric": "Average authentication/search time for 5,000 fingerprints",
        "their_result": "~0.65 s to identify a match within a pool of 5,000 fingerprints",
        "baseline_result": "~3.4 s (considering only feature vector encryption time)"
      }
    ],
    "performance_metrics_used": [
      "F1-score",
      "average search time (authentication latency)",
      "throughput of results per ciphertext"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can homomorphic encryption enable practical, accurate fingerprint authentication for web/cloud by keeping feature vectors encrypted during server-side inference?",
        "How can we partition a neural fingerprint matcher between client and server to minimize HE computation while maintaining accuracy?",
        "Can HE-compatible compression and clustered servers improve scalability for large user populations?"
      ],
      "gaps_identified": [
        "HE introduces significant computational overhead, especially for CNN layers (convolutions/pooling) under HE.",
        "Encrypted feature vectors are much larger than plaintext; reducing their size can degrade accuracy if not carefully designed.",
        "Prior HE-based fingerprint systems are too slow for real-world use (e.g., 3.4 s per 5,000 in DeepPrint for feature vector encryption).",
        "Existing HE-DNN approaches often do not accommodate CNNs with floating-point parameters needed for biometrics."
      ],
      "limitations": [
        "Authentication time still scales with ceil(N/512) without clustering; large user bases require the proposed cluster architecture.",
        "Feature vector size reduction under HE can lead to accuracy decreases if not carefully optimized (acknowledged challenge).",
        "Sigmoid is offloaded to the client to avoid HE cost; server returns encrypted scores requiring client decryption and thresholding (design trade-off)."
      ],
      "future_work": [],
      "motivation": "Enable privacy-preserving fingerprint authentication suitable for web/cloud by performing neural inference over encrypted features, addressing HE overheads while keeping high recognition performance.",
      "potential_research_ideas": [
        "Integrate polynomial-approximated nonlinearities to move more of the post-FC computation server-side without decryption, reducing client workload.",
        "Neural architecture search with a multi-objective cost (accuracy vs. HE compute/multiplicative depth/slot packing efficiency) to discover better feature dimensions than 16.",
        "Hybrid privacy approaches combining HE with trusted execution environments (TEE) or secure MPC to reduce multiplications and rotations while preserving privacy.",
        "Design LSH or coarse encrypted indexing to prefilter candidate sets before the HE FC-1, lowering complexity for very large N.",
        "Template protection extensions (revocability/cancelability) layered on the encrypted features to address compromise scenarios and re-issuance.",
        "Adopt GPU-accelerated HE libraries and batching strategies to further cut latency; evaluate vectorized CKKS parameters and bootstrapping trade-offs.",
        "Extend to multimodal biometrics (face+fingerprint) with encrypted score-level fusion under HE.",
        "Formal leakage analysis of side-channel vectors (timing, ciphertext size patterns) and defenses in the proposed pipeline."
      ],
      "architectural_improvement_recommendations": [
        "Replace square + client-side sigmoid with higher-order polynomial sigmoid approximation under HE to keep the entire scoring pipeline encrypted on the server when feasible.",
        "Introduce encrypted approximate nearest neighbor search (e.g., encrypted cosine or Hamming distance via binarized embeddings) to reduce server multiplications.",
        "Adopt binarized or quantized embeddings trained with HE-aware loss to improve slot packing (e.g., 8-bit or 1-bit) and reduce ciphertext size.",
        "Use scheme/parameters tuning (e.g., CKKS precision levels, rescaling strategy, minimal rotations) and layout-aware packing to cut multiplicative depth and memory.",
        "Add hierarchical clustering or sharding-aware indexing across clusters to reduce per-cluster candidate set and network overhead.",
        "Implement precomputation of rotated Cr variants to amortize rotation cost during high QPS periods."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/hm-choi/blind-touch",
      "frameworks": [
        "SEAL-Python",
        "Microsoft SEAL"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Reported average search time ~0.65 s for matching among 5,000. CKKS via SEAL-Python; example parameters mention poly modulus degree d=16,384 and log scale factor 40. Ciphertext size examples (KB) by multiplicative depth: depth 1: 459, 2: 658, 3: 855, 4: 1,075, 5: 1,280. Specific training hardware not stated."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Client devices with fingerprint scanners (plaintext CNN + encryption) and web/cloud servers (HE inference), with optional clustered servers",
      "scalability_discussed": true,
      "inference_time": "~0.65 seconds to search among 5,000 fingerprints",
      "deployment_challenges": [
        "HE computational overhead (multiplications, rotations, relinearization) and ciphertext size growth with depth.",
        "Network overhead due to large ciphertexts; need for compression to reduce returned ciphertexts.",
        "Scalability with user count (ceil(N/512) without clusters), motivating clustered architecture.",
        "Key management and secure distribution of public/evaluation keys to servers and secret keys on clients."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": true,
      "fairness_bias": false
    },
    "contributions": [
      "Blind-Touch: a practical distributed HE-based fingerprint authentication system enabling efficient encrypted neural inference.",
      "Three efficiency strategies for HE: (1) distributed architecture with client-side FC-16 and server-side FC-1 to minimize encrypted computation; (2) HE-compatible one-hot compression to handle up to 8,192 authentication results per ciphertext; (3) clustered server architecture for parallel processing and scalability.",
      "High performance: 93.6% F1-score on PolyU and 98.2% F1-score on Sokoto; average search time ~650 ms among 5,000 fingerprints.",
      "Formal security analysis showing no PPT adversary can extract information about encrypted fingerprint features under IND-CPA."
    ]
  },
  {
    "arxiv_id": "2312.00483v2",
    "title": "MalDicom: A Memory Forensic Framework for Detecting Malicious Payload in DICOM Files",
    "authors": "Ayushi Mishra; Priyanka Bagade",
    "abstract": "Digital Imaging and Communication System (DICOM) is widely used throughout the public health sector for portability in medical imaging. However, these DICOM files have vulnerabilities present in the preamble section. Successful exploitation of these vulnerabilities can allow attackers to embed executable codes in the 128-Byte preamble of DICOM files. Embedding the malicious executable will not interfere with the readability or functionality of DICOM imagery. However, it will affect the underline system silently upon viewing these files. This paper shows the infiltration of Windows malware executables into DICOM files. On viewing the files, the malicious DICOM will get executed and eventually infect the entire hospital network through the radiologist's workstation. The code injection process of executing malware in DICOM files affects the hospital networks and workstations' memory. Memory forensics for the infected radiologist's workstation is crucial as it can detect which malware disrupts the hospital environment, and future detection methods can be deployed. In this paper, we consider the machine learning (ML) algorithms to conduct memory forensics on three memory dump categories: Trojan, Spyware, and Ransomware, taken from the CIC-MalMem-2022 dataset. We obtain the highest accuracy of 75% with the Random Forest model. For estimating the feature importance for ML model prediction, we leveraged the concept of Shapley values.",
    "published_date": "2023-12-01",
    "pdf_link": "https://arxiv.org/pdf/2312.00483v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Healthcare Security",
      "subdomain": "Medical Imaging and PACS Security",
      "specific_problem": "Code injection into DICOM preamble to embed malware and post-incident malware classification via memory forensics on radiologist/PACS workstations",
      "attack_types": [
        "Code Injection",
        "Man-in-the-Middle (MITM)",
        "Memory-resident malware",
        "Ransomware",
        "Trojan",
        "Spyware"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Ensemble (Tree-based)",
        "specific": "Random Forest",
        "novel_contribution": "Applied as the top-performing classifier for multiclass malware family detection from memory forensic features; reported highest accuracy of 75%"
      },
      {
        "type": "primary",
        "category": "Explainability/Attribution",
        "specific": "Shapley values (SHAP)",
        "novel_contribution": "Used to estimate feature importance and provide mathematical explainability for malware classification decisions"
      },
      {
        "type": "primary",
        "category": "Sampling/Imbalanced Learning",
        "specific": "SMOTE",
        "novel_contribution": "Used to oversample minority classes in unbalanced malware/benign family distributions"
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "CIC-MalMem-2022",
        "type": "public",
        "domain": "memory_dumps",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "TCIA Lung CT DICOM samples (The Cancer Imaging Archive)",
        "type": "public",
        "domain": "medical_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Accuracy"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can DICOM vulnerabilities be exploited by implementing the code injection method to infiltrate malware in medical images and detect it using memory forensic technique?",
        "How can a code injection attack affect the DICOM viewer?"
      ],
      "gaps_identified": [
        "Limited research on DICOM security, with exploitable vulnerability in the 128-Byte preamble",
        "Encryption of DICOM files is rarely implemented in practice due to interoperability, performance, and collaboration challenges",
        "Prior DICOM attacks modified file content; code injection that preserves readability/functionality is largely undetectable by current methods",
        "Memory forensics research has been mostly restricted to computer systems; limited focus on medical devices/MIoT and PACS environments"
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Demonstrate a practical code-injection attack vector on DICOM files via the preamble and provide a memory forensics framework (MalDicom) using ML and SHAP to identify malware families impacting hospital networks.",
      "potential_research_ideas": [
        "Construct a dedicated dataset of real memory dumps collected from controlled DICOM code-injection executions across diverse DICOM viewers, OS versions, and PACS vendors to bridge the sim-to-real gap.",
        "Develop DICOM-aware static scanners that validate and sanitize the 128-Byte preamble and private tags, detecting anomalous PE structures and pointer inconsistencies (e.g., elfanew checks) before viewing.",
        "Design real-time memory sensors for PACS/viewer hosts (eBPF/ETW-based) to detect injected code regions and reflective loading, correlating with volatility-style artifacts.",
        "Explore self-supervised or contrastive representation learning on raw memory images and volatility artifacts to improve generalization across malware families and obfuscation.",
        "Integrate multi-modal detection by fusing network PCAP from MITM paths with host memory and process telemetry to catch staged DICOM payload delivery and execution.",
        "Evaluate robustness against packing/obfuscation and adversarial ML (evasion) specifically tailored to memory features and volatility outputs."
      ],
      "architectural_improvement_recommendations": [
        "Evaluate stronger tabular learners (XGBoost/LightGBM, CatBoost) and well-regularized linear/SVM baselines with thorough hyperparameter tuning and calibration; compare against RF.",
        "Improve class imbalance handling with SMOTE variants (SMOTE-Tomek, SMOTE-ENN) or cost-sensitive learning and focal loss where applicable.",
        "Engineer temporal/relational features: sequences of volatility artifacts, process-DLL-handle graphs, and cross-artifact consistency checks; consider GNNs over process graphs.",
        "Harden evaluation with family-wise stratified CV, group splits by malware family, and leakage controls; report per-class F1, macro-F1, and calibration metrics.",
        "Use SHAP interaction values and global surrogate models to capture cross-feature interactions (e.g., malfind with ldrmodules anomalies).",
        "Prototype an inline pre-viewer gatekeeper service that validates DICOM structure (preamble, tags) and quarantines suspicious files before viewer execution."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Hospital network lab setup with PACS server and Windows modality/radiologist workstation; MITM via Ettercap/Wireshark; memory dumps analyzed post-infection",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Obtaining memory dumps from production radiologist workstations is intrusive and time-sensitive; requires system seizure to avoid evidence tampering.",
        "Legacy Windows-based modality/viewer systems may harbor unpatched vulnerabilities; heterogeneous environments complicate standardized defenses.",
        "If DICOM encryption/signing is enabled, MITM interception and manipulation becomes harder; operational trade-offs exist.",
        "Integration with PACS workflows and change management in clinical environments can be challenging."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Novel attack on DICOM files by inserting a malicious payload using the code injection attack into the 128-Byte preamble.",
      "Demonstrated MITM interception of DICOM in transit, modification, and re-insertion into the hospital network without breaking DICOM readability.",
      "Development of a memory forensic framework (MalDicom) to analyze memory dumps from infected machines and classify malware families.",
      "Use of machine learning algorithms on Volatility-derived features for multiclass malware detection from memory dumps.",
      "Use of Shapley values for explainability of ML predictions, identifying high-impact features (e.g., malfindCommitCharge).",
      "Reported result: “We obtain the highest accuracy of 75% with the Random Forest model.”"
    ]
  },
  {
    "arxiv_id": "2311.16940v1",
    "title": "FP-Fed: Privacy-Preserving Federated Detection of Browser Fingerprinting",
    "authors": "Meenatchi Sundaram Muthu Selva Annamalai; Igor Bilogrevic; Emiliano De Cristofaro",
    "abstract": "Browser fingerprinting often provides an attractive alternative to third-party cookies for tracking users across the web. In fact, the increasing restrictions on third-party cookies placed by common web browsers and recent regulations like the GDPR may accelerate the transition. To counter browser fingerprinting, previous work proposed several techniques to detect its prevalence and severity. However, these rely on 1) centralized web crawls and/or 2) computationally intensive operations to extract and process signals (e.g., information-flow and static analysis). To address these limitations, we present FP-Fed, the first distributed system for browser fingerprinting detection. Using FP-Fed, users can collaboratively train on-device models based on their real browsing patterns, without sharing their training data with a central entity, by relying on Differentially Private Federated Learning (DP-FL). To demonstrate its feasibility and effectiveness, we evaluate FP-Fed's performance on a set of 18.3k popular websites with different privacy levels, numbers of participants, and features extracted from the scripts. Our experiments show that FP-Fed achieves reasonably high detection performance and can perform both training and inference efficiently, on-device, by only relying on runtime signals extracted from the execution trace, without requiring any resource-intensive operation.",
    "published_date": "2023-11-28",
    "pdf_link": "https://arxiv.org/pdf/2311.16940v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Web Privacy",
      "subdomain": "Tracking Detection",
      "specific_problem": "Browser fingerprinting detection in the wild with privacy-preserving federated learning",
      "attack_types": [
        "Browser fingerprinting",
        "Canvas fingerprinting",
        "Canvas font fingerprinting",
        "WebRTC fingerprinting",
        "AudioContext fingerprinting"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Federated Learning",
        "specific": "DP-FedAvg (Central Differential Privacy, participant-level DP)",
        "novel_contribution": "First distributed system for browser fingerprinting detection using differentially private federated learning; on-device training/inference using only runtime signals"
      },
      {
        "type": "primary",
        "category": "Feature Engineering",
        "specific": "Dynamic/runtime feature extraction from JS API execution traces",
        "novel_contribution": "Shows that a compact feature set (149 dynamic features) suffices for DP-FL fingerprinting detection, avoiding resource-intensive static/flow analyses"
      },
      {
        "type": "baseline",
        "category": "Centralized Training",
        "specific": "Fully centralized supervised classifier (architecture unspecified)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Local-only Training",
        "specific": "Each client trains only on its local data (no federation)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Federated Learning",
      "Differentially Private Learning"
    ],
    "datasets": [
      {
        "name": "18.3k popular websites crawl (Puppeteer/Chrome M114) execution traces",
        "type": "private",
        "domain": "web_script_execution_traces",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Small-scale study on top 300 Tranco domains (user vs automated crawls)",
        "type": "private",
        "domain": "web_script_execution_traces",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Tranco ranking list",
        "type": "public",
        "domain": "website_rankings",
        "link": "https://tranco-list.eu",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Fully centralized training",
        "paper_reference": null,
        "metric": "AUPRC",
        "their_result": "“With ε = 10, FP-Fed achieves a comparable performance to a fully centralized approach (0.95 vs. 0.97 AUPRC)”",
        "baseline_result": "0.97 AUPRC (centralized)"
      },
      {
        "method_name": "Local-only client training",
        "paper_reference": null,
        "metric": "AUPRC",
        "their_result": "0.95 AUPRC (FP-Fed with ε = 10) or 0.86 AUPRC (ε = 1, 1M participants)",
        "baseline_result": "“each client only training on their local dataset (0.78 AUPRC)”"
      }
    ],
    "performance_metrics_used": [
      "AUPRC (Area Under Precision-Recall Curve)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can browser fingerprinting be detected effectively using only runtime signals in a privacy-preserving, on-device federated learning setting?",
        "What is the privacy-utility tradeoff when applying participant-level central differential privacy in FL for this task?",
        "How do the number of participants, privacy level (ε), and feature sets affect detection performance and feasibility?",
        "Is a compact set of dynamic features sufficient to achieve high performance without resource-intensive static/flow analyses?"
      ],
      "gaps_identified": [
        "Centralized web crawls cannot replicate human-like browsing (logins, paywalls, CAPTCHAs) and are often blocked by bot detectors.",
        "Script behavior may vary across device/OS; centralized crawls cannot feasibly cover the diversity.",
        "Prior detection techniques rely on computationally intensive static or information-flow analyses unsuitable for on-device deployment.",
        "Static-feature-based models are not robust to obfuscation.",
        "Collecting centralized user browsing data raises privacy concerns; need for training without sharing raw data."
      ],
      "limitations": [
        "Fingerprinting ground truth relies on a conservative heuristic (FP-Inspector’s signatures), which may miss some fingerprinting scripts (potential false negatives).",
        "Data used in experiments are collected via a web crawl and do not fully represent distributed real-user browsing behavior.",
        "The approach relies on central differential privacy with a trusted server for aggregating and noising updates (secure aggregation not implemented).",
        "Evaluation focuses on Chrome/Puppeteer; cross-browser generality is not demonstrated in the reported text."
      ],
      "future_work": [
        "Periodically repeat data collection and FL training to track evolving fingerprinting techniques.",
        "Explore broader instrumentation of JavaScript APIs and evolving feature sets as the web changes."
      ],
      "motivation": "Provide a privacy-preserving, scalable alternative to centralized and resource-intensive fingerprinting detectors, enabling collaborative learning from real browsing patterns without sharing raw data.",
      "potential_research_ideas": [
        "Integrate secure aggregation (e.g., homomorphic encryption or MPC) to achieve distributed DP while maintaining utility.",
        "Develop personalized FL (e.g., FedProx, pFedMe, meta-learning) to handle non-IID browsing behavior across users and devices.",
        "Semi-supervised or weakly supervised FL to mitigate labeling gaps from conservative heuristics and leverage unlabeled traces.",
        "Design lightweight sequence models over API call traces (e.g., compact RNN/Transformer) under DP constraints to capture temporal patterns.",
        "Online/continual learning with DP to handle concept drift in fingerprinting behaviors.",
        "Cross-browser and cross-platform generalization studies, including Safari and Firefox, and OS diversity-aware modeling.",
        "Adversarial robustness evaluation against evasive scripts that mimic benign API usage or manipulate argument patterns.",
        "Explainability methods tailored to API-trace features to support analyst validation and policy enforcement."
      ],
      "architectural_improvement_recommendations": [
        "Add secure aggregation to the DP-FL pipeline to prevent server access to individual updates while retaining central DP noise levels.",
        "Adopt adaptive clipping and per-round privacy accounting to optimize privacy-utility tradeoffs.",
        "Introduce personalized FL variants (FedProx, pFedAvg) to mitigate client drift and heterogeneity.",
        "Incorporate feature hashing or learned embeddings for high-cardinality API arguments to improve scalability and privacy.",
        "Augment the classifier with lightweight sequence encoders over ordered API call traces to capture temporal dependencies.",
        "Use federated feature selection and sparsity-inducing regularization to maintain a compact feature set.",
        "Implement drift detection and scheduled model refresh to handle changing web scripts."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "Puppeteer"
      ],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "On-device resource constraints and heterogeneous client capabilities.",
        "Complexity of secure aggregation protocols at scale and their practical pitfalls.",
        "Balancing DP noise with utility under participant-level guarantees.",
        "Need to instrument browser APIs reliably without causing website breakage.",
        "Model and data drift due to changing web content and scripts.",
        "Potential label noise or coverage gaps from conservative heuristics."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": true,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces FP-Fed, the first distributed system for detecting browser fingerprinting using differentially private federated learning.",
      "Demonstrates on-device training and inference using only runtime signals from execution traces (no resource-intensive static or information-flow analyses).",
      "Evaluates on a dataset of 18.3k popular websites; shows high performance with participant-level central DP (e.g., 0.86 AUPRC at ε=1 with 1M participants).",
      "Shows FP-Fed with ε=10 achieves performance comparable to centralized training (0.95 vs. 0.97 AUPRC) and outperforms local-only training (0.78 AUPRC).",
      "Finds that a compact set of 149 dynamic features suffices for effective detection in a DP-FL setting.",
      "Analyzes tradeoffs across privacy levels, number of participants, and feature sets, highlighting deployment feasibility."
    ]
  },
  {
    "arxiv_id": "2312.17683v2",
    "title": "Malware Detection in IOT Systems Using Machine Learning Techniques",
    "authors": "Ali Mehrban; Pegah Ahadian",
    "abstract": "Malware detection in IoT environments necessitates robust methodologies. This study introduces a CNN-LSTM hybrid model for IoT malware identification and evaluates its performance against established methods. Leveraging K-fold cross-validation, the proposed approach achieved 95.5% accuracy, surpassing existing methods. The CNN algorithm enabled superior learning model construction, and the LSTM classifier exhibited heightened accuracy in classification. Comparative analysis against prevalent techniques demonstrated the efficacy of the proposed model, highlighting its potential for enhancing IoT security. The study advocates for future exploration of SVMs as alternatives, emphasizes the need for distributed detection strategies, and underscores the importance of predictive analyses for a more powerful IOT security. This research serves as a platform for developing more resilient security measures in IoT ecosystems.",
    "published_date": "2023-12-29",
    "pdf_link": "https://arxiv.org/pdf/2312.17683v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "IoT Security",
      "subdomain": "Malware/Intrusion Detection",
      "specific_problem": "Detecting IoT malware/intrusions from network traffic using a hybrid CNN-LSTM model with dimensionality reduction and feature selection",
      "attack_types": [
        "DDoS",
        "DoS",
        "OS and Service Scan",
        "Keylogging",
        "Data Exfiltration",
        "Worms",
        "Backdoors",
        "Fuzzers"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Hybrid (CNN+LSTM)",
        "specific": null,
        "novel_contribution": "Hybrid pipeline: SVD dimensionality reduction, CNN-based feature selection/representation, and LSTM classifier for IoT malware/intrusion detection with K-fold cross-validation"
      },
      {
        "type": "primary",
        "category": "CNN",
        "specific": null,
        "novel_contribution": "Used for feature selection/ranking and representation learning prior to classification"
      },
      {
        "type": "primary",
        "category": "RNN (LSTM)",
        "specific": "LSTM",
        "novel_contribution": "Used as the final classifier to capture temporal dependencies in features/sequences"
      },
      {
        "type": "primary",
        "category": "Dimensionality Reduction",
        "specific": "SVD (Singular Value Decomposition)",
        "novel_contribution": "Applied to reduce feature dimensionality before CNN/LSTM modeling"
      },
      {
        "type": "primary",
        "category": "Feature Selection",
        "specific": "Chi-Squared",
        "novel_contribution": "Statistical filter-based selection noted in methodology description (in addition to CNN-based selection)"
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "ResNet-50",
        "novel_contribution": "Used in prior work [19] as a comparator in the paper’s accuracy comparison"
      },
      {
        "type": "baseline",
        "category": "DNN",
        "specific": null,
        "novel_contribution": "Deep learning model from Ren et al. [21] used as a comparator in the paper’s accuracy comparison"
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "UNSW-NB15",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "BoT-IoT",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CSE-CIC-IDS2018",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "BaIoT-N (N-BaIoT)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "IoTPOT malware binaries (Mirai, Bashlite)",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "VirusShare APKs",
        "type": "proprietary",
        "domain": "mobile_apks",
        "link": null,
        "is_new_contribution": false,
        "availability": "available_on_request"
      },
      {
        "name": "Google Play APKs (benign)",
        "type": "public",
        "domain": "mobile_apks",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Dataset by E. Cozzi et al. (Linux IoT malware)",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "available_on_request"
      },
      {
        "name": "Mobile Leopard dataset",
        "type": "public",
        "domain": "malware_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "ResNet-50 (from Bandiyab et al. [19])",
        "paper_reference": "[19]",
        "metric": "Accuracy",
        "their_result": "95.5%",
        "baseline_result": "94.5%"
      },
      {
        "method_name": "Deep learning model (Ren et al. [21])",
        "paper_reference": "[21]",
        "metric": "Accuracy",
        "their_result": "95.5%",
        "baseline_result": "93.4%"
      }
    ],
    "performance_metrics_used": [
      "False Positive Rate (FPR)",
      "False Rejection Rate (FRR)",
      "Accuracy",
      "Precision",
      "Recall",
      "F1-score"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Limited research on static analysis specifically for IoT malware despite extensive work on IoT security",
        "Non-graph-based methods can lose accuracy under customization/obfuscation",
        "Graph-based methods can better capture control flow but are complex and resource-intensive",
        "Need for lightweight graph-based detection suitable for IoT constraints",
        "Centralized vs distributed detection strategies have performance trade-offs in IoT contexts"
      ],
      "limitations": [],
      "future_work": [
        "Explore SVMs as alternative classifiers",
        "Develop distributed detection strategies for IoT environments",
        "Emphasize predictive analyses for stronger IoT security",
        "Design a lightweight graph-based detection method for IoT malware"
      ],
      "motivation": "Rising IoT-targeted attacks and malware (e.g., Mirai) necessitate robust, accurate, and efficient detection methods for IoT ecosystems.",
      "potential_research_ideas": [
        "Graph-based IoT malware detection using control-flow graphs and graph neural networks tailored to embedded binaries",
        "Federated or hierarchical distributed intrusion/malware detection across edge gateways and cloud for privacy and scalability",
        "Domain adaptation and cross-dataset generalization from enterprise IDS datasets to IoT-specific traffic",
        "Lightweight on-device detection with model compression/distillation for constrained IoT nodes",
        "Self-supervised pretraining on large unlabeled IoT traffic to improve downstream supervised detection",
        "Explainable detection to attribute indicators (flows, features, opcodes) for analyst trust and response",
        "Adversarially robust training against traffic manipulation and evasion (e.g., GAN-based adversaries)"
      ],
      "architectural_improvement_recommendations": [
        "Replace/augment LSTM with Temporal CNN or lightweight Transformer encoders for sequence modeling efficiency",
        "Use unified 1D-CNN feature extractor over raw flow sequences rather than ad hoc feature selection, with learnable attention pooling",
        "Calibrate the classifier (e.g., temperature scaling) and thresholding per attack class; report ROC/PR curves",
        "Adopt proper feature selection (mutual information, Boruta, L1) and compare against CNN-based ranking to avoid selection bias",
        "Handle class imbalance via focal loss or cost-sensitive learning; add per-class metrics",
        "Perform cross-dataset evaluation (train on BoT-IoT, test on CSE-CIC-IDS2018/UNSW-NB15) to measure generalization",
        "Incorporate unsupervised pretraining (autoencoders/contrastive) and semi-supervised fine-tuning to leverage unlabeled traffic",
        "Model compression (quantization/pruning) for edge deployment and latency constraints"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Resource constraints and heterogeneity of IoT devices (MIPS, ARM, PowerPC, Sparc)",
        "Centralized vs distributed detection trade-offs in performance and deployment",
        "Potential sparsity/limited network traffic for some devices reduces detection capability",
        "Need for lightweight methods for on-device or edge deployment"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces a CNN-LSTM hybrid model for IoT malware/intrusion detection",
      "Implements dimensionality reduction (SVD) and CNN-based feature selection prior to classification",
      "Uses K-fold cross-validation and reports average performance across runs",
      "Evaluates on BoT-IoT and CSE-CIC-IDS2018 (and references UNSW-NB15) with multiple metrics",
      "Reports 95.5% accuracy, exceeding baselines [21] (93.4%) and [19] (94.5%)",
      "Advocates future exploration of SVMs, distributed detection strategies, predictive analyses, and lightweight graph-based detection"
    ]
  },
  {
    "arxiv_id": "2311.07760v1",
    "title": "Ransomware Detection Using Federated Learning with Imbalanced Datasets",
    "authors": "Aldin Vehabovic; Hadi Zanddizari; Nasir Ghani; G. Javidi; S. Uluagac; M. Rahouti; E. Bou-Harb; M. Safaei Pour",
    "abstract": "Ransomware is a type of malware which encrypts user data and extorts payments in return for the decryption keys. This cyberthreat is one of the most serious challenges facing organizations today and has already caused immense financial damage. As a result, many researchers have been developing techniques to counter ransomware. Recently, the federated learning (FL) approach has also been applied for ransomware analysis, allowing corporations to achieve scalable, effective detection and attribution without having to share their private data. However, in reality there is much variation in the quantity and composition of ransomware data collected across multiple FL client sites/regions. This imbalance will inevitably degrade the effectiveness of any defense mechanisms. To address this concern, a modified FL scheme is proposed using a weighted cross-entropy loss function approach to mitigate dataset imbalance. A detailed performance evaluation study is then presented for the case of static analysis using the latest Windows-based ransomware families. The findings confirm improved ML classifier performance for a highly imbalanced dataset.",
    "published_date": "2023-11-13",
    "pdf_link": "https://arxiv.org/pdf/2311.07760v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Malware Analysis",
      "subdomain": "Ransomware Detection and Attribution",
      "specific_problem": "Federated learning for Windows ransomware detection and family attribution under imbalanced client datasets (static PE analysis)",
      "attack_types": [
        "Ransomware (Babuk/Babyk)",
        "Ransomware (BlackCat/ALPHV)",
        "Ransomware (Chaos)",
        "Ransomware (DJVu/STOP)",
        "Ransomware (Hive)",
        "Ransomware (LockBit)",
        "Ransomware (Netwalker)",
        "Ransomware (Sodinokibi/REvil)",
        "Ransomware (WannaCry)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Federated Learning",
        "specific": "FedAvg",
        "novel_contribution": "Integrates inverse class-frequency weighted cross-entropy at client-side training within the FL loop to mitigate inter-client data imbalance."
      },
      {
        "type": "primary",
        "category": "MLP/Feedforward Neural Network",
        "specific": "FNN (tabular PE features)",
        "novel_contribution": "Used as the client and global model for both binary detection and multi-class attribution in the FL setting."
      },
      {
        "type": "primary",
        "category": "Loss Function",
        "specific": "Weighted cross-entropy (inverse class frequency per client)",
        "novel_contribution": "Weights α_j computed from inverse of per-class counts at each client site to upweight minority classes during local training."
      },
      {
        "type": "baseline",
        "category": "Loss Function",
        "specific": "Standard cross-entropy",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Empirical Windows PE ransomware/benign dataset (this study; 9 families + benign)",
        "type": "private",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "MalwareBazaar",
        "type": "public",
        "domain": "malware_binaries",
        "link": "https://bazaar.abuse.ch/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Triage (Hatching Triage malware sandbox/repository)",
        "type": "public",
        "domain": "malware_binaries",
        "link": "https://tria.ge/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "VirusShare",
        "type": "public",
        "domain": "malware_binaries",
        "link": "https://virusshare.com/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "VirusTotal",
        "type": "public",
        "domain": "malware_binaries",
        "link": "https://www.virustotal.com/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "FedAvg + standard cross-entropy (balanced dataset)",
        "paper_reference": null,
        "metric": "Binary detection (Global Accuracy)",
        "their_result": "Proposed (weighted CE, imbalanced): 94.67%",
        "baseline_result": "95.08%"
      },
      {
        "method_name": "FedAvg + standard cross-entropy (imbalanced dataset)",
        "paper_reference": null,
        "metric": "Binary detection (Global Accuracy)",
        "their_result": "Proposed (weighted CE, imbalanced): 94.67%",
        "baseline_result": "93.86%"
      },
      {
        "method_name": "FedAvg + standard cross-entropy (balanced dataset)",
        "paper_reference": null,
        "metric": "Multi-class attribution (Global Accuracy)",
        "their_result": "Proposed (weighted CE, imbalanced): 84.15%",
        "baseline_result": "92.11%"
      },
      {
        "method_name": "FedAvg + standard cross-entropy (imbalanced dataset)",
        "paper_reference": null,
        "metric": "Multi-class attribution (Global Accuracy)",
        "their_result": "Proposed (weighted CE, imbalanced): 84.15%",
        "baseline_result": "82.55%"
      },
      {
        "method_name": "FedAvg + standard cross-entropy (imbalanced dataset)",
        "paper_reference": null,
        "metric": "Multi-class attribution (Global F1)",
        "their_result": "Proposed (weighted CE, imbalanced): 82.35%",
        "baseline_result": "82.80%"
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "Precision",
      "Recall",
      "F1 Score"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How does inter-client dataset imbalance in federated learning affect ransomware detection and family attribution performance?",
        "Can client-side inverse class-frequency weighted cross-entropy mitigate performance degradation under imbalanced federated datasets?"
      ],
      "gaps_identified": [
        "Real-world FL settings exhibit uneven class distributions across client sites; most prior FL ransomware studies assume balanced data.",
        "Centralized collection/training is hindered by privacy and scalability concerns.",
        "Many existing ransomware datasets use older families (Windows 7/8 era), necessitating up-to-date repositories for Windows 10/11.",
        "Quote: \"this initial study assumes even data distribution, with local sites having the same number of samples of each family. Clearly, ideal balanced scenarios will rarely exist in practice.\""
      ],
      "limitations": [
        "Evaluation limited to static analysis of Windows PE features (15 features), not dynamic host/network behaviors.",
        "Only feedforward neural networks are evaluated; no comparison with other classifiers (e.g., gradient boosting, CNNs over bytes, transformers for tabular).",
        "FL setup uses K=3 client sites; scalability to many clients and heterogeneous devices not empirically tested.",
        "Per-client weighting uses simple inverse class frequency; no exploration of other imbalance-aware losses (e.g., focal loss, class-balanced loss, LDAM).",
        "Dataset size is modest (140 samples per ransomware family; 2,000 benign), which may limit generalization.",
        "No code or dataset release details provided.",
        "No privacy-preserving mechanisms beyond basic FL (e.g., secure aggregation, differential privacy) are implemented or evaluated.",
        "No evaluation of robustness to distribution shift or adversarial manipulation."
      ],
      "future_work": [
        "Quote: \"these findings indicate a further need to develop new schemes for imbalanced scenarios.\"",
        "Expand to dynamic analysis (network- and host-based telemetry) and multi-modal fusion with static PE features.",
        "Study non-IID settings beyond class imbalance, including client personalization and distribution shift.",
        "Evaluate additional imbalance mitigation techniques (e.g., focal loss, logit adjustment, class-balanced reweighting) within FL.",
        "Scale experiments to more clients and varying client participation rates; explore stratified client selection.",
        "Incorporate privacy enhancements (secure aggregation, differential privacy) and study their accuracy-privacy tradeoffs under imbalance."
      ],
      "motivation": "Enable privacy-preserving, scalable ransomware detection and attribution via federated learning while addressing realistic inter-client dataset imbalance that degrades classifier performance.",
      "potential_research_ideas": [
        "Federated class-balanced loss: combine effective number of samples-based weights with FedAvg for better minority-class recall under severe imbalance.",
        "Personalized FL for ransomware (e.g., FedPer, pFedMe): client-specific heads to capture local family prevalence while sharing a global backbone.",
        "Adaptive client sampling: prioritize clients containing minorities (estimated privately) to rebalance aggregated gradients without sharing raw distributions.",
        "Federated logit adjustment or LDAM-DRW: apply margin-based or logit-shift methods to counter long-tailed class distributions across clients.",
        "Hybrid data-level augmentation with client-local generative models (e.g., GAN/VAE) and privacy-preserving noise to boost minority families.",
        "Multi-modal FL: fuse static PE features with dynamic API call sequences and network telemetry using late fusion across clients.",
        "Self-supervised pretraining on raw PE bytes or embeddings, followed by supervised fine-tuning in FL to improve data efficiency.",
        "Fairness-aware FL for security: enforce per-class or per-client performance constraints to avoid neglecting minority ransomware families.",
        "Robust FL aggregation (FedProx, FedNova, Scaffold) to stabilize training under non-IID and imbalanced conditions.",
        "Calibration-aware training: improve calibrated probabilities to reduce overconfidence on minority classes in attribution."
      ],
      "architectural_improvement_recommendations": [
        "Replace/augment FNN with stronger tabular models (e.g., XGBoost, TabNet) and evaluate within FL via model-agnostic aggregation.",
        "Adopt focal loss or class-balanced loss alongside inverse frequency weighting; evaluate LDAM-DRW for attribution.",
        "Use stratified client participation and per-class weighted FedAvg that accounts for minority presence at selected clients.",
        "Introduce personalization layers (shared backbone + client-specific classification heads) to handle non-IID and imbalance jointly.",
        "Incorporate secure aggregation and optional differential privacy to strengthen privacy guarantees without leaking class distributions.",
        "Explore representation learning on raw PE bytes (1D CNN/Transformer) to capture richer signals than handcrafted PE features."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "Keras",
        "TensorFlow",
        "Pandas",
        "scikit-learn"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Inter-client data imbalance and non-IID distributions bias global model training.",
        "Privacy constraints prevent data sharing across organizations, limiting rebalancing options.",
        "Limited availability of up-to-date ransomware samples reduces training data for emerging families.",
        "Potential performance drop for multi-class attribution under severe imbalance."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": true,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes a federated learning scheme for ransomware analysis that integrates inverse class-frequency weighted cross-entropy at client sites to handle imbalanced datasets.",
      "Constructs and evaluates on an up-to-date Windows ransomware repository covering 9 families plus a benign class using static PE features (15 features).",
      "Provides empirical results for both binary detection and multi-class attribution under balanced vs. imbalanced client splits with K=3 clients.",
      "Quantifies that weighted cross-entropy recovers much of the accuracy lost to imbalance in binary detection: \"yielding an average accuracy within 0.5% of that with the balanced dataset\" (Global: 94.67% vs. 95.08%).",
      "Shows multi-class attribution remains challenging under imbalance, with \"average accuracy and F1 scores are 8-10% lower than the perfectly balanced scenarios\", while weighted loss improves accuracy over unweighted imbalance (84.15% vs. 82.55%).",
      "Highlights the practical gap between idealized balanced FL assumptions and real-world imbalanced client datasets, motivating further research: \"These findings indicate a further need to develop new schemes for imbalanced scenarios.\""
    ]
  },
  {
    "arxiv_id": "2312.00024v5",
    "title": "Can LLMs Patch Security Issues?",
    "authors": "Kamel Alrashedy; Abdullah Aljasser; Pradyumna Tambwekar; Matthew Gombolay",
    "abstract": "Large Language Models (LLMs) have shown impressive proficiency in code generation. Unfortunately, these models share a weakness with their human counterparts: producing code that inadvertently has security vulnerabilities. These vulnerabilities could allow unauthorized attackers to access sensitive data or systems, which is unacceptable for safety-critical applications. In this work, we propose Feedback-Driven Security Patching (FDSP), where LLMs automatically refine generated, vulnerable code. Our approach leverages automatic static code analysis to empower the LLM to generate and implement potential solutions to address vulnerabilities. We address the research communitys needs for safe code generation by introducing a large-scale dataset, PythonSecurityEval, covering the diversity of real-world applications, including databases, websites and operating systems. We empirically validate that FDSP outperforms prior work that uses self-feedback from LLMs by up to 17.6% through our procedure that injects targeted, external feedback. Code and data are available at \\url{https://github.com/Kamel773/LLM-code-refine}",
    "published_date": "2023-11-13",
    "pdf_link": "https://arxiv.org/pdf/2312.00024v5",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "benchmark",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Secure Code Generation and Repair",
      "specific_problem": "Automatically patching security vulnerabilities in LLM-generated Python code using static analysis feedback",
      "attack_types": [
        "SQL injection",
        "Cross-site scripting (XSS)",
        "Command injection",
        "Broken access control"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "LLM + Tool-augmented prompting",
        "specific": "Feedback-Driven Security Patching (FDSP) using Bandit static analysis and iterative solution generation",
        "novel_contribution": "Leverages static code analysis (Bandit) to produce targeted external feedback; LLM generates multiple potential solutions (J) and iteratively refines code (K) until Bandit reports no security issues"
      },
      {
        "type": "baseline",
        "category": "LLM",
        "specific": "GPT-4",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "LLM",
        "specific": "GPT-3.5 (gpt-3.5-turbo-instruct)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "LLM",
        "specific": "CodeLlama (CodeLlama-Instruct-34B)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Self-debugging",
        "specific": "Self-Debugging (Chen et al., 2023)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Prompting",
        "specific": "Direct prompting (ask LLM to detect and fix security issue)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Tool feedback prompting",
        "specific": "Bandit feedback (use Bandit’s report directly as feedback)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Tool feedback prompting",
        "specific": "Verbalization of Bandit feedback via LLM",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Zero-shot prompting",
      "Tool-augmented self-refinement"
    ],
    "datasets": [
      {
        "name": "PythonSecurityEval",
        "type": "public",
        "domain": "source_code",
        "link": "https://github.com/Kamel773/LLM-code-refine",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "LLMSecEval",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "SecurityEval",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Generated code (no refinement)",
        "paper_reference": null,
        "metric": "Bandit vulnerability rate on LLMSecEval (GPT-4)",
        "their_result": "FDSP: 6.0%",
        "baseline_result": "Generated: 38.2%"
      },
      {
        "method_name": "Direct prompting",
        "paper_reference": null,
        "metric": "Bandit vulnerability rate on LLMSecEval (GPT-4)",
        "their_result": "FDSP: 6.0%",
        "baseline_result": "Direct prompting: 35.3%"
      },
      {
        "method_name": "Self-Debugging (Chen et al., 2023)",
        "paper_reference": "Chen et al., 2023",
        "metric": "Bandit vulnerability rate on LLMSecEval (GPT-4)",
        "their_result": "FDSP: 6.0%",
        "baseline_result": "Self-debugging: 24.0%"
      },
      {
        "method_name": "Bandit feedback",
        "paper_reference": null,
        "metric": "Bandit vulnerability rate on LLMSecEval (GPT-4)",
        "their_result": "FDSP: 6.0%",
        "baseline_result": "Bandit feedback: 8.0%"
      },
      {
        "method_name": "Verbalization of Bandit feedback",
        "paper_reference": null,
        "metric": "Bandit vulnerability rate on LLMSecEval (GPT-4)",
        "their_result": "FDSP: 6.0%",
        "baseline_result": "Verbalization: 7.3%"
      },
      {
        "method_name": "Bandit feedback",
        "paper_reference": null,
        "metric": "Bandit vulnerability rate on LLMSecEval (GPT-3.5)",
        "their_result": "FDSP: 12.6%",
        "baseline_result": "Bandit feedback: 18.6%"
      },
      {
        "method_name": "Verbalization of Bandit feedback",
        "paper_reference": null,
        "metric": "Bandit vulnerability rate on LLMSecEval (GPT-3.5)",
        "their_result": "FDSP: 12.6%",
        "baseline_result": "Verbalization: 18.0%"
      },
      {
        "method_name": "Bandit feedback",
        "paper_reference": null,
        "metric": "Bandit vulnerability rate on LLMSecEval (CodeLlama)",
        "their_result": "FDSP: 14.6%",
        "baseline_result": "Bandit feedback: 18.0%"
      },
      {
        "method_name": "Verbalization of Bandit feedback",
        "paper_reference": null,
        "metric": "CodeQL vulnerability rate on LLMSecEval (CodeLlama)",
        "their_result": "FDSP: 9.1%",
        "baseline_result": "Verbalization: 10.7%"
      },
      {
        "method_name": "Self-Debugging (Chen et al., 2023)",
        "paper_reference": "Chen et al., 2023",
        "metric": "CodeQL vulnerability rate on LLMSecEval (GPT-3.5)",
        "their_result": "FDSP: 8.1%",
        "baseline_result": "Self-debugging: 8.7%"
      }
    ],
    "performance_metrics_used": [
      "Bandit vulnerability rate (fraction of generated programs flagged as vulnerable by Bandit; lower is better)",
      "CodeQL vulnerability rate (fraction of generated programs matching vulnerability patterns via CodeQL; lower is better)",
      "Absolute reduction in vulnerability rate vs generated code (reported as ↓ values)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "RQ1. What is the fundamental capability of LLMs in refining security vulnerabilities?",
        "RQ2. How does Bandit feedback affect the ability of LLMs to refine code vulnerabilities?",
        "RQ3. How does FDSP improve LLM performance in fixing code vulnerabilities?",
        "RQ4. How important are the multiple generated solutions and iterations of FDSP?"
      ],
      "gaps_identified": [
        "Existing security evaluation datasets (LLMSecEval, SecurityEval) are limited in size and diversity and insufficient for large-scale evaluation",
        "LLMs may lack specific security knowledge and struggle to recognize and fix security issues without targeted feedback",
        "Training approaches would require large, costly human-labeled datasets and expert feedback",
        "Prior refinement methods rely primarily on self-feedback or generic tool outputs, which can be insufficient for complex vulnerabilities"
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Enable safe code generation by automatically detecting and patching security vulnerabilities in LLM-generated code; provide a larger, diverse benchmark to evaluate secure code generation.",
      "potential_research_ideas": [
        "Incorporate dynamic analysis and fuzzing alongside static analysis to catch runtime-only vulnerabilities and improve patch validation",
        "Extend FDSP to multi-language and cross-framework settings (e.g., Java, JavaScript, Rust) and to broader CWE coverage",
        "Integrate retrieval of secure coding patterns and CWE-specific best practices to guide solution generation",
        "Use program-repair search (e.g., genetic programming or constraint solving) guided by static/dynamic verifiers within the FDSP loop",
        "Train or fine-tune a specialized security-aware LLM using Bandit/CodeQL-derived weak labels to improve zero-shot patch quality",
        "Add a formal verifier or symbolic execution step to validate security properties after each refinement iteration",
        "Develop an automated risk/priority ranking mechanism to choose among multiple generated patches based on exploitability and side effects"
      ],
      "architectural_improvement_recommendations": [
        "Add a dynamic analysis phase (e.g., unit tests, taint tests, fuzzing) and fuse its feedback with Bandit reports before solution generation",
        "Introduce an AST-level transformation module to propose structured patches aligned with static analysis findings",
        "Use retrieval-augmented generation to inject CWE-specific remediation snippets and secure API usage examples",
        "Adopt a verify-select loop: generate multiple candidate patches, validate with Bandit+CodeQL+tests, then select via a scoring model",
        "Incorporate a lightweight reward model to rank patches by security reduction and minimal code deviation",
        "Cache and reuse successful patch rationales and transformations for similar vulnerability patterns (case-based reasoning)"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/Kamel773/LLM-code-refine",
      "frameworks": [
        "OpenAI API",
        "HuggingFace Transformers",
        "Bandit (static analysis)",
        "CodeQL"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "FDSP: A feedback-driven security patching method that uses static code analysis (Bandit) to generate targeted feedback, then has the LLM propose multiple solutions and iteratively refine code until no issues are reported",
      "PythonSecurityEval: A new benchmark of 470 natural language prompts covering diverse real-world applications (databases, websites, operating systems) for evaluating secure code generation",
      "Empirical validation on three benchmarks with multiple LLMs showing FDSP outperforms prior self-feedback methods by up to 17.6%"
    ]
  },
  {
    "arxiv_id": "2312.06802v3",
    "title": "On the Feasibility of Fingerprinting Collaborative Robot Network Traffic",
    "authors": "Cheng Tang; Diogo Barradas; Urs Hengartner; Yue Hu",
    "abstract": "This study examines privacy risks in collaborative robotics, focusing on the potential for traffic analysis in encrypted robot communications. While previous research has explored low-level command recovery in teleoperation setups, our work investigates high-level motion recovery from script-based control interfaces. We evaluate the efficacy of prominent website fingerprinting techniques (e.g., Tik-Tok, RF) and their limitations in accurately identifying robotic actions due to their inability to capture detailed temporal relationships. To address this, we introduce a traffic classification approach using signal processing techniques, demonstrating high accuracy in action identification and highlighting the vulnerability of encrypted communications to privacy breaches. Additionally, we explore defenses such as packet padding and timing manipulation, revealing the challenges in balancing traffic analysis resistance with network efficiency. Our findings emphasize the need for continued development of practical defenses in robotic privacy and security.",
    "published_date": "2023-12-11",
    "pdf_link": "https://arxiv.org/pdf/2312.06802v3",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "IoT/Robotics Security",
      "subdomain": "Privacy and Traffic Analysis",
      "specific_problem": "Fingerprinting and identifying high-level robotic actions from encrypted robot-controller network traffic (script-based/high-level API control)",
      "attack_types": [
        "encrypted traffic analysis",
        "traffic fingerprinting",
        "website-fingerprinting-inspired classification"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Signal processing + Classical ML",
        "specific": null,
        "novel_contribution": "Signal-correlation and convolution-based feature extraction capturing temporal dependencies among command-message sub-patterns, followed by classical ML classification for action identification"
      },
      {
        "type": "baseline",
        "category": "Classical ML",
        "specific": "CUMUL",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Ensemble Trees",
        "specific": "k-FP (Random Forest-based website fingerprinting)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN/Deep Learning",
        "specific": "Tik-Tok",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN/Deep Learning",
        "specific": "Robust Fingerprinting (RF)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Kinova Gen3 collaborative robot encrypted traffic traces for four scripted actions (pick-and-place, pour water, turn on switch, press key)",
        "type": "private",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "CUMUL",
        "paper_reference": "[29] (website fingerprinting)",
        "metric": "Accuracy",
        "their_result": "Overall action identification accuracy up to 97% with the proposed signal-processing approach (baseline-specific results not provided in excerpt)",
        "baseline_result": null
      },
      {
        "method_name": "k-FP",
        "paper_reference": "[19] (website fingerprinting via Random Forest)",
        "metric": "Accuracy",
        "their_result": "Proposed approach: 97% accuracy (baseline-specific results not provided)",
        "baseline_result": null
      },
      {
        "method_name": "Tik-Tok",
        "paper_reference": "[31] (DL-based website fingerprinting)",
        "metric": "Accuracy",
        "their_result": "Proposed approach: 97% accuracy (baseline-specific results not provided)",
        "baseline_result": null
      },
      {
        "method_name": "Robust Fingerprinting (RF)",
        "paper_reference": "[35] (DL-based robust WF)",
        "metric": "Accuracy",
        "their_result": "Proposed approach: 97% accuracy (baseline-specific results not provided)",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "accuracy",
      "confusion_matrix",
      "bandwidth_utilization_overhead",
      "latency_increase"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can encrypted robot-controller traffic be fingerprinted to recover high-level robotic actions in script-based control settings?",
        "How well do prominent website fingerprinting techniques (e.g., Tik-Tok, RF, k-FP, CUMUL) identify robotic actions from encrypted traffic?",
        "Can a signal-processing-based approach that captures temporal dependencies improve action identification accuracy?",
        "What are the effectiveness and costs of practical defenses (packet padding and constant-rate/timing manipulation) against such traffic analysis?"
      ],
      "gaps_identified": [
        "Prior work focused on teleoperation and low-level command recovery; limited exploration of high-level motion recovery in script-based control.",
        "Prominent website fingerprinting methods struggle to capture detailed temporal relationships among command-message sequences critical for robotic action identification.",
        "Lack of practical defenses that balance traffic-analysis resistance with bandwidth/latency efficiency in robotic control traffic."
      ],
      "limitations": [
        "Closed-world evaluation setting.",
        "Single robot platform (Kinova Gen3) and high-level API; four action classes.",
        "Small dataset (200 samples; 50 per action) collected via manual scripted executions; physical operation limited parallelism.",
        "Traffic captured in a wired Ethernet lab setup; results may vary across networks or robots.",
        "Models trained with default hyperparameters; no separate validation set beyond 10-fold CV."
      ],
      "future_work": [
        "“Casting the need for additional work towards the development of practical defenses that can be widely applied to the robot operation scenario.”",
        "“We leave the exploration of other approaches to generate command traffic sequences automatically (e.g., generative adversarial networks [42]) to future work.”"
      ],
      "motivation": "Robotic systems in close human contact (e.g., healthcare) may leak sensitive operational and user information through encrypted traffic metadata; demonstrate risks and explore defenses.",
      "potential_research_ideas": [
        "Open-world and larger-scale evaluations across diverse robots, environments, and network conditions to assess generalization.",
        "Cross-robot transfer learning and domain adaptation to fingerprint actions without requiring the exact same robot model.",
        "Self-supervised or contrastive pretraining on unlabeled robot traffic to reduce labeled data needs while capturing temporal structure.",
        "Sequence-modeling approaches (e.g., TCNs/Transformers) tailored to traffic sub-patterns for improved temporal-dependency capture and interpretability.",
        "Synthetic traffic generation using GANs/diffusion models for data augmentation and to stress-test defenses.",
        "Defense design co-optimized with control constraints: adaptive stochastic padding, control-aware randomized scheduling, and traffic morphing with formal privacy-utility trade-offs.",
        "Evaluation under realistic network impairments (jitter, loss) and adversarial settings (adaptive attackers) to quantify robustness.",
        "Formal privacy metrics (e.g., empirical mutual information, Bayes error bounds) to guide defense parameterization."
      ],
      "architectural_improvement_recommendations": [
        "Augment signal-processing pipeline with multi-scale temporal convolutions or dilated TCNs to capture long-range dependencies while remaining data-efficient.",
        "Use attention-based temporal pooling over correlation/convolution features to emphasize discriminative sub-patterns.",
        "Apply self-supervised pretext tasks (e.g., masked time segments, contrastive augmentations) on traffic to pretrain feature extractors before supervised fine-tuning.",
        "Incorporate HMM/CRF layers on top of per-command detections to explicitly model command sequence grammar of actions.",
        "Calibrate classifiers and add uncertainty estimation to enable abstention in open-world scenarios.",
        "For defenses, implement adaptive rate/padding schemes that react to action phase while meeting control-loop deadlines; optimize parameters via multi-objective search (privacy vs. bandwidth/latency)."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Robotics lab: Kinova Gen3 arm connected via Ethernet to Ubuntu 20.04 controller; TLS-encrypted channels",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Balancing privacy defenses (padding/timing manipulation) with bandwidth and latency constraints required by robot control.",
        "Closed-world assumptions may not hold in deployment; action sets can be larger and evolving.",
        "Adversary model assumes access to same robot model to learn fingerprints; generalization across models unknown."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": true,
      "fairness_bias": false
    },
    "contributions": [
      "Show that encrypted traffic from a collaborative robot (script-based/high-level API) can be fingerprinted to recover high-level actions.",
      "Empirically evaluate website-fingerprinting methods (CUMUL, k-FP, Tik-Tok, RF) on robotic traffic and analyze their limitations for action identification.",
      "Introduce a signal-processing-based traffic classification approach (correlation and convolution over command sub-patterns) achieving up to 97% accuracy on four actions.",
      "Design and evaluate two defenses (packet padding and constant-rate/timing manipulation), quantifying their accuracy reduction and bandwidth/latency costs.",
      "Construct a dataset of 200 encrypted traffic traces (Kinova Gen3; four actions) and characterize command/action-level traffic signatures."
    ]
  },
  {
    "arxiv_id": "2311.18539v2",
    "title": "Bridging Both Worlds in Semantics and Time: Domain Knowledge Based Analysis and Correlation of Industrial Process Attacks",
    "authors": "Moses Ike; Kandy Phan; Anwesh Badapanda; Matthew Landen; Keaton Sadoski; Wanda Guo; Asfahan Shah; Saman Zonouz; Wenke Lee",
    "abstract": "Modern industrial control systems (ICS) attacks infect supervisory control and data acquisition (SCADA) hosts to stealthily alter industrial processes, causing damage. To detect attacks with low false alarms, recent work detects attacks in both SCADA and process data. Unfortunately, this led to the same problem - disjointed (false) alerts, due to the semantic and time gap in SCADA and process behavior, i.e., SCADA execution does not map to process dynamics nor evolve at similar time scales. We propose BRIDGE to analyze and correlate SCADA and industrial process attacks using domain knowledge to bridge their unique semantic and time evolution. This enables operators to tie malicious SCADA operations to their adverse process effects, which reduces false alarms and improves attack understanding. BRIDGE (i) identifies process constraints violations in SCADA by measuring actuation dependencies in SCADA process-control, and (ii) detects malicious SCADA effects in processes via a physics-informed neural network that embeds generic knowledge of inertial process dynamics. BRIDGE then dynamically aligns both analysis (i and ii) in a time-window that adjusts their time evolution based on process inertial delays. We applied BRIDGE to 11 diverse real-world industrial processes, and adaptive attacks inspired by past events. BRIDGE correlated 98.3% of attacks with 0.8% false positives (FP), compared to 78.3% detection accuracy and 13.7% FP of recent work.",
    "published_date": "2023-11-30",
    "pdf_link": "https://arxiv.org/pdf/2311.18539v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Industrial Control Systems (ICS) Security",
      "subdomain": "Process/Anomaly Detection and SCADA-host Attack Detection",
      "specific_problem": "Semantic and temporal correlation of SCADA operations with industrial process dynamics to detect process manipulation attacks with low false positives",
      "attack_types": [
        "SCADA-host compromise",
        "Setpoint/parameter manipulation",
        "Actuator manipulation",
        "Stealthy process perturbations",
        "HMI compromise",
        "PLC parameter changes without logic modification"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer Autoencoder",
        "specific": "Transformer-based Autoencoder with Attention",
        "novel_contribution": "Embeds a physics-informed PDE loss (PINN) to capture inertial process dynamics, enabling better generalization with limited data and timely correlation with SCADA events."
      },
      {
        "type": "primary",
        "category": "Physics-Informed Neural Network (PINN)",
        "specific": null,
        "novel_contribution": "Introduces a domain PDE loss function capturing inertial dynamics of actuators/sensors for anomaly detection in industrial processes."
      },
      {
        "type": "primary",
        "category": "Statistical Modeling",
        "specific": "Coefficient of Variation aggregation of control constraints",
        "novel_contribution": "Measures and aggregates control-time, control-burst, and control-frequency constraints from SCADA process-control to be robust across process setpoints."
      },
      {
        "type": "baseline",
        "category": "Rules/Signatures",
        "specific": "Process invariants and SCADA/physical signatures (Invariant, Scaphy)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Unsupervised",
      "Physics-informed",
      "Anomaly Detection",
      "Hybrid (statistical + ML)"
    ],
    "datasets": [
      {
        "name": "Public real-world data from 11 diverse industrial processes",
        "type": "public",
        "domain": "sensor_actuator_timeseries + scada_host_logs",
        "link": "https://github.com/lordmoses/Scaphy",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Scaphy",
        "paper_reference": "[1]",
        "metric": "Detection/Correlation rate",
        "their_result": "98.3% correlated attacks",
        "baseline_result": "78.3% detection accuracy"
      },
      {
        "method_name": "Scaphy",
        "paper_reference": "[1]",
        "metric": "False positive rate",
        "their_result": "0.8% FP",
        "baseline_result": "13.7% FP"
      },
      {
        "method_name": "Invariant",
        "paper_reference": "[16]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Detection accuracy",
      "False positive rate",
      "Correlation rate"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "How can we semantically connect SCADA process-control operations to industrial process dynamics to identify attacks?",
        "How can we align the time evolution of discrete SCADA execution and continuous physical processes to correlate cause and effect?",
        "Can embedding domain physics (inertial dynamics) into learning models improve process anomaly detection with limited data?"
      ],
      "gaps_identified": [
        "Disjointed analysis of SCADA and process data leads to false alarms due to lack of semantic and temporal correlation.",
        "Signature-based approaches (e.g., Scaphy) cannot detect unknown attacks that use normal tools/system calls.",
        "Standard AEs trained on limited data struggle with noisy, inertia-affected process dynamics."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Reduce false alarms and improve attack understanding by tying malicious SCADA operations to their adverse process effects, bridging semantic and time gaps between SCADA and physical processes.",
      "potential_research_ideas": [
        "Automatic discovery/learning of process-specific PDEs or physics priors (e.g., sparse identification of dynamics) to generalize PINNs across more diverse processes.",
        "Causal inference frameworks to explicitly model SCADA-to-process cause-effect with uncertainty quantification.",
        "Online/continual PINN adaptation to process drift, maintenance events, and seasonal changes.",
        "Multi-modal fusion (host execution, PLC logic diffs, network telemetry) with cross-attention to improve correlation robustness.",
        "Formal temporal logic constraints integrated with learned models to reduce false correlations and provide verifiable guarantees.",
        "Adversarial robustness evaluation against mimicry and low-and-slow perturbations targeting the PDE loss and attention mechanisms.",
        "Human-in-the-loop explanations linking specific SCADA commands to predicted physical trajectories and constraint violations."
      ],
      "architectural_improvement_recommendations": [
        "Replace fixed PDE loss with a learnable physics prior (e.g., meta-learned PDE parameters per actuator class) to adapt across processes.",
        "Introduce probabilistic PINN (Bayesian layers) for calibrated uncertainty in anomaly scores within the evolution window.",
        "Contrastive pretraining on benign sequences across processes to learn invariant representations before PINN fine-tuning.",
        "Hybrid graph-structured Transformer encoding device dependencies explicitly (graph attention) rather than implicit sequence modeling.",
        "Joint training objective combining semantic constraint violation scores (from SCADA) with PINN reconstruction to co-regularize both sides.",
        "Adaptive inertia time-block estimation using change-point detection and multi-scale derivatives rather than a single rate-of-change heuristic."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://anonymous.4open.science/r/bridge/",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "ICS Purdue model: SCADA hosts (Layer 2) and PLC/sensors/actuators (Layers 0–1)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Semantic gap between discrete SCADA execution and continuous process dynamics",
        "Time-scale mismatch due to inertial delays",
        "Limited training data and noise/irregularities in process signals",
        "Cross-plant heterogeneity in devices and setpoints",
        "Need for trusted agents on SCADA hosts and PLC-side logging"
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "A new technique to correlate SCADA and process attacks semantically and temporally via domain knowledge bridging their unique behaviors and time evolution.",
      "A physics-informed learning architecture (PINN) with a domain PDE loss deployed in a Transformer-based autoencoder to capture inertial process dynamics and enable timely correlation with SCADA.",
      "A technique to derive intrinsic process-control constraints from SCADA execution (control-time, control-burst, control-frequency) that generalizes across setpoints using coefficient of variation.",
      "Extensive evaluation on public real-world data from 11 industrial processes with adaptive attacks, achieving 98.3% correlated attacks and 0.8% FP, outperforming Scaphy and Invariant."
    ]
  },
  {
    "arxiv_id": "2312.00507v2",
    "title": "VEXIR2Vec: An Architecture-Neutral Embedding Framework for Binary Similarity",
    "authors": "S. VenkataKeerthy; Soumya Banerjee; Sayan Dey; Yashas Andaluri; Raghul PS; Subrahmanyam Kalyanasundaram; Fernando Magno Quintão Pereira; Ramakrishna Upadrasta",
    "abstract": "Binary similarity involves determining whether two binary programs exhibit similar functionality, often originating from the same source code. In this work, we propose VexIR2Vec, an approach for binary similarity using VEX-IR, an architecture-neutral Intermediate Representation (IR). We extract the embeddings from sequences of basic blocks, termed peepholes, derived by random walks on the control-flow graph. The peepholes are normalized using transformations inspired by compiler optimizations. The VEX-IR Normalization Engine mitigates, with these transformations, the architectural and compiler-induced variations in binaries while exposing semantic similarities. We then learn the vocabulary of representations at the entity level of the IR using the knowledge graph embedding techniques in an unsupervised manner. This vocabulary is used to derive function embeddings for similarity assessment using VexNet, a feed-forward Siamese network designed to position similar functions closely and separate dissimilar ones in an n-dimensional space. This approach is amenable for both diffing and searching tasks, ensuring robustness against Out-Of-Vocabulary (OOV) issues.   We evaluate VexIR2Vec on a dataset comprising 2.7M functions and 15.5K binaries from 7 projects compiled across 12 compilers targeting x86 and ARM architectures. In diffing experiments, VexIR2Vec outperforms the nearest baselines by $40\\%$, $18\\%$, $21\\%$, and $60\\%$ in cross-optimization, cross-compilation, cross-architecture, and obfuscation settings, respectively. In the searching experiment, VexIR2Vec achieves a mean average precision of $0.76$, outperforming the nearest baseline by $46\\%$. Our framework is highly scalable and is built as a lightweight, multi-threaded, parallel library using only open-source tools. VexIR2Vec is $3.1$-$3.5 \\times$ faster than the closest baselines and orders-of-magnitude faster than other tools.",
    "published_date": "2023-12-01",
    "pdf_link": "https://arxiv.org/pdf/2312.00507v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Binary Analysis and Hardening",
      "specific_problem": "Cross-compiler, cross-optimization, cross-architecture, and obfuscation-robust binary function similarity for diffing and search",
      "attack_types": [
        "Code obfuscation (e.g., control-flow flattening, dead control flow)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Knowledge graph embedding",
        "specific": null,
        "novel_contribution": "Unsupervised learning of a vocabulary over VEX-IR entities (opcodes, types, arguments) to avoid OOV and decouple representation learning from task-specific fine-tuning"
      },
      {
        "type": "primary",
        "category": "Siamese Network",
        "specific": "Feed-forward Siamese network (VexNet)",
        "novel_contribution": "Task-specific fine-tuning to position similar functions close and dissimilar far in embedding space for both diffing and searching"
      },
      {
        "type": "primary",
        "category": "Representation learning from graphs",
        "specific": "Random walks over CFG",
        "novel_contribution": "Generates ‘peephole’ sequences (basic block chains) via random walks to capture structural properties efficiently"
      },
      {
        "type": "baseline",
        "category": "Graph embedding / Graph-based similarity",
        "specific": "DeepBinDiff",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN/Sequence embedding",
        "specific": "SAFE",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Feature-based ML",
        "specific": "BinFinder",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Graph matching / Heuristic",
        "specific": "BinDiff",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Bag-of-operations",
        "specific": "Opcode histogram",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Unsupervised",
      "Supervised",
      "Metric Learning"
    ],
    "datasets": [
      {
        "name": "VexIR2Vec evaluation corpus (2.7M functions; 15.5K binaries; 7 projects; 12 compilers; x86 and ARM)",
        "type": "private",
        "domain": "compiled_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "GNU Findutils",
        "type": "public",
        "domain": "source_code -> compiled_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "GNU Diffutils",
        "type": "public",
        "domain": "source_code -> compiled_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "GNU Coreutils",
        "type": "public",
        "domain": "source_code -> compiled_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "cURL",
        "type": "public",
        "domain": "source_code -> compiled_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Lua",
        "type": "public",
        "domain": "source_code -> compiled_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "PuTTY",
        "type": "public",
        "domain": "source_code -> compiled_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "GNU Gzip",
        "type": "public",
        "domain": "source_code -> compiled_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "SAFE",
        "paper_reference": "[66]",
        "metric": "Inference speed",
        "their_result": "VexIR2Vec is ≈3.1× faster than SAFE",
        "baseline_result": null
      },
      {
        "method_name": "BinFinder",
        "paper_reference": "[79]",
        "metric": "Inference speed",
        "their_result": "VexIR2Vec is ≈3.5× faster than BinFinder",
        "baseline_result": null
      },
      {
        "method_name": "DeepBinDiff",
        "paper_reference": "[27]",
        "metric": "Training time",
        "their_result": "DeepBinDiff training reached ≈7.5 hours/epoch; VexNet trains in ≈5–8 seconds/epoch",
        "baseline_result": null
      },
      {
        "method_name": "BinDiff",
        "paper_reference": "[114]",
        "metric": "F1 Score (diffing)",
        "their_result": "“VexIR2Vec’s F1 Score outperforms the nearest baselines in cross-optimization, cross-compiler, cross-architecture, and obfuscation settings by 40%, 18%, 21%, and 60% respectively.”",
        "baseline_result": null
      },
      {
        "method_name": "DeepBinDiff",
        "paper_reference": "[27]",
        "metric": "F1 Score (diffing)",
        "their_result": "Nearest-baseline improvements reported above include graph-based methods",
        "baseline_result": null
      },
      {
        "method_name": "SAFE",
        "paper_reference": "[66]",
        "metric": "F1 Score (diffing)",
        "their_result": "Nearest-baseline improvements reported above include sequence-embedding methods",
        "baseline_result": null
      },
      {
        "method_name": "BinFinder",
        "paper_reference": "[79]",
        "metric": "F1 Score (diffing)",
        "their_result": "Nearest-baseline improvements reported above include feature-based methods",
        "baseline_result": null
      },
      {
        "method_name": "Opcode histogram (Damásio et al.)",
        "paper_reference": "[20]",
        "metric": "Diffing/search accuracy",
        "their_result": "Compared as a simple representation baseline; VexIR2Vec reported higher precision",
        "baseline_result": null
      },
      {
        "method_name": "Nearest baseline (unspecified)",
        "paper_reference": null,
        "metric": "Mean Average Precision (searching)",
        "their_result": "mAP = 0.76; outperforms the nearest baseline by 46%",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "F1 Score",
      "Mean Average Precision (mAP)",
      "Training time per epoch",
      "Inference speed (x faster)",
      "Throughput/Scalability"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can an architecture-neutral IR (VEX-IR) with peephole normalization yield robust cross-compiler and cross-architecture binary function embeddings?",
        "Does decoupling vocabulary learning (unsupervised) from task-specific fine-tuning (Siamese network) improve accuracy and scalability for diffing and search?",
        "Can simple feed-forward models achieve competitive or superior performance and orders-of-magnitude faster training/inference compared to prior ML-based binary similarity methods?"
      ],
      "gaps_identified": [
        "Architecture-specific training limits cross-architecture comparison for assembly-based models",
        "Pairwise-comparison approaches are impractical for large-scale search due to quadratic complexity",
        "Transformer/LLM-based methods have prohibitive training times and GPU requirements",
        "GNN-based CFG encodings suffer from high computational cost and poor scalability",
        "Limited availability and dependence on proprietary disassemblers impedes reproducibility"
      ],
      "limitations": [
        "Normalization rules are local and unsound by design: “these rules are applied to local sequences of instructions (the peepholes) without global code knowledge; hence, they are unsound.”",
        "Evaluation limited to x86 and ARM; generalization to other architectures not demonstrated in the provided text",
        "Dataset of compiled binaries (2.7M functions) is not explicitly released in the provided text"
      ],
      "future_work": [],
      "motivation": "Deliver a scalable, accurate, and architecture-neutral binary similarity framework that is robust to compiler/optimization/architecture variations and obfuscation while improving availability and reproducibility.",
      "potential_research_ideas": [
        "Extend to additional architectures (e.g., RISC-V, MIPS, PowerPC) and heterogeneous ISAs in a single model",
        "Introduce self-supervised pretraining on VEX-IR peepholes (e.g., masked-entity modeling) before Siamese fine-tuning",
        "Domain-adaptive alignment (adversarial/domain-invariant training) to better handle cross-architecture distribution shifts",
        "Obfuscation-aware data augmentation and curriculum learning targeting advanced obfuscations",
        "Incorporate inter-procedural context (call graphs, API sequences) and symbolic semantics into embeddings",
        "Efficient large-scale search via approximate nearest neighbor indexing with dynamic updates",
        "Calibrated similarity scores with uncertainty estimation for safer triage in security workflows",
        "Joint training on multiple IRs (e.g., LLVM-IR alongside VEX-IR) to improve robustness to lifting artifacts"
      ],
      "architectural_improvement_recommendations": [
        "Augment Siamese objective with contrastive losses (InfoNCE) and hard negative mining across compilers/architectures",
        "Multi-view learning that fuses peephole sequences with lightweight CFG summaries (e.g., node-degree histograms) without full GNN overhead",
        "Replace or complement knowledge-graph embeddings with learned subtokenization (BPE) over IR entities to further reduce OOV risk",
        "Add cross-architecture adversarial/domain confusion layer to encourage architecture-invariant embeddings",
        "Integrate ANN backends (e.g., product quantization) and quantized embeddings for sub-linear search at scale",
        "Incorporate lightweight semantics via partial symbolic execution summaries as additional channels"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Simple feed-forward models; VexNet trains in ≈5–8 seconds per epoch. Inference: VexIR2Vec is ≈3.1× faster than SAFE and ≈3.5× faster than BinFinder; orders-of-magnitude faster than DeepBinDiff. Built in Python using open-source tools (angr for lifting to VEX-IR)."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": "VexIR2Vec is ≈3.1–3.5× faster than closest baselines; VexNet trains in ≈5–8 s/epoch",
      "deployment_challenges": [
        "Generalization to architectures beyond x86/ARM not demonstrated",
        "Dependence on IR lifting quality (angr/VEX-IR) may introduce variance",
        "Handling highly aggressive or custom obfuscations may require additional normalization/augmentation"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": true,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces VexIR2Vec, an architecture-neutral embedding framework operating on VEX-IR",
      "Proposes peephole sequences via random walks on CFGs to capture structural properties efficiently",
      "Develops VEX-IR Normalization Engine (VexINE) applying compiler-inspired local rewrites",
      "Learns an IR-entity vocabulary unsupervised with knowledge-graph embedding techniques, avoiding OOV",
      "Designs VexNet, a feed-forward Siamese network for function-level embedding fine-tuning for diffing/search",
      "Demonstrates superior accuracy and scalability across cross-optimization, cross-compiler, cross-architecture, and obfuscation settings",
      "Implements a parallel Python library available as CLI and web interface, using only open-source tooling"
    ]
  },
  {
    "arxiv_id": "2311.06530v2",
    "title": "Exploring ChatGPT's Capabilities on Vulnerability Management",
    "authors": "Peiyu Liu; Junming Liu; Lirong Fu; Kangjie Lu; Yifan Xia; Xuhong Zhang; Wenzhi Chen; Haiqin Weng; Shouling Ji; Wenhai Wang",
    "abstract": "Recently, ChatGPT has attracted great attention from the code analysis domain. Prior works show that ChatGPT has the capabilities of processing foundational code analysis tasks, such as abstract syntax tree generation, which indicates the potential of using ChatGPT to comprehend code syntax and static behaviors. However, it is unclear whether ChatGPT can complete more complicated real-world vulnerability management tasks, such as the prediction of security relevance and patch correctness, which require an all-encompassing understanding of various aspects, including code syntax, program semantics, and related manual comments.   In this paper, we explore ChatGPT's capabilities on 6 tasks involving the complete vulnerability management process with a large-scale dataset containing 70,346 samples. For each task, we compare ChatGPT against SOTA approaches, investigate the impact of different prompts, and explore the difficulties. The results suggest promising potential in leveraging ChatGPT to assist vulnerability management. One notable example is ChatGPT's proficiency in tasks like generating titles for software bug reports. Furthermore, our findings reveal the difficulties encountered by ChatGPT and shed light on promising future directions. For instance, directly providing random demonstration examples in the prompt cannot consistently guarantee good performance in vulnerability management. By contrast, leveraging ChatGPT in a self-heuristic way -- extracting expertise from demonstration examples itself and integrating the extracted expertise in the prompt is a promising research direction. Besides, ChatGPT may misunderstand and misuse the information in the prompt. Consequently, effectively guiding ChatGPT to focus on helpful information rather than the irrelevant content is still an open problem.",
    "published_date": "2023-11-11",
    "pdf_link": "https://arxiv.org/pdf/2311.06530v2",
    "paper_types": [
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Vulnerability Management",
      "specific_problem": "Evaluating and prompting ChatGPT for six vulnerability management tasks: bug report summarization, security bug report identification, vulnerability severity evaluation, vulnerability repair, patch correctness assessment, and stable patch classification",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "ChatGPT (gpt-4-0314; gpt-3.5-turbo-0301)",
        "novel_contribution": "Large-scale, cross-task evaluation showing ChatGPT can outperform or match SOTA on some vulnerability management tasks without fine-tuning"
      },
      {
        "type": "primary",
        "category": "Other",
        "specific": "Prompt Engineering",
        "novel_contribution": "Systematic prompt template suite across tasks: zero-shot, 1-shot, few-shot, general-info (role + zero-CoT), expertise (manual domain knowledge), and a self-heuristic template that first asks ChatGPT to extract task-specific expertise from demonstrations and then integrates it into the prompt"
      },
      {
        "type": "primary",
        "category": "Other",
        "specific": "In-context Learning",
        "novel_contribution": "Empirical study of how different in-context strategies impact performance on six vulnerability management tasks"
      },
      {
        "type": "baseline",
        "category": "Other",
        "specific": "iTAPE (bug report summarization)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Other",
        "specific": "Farsec (security bug report identification)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Other",
        "specific": "DKG (security bug report identification)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Other",
        "specific": "CASMS (security bug report identification)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Other",
        "specific": "DiffCVSS (vulnerability severity evaluation)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Other",
        "specific": "LLMset (vulnerability repair; dataset/method referenced in [37])",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Other",
        "specific": "ExtractFix (vulnerability repair)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Other",
        "specific": "Quatrain (patch correctness assessment)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Other",
        "specific": "Invalidator (patch correctness assessment)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Other",
        "specific": "Panther (patch correctness assessment)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Other",
        "specific": "PatchNet (stable patch classification)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "In-context learning",
      "Zero-shot",
      "One-shot",
      "Few-shot"
    ],
    "datasets": [
      {
        "name": "Bug report summarization dataset (from iTAPE [18])",
        "type": "public",
        "domain": "software_bug_reports",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Security bug report identification dataset (used by Farsec [49], DKG [57], CASMS [35])",
        "type": "public",
        "domain": "software_bug_reports",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "DiffCVSS dataset (vulnerability severity evaluation)",
        "type": "public",
        "domain": "code_metadata_text",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "LLMset hand-crafted vulnerabilities [37] (used for probe-test in vulnerability repair)",
        "type": "public",
        "domain": "code_snippets",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "ExtractFix dataset (vulnerability repair)",
        "type": "public",
        "domain": "code_patches",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Quatrain dataset (patch correctness assessment)",
        "type": "public",
        "domain": "code_patches",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Invalidator dataset (patch correctness assessment)",
        "type": "public",
        "domain": "code_patches",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Panther dataset (patch correctness assessment)",
        "type": "public",
        "domain": "code_patches",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "PatchNet dataset (Linux stable patch classification)",
        "type": "public",
        "domain": "code_patches",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "iTAPE",
        "paper_reference": "[18]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Farsec",
        "paper_reference": "[49]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "DKG",
        "paper_reference": "[57]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "CASMS",
        "paper_reference": "[35]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "DiffCVSS",
        "paper_reference": "[48]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "LLMset",
        "paper_reference": "[37]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "ExtractFix",
        "paper_reference": "[24]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Quatrain",
        "paper_reference": "[46]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Invalidator",
        "paper_reference": "[31]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Panther",
        "paper_reference": "[44]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "PatchNet",
        "paper_reference": "[25]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "RQ1: Does ChatGPT achieve capability on par with the SOTAs?",
        "RQ2: How do prompt engineering methods impact ChatGPT’s performance?",
        "RQ3: What is the promising future direction to improve ChatGPT’s performance on each task?"
      ],
      "gaps_identified": [
        "ChatGPT’s adoption in security-focused vulnerability management remains underexplored despite progress in general software engineering.",
        "Lack of holistic evaluation across the full vulnerability management lifecycle.",
        "Automatic prompt generation is not well addressed; current prompts require manual design.",
        "Providing random demonstration examples does not consistently yield good performance for vulnerability management tasks.",
        "ChatGPT may misunderstand and misuse information when too much or irrelevant context is provided; guiding it to focus on helpful information is an open problem."
      ],
      "limitations": [
        "Probe-test dataset sizes were limited for some tasks due to time and cost constraints with API evaluations.",
        "For some tasks, summarizing expert domain knowledge is non-trivial; the paper resorts to self-heuristic knowledge extraction by ChatGPT.",
        "Potential for model misunderstanding when prompts include excessive or noisy information.",
        "Evaluation depends on external, closed models (gpt-3.5/gpt-4) which may change over time."
      ],
      "future_work": [
        "Leverage self-heuristic prompting where the model first extracts domain expertise from demonstrations and then uses it in the final prompt.",
        "Develop methods to guide ChatGPT to prioritize relevant and constructive information over irrelevant content within prompts.",
        "Systematically explore prompt engineering strategies tailored to each vulnerability management subtask.",
        "Investigate the effect of different underlying LLMs and configurations beyond gpt-3.5 and gpt-4."
      ],
      "motivation": "Assess whether ChatGPT can assist maintainers across the entire vulnerability management process and how prompt engineering affects performance, with the goal of revealing potentials, bottlenecks, and future directions.",
      "potential_research_ideas": [
        "Design retrieval-augmented prompting that pulls project- and vulnerability-specific artifacts (e.g., code context, historical patches, CVEs, advisories) to condition ChatGPT for each task.",
        "Integrate static/dynamic program analysis tools as tool-augmented LLM agents (e.g., AST diff parsers, taint analyzers) to improve repair and patch correctness assessment.",
        "Develop automated prompt optimization for security tasks (prompt search, reinforcement learning, or bandit-based selection) under API budget constraints.",
        "Construct a curated, open benchmark covering all six tasks with unified splits, metrics, and long-context inputs for LLMs.",
        "Explore multi-agent workflows (triager agent, fixer agent, reviewer agent) with role specialization and debate for vulnerability management.",
        "Calibrate LLM confidence and uncertainty estimates for risk-aware triaging and severity prediction.",
        "Constrain generation with structured output schemas (e.g., CVSS vector fields, patch metadata) and use constrained decoding to reduce misuse of context.",
        "Use self-critique or judge models to verify and refine LLM outputs for patch correctness and severity scoring.",
        "Build domain-adaptive instruction tuning datasets from open-source security repositories to specialize LLMs for vulnerability tasks."
      ],
      "architectural_improvement_recommendations": [
        "Adopt retrieval-augmented generation with a security knowledge base (CVEs, CWE/CAPEC, historical patches, project docs) to ground answers.",
        "Incorporate external program analysis tools via tool-use APIs, enabling the LLM to request and reason over ASTs, CFGs, diffs, and test results.",
        "Employ structured prompting and constrained decoding for tasks like CVSS prediction and patch classification to reduce hallucinations.",
        "Apply multi-step reasoning (chain-of-thought or plan-and-solve) with distilled, concise highlights to mitigate overload from excessive context.",
        "Use a two-stage self-heuristic pipeline: (1) expertise extraction from demonstrations, (2) short, focused guidance injected into the final prompt.",
        "Introduce a verifier/judge model to re-check patch correctness decisions and severity predictions before finalizing outputs."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/Jamrot/ChatGPT-Vulnerability-Management",
      "frameworks": [
        "OpenAI ChatGPT API"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Evaluations via ChatGPT API; gpt-3.5-turbo-0301 used for template design/selection; gpt-4-0314 used for large-scale tests; temperature=0, top_p=1.0."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "ChatGPT may misunderstand and misuse information when prompts include excessive or irrelevant content.",
        "Random demonstration examples in prompts do not consistently yield good performance.",
        "Need mechanisms to guide the model to focus on helpful information.",
        "Manual prompt engineering effort and template selection under API cost constraints."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "First large-scale evaluation of ChatGPT for vulnerability management tasks across six subtasks and 70,346 samples (19,355,711 tokens).",
      "Systematic investigation of multiple prompt engineering methods (zero-/one-/few-shot, general-info with zero-CoT, expertise, self-heuristic) and their impact on performance.",
      "Identification of bottlenecks and promising directions, including the benefits of self-heuristic knowledge extraction and the need to guide ChatGPT toward relevant information."
    ]
  },
  {
    "arxiv_id": "2312.11550v1",
    "title": "A Study on Transferability of Deep Learning Models for Network Intrusion Detection",
    "authors": "Shreya Ghosh; Abu Shafin Mohammad Mahdee Jameel; Aly El Gamal",
    "abstract": "In this paper, we explore transferability in learning between different attack classes in a network intrusion detection setup. We evaluate transferability of attack classes by training a deep learning model with a specific attack class and testing it on a separate attack class. We observe the effects of real and synthetically generated data augmentation techniques on transferability. We investigate the nature of observed transferability relationships, which can be either symmetric or asymmetric. We also examine explainability of the transferability relationships using the recursive feature elimination algorithm. We study data preprocessing techniques to boost model performance. The code for this work can be found at https://github.com/ghosh64/transferability.",
    "published_date": "2023-12-17",
    "pdf_link": "https://arxiv.org/pdf/2312.11550v1",
    "paper_types": [
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Transferability of deep learning-based network intrusion detection models across attack classes (class-to-class generalization)",
      "attack_types": [
        "BENIGN",
        "Bot",
        "DDoS",
        "DoS GoldenEye",
        "DoS Hulk",
        "DoS Slowhttptest",
        "DoS Slowloris",
        "FTP-Patator",
        "Heartbleed",
        "Infiltration",
        "PortScan",
        "SSH-Patator",
        "Web Attack Brute Force",
        "Web Attack SQL Injection",
        "Web Attack XSS"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "MLP (Feedforward DNN)",
        "specific": null,
        "novel_contribution": "A tailored deep neural network for multi-class NIDS and a two-class variant used as the vehicle to study transferability across attack classes"
      },
      {
        "type": "primary",
        "category": "Feature selection",
        "specific": "Recursive Feature Elimination (RFE)",
        "novel_contribution": "Used to explain symmetric/asymmetric transferability by identifying common and dominant features between attack classes"
      },
      {
        "type": "primary",
        "category": "Data augmentation/Oversampling",
        "specific": "SMOTE (k=5)",
        "novel_contribution": "Evaluated for balancing attack classes to study its effect on transferability; found not consistently improving performance"
      },
      {
        "type": "primary",
        "category": "Resampling",
        "specific": "Bootstrapping (class-wise resampling)",
        "novel_contribution": "Balanced training by resampling real attack data to benign count; improved transferability compared to no augmentation and SMOTE"
      },
      {
        "type": "primary",
        "category": "Signal preprocessing",
        "specific": "Temporal averaging",
        "novel_contribution": "Proposed preprocessing that boosted transferability accuracy across several train-test pairs versus other preprocessing and no preprocessing"
      },
      {
        "type": "primary",
        "category": "Signal preprocessing",
        "specific": "Differential inputs",
        "novel_contribution": "Explored as a privacy-friendly preprocessing for NIDS; compared in transferability study"
      },
      {
        "type": "primary",
        "category": "Transform-based preprocessing",
        "specific": "Discrete Cosine Transform (DCT)",
        "novel_contribution": "Explored as an alternative preprocessing to potentially improve transferability"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Transfer learning"
    ],
    "datasets": [
      {
        "name": "CIC-IDS 2017",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CAIDA DDoS 2007",
        "type": "public",
        "domain": "network_traffic",
        "link": "https://catalog.caida.org/details/dataset/ddos_attack_2007",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "DARPA 1998",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      },
      {
        "name": "KDD Cup 1999",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "accuracy",
      "confusion matrix",
      "per-class (attack-wise) accuracy"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How transferable is learning between different network attack classes in a NIDS setup when training on one attack class and testing on another?",
        "Are transferability relationships between attack classes symmetric or asymmetric, and why?",
        "How do real-data bootstrapping versus synthetic oversampling (SMOTE) affect transferability?",
        "Can feature selection (RFE) explain observed transferability relationships via shared important features?",
        "Do preprocessing techniques (differential inputs, temporal averaging, DCT) boost transferability performance?"
      ],
      "gaps_identified": [
        "Existing NIDS deep learning models detect known attacks but are not always robust to novel or rare attacks.",
        "Anomaly-detection studies often treat all unknown intrusions as one class, obscuring class-to-class transferability.",
        "Prior transferability work either trains on one dataset and tests on another, or pre-selects training/testing attack classes, limiting comprehensive analysis of symmetric/asymmetric transferability."
      ],
      "limitations": [
        "Transferability analysis excluded Heartbleed (Attack 8), Infiltration (Attack 9), and Web Attack SQL Injection (Attack 13) due to extremely small sample sizes.",
        "Most observed transferability correlations occurred among DoS-type attacks; similar correlations were not found for other attack types such as Botnets, FTP-Patator, Heartbleed, Infiltration, and SQL Injection.",
        "Training with synthetic data via SMOTE did not consistently improve transferability performance.",
        "Study conducted on a single dataset (CIC-IDS 2017)."
      ],
      "future_work": [
        "Study transferability in different settings, including other datasets.",
        "Explore different deep learning paradigms, e.g., distributed learning.",
        "Analyze trends in specific or selected features to understand transferability correlations.",
        "Identify attributes that could boost transferable learning for non-DoS attack types such as Botnets, FTP-Patator, Heartbleed, Infiltration, and SQL Injection."
      ],
      "motivation": "Leverage transfer learning so NIDS can detect novel or rare attacks beyond the trained classes, reduce training time and memory on resource-constrained devices by training on representative attack subsets, and provide a nuanced understanding of class-to-class transferability.",
      "potential_research_ideas": [
        "Develop a unified representation learning framework (e.g., contrastive/self-supervised pretraining on benign and mixed flows) to enhance class-to-class transfer without needing target-class labels.",
        "Meta-learning or few-shot adaptation for rapid generalization from a small number of samples of rare/novel attack classes.",
        "Investigate domain generalization across capture days/environments and across datasets (CIC-IDS 2017 -> other corpora) to test broader generalization.",
        "Use generative modeling (e.g., CTGAN, copula-based synthesis, diffusion models) for class-conditional data augmentation and compare against SMOTE/bootstrapping for transferability.",
        "Model temporal and session structure explicitly with sequence models (Transformers/Temporal CNNs) to capture dynamics that may drive transferability.",
        "Construct a feature-invariance objective to encourage learning features common within attack families (e.g., DoS variants) while suppressing port/service-specific leakage.",
        "Integrate explainability at training time (e.g., feature attribution regularization) to enforce reliance on stable, generalizable features."
      ],
      "architectural_improvement_recommendations": [
        "Replace/augment the MLP with temporal architectures (Temporal CNNs, LSTMs, or Transformers) to capture flow dynamics relevant for transferability.",
        "Multi-task learning that jointly predicts attack family and specific class to promote shared representations across related attacks.",
        "Feature normalization and de-biasing to reduce over-reliance on Destination Port and other dataset-specific features; consider randomized/hashed ports during training as augmentation.",
        "Use class-balanced loss (e.g., focal loss, class reweighting) and mixup/cutmix variants in tabular space to address imbalance and improve generalization.",
        "Adopt advanced synthetic augmentation (CTGAN/diffusion) with validation via statistical similarity tests to avoid artifacts from SMOTE.",
        "Calibrate outputs (temperature scaling) and evaluate OOD detection to better handle unseen attacks in deployment."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/ghosh64/transferability",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": true,
      "fairness_bias": false
    },
    "contributions": [
      "Developed a deep neural network architecture able to extract meaningful features from high-dimensional flow data with limited samples for under-represented attacks.",
      "Conducted a comprehensive transferability study across attack classes without pre-dividing classes into train/test sets, enabling analysis of symmetric vs. asymmetric relationships.",
      "Evaluated training data augmentation via bootstrapped real data and synthetic data (SMOTE) to assess effects on transferability.",
      "Used RFE-based feature selection to explain observed transferability, identifying common/dominant features across attack pairs.",
      "Studied preprocessing techniques (differential inputs, temporal averaging, DCT) and identified temporal averaging as a technique that boosts transferability.",
      "Reported multi-class performance where 8 of 11 attack classes achieved >98% accuracy; provided attack-wise transferability observations (e.g., symmetric relationships among DoS GoldenEye, Slowhttptest, Slowloris; HULK <-> DDoS)."
    ]
  },
  {
    "arxiv_id": "2312.04758v1",
    "title": "Physics-Informed Convolutional Autoencoder for Cyber Anomaly Detection in Power Distribution Grids",
    "authors": "Mehdi Jabbari Zideh; Sarika Khushalani Solanki",
    "abstract": "The growing trend toward the modernization of power distribution systems has facilitated the installation of advanced measurement units and promotion of the cyber communication systems. However, these infrastructures are still prone to stealth cyber attacks. The existing data-driven anomaly detection methods suffer from a lack of knowledge about the system's physics, lack of interpretability, and scalability issues hindering their practical applications in real-world scenarios. To address these concerns, physics-informed neural networks (PINNs) were introduced. This paper proposes a multivariate physics-informed convolutional autoencoder (PIConvAE) to detect stealthy cyber-attacks in power distribution grids. The proposed model integrates the physical principles into the loss function of the neural network by applying Kirchhoff's law. Simulations are performed on the modified IEEE 13-bus and 123-bus systems using OpenDSS software to validate the efficacy of the proposed model for stealth attacks. The numerical results prove the superior performance of the proposed PIConvAE in three aspects: a) it provides more accurate results compared to the data-driven ConvAE model, b) it requires less training time to converge c) the model excels in effectively detecting a wide range of attack magnitudes making it powerful in detecting stealth attacks.",
    "published_date": "2023-12-08",
    "pdf_link": "https://arxiv.org/pdf/2312.04758v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Industrial Control Systems Security",
      "subdomain": "Power Grid/Smart Grid Security",
      "specific_problem": "Cyber anomaly detection for False Data Injection Attacks in power distribution grids using µPMU/SCADA measurements",
      "attack_types": [
        "False Data Injection (FDIA)",
        "Stealth attacks",
        "Additive attacks",
        "Deductive attacks",
        "Combined additive/deductive attacks"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Autoencoder (Convolutional)",
        "specific": "Physics-Informed Convolutional Autoencoder (PIConvAE)",
        "novel_contribution": "Integrates Kirchhoff’s law into the loss via physics-based penalties (active/reactive power consistency terms LPhyP and LPhyQ) and uses a combined anomaly score (reconstruction error + physics-based mismatch)."
      },
      {
        "type": "baseline",
        "category": "Autoencoder (Convolutional)",
        "specific": "Convolutional Autoencoder (ConvAE, data-driven)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "IEEE 13-bus distribution test feeder (modified)",
        "type": "public",
        "domain": "power_grid_measurements",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "IEEE 123-bus distribution test feeder (modified)",
        "type": "public",
        "domain": "power_grid_measurements",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "OpenDSS-simulated measurement time series (this study)",
        "type": "synthetic",
        "domain": "power_grid_measurements",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "San Diego city solar/wind and ambient conditions (2021 week 1) [23]",
        "type": "public",
        "domain": "weather_time_series",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "SDG&E historical load profiles",
        "type": "public",
        "domain": "load_profiles",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "ConvAE (data-driven)",
        "paper_reference": null,
        "metric": "Precision (IEEE 13-bus)",
        "their_result": "100% (\"perfect precision\")",
        "baseline_result": null
      },
      {
        "method_name": "ConvAE (data-driven)",
        "paper_reference": null,
        "metric": "Recall (IEEE 13-bus)",
        "their_result": "97.5%",
        "baseline_result": null
      },
      {
        "method_name": "ConvAE (data-driven)",
        "paper_reference": null,
        "metric": "F1 (IEEE 13-bus)",
        "their_result": "98.73%",
        "baseline_result": null
      },
      {
        "method_name": "ConvAE (data-driven)",
        "paper_reference": null,
        "metric": "Precision (IEEE 123-bus)",
        "their_result": "100% (\"perfect precision\")",
        "baseline_result": null
      },
      {
        "method_name": "ConvAE (data-driven)",
        "paper_reference": null,
        "metric": "Recall (IEEE 123-bus)",
        "their_result": "95%",
        "baseline_result": null
      },
      {
        "method_name": "ConvAE (data-driven)",
        "paper_reference": null,
        "metric": "F1 (IEEE 123-bus)",
        "their_result": "97.44%",
        "baseline_result": null
      },
      {
        "method_name": "ConvAE (data-driven)",
        "paper_reference": null,
        "metric": "Epochs to convergence (IEEE 13-bus)",
        "their_result": "~200 epochs (early stop to avoid overfitting)",
        "baseline_result": "~1000 epochs"
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "Precision",
      "Recall",
      "F1-score"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can embedding Kirchhoff’s law into a convolutional autoencoder improve detection of stealth cyber-attacks in distribution grids?",
        "Does physics-informed training reduce training time while improving precision/recall for small-magnitude (±5%) falsification attacks?",
        "Can an unsupervised, multivariate PIConvAE detect attacks without prior labels?"
      ],
      "gaps_identified": [
        "Existing data-driven anomaly detectors lack system physics, interpretability, and face scalability/extrapolation limitations.",
        "Operators struggle to trust black-box models due to poor alignment with physical laws.",
        "Stealth FDIAs with small magnitudes are difficult to detect with purely data-driven models."
      ],
      "limitations": [
        "Evaluated only in simulation using OpenDSS on IEEE 13-bus and 123-bus feeders (no real-world deployment).",
        "Attack model limited to multiplicative falsification within −5% to +5%.",
        "Attacks injected on specific measurements (e.g., current at bus 671 for 13-bus; voltage at bus 35 for 123-bus).",
        "Model shows overfitting if trained beyond early-stop epochs (e.g., 200/500), performance degrades after that.",
        "No results reported for robustness to measurement noise, missing data, or varying sensor placement density.",
        "Comparisons limited to a single baseline (data-driven ConvAE)."
      ],
      "future_work": [],
      "motivation": "Improve cyber anomaly detection in smart distribution grids by addressing lack of physics, interpretability, and scalability in existing data-driven methods via physics-informed learning.",
      "potential_research_ideas": [
        "Extend physics constraints to full unbalanced three-phase power flow (including line/phase coupling) and enforce KCL/KVL across the network, not only per-node power equations.",
        "Incorporate grid topology explicitly via graph neural networks with physics-informed regularizers for spatial consistency.",
        "Evaluate and adapt the method on real µPMU/SCADA datasets with measurement noise, missing data, and varying sampling rates; develop domain adaptation across feeders.",
        "Design online/streaming PIConvAE with concept drift handling and adaptive thresholding for anomaly score.",
        "Broaden adversary models (e.g., coordinated multi-point FDIAs, replay/shift attacks, topology falsification) and study detectability limits.",
        "Integrate uncertainty quantification and explainability (e.g., physics residual attributions per variable/bus) for operator trust.",
        "Joint learning with state estimation: couple PIConvAE latent space with SE residuals for improved detection under low observability.",
        "Automate αd/αphy balancing via learnable weighting or curriculum schedules; multi-objective optimization for reconstruction vs physics consistency."
      ],
      "architectural_improvement_recommendations": [
        "Use a graph-convolutional autoencoder that respects feeder topology; add physics penalties on edges (power flow on lines) and nodes (injections).",
        "Augment loss with differentiable power flow residuals (including voltage drops, line impedances) and three-phase equations.",
        "Adopt hybrid models: pre-train on physics-simulated data, fine-tune with self-supervised objectives on real streams; add noise/missing-data augmentations.",
        "Introduce attention mechanisms over time to capture nonstationarity; consider temporal CNN + Transformer encoder for long horizons.",
        "Implement adaptive loss weighting (learnable αd, αphy) and early-stopping with validation on physics residuals.",
        "Add adversarial training with physically-constrained perturbations to increase robustness to stealthy coordinated attacks.",
        "Calibrate anomaly thresholds via extreme value theory or conformal prediction for rigorous false-alarm control."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "TensorFlow"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Implemented in Python/TensorFlow on Nvidia RTX 3070 (8GB) GPU and 2.1 GHz Intel Core i7 CPU with 32 GB RAM. Window size 16; decaying learning rate (initial 0.001); dropout 0.2; batch normalization on all layers. PIConvAE converged around 200 epochs (13-bus) and ~500 epochs plateau (123-bus); baseline ConvAE trained up to ~1000 epochs."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Power distribution grid monitoring with µPMUs/SCADA (simulated)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Validated only on simulated feeders; real-world sensor noise/latency and data quality not studied.",
        "Requires availability and placement of µPMU/SCADA measurements; sensitivity to observability not evaluated.",
        "Thresholding of anomaly score (reconstruction + physics residual) and early stopping strategy need operational tuning."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes a multivariate Physics-Informed Convolutional Autoencoder (PIConvAE) for cyber anomaly detection in distribution grids.",
      "Encodes Kirchhoff’s law into the loss via active/reactive power consistency terms (LPhyP, LPhyQ).",
      "Operates in an unsupervised manner without attack labels.",
      "Validated on IEEE 13-bus and 123-bus feeders simulated in OpenDSS with DERs and realistic weather/load profiles.",
      "Demonstrates faster training (e.g., ~200 epochs vs ~1000 for ConvAE on 13-bus) and reduced computational time.",
      "Shows improved detection performance, including \"perfect precision\" and high recall/F1 (e.g., 97.5% recall, 98.73% F1 on 13-bus; 95% recall, 97.44% F1 on 123-bus).",
      "Demonstrates effectiveness across a range of small-magnitude (±5%) stealth FDIAs."
    ]
  },
  {
    "arxiv_id": "2312.00802v1",
    "title": "Continuous Authentication Using Mouse Clickstream Data Analysis",
    "authors": "Sultan Almalki; Prosenjit Chatterjee; Kaushik Roy",
    "abstract": "Biometrics is used to authenticate an individual based on physiological or behavioral traits. Mouse dynamics is an example of a behavioral biometric that can be used to perform continuous authentication as protection against security breaches. Recent research on mouse dynamics has shown promising results in identifying users; however, it has not yet reached an acceptable level of accuracy. In this paper, an empirical evaluation of different classification techniques is conducted on a mouse dynamics dataset, the Balabit Mouse Challenge dataset. User identification is carried out using three mouse actions: mouse move, point and click, and drag and drop. Verification and authentication methods are conducted using three machine-learning classifiers: the Decision Tree classifier, the K-Nearest Neighbors classifier, and the Random Forest classifier. The results show that the three classifiers can distinguish between a genuine user and an impostor with a relatively high degree of accuracy. In the verification mode, all the classifiers achieve a perfect accuracy of 100%. In authentication mode, all three classifiers achieved the highest accuracy (ACC) and Area Under Curve (AUC) from scenario B using the point and click action data: (Decision Tree ACC:87.6%, AUC:90.3%), (K-Nearest Neighbors ACC:99.3%, AUC:99.9%), and (Random Forest ACC:89.9%, AUC:92.5%).",
    "published_date": "2023-11-23",
    "pdf_link": "https://arxiv.org/pdf/2312.00802v1",
    "paper_types": [
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Authentication",
      "subdomain": "Continuous Authentication",
      "specific_problem": "Behavioral biometrics using mouse dynamics (clickstream) for user verification and authentication",
      "attack_types": [
        "Impersonation",
        "Account takeover"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Decision Tree",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "K-Nearest Neighbors",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Random Forest",
        "specific": null,
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Balabit Mouse Challenge dataset",
        "type": "public",
        "domain": "user_interaction_logs",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Decision Tree (Scenario B, Point & Click)",
        "paper_reference": null,
        "metric": "ACC/AUC",
        "their_result": "ACC: 87.6%, AUC: 90.3%",
        "baseline_result": null
      },
      {
        "method_name": "K-Nearest Neighbors (Scenario B, Point & Click)",
        "paper_reference": null,
        "metric": "ACC/AUC",
        "their_result": "ACC: 99.3%, AUC: 99.9%",
        "baseline_result": null
      },
      {
        "method_name": "Random Forest (Scenario B, Point & Click)",
        "paper_reference": null,
        "metric": "ACC/AUC",
        "their_result": "ACC: 89.9%, AUC: 92.5%",
        "baseline_result": null
      },
      {
        "method_name": "Antal et al. (2018) Random Forest on Balabit",
        "paper_reference": "Antal et al., 2018 [5]",
        "metric": "ACC/AUC (average across users)",
        "their_result": null,
        "baseline_result": "ACC: 80.17%, AUC: 0.87 (avg). Highest per-user ACC 93% (user 7), AUC 0.97; lowest ACC 72% (user 8), AUC 0.80."
      },
      {
        "method_name": "Nakkabi et al. (2010) fuzzy classification",
        "paper_reference": "Nakkabi et al., 2010 [6]",
        "metric": "FAR/FRR",
        "their_result": null,
        "baseline_result": "FAR: 0%, FRR: 0.36% (requires >2000 mouse events)."
      },
      {
        "method_name": "Feher et al. (2012) RF framework",
        "paper_reference": "Feher et al., 2012 [7]",
        "metric": "EER",
        "their_result": null,
        "baseline_result": "EER: 1.01% (based on 30 actions)."
      },
      {
        "method_name": "Zheng et al. (2011) SVM (angle-based)",
        "paper_reference": "Zheng et al., 2011 [8]",
        "metric": "EER",
        "their_result": null,
        "baseline_result": "EER: 1.3% (20 mouse clicks)."
      },
      {
        "method_name": "Shen et al. (2012) SVM with pattern growth",
        "paper_reference": "Shen et al., 2012 [9]",
        "metric": "FAR/FRR",
        "their_result": null,
        "baseline_result": "FAR: 0.37%, FRR: 1.12%."
      },
      {
        "method_name": "Schulz (2006) Distance-based",
        "paper_reference": "Schulz, 2006 [10]",
        "metric": "EER",
        "their_result": null,
        "baseline_result": "EER: 24.3% (60 curves); EER: 11.2% (3600 curves)."
      },
      {
        "method_name": "Bours et al. (2009) maze task (distance metrics)",
        "paper_reference": "Bours et al., 2009 [11]",
        "metric": "EER",
        "their_result": null,
        "baseline_result": "EER: 26.8% (horizontal), 27.0% (vertical)."
      }
    ],
    "performance_metrics_used": [
      "Accuracy (ACC)",
      "Area Under the ROC Curve (AUC)",
      "False Acceptance Rate (FAR)",
      "False Rejection Rate (FRR)",
      "Equal Error Rate (EER)",
      "Receiver Operating Characteristic (ROC)",
      "True Positive Rate (TPR)",
      "True Negative Rate (TNR)",
      "False Positive Rate (FPR)",
      "False Negative Rate (FNR)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How well do standard classifiers (Decision Tree, KNN, Random Forest) perform for continuous authentication using mouse dynamics on the Balabit dataset?",
        "Which mouse action type (mouse movement, point and click, drag and drop) yields the best verification/authentication performance?"
      ],
      "gaps_identified": [
        "Recent research on mouse dynamics has shown promising results in identifying users; however, it has not yet reached an acceptable level of accuracy.",
        "Many prior approaches require a large number of mouse events/actions to achieve low error rates.",
        "Public datasets for mouse dynamics are limited in user count (Balabit has only 10 users), raising generalizability concerns."
      ],
      "limitations": [
        "Experiments conducted on a single dataset (Balabit) with only 10 users.",
        "No reported deployment or real-world online evaluation; results are from offline supervised experiments.",
        "Classifier and feature engineering choices limited to hand-crafted 39 features; no representation learning explored."
      ],
      "future_work": [],
      "motivation": "To empirically evaluate behavioral biometric continuous authentication using mouse dynamics and improve identification/verification accuracy, given that existing mouse-based biometrics have not yet reached acceptable accuracy levels.",
      "potential_research_ideas": [
        "Sequence modeling of raw mouse trajectories with deep learning (e.g., temporal CNNs, RNNs, Transformers) to learn richer behavioral embeddings beyond the 39 hand-crafted features.",
        "Online and incremental learning for per-user adaptation to capture behavioral drift over time.",
        "Multi-modal continuous authentication by fusing mouse dynamics with keystroke dynamics and application context.",
        "Domain adaptation and cross-device generalization to handle different hardware, resolutions, and OS settings.",
        "Self-supervised pretraining on unlabeled mouse event streams to improve data efficiency.",
        "Robustness studies and defenses against spoofing/synthetic mouse event injection and bot-driven impersonation.",
        "Calibrated thresholding and score normalization across users to reduce FRR while maintaining low FAR.",
        "Larger-scale dataset collection with diverse users and tasks; define standardized train/test protocols to enable fair benchmarking."
      ],
      "architectural_improvement_recommendations": [
        "Replace or complement DT/RF/KNN with gradient boosting (e.g., XGBoost/LightGBM) and calibrate probabilities (Platt scaling/Isotonic).",
        "Adopt per-user one-class models (e.g., One-Class SVM, Isolation Forest) for verification, then ensemble with global discriminative models.",
        "Use session-based splits to prevent temporal leakage; ensure strict user-disjoint or session-disjoint evaluations.",
        "Learn sequence embeddings using Transformer encoders over event sequences and aggregate at action/session level.",
        "Implement online learning with sliding windows and concept-drift detectors (e.g., ADWIN) for continuous adaptation.",
        "Augment data via trajectory perturbations (e.g., time warping, jitter) while preserving behavior semantics.",
        "Calibrate decision thresholds per-user using ROC analysis to balance FAR/FRR according to risk profiles."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "scikit-learn"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Empirical evaluation of Decision Tree, KNN, and Random Forest for continuous authentication using mouse dynamics on the Balabit dataset.",
      "Two evaluation scenarios introduced: (A) single user with all actions (MM, PC, DD) and (B) all users with a single action, analyzing action-specific performance.",
      "Finding that point-and-click (PC) action yields the highest accuracy and AUC across classifiers, with KNN achieving up to ACC 99.3% and AUC 99.9% in Scenario B.",
      "100% verification accuracy reported across all three classifiers in verification mode.",
      "Comprehensive reporting of ACC, AUC, FAR, FRR, EER, and ROC analyses for both verification and authentication stages."
    ]
  },
  {
    "arxiv_id": "2312.00034v1",
    "title": "Enhancing IoT Security via Automatic Network Traffic Analysis: The Transition from Machine Learning to Deep Learning",
    "authors": "Mounia Hamidouche; Eugeny Popko; Bassem Ouni",
    "abstract": "This work provides a comparative analysis illustrating how Deep Learning (DL) surpasses Machine Learning (ML) in addressing tasks within Internet of Things (IoT), such as attack classification and device-type identification. Our approach involves training and evaluating a DL model using a range of diverse IoT-related datasets, allowing us to gain valuable insights into how adaptable and practical these models can be when confronted with various IoT configurations. We initially convert the unstructured network traffic data from IoT networks, stored in PCAP files, into images by processing the packet data. This conversion process adapts the data to meet the criteria of DL classification methods. The experiments showcase the ability of DL to surpass the constraints tied to manually engineered features, achieving superior results in attack detection and maintaining comparable outcomes in device-type identification. Additionally, a notable feature extraction time difference becomes evident in the experiments: traditional methods require around 29 milliseconds per data packet, while DL accomplishes the same task in just 2.9 milliseconds. The significant time gap, DL's superior performance, and the recognized limitations of manually engineered features, presents a compelling call to action within the IoT community. This encourages us to shift from exploring new IoT features for each dataset to addressing the challenges of integrating DL into IoT, making it a more efficient solution for real-world IoT scenarios.",
    "published_date": "2023-11-20",
    "pdf_link": "https://arxiv.org/pdf/2312.00034v1",
    "paper_types": [
      "position",
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "IoT Security",
      "subdomain": "Intrusion Detection and Device-Type Identification",
      "specific_problem": "End-to-end attack detection (binary, 8-class, 34-class) and device-type identification (3 classes) from IoT PCAP traffic converted to images",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "CNN",
        "specific": null,
        "novel_contribution": "End-to-end image-based classifier operating directly on PCAP-to-image representations to perform both IoT attack detection and device-type identification with a single DL pipeline"
      },
      {
        "type": "primary",
        "category": "DNN",
        "specific": null,
        "novel_contribution": "Unified pipeline: PCAP parsing (Session + All, first 784 bytes) -> 23x23 grayscale image -> normalized tensor -> DNN classifier"
      },
      {
        "type": "baseline",
        "category": "SVM",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Decision Tree",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "k-NN",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Naive Bayes",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Rule-based",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Shallow Neural Network",
        "specific": null,
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "UNSW-Sydney 2018 (TMC)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CIC IoT 2022 (CIC22)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "IMC 2019 (IMC)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CIC IoT 2023 (CIC23)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Traditional ML pipeline with hand-crafted features (feature extraction stage)",
        "paper_reference": null,
        "metric": "Feature extraction time per packet",
        "their_result": "2.9 ms per packet (DL)",
        "baseline_result": "29 ms per packet (traditional methods)"
      },
      {
        "method_name": "Traditional ML pipeline with hand-crafted features (device identification setting)",
        "paper_reference": null,
        "metric": "Feature extraction time per packet",
        "their_result": "0.3 ms per packet (DL)",
        "baseline_result": "26 ms per packet (hand-crafted features)"
      },
      {
        "method_name": "Classical ML with hand-crafted features (e.g., Decision Tree, SVM, k-NN, Naive Bayes, Rule-based)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "feature extraction time per packet"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Laborious and dataset-specific feature engineering dominates prior IoT intrusion/device-type work",
        "Fragmentation of feature sets across IoT datasets hinders comparability and generalization",
        "Hand-crafted features struggle with large-scale IoT datasets due to complexity and computational burden",
        "IoT data is often unstructured (PCAP), creating barriers for end-to-end learning without conversion"
      ],
      "limitations": [
        "Interpretability of DL models is a challenge in IoT scenarios",
        "Deployment on resource-constrained IoT devices poses computation, energy, and real-time constraints",
        "Scalability, data privacy, adaptability to dynamic environments, and resilience remain open challenges"
      ],
      "future_work": [
        "Address interpretability for DL in IoT security",
        "Optimize DL for deployment on resource-constrained IoT devices with real-time constraints",
        "Tackle scalability and privacy for end-to-end DL across diverse IoT environments",
        "Improve adaptability to dynamic environments and resilience to environmental factors"
      ],
      "motivation": "Reduce labor-intensive, dataset-specific feature engineering and improve generalization and efficiency by adopting end-to-end DL that learns directly from raw IoT network traffic.",
      "potential_research_ideas": [
        "Design self-supervised or contrastive pretraining on large unlabeled PCAP corpora to improve generalization across IoT environments",
        "Develop domain adaptation and continual learning to handle distribution shifts across networks, devices, and time",
        "Combine PCAP-to-image with auxiliary flow/session metadata (e.g., timing, lengths, protocol tags) as multi-channel inputs",
        "Explore lightweight architectures (mobile CNNs, 1D CNNs over bytes) for on-device or edge inference",
        "Incorporate temporal modeling (CNN-LSTM or Transformers over sequences of packet-images) for richer behavior capture",
        "Investigate explainability methods tailored to byte-level or image representations of packets",
        "Evaluate robustness to adversarial manipulations of packet bytes and implement robust training",
        "Leverage federated or privacy-preserving learning across sites to address data siloing",
        "Study few-shot and open-set recognition to detect unseen devices and novel attack types",
        "Systematically compare alternative PCAP-to-representation encodings (e.g., protocol-aware layouts, learnable byte embeddings)"
      ],
      "architectural_improvement_recommendations": [
        "Replace fixed 23x23 grayscale with multi-channel inputs encoding bytes, lengths, timing, and protocol indicators",
        "Adopt modern lightweight CNN backbones (e.g., MobileNet-style) or efficient Transformers for small images",
        "Model sequences of packets/sessions explicitly with temporal attention or sequence encoders",
        "Use multi-task learning to jointly predict device-type and attack class, sharing a common backbone",
        "Apply extensive data augmentation for byte/image inputs and class-imbalance strategies",
        "Introduce calibration and uncertainty estimation for operational reliability"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://owl-light-racer.ngrok-free.app/mounia.hamidouche/iot-securedl",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": "Feature extraction time reported: 29 ms -> 2.9 ms per packet (abstract); also 26 ms -> 0.3 ms per packet (device identification context)",
      "deployment_challenges": [
        "Interpretability needs",
        "Resource constraints on IoT devices (compute, energy, real-time)",
        "Scalability across large, diverse IoT deployments",
        "Data privacy considerations",
        "Adaptability to dynamic environments",
        "Resilience to environmental factors"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Empirical comparison showing DL surpasses ML with hand-crafted features for IoT attack detection and remains comparable for device-type identification",
      "Unified end-to-end pipeline converting PCAP to 23x23 images and training a DNN classifier applicable across multiple IoT datasets and tasks",
      "Demonstrated significant reduction in feature extraction time (e.g., 29 ms to 2.9 ms per packet; 26 ms to 0.3 ms per packet)",
      "Released code, datasets, preprocessing scripts, and detection model"
    ]
  },
  {
    "arxiv_id": "2311.01532v1",
    "title": "VFCFinder: Seamlessly Pairing Security Advisories and Patches",
    "authors": "Trevor Dunlap; Elizabeth Lin; William Enck; Bradley Reaves",
    "abstract": "Security advisories are the primary channel of communication for discovered vulnerabilities in open-source software, but they often lack crucial information. Specifically, 63% of vulnerability database reports are missing their patch links, also referred to as vulnerability fixing commits (VFCs). This paper introduces VFCFinder, a tool that generates the top-five ranked set of VFCs for a given security advisory using Natural Language Programming Language (NL-PL) models. VFCFinder yields a 96.6% recall for finding the correct VFC within the Top-5 commits, and an 80.0% recall for the Top-1 ranked commit. VFCFinder generalizes to nine different programming languages and outperforms state-of-the-art approaches by 36 percentage points in terms of Top-1 recall. As a practical contribution, we used VFCFinder to backfill over 300 missing VFCs in the GitHub Security Advisory (GHSA) database. All of the VFCs were accepted and merged into the GHSA database. In addition to demonstrating a practical pairing of security advisories to VFCs, our general open-source implementation will allow vulnerability database maintainers to drastically improve data quality, supporting efforts to secure the software supply chain.",
    "published_date": "2023-11-02",
    "pdf_link": "https://arxiv.org/pdf/2311.01532v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Vulnerability Management",
      "specific_problem": "Automatically matching security advisories to vulnerability fixing commits (VFCs) / patch link recovery",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "CodeBERT",
        "novel_contribution": "Fine-tuned NL-PL model for (1) VFC identification (binary) and (2) OWASP Top-10 vulnerability type classification; per-file tokenization of commit message + git diff with commit-level aggregation."
      },
      {
        "type": "primary",
        "category": "Sentence Embeddings",
        "specific": "SentenceTransformers (all-mpnet-base-v2)",
        "novel_contribution": "Used to compute semantic similarity between advisory text and commit messages as a feature."
      },
      {
        "type": "primary",
        "category": "Gradient Boosted Trees",
        "specific": "XGBoost (learning-to-rank)",
        "novel_contribution": "Ranks commits within the commit window using five features: VFC fix probability, VFC type, advisory–commit semantic similarity, commit rank location in window, and presence of CVE/GHSA ID."
      },
      {
        "type": "baseline",
        "category": "Logistic Regression",
        "specific": null,
        "novel_contribution": "Used by FixFinder in prior work; included for comparison."
      },
      {
        "type": "baseline",
        "category": "Learning-to-Rank (Neural)",
        "specific": "RankNet",
        "novel_contribution": "Used by PatchScout in prior work; included for comparison."
      },
      {
        "type": "baseline",
        "category": "Other",
        "specific": "VCMatch (100 features; three ML models)",
        "novel_contribution": "Top prior approach; included for comparison."
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Learning-to-Rank"
    ],
    "datasets": [
      {
        "name": "GHSA advisories with known patch links (evaluation dataset)",
        "type": "public",
        "domain": "security_advisories",
        "link": "https://github.com/advisories",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "GHSA advisories without patch links (deployed backfill subset, ~300 advisories)",
        "type": "public",
        "domain": "security_advisories",
        "link": "https://github.com/advisories",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "NVD (used for fine-tuning labels)",
        "type": "public",
        "domain": "security_advisories",
        "link": "https://nvd.nist.gov",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "OSV (used for fine-tuning labels and advisory schema)",
        "type": "public",
        "domain": "security_advisories",
        "link": "https://osv.dev",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "VulasDB (used for fine-tuning labels)",
        "type": "public",
        "domain": "security_advisories",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "VCMatch dataset (for cross-evaluation reference)",
        "type": "public",
        "domain": "security_advisories",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "VCMatch",
        "paper_reference": "VCMatch [52]",
        "metric": "Top-1 recall / Top-5 recall (on VFCFinder's GHSA dataset)",
        "their_result": "Top-1 recall 80.0%, Top-5 recall 96.6%",
        "baseline_result": "Top-1 recall 44.0%, Top-5 recall 70.1%"
      },
      {
        "method_name": "FixFinder",
        "paper_reference": "FixFinder [20]",
        "metric": "Top-1 recall / Top-5 recall (reported on single Java dataset)",
        "their_result": null,
        "baseline_result": "Top-1 recall 65.1%, Top-5 recall 77.7%"
      },
      {
        "method_name": "PatchScout",
        "paper_reference": "PatchScout [50]",
        "metric": "Top-1 recall / Top-5 recall (reported across C/C++ and a single Java project)",
        "their_result": null,
        "baseline_result": "Top-1 recall 69.5%, Top-5 recall 85.4%"
      },
      {
        "method_name": "VCMatch (reported on its own dataset)",
        "paper_reference": "VCMatch [52]",
        "metric": "Top-1 recall / Top-5 recall (reported across 10 OSS projects)",
        "their_result": null,
        "baseline_result": "Top-1 recall 88.9%, Top-5 recall 95.33%"
      }
    ],
    "performance_metrics_used": [
      "Top-1 recall",
      "Top-5 recall"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can an NL-PL approach with a small, intuitive feature set accurately rank VFCs for a given advisory across many languages and projects?",
        "Does contiguous sampling (by version windows) provide a more realistic evaluation for VFC ranking than non-contiguous random sampling?",
        "Can the approach generalize to real-world GHSA advisories missing patch links and support large-scale backfilling?"
      ],
      "gaps_identified": [
        "Lack of representative training/evaluation data: prior work evaluated rigorously maintained projects with strict contributing guidelines, not representative of the broader ecosystem.",
        "Non-contiguous data sampling in prior work can inflate recall by leaking temporal signals (e.g., CVE file date vs commit time).",
        "High model complexity and many features in prior work increase overfitting risk and reduce generalization.",
        "Security advisories frequently miss patch links (63% missing) and commit messages are often low quality, hindering automated matching.",
        "Branch-based commit selection is misaligned with tag-based versioning used for releases."
      ],
      "limitations": [
        "Assumes the VFC lies within the commit window between fixed and prior version tags; approximately 6% of cases fall outside due to factors like unreliable version data.",
        "OWASP Top-10 VFC type classification is coarse due to sparse training data, avoiding fine-grained CWE classification.",
        "Sequence length limits (e.g., 512 tokens) require truncation/chunking of large diffs, which may omit context.",
        "Backports and multiple versions per advisory complicate one-to-one matching and may require identifying multiple VFCs.",
        "Approach depends on accurate and consistent git tagging and semantic version sorting to define windows."
      ],
      "future_work": [],
      "motivation": "63% of vulnerability database reports are missing patch links; improving advisory-to-patch pairing enhances SCA tools, patch verification, and supply chain security.",
      "potential_research_ideas": [
        "Model multi-VFC and backport relationships explicitly (graph over versions/branches) to identify all related fixes per advisory.",
        "Incorporate code-aware diff representations (e.g., AST-level or token-level edit encoders) to improve VFC probability beyond raw diffs.",
        "Leverage retrieval-augmented generation or LLMs to rewrite sparse commit messages and enrich features for similarity scoring.",
        "Active learning with curator-in-the-loop to iteratively refine the ranker on hard cases and new projects.",
        "Temporal and repository-metadata features (issue links, PR reviews, maintainers) under a contiguous sampling regime, carefully debiased.",
        "Cross-database linking (NVD, OSV, GHSA) via entity resolution to improve advisory normalization and reduce version/tag mismatches.",
        "Assess adversarial robustness to intentionally misleading commit messages and harden features accordingly."
      ],
      "architectural_improvement_recommendations": [
        "Replace cosine similarity with cross-encoder scoring (e.g., NL-PL cross-encoder) for advisory–commit pairs to capture fine-grained interactions.",
        "Use XGBoost ranker with calibrated probabilities or pairwise/listwise ranking losses tailored to the ranking objective.",
        "Multi-task fine-tuning of CodeBERT jointly on VFC identification and type classification to share NL-PL representations.",
        "Language-specific adapters or LoRA modules per language to better generalize across the nine languages without overfitting.",
        "Integrate git topology features (e.g., PR merge status, file ownership, code churn) while maintaining low feature count."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/s3c2/vfcfinder",
      "frameworks": [
        "HuggingFace Transformers",
        "PyTorch",
        "SentenceTransformers",
        "XGBoost"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Vulnerability database curation (GitHub Security Advisory database)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Dependence on accurate git tags and semantic versioning to define commit windows.",
        "Handling backports and multiple fixed versions per advisory.",
        "Poor or misleading commit messages can reduce semantic similarity signal.",
        "Unreliable version data in advisories can place VFCs outside expected windows."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces VFCFinder, an NL-PL based ranking system that uses only five features to match advisories to VFCs across nine programming languages.",
      "Proposes a contiguous, tag-based sampling and evaluation methodology that better reflects real analyst workflows compared to prior non-contiguous sampling.",
      "Achieves \"96.6% recall\" for Top-5 and \"80.0%\" for Top-1 on GHSA advisories with known patch links; on ~300 missing-link GHSA advisories, achieves Top-5 \"96.1%\" and Top-1 \"81.2%\".",
      "Outperforms the state-of-the-art (VCMatch) by \"36 percentage points\" in Top-1 recall on the VFCFinder dataset (80.0% vs 44.0%).",
      "Backfilled over 300 missing VFCs in GHSA; all were accepted and merged by GitHub’s security team.",
      "Releases an open-source implementation to assist vulnerability database maintainers."
    ]
  },
  {
    "arxiv_id": "2312.10669v1",
    "title": "Analisis Eksploratif Dan Augmentasi Data NSL-KDD Menggunakan Deep Generative Adversarial Networks Untuk Meningkatkan Performa Algoritma Extreme Gradient Boosting Dalam Klasifikasi Jenis Serangan Siber",
    "authors": "K. P. Santoso; F. A. Madany; H. Suryotrisongko",
    "abstract": "This study proposes the implementation of Deep Generative Adversarial Networks (GANs) for augmenting the NSL-KDD dataset. The primary objective is to enhance the efficacy of eXtreme Gradient Boosting (XGBoost) in the classification of cyber-attacks on the NSL-KDD dataset. As a result, the method proposed in this research achieved an accuracy of 99.53% using the XGBoost model without data augmentation with GAN, and 99.78% with data augmentation using GAN.",
    "published_date": "2023-12-17",
    "pdf_link": "https://arxiv.org/pdf/2312.10669v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Multiclass classification of network intrusion attack types on NSL-KDD with severe class imbalance; evaluate whether GAN-based augmentation improves XGBoost performance",
      "attack_types": [
        "normal",
        "DDoS",
        "ipsweep",
        "neptune",
        "nmap",
        "portsweep",
        "satan",
        "smurf"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Boosted Trees",
        "specific": "XGBoost",
        "novel_contribution": "Used as the main classifier; paired with GAN-based tabular data augmentation to mitigate class imbalance and improve NSL-KDD multiclass classification (accuracy improved from 99.5362% to 99.7886%)."
      },
      {
        "type": "primary",
        "category": "GAN",
        "specific": "Deep feed-forward GAN with LeakyReLU and BatchNorm",
        "novel_contribution": "Class-wise synthetic data generation for NSL-KDD features to balance minority classes; 4 generator blocks (FFN+LeakyReLU 0.2+BatchNorm), 3 discriminator blocks (FFN+LeakyReLU 0.2), trained 5000 epochs with binary cross-entropy loss."
      },
      {
        "type": "baseline",
        "category": "Anomaly Detection",
        "specific": "Isolation Forest",
        "novel_contribution": "Used for exploratory anomaly analysis to identify rare/atypical attack vectors; not part of the main classifier."
      }
    ],
    "learning_paradigm": [
      "Supervised (XGBoost classification)",
      "Adversarial generative modeling (GAN) for data augmentation",
      "Imbalanced learning"
    ],
    "datasets": [
      {
        "name": "NSL-KDD",
        "type": "public",
        "domain": "network_traffic",
        "link": "https://www.unb.ca/cic/datasets/nsl.html",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "XGBoost (no GAN augmentation)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "99.7886% (with GAN augmentation)",
        "baseline_result": "99.5362% (without GAN)"
      },
      {
        "method_name": "CNN (Hadi et al., 2022)",
        "paper_reference": "Radhi Hadi, M., & Saher Mohammed, A. (2022). A Novel Approach to Network Intrusion Detection System using Deep Learning for SDN.",
        "metric": "Accuracy",
        "their_result": "99.78% (this work; NSL-KDD)",
        "baseline_result": "98.63% (reported)"
      },
      {
        "method_name": "DNN (Hadi et al., 2022)",
        "paper_reference": "Radhi Hadi, M., & Saher Mohammed, A. (2022).",
        "metric": "Accuracy",
        "their_result": "99.78% (this work; NSL-KDD)",
        "baseline_result": "98.53% (reported)"
      },
      {
        "method_name": "RNN (Hadi et al., 2022)",
        "paper_reference": "Radhi Hadi, M., & Saher Mohammed, A. (2022).",
        "metric": "Accuracy",
        "their_result": "99.78% (this work; NSL-KDD)",
        "baseline_result": "98.13% (reported)"
      },
      {
        "method_name": "LSTM (Hadi et al., 2022)",
        "paper_reference": "Radhi Hadi, M., & Saher Mohammed, A. (2022).",
        "metric": "Accuracy",
        "their_result": "99.78% (this work; NSL-KDD)",
        "baseline_result": "98.04% (reported)"
      },
      {
        "method_name": "GRU (Hadi et al., 2022)",
        "paper_reference": "Radhi Hadi, M., & Saher Mohammed, A. (2022).",
        "metric": "Accuracy",
        "their_result": "99.78% (this work; NSL-KDD)",
        "baseline_result": "97.78% (reported)"
      },
      {
        "method_name": "E-CNN with GAN (Soleymanzadeh & Kashef, 2022)",
        "paper_reference": "Raha Soleymanzadeh, & Rasha Kashef. (2022). A Stable GAN Architecture for Network Intrusion Detection.",
        "metric": "Accuracy",
        "their_result": "99.78% (this work; NSL-KDD)",
        "baseline_result": "86.36% (reported)"
      },
      {
        "method_name": "E-CNN with GAN (Soleymanzadeh & Kashef, 2022)",
        "paper_reference": "Raha Soleymanzadeh, & Rasha Kashef. (2022).",
        "metric": "F1-score",
        "their_result": "Macro F1 up to 0.9990 class-wise; overall accuracy 99.7886% (this work)",
        "baseline_result": "85.81% (reported)"
      },
      {
        "method_name": "Stacked Autoencoder + Random Forest (Shone et al., 2018)",
        "paper_reference": "Shone, N., et al. (2018). A Deep Learning Approach to Network Intrusion Detection. IEEE TETCI.",
        "metric": "Accuracy",
        "their_result": "99.78% (this work; NSL-KDD)",
        "baseline_result": "85.42% (reported)"
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "Precision",
      "Recall",
      "F1-score"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can GAN-based data augmentation alleviate NSL-KDD class imbalance and improve XGBoost multiclass cyber-attack classification performance?",
        "Which features are most influential for classifying normal vs attack traffic in NSL-KDD?",
        "Can anomaly analysis (Isolation Forest) surface rare or atypical attack vectors within NSL-KDD?"
      ],
      "gaps_identified": [
        "NSL-KDD suffers significant class imbalance that harms minority-class performance.",
        "Traditional oversampling/undersampling have limitations (overfitting via SMOTE or loss of information via undersampling).",
        "NSL-KDD may not adequately represent recent/modern attack patterns."
      ],
      "limitations": [
        "Despite higher overall accuracy, precision for underrepresented classes requires further examination to avoid excessive false positives.",
        "Anomaly detection with Isolation Forest requires assessing false positive rates to ensure detected anomalies correspond to rare attacks, not misclassifications."
      ],
      "future_work": [
        "Investigate precision and false positive behavior for minority classes post-augmentation.",
        "Analyze feature interactions to refine classification performance.",
        "Validate anomalies detected by Isolation Forest and quantify false positives."
      ],
      "motivation": "Escalating cyber threats and costs combined with NSL-KDD class imbalance motivate exploring GAN-based augmentation to boost classifier efficacy for intrusion detection.",
      "potential_research_ideas": [
        "Evaluate the GAN+XGBoost pipeline on modern intrusion datasets (e.g., UNSW-NB15, CIC-IDS2017/2018, CSE-CIC-IDS2018, TON_IoT) to test generalization beyond NSL-KDD.",
        "Adopt class-conditional GANs (AC-GAN), CTGAN/TVAE, or WGAN-GP to better model tabular feature distributions and minority classes.",
        "Integrate synthetic sample quality control via TSTR/TSNR protocols, classifier two-sample tests, or tabular fidelity/utility metrics before augmenting training data.",
        "Combine augmentation with cost-sensitive/weighted XGBoost or focal loss-style reweighting for further minority recall gains without harming precision.",
        "Use representation learning (autoencoders or supervised embeddings) prior to XGBoost to capture non-linear feature interactions; compare against pure tree methods.",
        "Benchmark against advanced imbalanced learning baselines (SMOTE variants, ADASYN, Borderline-SMOTE, k-means SMOTE) under identical splits with macro-F1/PR-AUC.",
        "Assess robustness to distribution shift and concept drift with incremental/online learning and streaming evaluation.",
        "Quantify and mitigate risks of overfitting to synthetic data and potential data leakage; perform ablation on augmentation ratios per class.",
        "Incorporate explainability (e.g., SHAP) to validate that augmented data preserves causal/semantic relationships among features.",
        "Study privacy risks (e.g., membership inference) from GAN-generated tabular data; consider DP-GANs."
      ],
      "architectural_improvement_recommendations": [
        "Switch to WGAN-GP or CTGAN for tabular data; add class conditioning (AC-GAN) to control sample quality per attack class.",
        "Use Bayesian hyperparameter optimization for XGBoost and add class weights/scale_pos_weight per class.",
        "Calibrate classifier outputs (Platt/Isotonic) and apply class-specific thresholds optimized for F1 or cost-sensitive objectives.",
        "Filter synthetic samples using a discriminator acceptance threshold or density estimators to improve augmentation fidelity.",
        "Ensemble multiple augmenters (CTGAN + CopulaGAN) and stacking classifiers (XGBoost + LightGBM + RF) for robustness.",
        "Perform feature selection/regularization informed by SHAP to reduce spurious correlations introduced by augmentation."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Risk of model bias toward majority classes and elevated false positives on minority attacks, even after augmentation.",
        "Need to validate anomaly detections to avoid operational noise from false positives.",
        "NSL-KDD’s outdated distribution may not reflect modern network traffic and attack vectors, limiting direct deployability."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Applied Deep GAN-based tabular augmentation per attack class to address NSL-KDD class imbalance.",
      "Improved XGBoost performance from 99.5362% to 99.7886% accuracy; substantially increased recall for 'nmap' from 0.7480 to 0.9906.",
      "Conducted exploratory data analysis including feature importance (XGBoost gain) and class-wise distribution insights; identified top features ('flag', 'dst_bytes', 'dst_host_srv_serror_rate').",
      "Performed anomaly analysis with Isolation Forest to surface rare/atypical attack vectors.",
      "Specified GAN architecture and training configuration (4 generator blocks, 3 discriminator blocks, LeakyReLU 0.2, BatchNorm, 5000 epochs, BCE loss)."
    ]
  },
  {
    "arxiv_id": "2312.02490v1",
    "title": "Constrained Twin Variational Auto-Encoder for Intrusion Detection in IoT Systems",
    "authors": "Phai Vu Dinh; Quang Uy Nguyen; Dinh Thai Hoang; Diep N. Nguyen; Son Pham Bao; Eryk Dutkiewicz",
    "abstract": "Intrusion detection systems (IDSs) play a critical role in protecting billions of IoT devices from malicious attacks. However, the IDSs for IoT devices face inherent challenges of IoT systems, including the heterogeneity of IoT data/devices, the high dimensionality of training data, and the imbalanced data. Moreover, the deployment of IDSs on IoT systems is challenging, and sometimes impossible, due to the limited resources such as memory/storage and computing capability of typical IoT devices. To tackle these challenges, this article proposes a novel deep neural network/architecture called Constrained Twin Variational Auto-Encoder (CTVAE) that can feed classifiers of IDSs with more separable/distinguishable and lower-dimensional representation data. Additionally, in comparison to the state-of-the-art neural networks used in IDSs, CTVAE requires less memory/storage and computing power, hence making it more suitable for IoT IDS systems. Extensive experiments with the 11 most popular IoT botnet datasets show that CTVAE can boost around 1% in terms of accuracy and Fscore in detection attack compared to the state-of-the-art machine learning and representation learning methods, whilst the running time for attack detection is lower than 2E-6 seconds and the model size is lower than 1 MB. We also further investigate various characteristics of CTVAE in the latent space and in the reconstruction representation to demonstrate its efficacy compared with current well-known methods.",
    "published_date": "2023-12-05",
    "pdf_link": "https://arxiv.org/pdf/2312.02490v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "IoT Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Representation learning for network-based IoT botnet intrusion detection under resource constraints",
      "attack_types": [
        "IoT botnet",
        "anomaly"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "VAE (Autoencoder)",
        "specific": "Constrained Twin Variational Auto-Encoder (CTVAE)",
        "novel_contribution": "Supervised VAE-based architecture with Encoder–Hermaphrodite–Decoder; constrains latent variables to class-specific separable Gaussian distributions; uses only the Decoder at inference to output a low-dimensional 'reconstruction representation' that is more separable and lightweight for IDS classifiers."
      },
      {
        "type": "primary",
        "category": "VAE (Autoencoder)",
        "specific": "Twin Variational Auto-Encoder (TVAE)",
        "novel_contribution": "Unsupervised precursor; converts stochastic latent z to a deterministic reconstruction representation via a 'hermaphrodite' module and decoder; forms the basis for CTVAE."
      },
      {
        "type": "baseline",
        "category": "Autoencoder",
        "specific": "AE",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "VAE (Autoencoder)",
        "specific": "VAE",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "VAE (Autoencoder)",
        "specific": "β-VAE",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "VAE (Autoencoder)",
        "specific": "VQ-VAE",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Autoencoder",
        "specific": "MAE (Multi-distribution Autoencoder)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "VAE (Autoencoder)",
        "specific": "MVAE (Multi-distribution Variational Autoencoder)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Autoencoder (Convolutional/Sparse)",
        "specific": "CSAE / CSAEC",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Ensemble Tree",
        "specific": "XGBoost",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Linear Model",
        "specific": "Logistic Regression (LR)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "SVM",
        "specific": "Support Vector Machine (SVM)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Decision Tree",
        "specific": "Decision Tree (DT)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Ensemble Tree",
        "specific": "Random Forest (RF)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Clustering",
        "specific": "K-Means with silhouette analysis (auxiliary for class distribution estimation)",
        "novel_contribution": "Used within CTVAE to determine reasonable distributions of major classes to address class imbalance."
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "11 IoT botnet datasets (names not specified in provided text)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "AE",
        "paper_reference": null,
        "metric": "Accuracy, F1-score",
        "their_result": "“CTVAE can boost around 1% in terms of accuracy and Fscore … compared to the state-of-the-art machine learning and representation learning methods”",
        "baseline_result": null
      },
      {
        "method_name": "VAE",
        "paper_reference": null,
        "metric": "Accuracy, F1-score",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "β-VAE",
        "paper_reference": null,
        "metric": "Accuracy, F1-score",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "VQ-VAE",
        "paper_reference": null,
        "metric": "Accuracy, F1-score",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "MAE",
        "paper_reference": null,
        "metric": "Accuracy, F1-score",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "MVAE",
        "paper_reference": null,
        "metric": "Accuracy, F1-score",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "CSAE / CSAEC",
        "paper_reference": null,
        "metric": "Accuracy, F1-score",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "XGBoost",
        "paper_reference": null,
        "metric": "Accuracy, F1-score",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Logistic Regression",
        "paper_reference": null,
        "metric": "Accuracy, F1-score",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "SVM",
        "paper_reference": null,
        "metric": "Accuracy, F1-score",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Decision Tree",
        "paper_reference": null,
        "metric": "Accuracy, F1-score",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Random Forest",
        "paper_reference": null,
        "metric": "Accuracy, F1-score",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "F1-score",
      "Running time per sample",
      "Model size"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "AE cannot enforce a desired latent distribution to minimize redundancy for large-scale IoT IDS datasets.",
        "VAE latent representations are stochastic/unstable for downstream classifiers and often underperform AEs, especially on multi-class datasets.",
        "Prior supervised AE/VAE variants add penalty terms that trade off with reconstruction, degrading classifier accuracy.",
        "Handling high-dimensional, heterogeneous, and imbalanced IoT data with resource-constrained deployment remains challenging."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Address IoT IDS challenges: heterogeneous/high-dimensional and imbalanced data, zero-day/anomaly detection needs, and strict device resource constraints, by learning more separable, lower-dimensional representations with a tiny, fast model suitable for IoT deployment.",
      "potential_research_ideas": [
        "Extend CTVAE to online/streaming learning for evolving IoT traffic and concept drift.",
        "Incorporate supervised contrastive or metric learning losses in the latent space to further separate classes and improve zero-shot generalization.",
        "Adopt mixture-of-Gaussians or learnable priors (e.g., normalizing flows) to better model complex multi-class latent distributions.",
        "Federated or split learning versions of CTVAE for privacy-preserving cross-device training on IoT endpoints.",
        "Adversarially robust training of the representation (e.g., latent adversarial perturbations) to resist evasion attacks.",
        "Multi-modal extensions combining network traffic with device logs or system calls for richer representations.",
        "Automated compression (quantization/pruning/distillation) targeting microcontroller-class devices while preserving separability.",
        "Explainable variants that map reconstruction features back to interpretable network features for analyst trust."
      ],
      "architectural_improvement_recommendations": [
        "Replace fixed Gaussian assumptions with learnable mixture priors or flow-based posteriors to capture class manifolds.",
        "Add a supervised contrastive head and joint loss with the decoder reconstruction to stabilize class separation.",
        "Introduce center loss or prototype loss to tighten intra-class clusters in the reconstruction representation.",
        "Use lightweight convolutional or temporal encoders for sequence-aware traffic features and better efficiency.",
        "Apply knowledge distillation from CTVAE to tiny decoders for ultra-low memory deployment.",
        "Incorporate class-imbalance-aware training (e.g., focal loss or reweighting) alongside the K-Means/silhouette heuristic."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": "Reported: model size < 1 MB; inference time < 2e-6 seconds per sample (hardware unspecified)."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "IoT edge devices (resource-constrained)",
      "scalability_discussed": true,
      "inference_time": "Lower than 2E-6 seconds per sample",
      "deployment_challenges": [
        "Limited memory/storage and compute on IoT devices",
        "Heterogeneity of IoT devices and data formats",
        "Imbalanced datasets and scarcity of attack samples",
        "Generalization to zero-day attacks"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes CTVAE, a supervised architecture that produces more separable, lower-dimensional reconstruction representations for IDS classifiers.",
      "Evaluates CTVAE on 11 IoT botnet datasets, showing ≈1% improvements in accuracy and F1 over state-of-the-art ML and representation learning methods with model size <1 MB and per-sample detection time <2e-6 s.",
      "Leverages K-Means and silhouette values within CTVAE to handle class imbalance and support abnormal data detection.",
      "Analyzes latent and reconstruction spaces to justify CTVAE’s efficacy compared with AE/VAE-based methods."
    ]
  },
  {
    "arxiv_id": "2311.16143v1",
    "title": "Ransomware Detection and Classification using Machine Learning",
    "authors": "Kavitha Kunku; ANK Zaman; Kaushik Roy",
    "abstract": "Vicious assaults, malware, and various ransomware pose a cybersecurity threat, causing considerable damage to computer structures, servers, and mobile and web apps across various industries and businesses. These safety concerns are important and must be addressed immediately. Ransomware detection and classification are critical for guaranteeing rapid reaction and prevention. This study uses the XGBoost classifier and Random Forest (RF) algorithms to detect and classify ransomware attacks. This approach involves analyzing the behaviour of ransomware and extracting relevant features that can help distinguish between different ransomware families. The models are evaluated on a dataset of ransomware attacks and demonstrate their effectiveness in accurately detecting and classifying ransomware. The results show that the XGBoost classifier, Random Forest Classifiers, can effectively detect and classify different ransomware attacks with high accuracy, thereby providing a valuable tool for enhancing cybersecurity.",
    "published_date": "2023-11-05",
    "pdf_link": "https://arxiv.org/pdf/2311.16143v1",
    "paper_types": [
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Malware Analysis",
      "subdomain": "Ransomware Detection",
      "specific_problem": "Supervised detection (binary classification) of ransomware versus legitimate software using static PE/header features; exploratory claim of classifying ransomware families",
      "attack_types": [
        "ransomware"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Gradient Boosted Trees",
        "specific": "XGBoost classifier",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Random Forest",
        "specific": "Random Forest classifier",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Ransomware Detection Dataset (Kaggle)",
        "type": "public",
        "domain": "malware_binaries",
        "link": "https://www.kaggle.com/datasets/amdj3dax/ransomware-detection-data-set",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "MLRD-Machine-Learning-Ransomware-Detection (GitHub mirror)",
        "type": "public",
        "domain": "malware_binaries",
        "link": "https://github.com/securycore/MLRD-Machine-Learning-Ransomware-Detection",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Random Forest (Molina et al.)",
        "paper_reference": "Molina et al., IEEE TNSM 2022 [20]",
        "metric": "Accuracy",
        "their_result": "99.70% (Random Forest), 99.61% (XGBoost)",
        "baseline_result": "94.92%"
      },
      {
        "method_name": "Random Forest (Bae et al.)",
        "paper_reference": "Bae et al., Concurrency and Computation 2020 [21]",
        "metric": "Accuracy",
        "their_result": "99.70% (Random Forest), 99.61% (XGBoost)",
        "baseline_result": "98.65%"
      },
      {
        "method_name": "Markov Chain + Random Forest (Hwang et al.)",
        "paper_reference": "Hwang et al., Wireless Personal Communications 2020 [22]",
        "metric": "Accuracy",
        "their_result": "99.70% (Random Forest), 99.61% (XGBoost)",
        "baseline_result": "97.30%"
      },
      {
        "method_name": "Logistic Regression (Rawshan et al.)",
        "paper_reference": "Rawshan et al. (Mowri et al.) 2022 [6]",
        "metric": "Accuracy",
        "their_result": "99.70% (Random Forest), 99.61% (XGBoost)",
        "baseline_result": "99.15%"
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "Precision",
      "Recall",
      "F1 Score"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can supervised machine learning models accurately detect ransomware attacks in real time?",
        "Which features of ransomware behavior and static attributes are most useful to distinguish ransomware from legitimate software?",
        "Between XGBoost and Random Forest, which performs better for ransomware detection and classification?"
      ],
      "gaps_identified": [
        "Conventional signature-based malware detection dominates but struggles with evolving ransomware; behavior/feature-based ML approaches are needed.",
        "Need for accurate and rapid detection and classification (including attribution to ransomware families) to support timely response."
      ],
      "limitations": [
        "“It should be noted that the performance of the models may vary depending on the data set used and the specific parameters chosen for the algorithm.”",
        "Potential overfitting concern with XGBoost relative to Random Forest is acknowledged.",
        "Evaluation is on a single public dataset with an 80/20 split; real-time deployment and cross-dataset generalization are not assessed."
      ],
      "future_work": [
        "“In the future, the Convolutional Neural Network (CNN) based algorithms will be tested with the latest and larger scale datasets for detecting and classifying ransomware families.”"
      ],
      "motivation": "Ransomware is increasingly common and damaging; the authors aim to build an ML-based system for real-time detection and classification to aid victims and cybersecurity professionals.",
      "potential_research_ideas": [
        "Build a multi-class and hierarchical classifier for ransomware family attribution using richer static and dynamic features.",
        "Combine static PE/header features with dynamic behavior (API call sequences, filesystem and registry activity, network telemetry) for multimodal detection.",
        "Evaluate cross-time and cross-dataset generalization with temporal splits and holdout corpora to measure robustness to evolving ransomware.",
        "Apply explainable ML (e.g., SHAP) to interpret feature contributions, aiding analyst triage and model debugging.",
        "Develop online/streaming detection with concept drift handling and incremental learning for real-time environments.",
        "Investigate adversarial robustness against evasion attacks (feature manipulation/packing) and propose defenses.",
        "Use federated learning across organizations to improve models without sharing raw samples (privacy-preserving collaboration).",
        "Self-supervised or representation learning on large unlabeled PE corpora to improve downstream ransomware detection with limited labels."
      ],
      "architectural_improvement_recommendations": [
        "Perform systematic hyperparameter optimization (e.g., Bayesian search) with stratified k-fold cross-validation and fixed random seeds.",
        "Calibrate predicted probabilities (Platt scaling/Isotonic) and tune thresholds for desired precision-recall trade-offs.",
        "Introduce cost-sensitive learning or class weighting if class imbalance exists; assess macro/micro-averaged metrics.",
        "Augment feature set with additional PE header entropy, section statistics, import/export graph features, and string features; consider feature selection (e.g., mutual information).",
        "Adopt a two-stage pipeline: fast anomaly filter (One-Class SVM/Isolation Forest) followed by supervised classifier for high-precision verdicts.",
        "Add explainability via SHAP to understand model decisions and detect spurious correlations.",
        "Evaluate and harden against packing/obfuscation by adding unpacking/preprocessing or robust features (e.g., byte n-grams with CNN/1D-ResNet).",
        "Conduct time-split and cross-dataset evaluations; report ROC/PR AUC and calibration curves.",
        "Consider stacking/ensembling (e.g., combine XGBoost, RF, and a lightweight DNN) for improved robustness."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "scikit-learn",
        "XGBoost"
      ],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Empirical evaluation of XGBoost and Random Forest for ransomware detection on a public Kaggle dataset (62,485 samples, 18 features).",
      "Achieved high accuracy: Random Forest 99.70% and XGBoost 99.61% with strong precision/recall/F1.",
      "Comparison against results reported in prior works, claiming superior accuracy.",
      "Provided basic dataset analysis (feature correlations, observations of feature values distinguishing legitimate vs malware).",
      "Outlined a simple training pipeline with 80/20 split and standard evaluation metrics."
    ]
  },
  {
    "arxiv_id": "2311.17447v1",
    "title": "Learning-driven Zero Trust in Distributed Computing Continuum Systems",
    "authors": "Ilir Murturi; Praveen Kumar Donta; Victor Casamayor Pujol; Andrea Morichetta; Schahram Dustdar",
    "abstract": "Converging Zero Trust (ZT) with learning techniques can solve various operational and security challenges in Distributed Computing Continuum Systems (DCCS). Implementing centralized ZT architecture is seen as unsuitable for the computing continuum (e.g., computing entities with limited connectivity and visibility, etc.). At the same time, implementing decentralized ZT in the computing continuum requires understanding infrastructure limitations and novel approaches to enhance resource access management decisions. To overcome such challenges, we present a novel learning-driven ZT conceptual architecture designed for DCCS. We aim to enhance ZT architecture service quality by incorporating lightweight learning strategies such as Representation Learning (ReL) and distributing ZT components across the computing continuum. The ReL helps to improve the decision-making process by predicting threats or untrusted requests. Through an illustrative example, we show how the learning process detects and blocks the requests, enhances resource access control, and reduces network and computation overheads. Lastly, we discuss the conceptual architecture, processes, and provide a research agenda.",
    "published_date": "2023-11-29",
    "pdf_link": "https://arxiv.org/pdf/2311.17447v1",
    "paper_types": [
      "position",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Identity and Access Management",
      "subdomain": "Zero Trust Architecture",
      "specific_problem": "Decentralized Zero Trust authorization and access decisioning in distributed computing continuum (edge–fog–cloud) using lightweight learning on activity logs",
      "attack_types": [
        "unauthorized access",
        "insider threats",
        "lateral movement"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Probabilistic Graphical Model",
        "specific": "Bayesian Network Structure Learning (BNSL)",
        "novel_contribution": "Embedding BNSL into ZT (at PEP/PE) to learn from active logs and make local pre-filtering decisions, reducing load on the policy engine; demonstrates querying CPTs for allow/block decisions and causal effect analysis."
      },
      {
        "type": "primary",
        "category": "Representation Learning",
        "specific": null,
        "novel_contribution": "Use of lightweight representation learning on ZT activity logs to predict whether a request is authentic or fraudulent within DCCS."
      },
      {
        "type": "primary",
        "category": "Causal Analysis / Explainable Modeling",
        "specific": "Causal effect analysis via BN CPTs",
        "novel_contribution": "Uses learned BN to compute causal effects of log attributes on the 'allow' action to explain decisions."
      },
      {
        "type": "primary",
        "category": "Federated/Distributed Learning",
        "specific": "Gradient sharing (conceptual)",
        "novel_contribution": "Proposes distributing a shared model across the computing continuum with local training and sharing of gradients instead of raw data; conceptual only, not empirically evaluated."
      }
    ],
    "learning_paradigm": [
      "Unsupervised / Generative (probabilistic modeling on logs)",
      "Representation Learning",
      "Federated / Distributed training (proposed)",
      "Online / Incremental updates (proposed capability of BN)"
    ],
    "datasets": [
      {
        "name": "Synthetic active logs (illustrative example)",
        "type": "synthetic",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "log-likelihood",
      "Bayesian Information Criterion (BIC)",
      "allow/block probability from BN querying"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How can Zero Trust be implemented in a decentralized manner for distributed computing continuum systems under constraints of limited resources, connectivity, and visibility?",
        "Can lightweight representation learning on activity logs improve ZT decision-making (predict and block untrusted requests) and reduce network/computation overhead?",
        "How should ZT components (PE, PA, PEP, learning, resource management) be placed and orchestrated across cloud–fog–edge for low latency and resilience?",
        "How can learning models be configured, distributed, and updated across heterogeneous devices (e.g., via shared models and gradient sharing) while minimizing communication overhead and misuse of logs?",
        "What runtime and orchestration mechanisms (e.g., WebAssembly-based, self-adaptive orchestrator) best support learning-driven ZT in DCCS?"
      ],
      "gaps_identified": [
        "Existing ML-for-security works focus on attack/anomaly detection (often for SDN or IoT) rather than Zero Trust decisioning in the computing continuum.",
        "Centralized ZT architectures are unsuitable for the continuum due to limited connectivity and visibility; decentralized ZT remains underexplored.",
        "Learning strategies have not been considered within ZT for DCCS; existing ML approaches can be resource/time intensive and ill-suited for edge devices.",
        "Lack of approaches that integrate learning-driven access decisions with resource management and deployment across edge–fog–cloud."
      ],
      "limitations": [
        "Evaluation uses a small synthetic active-log dataset (33 entries) as an illustrative example; no real-world dataset evaluation.",
        "No comparative baselines or quantitative benchmarking against alternative methods.",
        "Resource management/orchestration aspects are treated as future work and are not implemented.",
        "Security, privacy, and adversarial robustness are discussed only conceptually; no empirical validation.",
        "Scalability and latency are argued conceptually; no end-to-end system measurements."
      ],
      "future_work": [
        "Develop and evaluate the self-adaptive, resilient runtime and orchestration for resource management (discovery, placement, provisioning, adaptive monitoring) across the continuum.",
        "Realize and evaluate distributed/federated training of the shared model with gradient sharing across cloud–fog–edge.",
        "Extend resource management functionalities and edge functions; explore deployment patterns (cloud vs edge-cloud hybrid).",
        "Broaden the learning features/attributes in active logs (networks, workloads, visibility/analytics, orchestration, etc.) and validate at scale."
      ],
      "motivation": "Perimeter-based security is insufficient for highly dynamic device–edge–cloud environments; a decentralized Zero Trust architecture augmented with lightweight learning is needed to improve access decisions and reduce overhead under limited resources, connectivity, and visibility.",
      "potential_research_ideas": [
        "Federated Bayesian Network structure/parameter learning with secure aggregation and differential privacy for ZT logs.",
        "Online/continual BN learning with concept drift detection to adapt to evolving behaviors and threats in DCCS.",
        "Hybrid risk scoring that combines BN-based causal reasoning with sequence models (e.g., temporal point processes) for time-aware access decisions.",
        "Policy–model co-design: integrate learned risk estimates with formal policy engines (e.g., OPA/Rego) and calibrate decision thresholds.",
        "Adversarially robust structure learning and query processing (e.g., robust score functions, outlier-resistant CPT estimation).",
        "Benchmarking learning-driven ZT on large, realistic multi-tenant datasets; creation of a standardized ZT activity-log benchmark.",
        "Trusted execution (TEEs) for PEP-side inference and model updates to harden against compromise.",
        "Formal verification of safety properties (no over-privilege, minimal lateral movement) under learned decision policies."
      ],
      "architectural_improvement_recommendations": [
        "Implement federated BN training with secure aggregation and differential privacy; add client selection and compression to reduce bandwidth.",
        "Add drift detection and active learning to trigger targeted data collection or human-in-the-loop review when uncertainty is high.",
        "Calibrate BN output probabilities (e.g., via isotonic regression) and adopt risk-based adaptive thresholds at PEPs.",
        "Introduce a hierarchical ZT control plane (edge PEP micro-controllers with local models; regional fog PDPs; cloud PE) with fallback strategies for connectivity loss.",
        "Package learning/inference as WebAssembly modules for portability, with a lightweight policy–inference API at PEP.",
        "Integrate with a latency-aware, resource-constrained scheduler for PEP/learning placement; use telemetry to auto-scale where request rates spike.",
        "Instrument comprehensive auditing/telemetry to evaluate decision latency, false positives/negatives, and policy impacts; define KPIs.",
        "Harden data pipelines with schema validation, provenance tracking, and secure logging to prevent data poisoning."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": "Not specified; authors argue BNSL time complexity is 'not time intensive' and can run on edge devices; propose gradient sharing to reduce communication."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Distributed computing continuum (cloud–fog–edge). Policy Administrator and core Resource Management in cloud; PEPs and learning components distributed to edge/fog; shared model across continuum.",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Limited resources on edge devices (compute, storage, networking).",
        "Limited/intermittent connectivity between PEPs and central PDP/PE.",
        "Limited visibility across distributed devices (monitoring difficulties).",
        "Heterogeneity and uncertainty in edge/fog environments.",
        "Communication overhead and energy constraints for model updates.",
        "Maintaining security posture when policy engine connectivity is unstable."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": true,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces a novel learning-driven Zero Trust conceptual architecture for DCCS, extending NIST ZT with two components: Learning and Resource Management.",
      "Considers Bayesian Network Structure Learning on historical activity logs to predict authenticity/fraud and assist PEP/PE decisions.",
      "Illustrative example showing the learning process can detect and block requests, enhance access control, and reduce network/computation overheads.",
      "Discusses deployment and resource management considerations (e.g., WebAssembly-based runtime, edge functions, cloud vs edge–cloud hybrid placements).",
      "Outlines a research agenda for learning-driven Zero Trust in distributed computing continuum systems."
    ]
  },
  {
    "arxiv_id": "2312.17270v1",
    "title": "Anticipated Network Surveillance -- An extrapolated study to predict cyber-attacks using Machine Learning and Data Analytics",
    "authors": "Aviral Srivastava; Dhyan Thakkar; Sharda Valiveti; Pooja Shah; Gaurang Raval",
    "abstract": "Machine learning and data mining techniques are utiized for enhancement of the security of any network. Researchers used machine learning for pattern detection, anomaly detection, dynamic policy setting, etc. The methods allow the program to learn from data and make decisions without human intervention, consuming a huge training period and computation power. This paper discusses a novel technique to predict an upcoming attack in a network based on several data parameters. The dataset is continuous in real-time implementation. The proposed model comprises dataset pre-processing, and training, followed by the testing phase. Based on the results of the testing phase, the best model is selected using which, event class which may lead to an attack is extracted. The event statistics are used for attack",
    "published_date": "2023-12-27",
    "pdf_link": "https://arxiv.org/pdf/2312.17270v1",
    "paper_types": [
      "empirical_analysis",
      "survey",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Threat Intelligence and Intrusion Prediction",
      "specific_problem": "Anticipating and predicting upcoming cyber-attacks in a network from network event/log data using an ML-driven architecture",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Pipeline/Architecture",
        "specific": "Model selection pipeline for event classification and attack anticipation",
        "novel_contribution": "Proposed threat intelligence architecture: data preprocessing, multi-model training/testing, best-model selection, event-class extraction, and event statistics for attack prediction"
      },
      {
        "type": "primary",
        "category": "Decision Tree",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Random Forest",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Boosting",
        "specific": "AdaBoost",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Boosting",
        "specific": "Gradient Boosting (unspecified)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Neural Network",
        "specific": "Artificial Neural Network (ANN)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Probabilistic",
        "specific": "Naive Bayes",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Probabilistic",
        "specific": "Gaussian Process Classifier",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "SVM",
        "specific": "Support Vector Classifier",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Linear Model",
        "specific": "Logistic Regression",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Graphical Model",
        "specific": "Attack Graph / Factor Graph",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Probabilistic",
        "specific": "Bayesian Network",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Probabilistic",
        "specific": "Markov Models (Markov Chain, HMM)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Time Series",
        "specific": "ARMA / GARMA / FARIMA / GARCH",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Boosting",
        "specific": "XGBoost",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Boosting",
        "specific": "LightGBM",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "DARPA 1998",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "DARPA 1999",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "DARPA 2000",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "KDD'99 (KDD Cup 1999)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "NSL-KDD",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "UNB ISCX 2012",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "UNSW-NB15",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "AFDA (system call traces, 2013)",
        "type": "public",
        "domain": "system_call_traces",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CICIDS 2017 (CSE-CIC-IDS2017)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CICIDS 2018 (CSE-CIC-IDS2018)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "VERIS database",
        "type": "public",
        "domain": "threat_reports",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Hackmageddon",
        "type": "public",
        "domain": "threat_reports",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Honeynet live data",
        "type": "proprietary",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Attack Graph (Cao et al.)",
        "paper_reference": "[34]",
        "metric": "Accuracy",
        "their_result": null,
        "baseline_result": "75% Accuracy (live)"
      },
      {
        "method_name": "Attack Graph - RTECA (Ramaki et al.)",
        "paper_reference": "[51]",
        "metric": "Accuracy",
        "their_result": null,
        "baseline_result": "95% Accuracy (DARPA 2000)"
      },
      {
        "method_name": "Bayesian Network (Okutan et al.)",
        "paper_reference": "[26]",
        "metric": "Accuracy",
        "their_result": null,
        "baseline_result": "63%–99% Accuracy (live; includes non-conventional signals)"
      },
      {
        "method_name": "Hidden Markov Model (Farhadi et al.)",
        "paper_reference": "[40]",
        "metric": "Accuracy",
        "their_result": null,
        "baseline_result": "81.33%–98.3% Accuracy (DARPA 2000)"
      },
      {
        "method_name": "Markov Model for exploitability (Abraham and Nair)",
        "paper_reference": "[41]",
        "metric": null,
        "their_result": null,
        "baseline_result": "Exploitability analysis; vulnerability life-cycle (no single metric reported)"
      },
      {
        "method_name": "Time Series (GARCH/FARIMA) (Zhan)",
        "paper_reference": "[44]",
        "metric": "Accuracy",
        "their_result": null,
        "baseline_result": "≈88% Accuracy with 1-hour prediction window"
      },
      {
        "method_name": "XGBoost (Khan et al.)",
        "paper_reference": "[47]",
        "metric": "Accuracy, F1-Score",
        "their_result": null,
        "baseline_result": "71.7% Accuracy; 0.67 F1 on UNSW-NB15"
      },
      {
        "method_name": "Multiple algorithms (R. Zuech et al.)",
        "paper_reference": "[49]",
        "metric": "F1-Score",
        "their_result": null,
        "baseline_result": "93.3 F1 on UNSW-NB15"
      },
      {
        "method_name": "LightGBM/XGBoost (J.L. Leevy et al.)",
        "paper_reference": "[53]",
        "metric": "Accuracy, F1-Score",
        "their_result": null,
        "baseline_result": "95% Accuracy; 88.905 F1 on CSE-CICIDS2017"
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "Precision",
      "Recall",
      "F1-Score",
      "Processing time"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Legacy datasets (DARPA series, KDDCup’99) are outdated and do not reflect modern traffic; UNSW-NB15 and CICIDS better mimic contemporary traffic.",
        "Manual processing of IDS alerts is slow and error-prone; dynamic, automated prediction is needed.",
        "Traditional IDS focuses on detection of observed events, whereas attack anticipation requires predicting future attacks and vulnerabilities.",
        "Time-series prediction works often consider limited attack types and may not provide precise identification of future attacks; they often provide aggregate security state forecasts."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Scale and sophistication of network attacks and unmanageable IDS alerts necessitate automated threat intelligence and proactive attack anticipation using ML and data analytics.",
      "potential_research_ideas": [
        "Integrate temporal point-process or sequence models (e.g., Transformer-based time-series, Hawkes processes) to predict not only attack occurrence but also attack type and timing.",
        "Leverage graph neural networks over dynamic attack graphs to jointly reason about host vulnerabilities, alerts, and lateral movement for multi-step attack prediction.",
        "Fuse OSINT/social media sentiment with network telemetry for early-warning signals in a multi-modal prediction model.",
        "Develop online/streaming learning with concept-drift adaptation for evolving network behaviors and emerging attack tactics.",
        "Calibrate predicted probabilities and decision thresholds for actionable early alerts with controlled false positives.",
        "Conduct cross-dataset generalization studies and domain adaptation to ensure robustness from lab datasets to real networks."
      ],
      "architectural_improvement_recommendations": [
        "Augment the pipeline with a dedicated temporal modeling stage (e.g., sequence classifier or temporal point-process) after event-class classification to capture inter-event dynamics.",
        "Integrate explainability tooling (e.g., SHAP/Anchors) to attribute predictions to features and produce human-interpretable ‘point of attack’ justifications.",
        "Adopt AutoML for model selection and hyperparameter optimization under latency constraints; include cost-sensitive learning to penalize false negatives.",
        "Implement a streaming feature store and incremental/online training to support real-time continuous datasets.",
        "Add anomaly/novelty detection (one-class or deep SVDD) to flag zero-day patterns outside known event classes.",
        "Use probabilistic graphical models or calibrated ensembles to aggregate uncertainty across models for robust alerting."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Manual alert processing is slow and error-prone.",
        "Identifying the point of attack is critical and non-trivial.",
        "Prediction and mitigation complexity grows with network size.",
        "Selecting and tuning suitable learning techniques for real-time intrusion prediction."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Exhaustive study of techniques and datasets for threat intelligence and attack prediction.",
      "Novel threat intelligence architecture to anticipate network attacks.",
      "Performance summary of different methodologies to implement the architecture.",
      "Analysis of the proposed architecture on multiple datasets (public network intrusion datasets).",
      "Method to anticipate the next attack by extracting key features and analyzing the space of possible events."
    ]
  },
  {
    "arxiv_id": "2312.07885v1",
    "title": "RAT: Reinforcement-Learning-Driven and Adaptive Testing for Vulnerability Discovery in Web Application Firewalls",
    "authors": "Mohammadhossein Amouei; Mohsen Rezvani; Mansoor Fateh",
    "abstract": "Due to the increasing sophistication of web attacks, Web Application Firewalls (WAFs) have to be tested and updated regularly to resist the relentless flow of web attacks. In practice, using a brute-force attack to discover vulnerabilities is infeasible due to the wide variety of attack patterns. Thus, various black-box testing techniques have been proposed in the literature. However, these techniques suffer from low efficiency. This paper presents Reinforcement-Learning-Driven and Adaptive Testing (RAT), an automated black-box testing strategy to discover injection vulnerabilities in WAFs. In particular, we focus on SQL injection and Cross-site Scripting, which have been among the top ten vulnerabilities over the past decade. More specifically, RAT clusters similar attack samples together. It then utilizes a reinforcement learning technique combined with a novel adaptive search algorithm to discover almost all bypassing attack patterns efficiently. We compare RAT with three state-of-the-art methods considering their objectives. The experiments show that RAT performs 33.53% and 63.16% on average better than its counterparts in discovering the most possible bypassing payloads and reducing the number of attempts before finding the first bypassing payload when testing well-configured WAFs, respectively.",
    "published_date": "2023-12-13",
    "pdf_link": "https://arxiv.org/pdf/2312.07885v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Web Application Security",
      "subdomain": "WAF testing and evaluation",
      "specific_problem": "Black-box testing to discover injection vulnerabilities in Web Application Firewalls",
      "attack_types": [
        "SQL injection (SQLi)",
        "Cross-site Scripting (XSS)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Reinforcement Learning",
        "specific": "Decayed ε-greedy policy (multi-armed bandit style selection)",
        "novel_contribution": "Uses decayed ε-greedy to guide cluster selection and balance exploration/exploitation during WAF testing"
      },
      {
        "type": "primary",
        "category": "Adaptive Search",
        "specific": null,
        "novel_contribution": "Novel adaptive search algorithm to rapidly discover bypassing payloads within selected clusters using only blocked attempts early on"
      },
      {
        "type": "primary",
        "category": "Clustering",
        "specific": "Hierarchical clustering",
        "novel_contribution": "Clusters tokens and then payloads to reduce feature space and exploit the tendency of bypassing payloads to cluster"
      },
      {
        "type": "primary",
        "category": "Representation Learning",
        "specific": "Word2Vec; Deep Embedding Network (DEN)",
        "novel_contribution": "Maps n-gram tokens to vectors with Word2Vec; uses DEN to cluster attack samples"
      },
      {
        "type": "primary",
        "category": "Feature Engineering",
        "specific": "n-gram tokenization; IDF weighting; binary encoding by token-cluster presence",
        "novel_contribution": "Lightweight n-gram based features that model sophisticated attack patterns with fewer features than prior work"
      },
      {
        "type": "baseline",
        "category": "Decision Tree (for comparison in prior work)",
        "specific": "ML-Driven E (from literature)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Adaptive Random Testing",
        "specific": "ART4SQLi; XSSART (from literature)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Reinforcement Learning",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "SQLi dataset",
        "type": "",
        "domain": "web_attack_payloads",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      },
      {
        "name": "XSS dataset",
        "type": "",
        "domain": "web_attack_payloads",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      }
    ],
    "baselines": [
      {
        "method_name": "ML-Driven E",
        "paper_reference": "Appelt et al.",
        "metric": "Total number of bypassing payloads discovered within a limited number of requests",
        "their_result": "“RAT can discover an average of 33.53% more bypassing attack than ML-Driven E within a limited number of requests.”",
        "baseline_result": null
      },
      {
        "method_name": "ART4SQLi",
        "paper_reference": "Zhang et al.",
        "metric": "Number of attempts/blocked payloads before finding the first bypassing payload",
        "their_result": "“Our adaptive search algorithm is an average of 61.43% faster than ART4SQLi when facing well-configured WAFs.”",
        "baseline_result": "“ART4SQLi could discover the first bypassing payload about 38.70% faster than RAT in testing a WAF with massive vulnerabilities.”"
      },
      {
        "method_name": "XSSART",
        "paper_reference": "Lv et al.",
        "metric": "Number of attempts before finding the first bypassing payload",
        "their_result": "“RAT is an average of 64.90% faster than XSSART in finding the first bypassing payload.”",
        "baseline_result": null
      },
      {
        "method_name": "Random Fuzzer",
        "paper_reference": null,
        "metric": "Used as a simple baseline method",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "number_of_bypassing_payloads_within_request_budget",
      "attempts_before_first_bypass",
      "requests_consumed",
      "relative_speedup_percentage"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Black-box testing techniques for WAFs are inefficient; many vulnerabilities remain undiscovered",
        "Prior learning-based methods (e.g., ML-Driven variants) require many observations and an initial set of passed and blocked attacks before learning becomes effective",
        "Feature spaces built from derivation tree subtrees explode combinatorially for longer payloads, harming practicality and efficiency",
        "Decision trees suffer from local minima/instability; ensembles add computational overhead with limited gains",
        "Payload spaces are sparse; bypassing payloads tend to cluster, but prior methods underutilize this structure"
      ],
      "limitations": [
        "ε-greedy policy can suffer from local optima and instability; results depend on cluster selection order (mitigated via decay, but not eliminated)",
        "Less effective than ART4SQLi when the target WAF has massive vulnerabilities (slower to first bypass in that setting)",
        "Targeted primarily at signature-based WAFs; not evaluated against ML-based WAFs",
        "Focuses on SQLi and XSS; not validated on other web attack types"
      ],
      "future_work": [
        "Combine RAT with adversarial mutation methods (e.g., WAF-A-MoLE) to further boost effectiveness and efficiency",
        "Evaluate and adapt RAT to ML-based WAFs",
        "Extend to additional web attack classes beyond SQLi and XSS"
      ],
      "motivation": "Design a practical automated black-box testing approach that efficiently uncovers WAF vulnerabilities despite the vast variety of attack patterns and high testing cost.",
      "potential_research_ideas": [
        "Integrate contextual bandits or Thompson sampling for cluster/payload selection to reduce susceptibility to local optima",
        "Combine RAT’s cluster-guided search with adversarial payload mutation to jointly explore structural and token-level transformations",
        "Transfer learning/meta-learning across WAFs to warm-start testing on unseen targets using knowledge from previous campaigns",
        "Use contrastive or transformer-based sequence embeddings for payloads to improve clustering and feature quality over Word2Vec + n-grams",
        "Develop adaptive request budgeting policies that allocate testing effort across clusters based on online estimates of bypass probability and diversity"
      ],
      "architectural_improvement_recommendations": [
        "Replace pure ε-greedy with UCB or Thompson sampling to better balance exploration–exploitation",
        "Adopt contextual bandits leveraging cluster features (e.g., token-IDF statistics) for informed selection",
        "Enhance the embedding with pretrained subword/byte-level models and contrastive fine-tuning on payload similarity",
        "Incorporate active feature selection to dynamically prune/expand token features per cluster",
        "Parallelize the adaptive search across clusters with shared statistics and early stopping rules"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "High testing cost due to the large variety of attack patterns",
        "Sparse payload spaces necessitating efficient search strategies",
        "Potential instability from local optima in selection policies",
        "Differences in performance across WAF configurations (well-configured vs. highly vulnerable)"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Use of n-gram tokenization with Word2Vec embeddings to model sophisticated attack patterns with significantly fewer features than prior ML-Driven E, improving efficiency",
      "A method to cluster similar attack payloads and evaluate the effects of clustering on efficiency and feature reduction",
      "A reinforcement-learning-driven (decayed ε-greedy) and novel adaptive search technique that enhances efficiency of black-box WAF testing"
    ]
  },
  {
    "arxiv_id": "2310.11325v1",
    "title": "Detection of Malicious DNS-over-HTTPS Traffic: An Anomaly Detection Approach using Autoencoders",
    "authors": "Sergio Salinas Monroy; Aman Kumar Gupta; Garrett Wahlstedt",
    "abstract": "To maintain the privacy of users' web browsing history, popular browsers encrypt their DNS traffic using the DNS-over-HTTPS (DoH) protocol. Unfortunately, encrypting DNS packets prevents many existing intrusion detection systems from using plaintext domain names to detect malicious traffic. In this paper, we design an autoencoder that is capable of detecting malicious DNS traffic by only observing the encrypted DoH traffic. Compared to previous works, the proposed autoencoder looks for anomalies in DoH traffic, and thus can detect malicious traffic that has not been previously observed, i.e., zero-day attacks. We run extensive experiments to evaluate the performance of our proposed autoencoder and compare it to that of other anomaly detection algorithms, namely, local outlier factor, one-class support vector machine, isolation forest, and variational autoencoders. We find that our proposed autoencoder achieves the highest detection performance, with a median F-1 score of 99\\% over several types of malicious traffic.",
    "published_date": "2023-10-17",
    "pdf_link": "https://arxiv.org/pdf/2310.11325v1",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Anomaly-based detection of malicious DNS-over-HTTPS (DoH) traffic without accessing plaintext DNS payloads",
      "attack_types": [
        "Domain Generation Algorithm (DGA) C2 discovery over DoH",
        "DNS tunneling/data exfiltration over DoH",
        "Zero-day malicious DoH traffic"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Autoencoder",
        "specific": null,
        "novel_contribution": "Privacy-preserving anomaly detection using a feed-forward autoencoder trained only on benign DoH flow statistics; per-DoH-server modeling to detect zero-day malicious DoH traffic"
      },
      {
        "type": "baseline",
        "category": "Isolation Forest",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Local Outlier Factor",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "One-Class SVM",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Variational Autoencoder",
        "specific": null,
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Unsupervised",
      "Anomaly detection (one-class)"
    ],
    "datasets": [
      {
        "name": "Authors' DoH traffic dataset (benign + DGA-generated malicious)",
        "type": "public",
        "domain": "network_traffic (DNS-over-HTTPS flows and statistics)",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "MontazeriShatoori et al. [21] DoH tunneling datasets",
        "type": "public",
        "domain": "network_traffic (DoH tunneling)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Isolation Forest",
        "paper_reference": null,
        "metric": null,
        "their_result": "\"up to a median F1-score of 99%\" for the proposed autoencoder; higher than baselines across metrics",
        "baseline_result": null
      },
      {
        "method_name": "Local Outlier Factor (LOF)",
        "paper_reference": null,
        "metric": null,
        "their_result": "\"up to a median F1-score of 99%\" for the proposed autoencoder; higher than baselines across metrics",
        "baseline_result": null
      },
      {
        "method_name": "One-Class SVM (OCSVM)",
        "paper_reference": null,
        "metric": null,
        "their_result": "\"up to a median F1-score of 99%\" for the proposed autoencoder; higher than baselines across metrics",
        "baseline_result": null
      },
      {
        "method_name": "Variational Autoencoder (VAE)",
        "paper_reference": null,
        "metric": null,
        "their_result": "\"up to a median F1-score of 99%\" for the proposed autoencoder; higher than baselines across metrics",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "F1-score",
      "Accuracy",
      "Area Under the ROC Curve (AUC)",
      "Precision",
      "Recall"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can an autoencoder trained only on benign DoH traffic detect malicious DoH traffic (including zero-day) using encrypted-flow statistics?",
        "How does the proposed autoencoder compare to other anomaly detection algorithms (LOF, OCSVM, Isolation Forest, VAE) on various malicious DoH types?",
        "Can a privacy-preserving approach (no plaintext DNS) achieve high detection performance?"
      ],
      "gaps_identified": [
        "Most prior malicious DoH detection uses supervised classification requiring malicious samples, limiting zero-day detection.",
        "Prior DoH works focus mainly on tunneling and ignore DGA-based abuse.",
        "Existing autoencoder-based anomaly detection has not targeted DoH traffic and used datasets without DoH flows."
      ],
      "limitations": [
        "Assumes networks approve only a few DoH servers and traffic can be filtered by destination DoH server.",
        "Assumes malware uses one of the approved public DoH servers; malware using non-approved servers is out of scope.",
        "Detection relies on statistical flow features; no access to plaintext DNS data."
      ],
      "future_work": [],
      "motivation": "DoH encrypts DNS payloads, preventing IDS from inspecting domains; need a privacy-preserving method that can detect previously unseen malicious DoH traffic.",
      "potential_research_ideas": [
        "Extend from per-DoH-server models to a unified or multi-domain model with domain adaptation across different DoH resolvers.",
        "Incorporate temporal/sequential modeling of DoH query timings (e.g., LSTM/Temporal CNN/Transformers) to better capture DGA/tunneling dynamics.",
        "Develop online/continual learning and concept-drift detection to adapt to evolving benign DoH usage patterns.",
        "Combine reconstruction-based anomaly scores with calibrated statistical models (e.g., extreme value theory) for thresholding and confidence estimation.",
        "Integrate multi-modal features such as TLS fingerprinting (JA3/JA4), flow directions, and host-level context while preserving privacy.",
        "Use self-supervised/contrastive pretraining on benign DoH flows to improve representations for anomaly detection.",
        "Investigate federated or on-prem privacy-preserving training to deploy across multiple organizations without sharing raw traffic."
      ],
      "architectural_improvement_recommendations": [
        "Evaluate denoising and sparse autoencoders, and compare with contractive or β-VAEs for more robust embeddings.",
        "Adopt sequence autoencoders or temporal convolutional networks over flow sequences rather than single-flow statistics.",
        "Ensemble anomaly detectors (autoencoder + Isolation Forest/LOF) with score-level fusion to reduce false positives.",
        "Apply attention mechanisms to highlight influential features and aid explainability of anomalies.",
        "Calibrate thresholds per host and per DoH server using robust statistics or EVT.",
        "Leverage metric learning/contrastive loss on benign data to tighten normality manifold."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Local network gateway (home LAN or enterprise network) monitoring approved DoH servers",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Requires maintaining a list of approved DoH servers and routing/labeling flows per resolver for model selection.",
        "Potential concept drift in benign DoH usage patterns may require periodic retraining or adaptive thresholds.",
        "Attackers may attempt traffic-shaping to mimic benign DoH flow statistics."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": true,
      "fairness_bias": false
    },
    "contributions": [
      "Design of a privacy-preserving anomaly detection autoencoder for malicious DoH traffic, capable of zero-day detection.",
      "Creation of a new DoH dataset with benign and DGA-generated malicious traffic; publicly available on Kaggle [30].",
      "Extensive evaluation against LOF, Isolation Forest, One-Class SVM, and Variational Autoencoders under various hyperparameters.",
      "Reported up to a median F1-score of 99%, outperforming other evaluated anomaly detection models."
    ]
  },
  {
    "arxiv_id": "2310.13079v1",
    "title": "Critical Path Prioritization Dashboard for Alert-driven Attack Graphs",
    "authors": "Sònia Leal Díaz; Sergio Pastrana; Azqa Nadeem",
    "abstract": "Although intrusion alerts can provide threat intelligence regarding attacker strategies, extracting such intelligence via existing tools is expensive and time-consuming. Earlier work has proposed SAGE, which generates attack graphs from intrusion alerts using unsupervised sequential machine learning. This paper proposes a querying and prioritization-enabled visual analytics dashboard for SAGE. The dashboard has three main components: (i) a Graph Explorer that presents a global view of all attacker strategies, (ii) a Timeline Viewer that correlates attacker actions chronologically, and (iii) a Recommender Matrix that highlights prevalent critical alerts via a MITRE ATT&CK-inspired attack stage matrix. We describe the utility of the proposed dashboard using intrusion alerts collected from a distributed multi-stage team-based attack scenario. We evaluate the utility of the dashboard through a user study. Based on the responses of a small set of security practitioners, we find that the dashboard is useful in depicting attacker strategies and attack progression, but can be improved in terms of usability.",
    "published_date": "2023-10-19",
    "pdf_link": "https://arxiv.org/pdf/2310.13079v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Security Operations & Monitoring",
      "subdomain": "Intrusion Alert Analytics / Attack Graph Analysis",
      "specific_problem": "Prioritization and interactive exploration of critical attack paths from IDS alert-driven attack graphs to support SOC triage and threat intelligence extraction",
      "attack_types": [
        "Data Exfiltration",
        "Data Manipulation",
        "Network DoS",
        "Host Discovery",
        "Privilege Escalation (Root)",
        "Web Application Exploits (e.g., CodeRed, ColdFusion)"
      ]
    },
    "ml_techniques": [
      {
        "type": "baseline",
        "category": "Sequential Probabilistic Automata",
        "specific": "Suffix-based Probabilistic Deterministic Finite Automaton (S-PDFA) within SAGE",
        "novel_contribution": "Used to learn alert-driven attack graphs from intrusion alerts without prior expert knowledge; this paper builds a dashboard on top of SAGE outputs"
      },
      {
        "type": "primary",
        "category": "Heuristic Scoring / Rule-based",
        "specific": "Urgency score based on severity weights and normalized prevalence",
        "novel_contribution": "MITRE ATT&CK-inspired Recommender Matrix that prioritizes Micro AIS using urgency score = severity_weight * normalized_prevalence, with analyst-tunable weights and thresholds"
      }
    ],
    "learning_paradigm": [
      "Unsupervised",
      "Heuristic"
    ],
    "datasets": [
      {
        "name": "CPTC 2018 Intrusion Alerts (Team-based multi-stage attack scenario)",
        "type": "private",
        "domain": "ids_alerts",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can a unified, interactive dashboard (Graph Explorer, Timeline Viewer, Recommender Matrix) improve analysts’ ability to understand attacker strategies and attack progression from IDS alerts?",
        "Does the Recommender Matrix effectively prioritize critical alerts by balancing severity and prevalence?",
        "Is the proposed dashboard usable and useful for security practitioners in realistic multi-stage attack scenarios?"
      ],
      "gaps_identified": [
        "“it is infeasible and time-consuming for security analysts to visualize each AG separately for finding global patterns.”",
        "“SAGE does not prioritize critical attack paths … that might need the urgent attention of security analysts.”",
        "“The AG nodes are not linked back to the intrusion alert signatures, omitting crucial details for understanding the vulnerabilities exploited by the attackers.”",
        "Lack of interactive filtering, temporal correlation, and consolidated cross-objective view in prior SAGE outputs; existing visual tools often expensive/time-consuming to extract threat intelligence."
      ],
      "limitations": [
        "“Based on the responses of a small set of security practitioners, we find that the dashboard is useful … but can be improved in terms of usability.”",
        "Preliminary virtual survey with a small number of practitioners; limited external validity.",
        "Operationalization demonstrated on one CPTC dataset subset (Team 1); generalizability to other environments untested.",
        "Automated report generation capability noted but “left as future work.”"
      ],
      "future_work": [
        "Automated report generation via the existing API endpoints (explicitly stated).",
        "Broader, controlled user studies with larger and more diverse analyst populations to assess usability and effectiveness.",
        "Evaluation on additional datasets and in live SOC environments; integration with SIEM pipelines.",
        "Refinements to interaction design and usability of the dashboard components."
      ],
      "motivation": "Reduce SOC alert fatigue by providing querying and prioritization-enabled visual analytics over alert-driven attack graphs to extract actionable threat intelligence more efficiently.",
      "potential_research_ideas": [
        "Learn severity weights and urgency thresholds from historical incident outcomes (e.g., supervised or reinforcement learning for prioritization).",
        "Integrate streaming/online SAGE to support near-real-time attack graph updates and continuous prioritization.",
        "Apply graph learning (e.g., GNN-based path ranking) to identify most critical paths and junctions beyond frequency/heuristics.",
        "Personalize prioritization via analyst profiles and active learning with human-in-the-loop feedback loops.",
        "Fuse heterogeneous telemetry (host logs, EDR, cloud logs) with IDS alerts to enrich nodes and improve context/precision.",
        "Automated narrative generation (LLM-based) to summarize selected paths and produce incident-ready reports.",
        "Add anomaly detection for rare-but-severe tactics/techniques using unsupervised sequence modeling on episodes."
      ],
      "architectural_improvement_recommendations": [
        "Replace/augment SQLite with a graph database (e.g., Neo4j) for scalable path queries and subgraph filtering.",
        "Implement incremental/streaming ingestion and incremental S-PDFA updates to handle live alert feeds.",
        "Augment Recommender Matrix with learned ranking models (learning-to-rank) that incorporate time decay, host criticality, and threat intel signals (STIX/TAXII).",
        "Use WebGL-based graph rendering (e.g., Sigma.js) and server-side graph layout for very large graphs; add progressive loading and level-of-detail rendering.",
        "Introduce community detection/path clustering to collapse similar strategies; add path templates and motif detection.",
        "Add role-based access control, multi-user annotations, and provenance tracking for analytical workflows.",
        "Export/import mappings to ATT&CK Navigator, STIX/TAXII, and SIEM connectors (e.g., Splunk/Elastic)."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "Django",
        "Flask",
        "vis.js (Network)",
        "D3 (timelines-chart)",
        "SQLite",
        "Bootstrap 5",
        "HTML5/Twig"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Usability issues with interactive components reported by practitioners; need for improved interaction design.",
        "Handling large, complex graphs (hundreds of nodes) may challenge client-side rendering and analyst cognition.",
        "Integration with live SOC/SIEM pipelines and data governance (permissions, formats) not addressed.",
        "Dataset availability/privacy for operational environments not discussed."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "A web-based critical path prioritization dashboard for alert-driven attack graphs with three components: Graph Explorer, Timeline Viewer, and Recommender Matrix.",
      "A MITRE ATT&CK-inspired Recommender Matrix computing an urgency score that balances severity and prevalence, with analyst-tunable weights and thresholds.",
      "Operationalization on a distributed multi-stage CPTC 2018 alert dataset; demonstration of actionable insights (e.g., exfiltration strategies, temporal coordination).",
      "Preliminary user study with security practitioners indicating usefulness for depicting strategies and progression, with noted usability improvements needed."
    ]
  },
  {
    "arxiv_id": "2310.16263v1",
    "title": "Enhancing Large Language Models for Secure Code Generation: A Dataset-driven Study on Vulnerability Mitigation",
    "authors": "Jiexin Wang; Liuwen Cao; Xitong Luo; Zhiping Zhou; Jiayuan Xie; Adam Jatowt; Yi Cai",
    "abstract": "Large language models (LLMs) have brought significant advancements to code generation, benefiting both novice and experienced developers. However, their training using unsanitized data from open-source repositories, like GitHub, introduces the risk of inadvertently propagating security vulnerabilities. To effectively mitigate this concern, this paper presents a comprehensive study focused on evaluating and enhancing code LLMs from a software security perspective. We introduce SecuCoGen\\footnote{SecuCoGen has been uploaded as supplemental material and will be made publicly available after publication.}, a meticulously curated dataset targeting 21 critical vulnerability types. SecuCoGen comprises 180 samples and serves as the foundation for conducting experiments on three crucial code-related tasks: code generation, code repair and vulnerability classification, with a strong emphasis on security. Our experimental results reveal that existing models often overlook security concerns during code generation, leading to the generation of vulnerable code. To address this, we propose effective approaches to mitigate the security vulnerabilities and enhance the overall robustness of code generated by LLMs. Moreover, our study identifies weaknesses in existing models' ability to repair vulnerable code, even when provided with vulnerability information. Additionally, certain vulnerability types pose challenges for the models, hindering their performance in vulnerability classification. Based on these findings, we believe our study will have a positive impact on the software engineering community, inspiring the development of improved methods for training and utilizing LLMs, thereby leading to safer and more trustworthy model deployment.",
    "published_date": "2023-10-25",
    "pdf_link": "https://arxiv.org/pdf/2310.16263v1",
    "paper_types": [
      "new_dataset",
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Software and Application Security",
      "subdomain": "Secure Coding and Code Analysis",
      "specific_problem": "Security-aware code generation, code repair, and vulnerability classification for LLM-generated code",
      "attack_types": [
        "Out-of-bounds Write (CWE-787)",
        "Cross-site Scripting (CWE-79)",
        "SQL Injection (CWE-89)",
        "OS Command Injection (CWE-78)",
        "Improper Input Validation (CWE-20)",
        "Out-of-bounds Read (CWE-125)",
        "Path Traversal (CWE-22)",
        "Cross-Site Request Forgery (CSRF) (CWE-352)",
        "Unrestricted File Upload (CWE-434)",
        "Missing/Improper Authentication/Authorization (CWE-862/CWE-863/CWE-287/CWE-306 merged)",
        "Deserialization of Untrusted Data (CWE-502)",
        "Command Injection (CWE-77)",
        "Use of Hard-coded Credentials (CWE-798)",
        "Server-Side Request Forgery (SSRF) (CWE-918)",
        "Race Condition (CWE-362)",
        "Improper Privilege Management (CWE-269)",
        "Code Injection (CWE-94)",
        "Incorrect Default Permissions (CWE-276)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Prompting",
        "specific": "CWE-aware problem prompting for code generation",
        "novel_contribution": "Rewriting task prompts to be CWE-aware to mitigate vulnerabilities during generation"
      },
      {
        "type": "primary",
        "category": "Prompting",
        "specific": "One-shot prompting with a secure code exemplar",
        "novel_contribution": "Providing a secure example (problem + secure solution) to reduce vulnerable generations"
      },
      {
        "type": "primary",
        "category": "Prompting",
        "specific": "Code repair with vulnerability explanation",
        "novel_contribution": "Supplying an explanation of why code is insecure to guide repair"
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "InCoder 6.7B",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "CodeGen-Mono 6B",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "In-context learning",
      "Zero-shot",
      "Few-shot (1-shot)"
    ],
    "datasets": [
      {
        "name": "SecuCoGen",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "SecurityEval",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "BIGPYTHON",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Plain Problem prompt (zero-shot code generation)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "CWE-aware Problem prompt (security-informed generation)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "1-shot prompting with secure exemplar for code generation",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Code repair without explanation",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Code repair with Insecure Code Explanation",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "InCoder 6.7B",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "CodeGen-Mono 6B",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "RQ1: How effective are current large language models in addressing security concerns during code generation, and are certain vulnerability types more likely to be successfully mitigated during code generation?",
        "RQ2: What effective approaches can be devised to improve the security of code generation by large language models, and to what extent can these proposed approaches mitigate security vulnerabilities?",
        "RQ3: How well do existing large language models perform in repairing insecure code?",
        "RQ4: To what extent does explaining the reasons why the code is insecure help in repairing the code by existing models?",
        "RQ5: Which vulnerability types pose challenges for large language models in vulnerability classification?",
        "RQ6: What are the implications of the research findings for the broader software engineering community, and how can developers and researchers leverage large language models more securely in real-world applications?"
      ],
      "gaps_identified": [
        "LLMs trained on open-source code may learn and propagate vulnerabilities during generation",
        "Lack of security-focused code generation datasets with rich context (problem, insecure/secure code, explanations)",
        "Limited prior work on concrete methods to enhance security of LLM-generated code or to repair insecure code",
        "Existing datasets (e.g., SecurityEval) provide limited information per instance"
      ],
      "limitations": [
        "SecuCoGen is Python-only",
        "Excludes four CWE types due to rarity/absence in Python and merges four related authorization CWEs into one category",
        "Dataset size is relatively small (180 samples, 21 vulnerability types)"
      ],
      "future_work": [
        "Develop improved methods for training and utilizing LLMs for safer code generation and deployment",
        "Further advance LLM capabilities for code repair, especially for challenging vulnerability types",
        "Improve vulnerability classification performance for difficult categories"
      ],
      "motivation": "Mitigate security risks in LLM-generated code arising from training on unsanitized open-source repositories by evaluating current models and proposing data- and prompt-driven approaches to improve security.",
      "potential_research_ideas": [
        "Fine-tune code LLMs on SecuCoGen-style secure/insecure pairs with CWE-aware instructions to internalize secure coding patterns",
        "Integrate static analysis or SAST tools in-the-loop with LLM generation (generate–analyze–repair cycles)",
        "Retrieval-augmented generation using CWE/Mitre knowledge and secure coding guidelines as context",
        "Multi-agent self-critique systems where one agent generates code and another performs security auditing before finalization",
        "Security-focused RLHF or constraint optimization with reward signals from vulnerability detectors and unit/security tests",
        "Automated test generation for security properties (fuzzing, property-based tests) to validate and guide LLM outputs",
        "Curriculum learning over vulnerability types to progressively teach LLMs robust secure coding practices"
      ],
      "architectural_improvement_recommendations": [
        "Add a security-discriminator head or post-hoc verifier to filter/repair generated code before output",
        "Use a tool-augmented LLM that calls static analyzers, linters, and sandboxed execution for taint/flow checks during decoding",
        "Implement constrained decoding guided by vulnerability patterns (negative constraints) learned from CWE exemplars",
        "Adopt RAG with CWE and secure coding corpora to condition generation on authoritative guidance",
        "Train with contrastive pairs (insecure vs secure) to penalize vulnerable patterns and reinforce mitigations"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "LLMs may generate insecure code due to training on unsanitized repositories",
        "Difficulty repairing vulnerable code even with vulnerability information",
        "Certain vulnerability types remain challenging for LLMs to detect/classify",
        "Ensuring robustness and trustworthiness of LLM-generated code in real deployments"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "SecuCoGen: a curated Python dataset of 180 samples across 21 critical CWE vulnerability types with six rich attributes (Problem, Insecure Code, Explanation, Secure Code, CWE-aware Problem)",
      "Comprehensive empirical study of code LLMs on security-aware code generation, code repair, and vulnerability classification",
      "Proposed effective prompting-based approaches (CWE-aware prompts, one-shot secure exemplars) to mitigate vulnerabilities in code generation",
      "Identified weaknesses in LLMs’ ability to repair insecure code, even with vulnerability explanations",
      "Highlighted vulnerability types that challenge LLMs in vulnerability classification, offering guidance for future improvements"
    ]
  },
  {
    "arxiv_id": "2312.04864v2",
    "title": "Critical Analysis of 5G Networks Traffic Intrusion using PCA, t-SNE and UMAP Visualization and Classifying Attacks",
    "authors": "Humera Ghani; Shahram Salekzamankhani; Bal Virdee",
    "abstract": "Networks, threat models, and malicious actors are advancing quickly. With the increased deployment of the 5G networks, the security issues of the attached 5G physical devices have also increased. Therefore, artificial intelligence based autonomous end-to-end security design is needed that can deal with incoming threats by detecting network traffic anomalies. To address this requirement, in this research, we used a recently published 5G traffic dataset, 5G-NIDD, to detect network traffic anomalies using machine and deep learning approaches. First, we analyzed the dataset using three visualization techniques: t-Distributed Stochastic Neighbor Embedding (t-SNE), Uniform Manifold Approximation and Projection (UMAP), and Principal Component Analysis (PCA). Second, we reduced the data dimensionality using mutual information and PCA techniques. Third, we solve the class imbalance issue by inserting synthetic records of minority classes. Last, we performed classification using six different classifiers and presented the evaluation metrics. We received the best results when K-Nearest Neighbors classifier was used: accuracy (97.2%), detection rate (96.7%), and false positive rate (2.2%).",
    "published_date": "2023-12-08",
    "pdf_link": "https://arxiv.org/pdf/2312.04864v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Anomaly/intrusion detection on 5G network traffic (binary classification: benign vs malicious) with dimensionality reduction and class-imbalance handling",
      "attack_types": [
        "UDPFlood",
        "HTTPFlood",
        "SlowrateDoS",
        "TCPConnectScan",
        "SYNScan",
        "UDPScan",
        "SYNFlood",
        "ICMPFlood"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Feature Selection",
        "specific": "Mutual Information (Information Gain)",
        "novel_contribution": "Used to rank features; top 22 features selected prior to PCA"
      },
      {
        "type": "primary",
        "category": "Dimensionality Reduction",
        "specific": "Principal Component Analysis (PCA)",
        "novel_contribution": "Transformed selected features into 11 principal components capturing 89.2% variance"
      },
      {
        "type": "primary",
        "category": "Imbalanced Learning",
        "specific": "SMOTE (Synthetic Minority Over-sampling Technique)",
        "novel_contribution": "Applied to address class imbalance in 5G-NIDD"
      },
      {
        "type": "primary",
        "category": "k-Nearest Neighbors",
        "specific": "KNN",
        "novel_contribution": "Best-performing classifier in the proposed pipeline (Accuracy 97.275%, DR 96.765%, FPR 2.213%)"
      },
      {
        "type": "baseline",
        "category": "Decision Tree",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Random Forest",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Multi-layer Perceptron",
        "specific": "Feed-forward MLP",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Naive Bayes",
        "specific": "Gaussian Naive Bayes (GNB)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Support Vector Machine",
        "specific": "Support Vector Classifier (SVC)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Dimensionality Reduction (Visualization)",
        "specific": "t-SNE",
        "novel_contribution": "Used for visual analysis of dataset (class overlap/within-class clusters)"
      },
      {
        "type": "baseline",
        "category": "Dimensionality Reduction (Visualization)",
        "specific": "UMAP",
        "novel_contribution": "Used for visual analysis of dataset (class overlap/within-class clusters)"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised (for visualization and PCA)"
    ],
    "datasets": [
      {
        "name": "5G-NIDD",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "UNSW-NB15",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "NSL-KDD",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CICIDS-2017",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Phishing dataset (unspecified)",
        "type": "public",
        "domain": "phishing_urls",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Decision Tree",
        "paper_reference": null,
        "metric": "Accuracy / Detection Rate / False Positive Rate",
        "their_result": "KNN: 97.275 / 96.765 / 2.213",
        "baseline_result": "DT: 97.150 / 96.618 / 2.318"
      },
      {
        "method_name": "Random Forest",
        "paper_reference": null,
        "metric": "Accuracy / Detection Rate / False Positive Rate",
        "their_result": "KNN: 97.275 / 96.765 / 2.213",
        "baseline_result": "RF: 97.168 / 96.650 / 2.314"
      },
      {
        "method_name": "Multi-layer Perceptron",
        "paper_reference": null,
        "metric": "Accuracy / Detection Rate / False Positive Rate",
        "their_result": "KNN: 97.275 / 96.765 / 2.213",
        "baseline_result": "MLP: 94.910 / 92.646 / 2.828"
      },
      {
        "method_name": "Support Vector Classifier",
        "paper_reference": null,
        "metric": "Accuracy / Detection Rate / False Positive Rate",
        "their_result": "KNN: 97.275 / 96.765 / 2.213",
        "baseline_result": "SVC: 92.090 / 90.490 / 6.320"
      },
      {
        "method_name": "Gaussian Naive Bayes",
        "paper_reference": null,
        "metric": "Accuracy / Detection Rate / False Positive Rate",
        "their_result": "KNN: 97.275 / 96.765 / 2.213",
        "baseline_result": "GNB: 87.425 / 92.428 / 17.559"
      },
      {
        "method_name": "[11] (Deep learning with CNN and Bi-LSTM; with class-imbalance handling)",
        "paper_reference": "[11] (as cited in paper)",
        "metric": "Accuracy",
        "their_result": "Proposed: 97.28% on 5G-NIDD",
        "baseline_result": "77.16% (UNSW-NB15), 83.58% (NSL-KDD)"
      },
      {
        "method_name": "[12] (WGAN + Stacked Autoencoder + cost-sensitive loss)",
        "paper_reference": "[12] (as cited in paper)",
        "metric": "Accuracy",
        "their_result": "Proposed: 97.28% on 5G-NIDD",
        "baseline_result": "93.27% (UNSW-NB15), 90.34% (NSL-KDD)"
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "Detection Rate",
      "False Positive Rate",
      "ROC AUC"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Current research in network traffic anomaly detection lacks presenting visual analysis of datasets (authors emphasize PCA, t-SNE, UMAP exploration).",
        "Prior use of ANOVA for feature selection on 5G-NIDD is inappropriate due to skewness and multi-modal features (non-normality).",
        "Class imbalance and class overlap are prevalent issues that hinder detection performance."
      ],
      "limitations": [
        "5G-NIDD is recently published; limited prior work on it to compare against.",
        "Processing power constraints prevented comprehensive hyperparameter tuning, which could yield better metrics."
      ],
      "future_work": [
        "Conduct thorough hyperparameter search for all classifiers.",
        "Benchmark against more approaches as additional research on 5G-NIDD becomes available."
      ],
      "motivation": "With rapid 5G deployment and evolving threats, an AI-based autonomous end-to-end security design is needed to detect network traffic anomalies in 5G networks.",
      "potential_research_ideas": [
        "Extend to multiclass attack classification on 5G-NIDD with per-attack metrics and cost-sensitive evaluation.",
        "Investigate supervised dimensionality reduction (e.g., LDA) or metric learning/NCA tailored to KNN to mitigate class overlap.",
        "Evaluate gradient-boosted trees (XGBoost/LightGBM/CatBoost) and stacking ensembles on 5G-NIDD.",
        "Develop streaming/online IDS for 5G core and edge with concept-drift detection.",
        "Domain adaptation/generalization across different 5G network deployments and traffic mixes.",
        "Self-supervised or contrastive pretraining on raw flows/pcap-derived features to improve rare attack detection.",
        "Adversarial robustness evaluation (evasion/poisoning) and robust training for IDS in 5G.",
        "Privacy-preserving training/inference (federated learning on 5G network slices).",
        "Explainability for NIDS decisions (e.g., SHAP on tree models; counterfactuals for KNN).",
        "Advanced imbalance handling (focal loss, class-weighting, ADAptive synthetic sampling variants) and rarity-aware metrics (G-mean, MCC, PR-AUC).",
        "GAN-based or diffusion-based traffic augmentation with fidelity/utility validation for rare 5G attacks."
      ],
      "architectural_improvement_recommendations": [
        "Perform nested cross-validation with SMOTE strictly inside training folds to avoid data leakage; add stratification by attack type.",
        "Tune number of top-ranked features and PCA components; compare PCA vs supervised LDA; try NCA for KNN distance metric.",
        "Calibrate classifiers (Platt/Isotonic) and report calibrated detection thresholds and PR curves.",
        "Add strong tabular baselines (XGBoost/LightGBM/CatBoost) and stacking/ensemble averaging.",
        "Use feature selection stability analysis and SHAP-based importance to validate MI rankings.",
        "Evaluate alternative distance metrics and metric learning for KNN; scale features with robust scalers to handle skew.",
        "Report per-class metrics and confusion matrices (especially for minority attacks) and use cost-sensitive evaluation.",
        "Integrate drift detection and incremental learning for deployment on streaming 5G traffic."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": "Not specified; authors note processing power was a limitation preventing hyperparameter search."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Class imbalance and class overlap in 5G traffic can hinder detection.",
        "Limited computational resources for tuning and real-time constraints in 5G environments.",
        "Generalization from a single 5G test network to diverse production deployments is uncertain."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Visual analyses of the 5G-NIDD dataset using PCA, t-SNE, and UMAP to reveal class imbalance, overlap, and within-class clustering.",
      "Dimensionality reduction via Mutual Information feature ranking (top 22) followed by PCA to 11 components capturing 89.2% variance.",
      "Addressed class imbalance using oversampling (SMOTE) to improve classification metrics.",
      "Binary classification of benign vs malicious traffic using DT, KNN, MLP, GNB, RF, and SVC; best KNN results: accuracy 97.275%, detection rate 96.765%, false positive rate 2.213%; ROC AUC 97.2%."
    ]
  },
  {
    "arxiv_id": "2312.00041v1",
    "title": "Presentation Attack Detection using Convolutional Neural Networks and Local Binary Patterns",
    "authors": "Justin Spencer; Deborah Lawrence; Prosenjit Chatterjee; Kaushik Roy; Albert Esterline; Jung-Hee Kim",
    "abstract": "The use of biometrics to authenticate users and control access to secure areas has become extremely popular in recent years, and biometric access control systems are frequently used by both governments and private corporations. However, these systems may represent risks to security when deployed without considering the possibility of biometric presentation attacks (also known as spoofing). Presentation attacks are a serious threat because they do not require significant time, expense, or skill to carry out while remaining effective against many biometric systems in use today. This research compares three different software-based methods for facial and iris presentation attack detection in images. The first method uses Inception-v3, a pre-trained deep Convolutional Neural Network (CNN) made by Google for the ImageNet challenge, which is retrained for this problem. The second uses a shallow CNN based on a modified Spoofnet architecture, which is trained normally. The third is a texture-based method using Local Binary Patterns (LBP). The datasets used are the ATVS-FIr dataset, which contains real and fake iris images, and the CASIA Face Anti-Spoofing Dataset, which contains real images as well as warped photos, cut photos, and video replay presentation attacks. We also present a third set of results, based on cropped versions of the CASIA images.",
    "published_date": "2023-11-23",
    "pdf_link": "https://arxiv.org/pdf/2312.00041v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Biometric Security",
      "subdomain": "Presentation Attack Detection (Anti-spoofing)",
      "specific_problem": "Static image-based PAD for face and iris modalities; effect of cropping/background on PAD",
      "attack_types": [
        "warped photo (printed photo moved)",
        "cut photo (eye holes cut in printed photo)",
        "video replay (tablet/screen)",
        "printed-and-rescanned iris"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "CNN",
        "specific": "Inception-v3 (transfer learning)",
        "novel_contribution": "Retrains ImageNet-pretrained Inception-v3 for PAD on iris and face datasets; compares effect of background/cropping."
      },
      {
        "type": "primary",
        "category": "CNN",
        "specific": "Modified Spoofnet (custom shallow CNN)",
        "novel_contribution": "Proposes a 6-layer shallow CNN (5x5 conv-16 + 3x3 maxpool s=3; 5x5 conv-32 + maxpool; flatten; dense-128 ReLU; dense-1 sigmoid) for PAD."
      },
      {
        "type": "primary",
        "category": "LBP",
        "specific": "Local Binary Patterns (texture descriptor)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Transfer Learning"
    ],
    "datasets": [
      {
        "name": "ATVS-FIr (ATVS iris spoofing dataset)",
        "type": "public",
        "domain": "iris_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CASIA Face Anti-Spoofing Dataset (CASIA-FASD)",
        "type": "public",
        "domain": "face_videos",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CASIA-cropped (face crops derived from CASIA-FASD)",
        "type": "private",
        "domain": "face_images",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Inception-v3 (transfer learning) on ATVS (R/F)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "97% (Modified Spoofnet)",
        "baseline_result": "100%"
      },
      {
        "method_name": "LBP on ATVS (R/F)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "97% (Modified Spoofnet)",
        "baseline_result": "100% (1x1 patch)"
      },
      {
        "method_name": "Inception-v3 (transfer learning) on CASIA (W/C/V/R, 4-class)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "90.5% (Modified Spoofnet)",
        "baseline_result": "98.7%"
      },
      {
        "method_name": "LBP on CASIA (W/C/V/R, 4-class)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "90.5% (Modified Spoofnet)",
        "baseline_result": "100% (1x1 patch)"
      },
      {
        "method_name": "Inception-v3 (transfer learning) on CASIA-cropped (W/C/V/R, 4-class)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "25% (Modified Spoofnet)",
        "baseline_result": "90.2%"
      },
      {
        "method_name": "LBP on CASIA-cropped (W/C/V/R, 4-class)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "25% (Modified Spoofnet)",
        "baseline_result": "100% (1x1 patch)"
      },
      {
        "method_name": "CASIA (binary R/F) — no external baseline reported",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "92.5% (Modified Spoofnet)",
        "baseline_result": null
      },
      {
        "method_name": "CASIA-cropped (binary R/F) — no external baseline reported",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "94.5% (Modified Spoofnet)",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "ROC AUC",
      "True Positive Rate (TPR)",
      "False Positive Rate (FPR)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How do a shallow CNN, a deep pretrained CNN (Inception-v3), and LBP compare for face and iris presentation attack detection on static images?",
        "What is the impact of cropping (removing background) on PAD performance, especially for CNNs?",
        "How does transfer learning (Inception-v3) compare to training a shallow CNN from scratch in terms of accuracy vs. complexity/performance?",
        "What is the accuracy/performance/complexity trade-off between shallow and deep CNN architectures for PAD?"
      ],
      "gaps_identified": [
        "CNN-based PAD can rely on background cues; performance drops sharply on cropped images where background is removed.",
        "Training instability of the shallow CNN (“fails to find a gradient about one-third of the time”).",
        "Memory/loading limitations: the custom CNN must load the entire dataset at once and cannot handle the full CASIA datasets.",
        "Static-image PAD evaluated; no temporal/dynamic cues considered though video-based PAD is common.",
        "Limited robustness analysis across varied capture conditions and unseen attack instruments (e.g., photorealistic masks mentioned as motivation)."
      ],
      "limitations": [
        "“Although our CNN obtains reasonable accuracy when it converges, it fails to find a gradient about one-third of the time.”",
        "Custom CNN requires fixed-size inputs and full-dataset loading; could not load the larger CASIA datasets in full (used 800 images per class instead).",
        "ROC analysis only reported for binary classification problems with the custom CNN.",
        "No cross-dataset generalization tests; results reported on the same datasets used for training/validation.",
        "No code release indicated; reproducibility may be affected by training instability and preprocessing specifics."
      ],
      "future_work": [],
      "motivation": "Biometric systems are vulnerable to low-cost, effective presentation (spoofing) attacks; need software-based PAD methods for faces and irises that are accurate and practical.",
      "potential_research_ideas": [
        "Design PAD models that are explicitly robust to background removal by focusing on intrinsic face/iris cues (e.g., micro-texture, rPPG signals from videos, frequency-domain cues).",
        "Cross-dataset and cross-sensor generalization studies, including domain generalization/adaptation to reduce reliance on dataset-specific backgrounds.",
        "Fusion of deep features with classical texture descriptors (e.g., LBP + CNN features) to leverage complementary strengths.",
        "Iris-specific segmentation and attention mechanisms to isolate the iris region and mitigate non-iris artifacts.",
        "Evaluate and release the CASIA-cropped protocol as a standardized benchmark split to encourage background-agnostic PAD.",
        "Stabilize shallow models with improved training (initialization, normalization, curriculum learning) and investigate compact modern backbones (e.g., MobileNetV3, EfficientNet-Lite) for edge deployment.",
        "Incorporate temporal modeling (e.g., 3D CNNs or transformers) for video PAD on CASIA-FASD to capture motion/liveness cues.",
        "Adversarial robustness assessment for PAD models, including attacks that mimic micro-texture or introduce subtle perturbations."
      ],
      "architectural_improvement_recommendations": [
        "Add batch normalization and dropout, and use learning-rate schedules/warmup to address gradient-finding failures.",
        "Use residual/efficient backbones (ResNet/EfficientNet/MobileNet) with global average pooling to improve generalization on cropped inputs.",
        "Introduce attention (spatial/channel) or patch-level supervision to prioritize facial/iris regions over backgrounds.",
        "Adopt data loading with mini-batching and on-the-fly augmentation to scale to full datasets and improve robustness.",
        "Explore loss functions beyond binary cross-entropy (e.g., focal loss, label smoothing) to handle class imbalance and hard negatives.",
        "For iris PAD, apply segmentation pre-processing and multi-scale feature extraction tailored to iris micro-textures."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "TensorFlow",
        "Keras",
        "OpenCV"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Inception-v3 retrained for 4000 epochs with 80/10/10 split and batch loading. Custom shallow CNN trained for 30 epochs with 50/50 split (first 30 test images as validation), requires fixed-size inputs and loads entire dataset into memory; used 800 images per class for CASIA and CASIA-cropped due to memory constraints."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "CNN reliance on background cues; performance degrades on cropped/realistic scenarios.",
        "Training instability in shallow CNN (failure to find gradient about one-third of runs).",
        "Memory constraints in training pipeline (no mini-batch dataset streaming for custom CNN).",
        "Static-image approach lacks temporal liveness cues needed in many real deployments."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Empirical comparison of three software-based PAD methods (LBP, shallow CNN, transfer-learned Inception-v3) across iris and face datasets.",
      "Design of a modified shallow Spoofnet-like CNN architecture and analysis of its performance vs. a deep pretrained CNN.",
      "Creation and evaluation of a CASIA-cropped dataset to study the effect of removing background on PAD.",
      "Quantitative results showing LBP and Inception-v3 achieve very high accuracy across datasets, while the shallow CNN struggles on cropped multi-class PAD.",
      "ROC analysis for binary PAD with the custom CNN, reporting AUC ≈ 0.98 across datasets."
    ]
  },
  {
    "arxiv_id": "2310.06752v2",
    "title": "Comparing AI Algorithms for Optimizing Elliptic Curve Cryptography Parameters in e-Commerce Integrations: A Pre-Quantum Analysis",
    "authors": "Felipe Tellez; Jorge Ortiz",
    "abstract": "This paper presents a comparative analysis between the Genetic Algorithm (GA) and Particle Swarm Optimization (PSO), two vital artificial intelligence algorithms, focusing on optimizing Elliptic Curve Cryptography (ECC) parameters. These encompass the elliptic curve coefficients, prime number, generator point, group order, and cofactor. The study provides insights into which of the bio-inspired algorithms yields better optimization results for ECC configurations, examining performances under the same fitness function. This function incorporates methods to ensure robust ECC parameters, including assessing for singular or anomalous curves and applying Pollard's rho attack and Hasse's theorem for optimization precision. The optimized parameters generated by GA and PSO are tested in a simulated e-commerce environment, contrasting with well-known curves like secp256k1 during the transmission of order messages using Elliptic Curve-Diffie Hellman (ECDH) and Hash-based Message Authentication Code (HMAC). Focusing on traditional computing in the pre-quantum era, this research highlights the efficacy of GA and PSO in ECC optimization, with implications for enhancing cybersecurity in third-party e-commerce integrations. We recommend the immediate consideration of these findings before quantum computing's widespread adoption.",
    "published_date": "2023-10-10",
    "pdf_link": "https://arxiv.org/pdf/2310.06752v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Cryptography",
      "subdomain": "Public-key Cryptography",
      "specific_problem": "Optimization of Elliptic Curve Cryptography (ECC) parameters for secure e-commerce third-party integrations (pre-quantum)",
      "attack_types": [
        "Elliptic Curve Discrete Logarithm Problem (ECDLP) hardness checks via Pollard's rho",
        "Anomalous curve vulnerability avoidance",
        "Singular curve avoidance"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Evolutionary Algorithm",
        "specific": "Genetic Algorithm (DEAP-based)",
        "novel_contribution": "Custom mutation operator with dynamic Gaussian perturbation, prime generation for p (256-bit), regeneration of generator point G, three-point multiparent crossover, elitism strategy; GA constants tuned (population=500, CXPB=0.5, MUTPB=0.2, NGEN=40, multiparent CXPB=0.1, elitism rate=0.1)."
      },
      {
        "type": "primary",
        "category": "Swarm Intelligence",
        "specific": "Particle Swarm Optimization",
        "novel_contribution": "Dynamic inertia weight decreasing from 0.9 to 0.4; explicit handling of generator point G updates; cognitive (C1) and social (C2) components for ECC parameter vector; comparison under identical fitness function."
      }
    ],
    "learning_paradigm": [
      "Population-based optimization",
      "Metaheuristic"
    ],
    "datasets": [
      {
        "name": "Simulated e-commerce order messages for third-party integrations",
        "type": "synthetic",
        "domain": "application_protocol_messages",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "secp256k1 standard curve (comparison reference during ECDH+HMAC transmission)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Computational time",
      "Convergence rate",
      "Optimization speed",
      "Robustness to initialization and noise",
      "Scalability with parameter/search-space size",
      "Security compliance checks (avoid singular/anomalous curves)",
      "Hasse's theorem bounds validation",
      "Resistance indication via Pollard's rho check",
      "Validity of ECC parameter set (a, b, p, G, n, h)",
      "Practical compatibility with existing systems",
      "Successful ECDH+HMAC message transmission in simulated environment"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Which AI algorithm (GA or PSO) is most efficient during the optimization process for ECC parameters?",
        "Which AI algorithm (GA or PSO) is most effective in terms of the quality of ECC parameters it produces for e-commerce third-party integrations?"
      ],
      "gaps_identified": [
        "Prior works focus on specific techniques (e.g., Pollard's Rho, PSO/Cuckoo Search, DNA-based cryptography) rather than comprehensive ECC parameter optimization in e-commerce contexts.",
        "Lack of comparative evaluations between multiple AI techniques (e.g., GA vs PSO) for ECC parameter optimization.",
        "Limited consideration of practical, real-world e-commerce integration scenarios in prior optimization studies."
      ],
      "limitations": [
        "Scope limited to pre-quantum computing context.",
        "Excludes other AI techniques (e.g., Simulated Annealing, Ant Colony Optimization, Artificial Neural Networks).",
        "Uses a simulated e-commerce environment with API simulations rather than a full production e-commerce system.",
        "Focuses on outbound integrations and specific business process (order creation to ERP) rather than broader scenarios."
      ],
      "future_work": [
        "Parameter tuning and hyperparameter search for GA/PSO.",
        "Parallelization of algorithms to improve performance.",
        "Hybrid algorithms combining GA and PSO.",
        "Exploration of alternative AI techniques for ECC parameter optimization.",
        "Fitness function improvements and inclusion of more diverse cryptographic threat models.",
        "Assessment of quantum computing implications and post-quantum directions."
      ],
      "motivation": "Enhance security and efficiency of ECC in e-commerce third-party integrations by identifying which population-based AI algorithm optimally tunes ECC parameters under classical (pre-quantum) computing.",
      "potential_research_ideas": [
        "Multi-objective optimization framework balancing security (ECDLP hardness, safe-curve checks) and performance (latency, resource use) for ECC parameters.",
        "Integrate formal safe-curve and cofactor checks (e.g., Montgomery ladder safety, twist security, small-subgroup checks) into the fitness function.",
        "Extend to standardized validation pipelines (SEC 1, NIST SP 800-56A) and automatic rejection sampling of unsafe curves.",
        "Empirical evaluation on real e-commerce platforms (A/B testing) to measure end-to-end latency and throughput impacts.",
        "Adversarial search to detect trapdoor/backdoored curves and add adversarial robustness constraints to the optimizer.",
        "Explore post-quantum transitional strategies: optimize ECC with hybrid KEM/TLS ciphersuites and measure operational overhead."
      ],
      "architectural_improvement_recommendations": [
        "Adopt a hybrid GA-PSO (memetic/island) approach with migration between subpopulations to combine exploration and exploitation.",
        "Use self-adaptive parameter control (adaptive CXPB, MUTPB; adaptive C1/C2/inertia) via reinforcement signals from fitness trends.",
        "Introduce constraint-handling via penalty/barrier methods or feasibility-first selection to strictly enforce cryptographic constraints.",
        "Parallelize evaluation of individuals/particles on multi-core/GPU; vectorize curve arithmetic with GMP-backed libraries.",
        "Augment fitness with fast ECDLP hardness estimators and small-subgroup/twist-security checks; incorporate constant-time arithmetic checks.",
        "Replace ad-hoc RNG with cryptographic RNG and seed management; log provenance for full reproducibility."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "github.com/cftellezc/GA PSO ECC parameter Optimization",
      "frameworks": [
        "Python",
        "DEAP",
        "numpy",
        "pandas",
        "matplotlib",
        "gmpy2",
        "requests",
        "tinyec"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Simulated e-commerce integration environment using web services for outbound order messages to an emulated ERP",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Ensuring compatibility with existing third-party systems (ERP/CRM/payment gateways).",
        "Balancing field size for security vs computational load.",
        "Adhering to cryptographic best practices and standards during parameter generation."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Comparative analysis of Genetic Algorithm (GA) vs Particle Swarm Optimization (PSO) for ECC parameter optimization under a unified fitness function.",
      "A fitness function incorporating checks for singular/anomalous curves, Hasse's theorem, and Pollard's rho-based robustness indication.",
      "Simulation-based evaluation framework for e-commerce third-party integrations using ECDH and HMAC in a pre-quantum setting.",
      "Open-source Python implementation leveraging DEAP and ECC libraries, including GA and PSO modules and custom operators.",
      "Parameterization for 256-bit prime generation and automated generator point selection within the optimization loop."
    ]
  },
  {
    "arxiv_id": "2311.17012v1",
    "title": "Counter-terrorism in cyber-physical spaces: Best practices and technologies from the state of the art",
    "authors": "Giuseppe Cascavilla; Damian A. Tamburri; Francesco Leotta; Massimo Mecella; WillemJan Van Den Heuvel",
    "abstract": "Context: The demand for protection and security of physical spaces and urban areas increased with the escalation of terroristic attacks in recent years. We envision with the proposed cyber-physical systems and spaces, a city that would indeed become a smarter urbanistic object, proactively providing alerts and being protective against any threat. Objectives: This survey intend to provide a systematic multivocal literature survey comprised of an updated, comprehensive and timely overview of state of the art in counter-terrorism cyber-physical systems, hence aimed at the protection of cyber-physical spaces. Hence, provide guidelines to law enforcement agencies and practitioners providing a description of technologies and best practices for the protection of public spaces. Methods: We analyzed 112 papers collected from different online sources, both from the academic field and from websites and blogs ranging from 2004 till mid-2022. Results: a) There is no one single bullet-proof solution available for the protection of public spaces. b) From our analysis we found three major active fields for the protection of public spaces: Information Technologies, Architectural approaches, Organizational field. c) While the academic suggest best practices and methodologies for the protection of urban areas, the market did not provide any type of implementation of such suggested approaches, which shows a lack of fertilization between academia and industry. Conclusion: The overall analysis has led us to state that there is no one single solution available, conversely, multiple methods and techniques can be put in place to guarantee safety and security in public spaces. The techniques range from architectural design to rethink the design of public spaces keeping security into account in continuity, to emerging technologies such as AI and predictive surveillance.",
    "published_date": "2023-11-28",
    "pdf_link": "https://arxiv.org/pdf/2311.17012v1",
    "paper_types": [
      "empirical_analysis",
      "survey"
    ],
    "security_domain": {
      "primary": "Cyber-Physical Systems Security",
      "subdomain": "Smart City/Public Space Protection",
      "specific_problem": "Counter-terrorism protection of cyber-physical public spaces (guidelines, technologies, and best practices)",
      "attack_types": [
        "terrorist attacks",
        "knife attacks",
        "vehicle ramming/hit-and-run",
        "improvised explosive devices (IEDs)/bombing",
        "public violence/harassment"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Topic Modeling/NLP",
        "specific": null,
        "novel_contribution": "Applied to analyze and synthesize topics in the multivocal literature corpus"
      },
      {
        "type": "primary",
        "category": "Topological Data Analysis",
        "specific": null,
        "novel_contribution": "Applied to structure/cluster the literature landscape in the survey"
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": null,
        "novel_contribution": "Discussed as a technique used in surveyed crowd-monitoring video analytics; not introduced by this paper"
      },
      {
        "type": "baseline",
        "category": "Classical Computer Vision",
        "specific": "Background Subtraction",
        "novel_contribution": "Discussed as an alternative to CNNs in surveyed works; not introduced by this paper"
      }
    ],
    "learning_paradigm": [
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "Corpus of 112 white and grey literature sources (2004–mid-2022)",
        "type": "public",
        "domain": "literature_corpus",
        "link": "https://shorturl.at/hoyJT",
        "is_new_contribution": true,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "MRQ: What are the best practices and CPS technologies available nowadays for the protection of urban spaces?",
        "SRQ1: To what extent do new technologies effectively enhance the protection of public spaces?",
        "SRQ2: What are the newest approaches available for the protection of urban areas?",
        "SRQ3: What is the extent of agreement among white and grey literature, and what are the research gaps and overlaps?"
      ],
      "gaps_identified": [
        "There is no single bullet-proof solution for protection of public spaces.",
        "Lack of fertilization between academia and industry; market did not implement many academic best practices and methodologies.",
        "Research and practice remain siloed and mono-disciplinary; need for multi-disciplinary cyber-physical perspectives.",
        "Legal and privacy protection challenges in intelligent video surveillance systems.",
        "Technical challenges in crowd/video analytics such as occlusion handling and synchronization across multiple camera views.",
        "Limited evidence of real-world deployments of integrated CPS-based counter-terrorism systems."
      ],
      "limitations": [
        "Details from focus-group workshops with municipalities/LEAs cannot be disclosed for security reasons.",
        "Study period limited to sources from 2004 to mid-2022.",
        "Multivocal SLR includes grey literature; quality and completeness of such sources may vary.",
        "Survey nature; no single prescriptive solution or quantitative performance benchmarking."
      ],
      "future_work": [
        "Bridge academia–industry gap to translate best practices into deployable solutions.",
        "Integrate architectural, organizational, and information technology approaches into cohesive CPS frameworks.",
        "Leverage AI and predictive surveillance responsibly for proactive threat detection in public spaces.",
        "Develop quantitative indicators and risk assessment tools for underdeveloped CPS areas.",
        "Explore participatory and citizen-science elements for threat reporting and situational awareness.",
        "Establish standards, testbeds, and procurement models to validate and scale CPS-based protection solutions."
      ],
      "motivation": "Provide a systematic multivocal overview of state-of-the-art CPS technologies and best practices for protecting public spaces from terrorism, offering actionable guidance for LEAs and practitioners.",
      "potential_research_ideas": [
        "Design an open CPS reference architecture for public space protection that explicitly integrates IT, architectural design, and organizational processes.",
        "Build a privacy-preserving, multi-modal predictive surveillance pipeline (video, acoustic, crowd density, environmental sensors) for early anomaly detection.",
        "Create a standard smart-city security testbed and simulation/digital twin for evaluating counter-terrorism CPS under realistic scenarios.",
        "Develop explainable AI for public-space threat detection to improve operator trust and accountable decision-making.",
        "Study adversarial robustness of urban surveillance models (e.g., adversarial patches against person/crowd detectors) and propose defenses.",
        "Construct a knowledge graph linking cyber threat intelligence with physical-world CPS events for cross-domain situational awareness.",
        "Investigate federated or split learning for cross-municipality model training without sharing raw video or sensor data.",
        "Use TDA to detect rare topological anomalies in CPS telemetry/logs indicative of pre-attack behaviors."
      ],
      "architectural_improvement_recommendations": [
        "Adopt an event-driven microservices architecture with pub/sub messaging (e.g., MQTT/Kafka) for scalable, real-time CPS analytics.",
        "Implement edge–cloud split processing to reduce latency and bandwidth for video/sensor analytics; deploy model compression/quantization on edge.",
        "Integrate identity, access control, and zero-trust networking across CPS components; apply secure update and attestation for IoT devices.",
        "Add cross-camera tracking and re-identification with unsupervised domain adaptation to handle camera heterogeneity and environmental drift.",
        "Introduce synthetic data generation and simulation to augment rare event training data for terrorism-relevant scenarios.",
        "Apply formal methods or runtime monitoring for safety constraints (e.g., crowd density thresholds) linked to automated response playbooks."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://shorturl.at/hoyJT",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Municipal public spaces (smart city CPS)",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Lack of industry adoption of academic approaches; limited market implementations.",
        "Legal and privacy constraints around surveillance and data processing.",
        "Data sharing barriers across agencies and municipalities.",
        "Operational challenges: occlusion handling, multi-camera synchronization, and real-time analytics at scale.",
        "Costs, integration complexity, and multi-stakeholder coordination."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": true,
      "fairness_bias": false
    },
    "contributions": [
      "Systematic multivocal literature survey (112 sources, 2004–mid-2022) on CPS for protecting public spaces against terrorism.",
      "Identification of three major fields for public space protection: Information Technologies, Architectural approaches, Organizational field.",
      "Quantitative indicators and mapping to reveal underdeveloped CPS areas and research opportunities.",
      "Synthesis of techniques, prevention measures, and assessment methods; best practices for governments and private organizations.",
      "Observation of limited market implementation of academic approaches, highlighting academia–industry gap.",
      "Replication package to support continuation and practical uptake of the surveyed technologies and methodologies."
    ]
  },
  {
    "arxiv_id": "2312.07575v1",
    "title": "TapTree: Process-Tree Based Host Behavior Modeling and Threat Detection Framework via Sequential Pattern Mining",
    "authors": "Mohammad Mamun; Scott Buffett",
    "abstract": "Audit logs containing system level events are frequently used for behavior modeling as they can provide detailed insight into cyber-threat occurrences. However, mapping low-level system events in audit logs to highlevel behaviors has been a major challenge in identifying host contextual behavior for the purpose of detecting potential cyber threats. Relying on domain expert knowledge may limit its practical implementation. This paper presents TapTree, an automated process-tree based technique to extract host behavior by compiling system events' semantic information. After extracting behaviors as system generated process trees, TapTree integrates event semantics as a representation of behaviors. To further reduce pattern matching workloads for the analyst, TapTree aggregates semantically equivalent patterns and optimizes representative behaviors. In our evaluation against a recent benchmark audit log dataset (DARPA OpTC), TapTree employs tree pattern queries and sequential pattern mining techniques to deduce the semantics of connected system events, achieving high accuracy for behavior abstraction and then Advanced Persistent Threat (APT) attack detection. Moreover, we illustrate how to update the baseline model gradually online, allowing it to adapt to new log patterns over time.",
    "published_date": "2023-12-10",
    "pdf_link": "https://arxiv.org/pdf/2312.07575v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Host-based Security",
      "subdomain": "Behavioral Anomaly Detection / Intrusion Detection",
      "specific_problem": "Process-tree based host behavior modeling and APT detection from system audit logs",
      "attack_types": [
        "Advanced Persistent Threat (APT)",
        "Insider Threat"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Tree/Graph Pattern Matching",
        "specific": "Process-tree exact/partial matching with DFS-based querying and weighted scoring",
        "novel_contribution": "Semantic aggregation of process-tree patterns, forward-pruning and optimized baseline growth; partial-match scoring emphasizing deeper nodes"
      },
      {
        "type": "primary",
        "category": "Sequential Pattern Mining / Sequence Classification",
        "specific": "Sequential pattern mining over traces extracted from task process-trees",
        "novel_contribution": "Use of mined temporal patterns from aggregated process-trees for supervised binary classification of malicious vs benign tasks"
      },
      {
        "type": "primary",
        "category": "Unsupervised One-Class Detection",
        "specific": "Tree pattern queries against baseline model",
        "novel_contribution": "Thresholdable partial-match scoring enabling recall/FPR trade-off on process-tree patterns"
      },
      {
        "type": "baseline",
        "category": "Ablation: No semantic aggregation",
        "specific": "Temporal tree set and clustered trees without semantic aggregation",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Unsupervised",
      "Supervised"
    ],
    "datasets": [
      {
        "name": "DARPA OpTC (Operationally Transparent Cyber) public dataset",
        "type": "public",
        "domain": "system_audit_logs",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Without semantic aggregation (Clustered Trees) vs With semantic aggregation",
        "paper_reference": null,
        "metric": "Pattern matching time (ms) on baseline queries (Table 1)",
        "their_result": "Semantic Aggregation: 47.0608 ms",
        "baseline_result": "Clustered Trees: 65.3992 ms"
      },
      {
        "method_name": "Raw temporal tree set vs aggregated model (size)",
        "paper_reference": null,
        "metric": "Number of trees (reduction)",
        "their_result": "36 trees after semantic aggregation",
        "baseline_result": "3501 trees in temporal tree set (≈98% reduction reported)"
      }
    ],
    "performance_metrics_used": [
      "accuracy",
      "recall",
      "false positive rate (FPR)",
      "pattern matching time (ms)",
      "baseline generation time (s)",
      "model size (#trees) reduction (%)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can low-level audit log events be automatically abstracted into high-level host behaviors without relying on expert-crafted rules?",
        "Can process-tree semantics and aggregation provide a distinctive and consistent baseline of host behavior for anomaly/APT detection with low FPR?",
        "How do tree pattern queries (unsupervised) and sequential pattern mining (supervised) perform for APT detection on OpTC?",
        "Can the baseline be updated online to adapt to new log patterns while maintaining detection effectiveness?"
      ],
      "gaps_identified": [
        "Existing methods often rely heavily on domain expert knowledge or rule stores for mapping low-level events to high-level behaviors.",
        "Sequence-only log modeling ignores hierarchical, filiation, and logical relationships across concurrent tasks/threads, reducing reliability.",
        "High event volume and noise make manual behavior extraction infeasible; need aggregation of semantically equivalent patterns.",
        "Provenance/knowledge graphs can be computationally heavy for large-scale detection."
      ],
      "limitations": [
        "Assumes integrity and security of the audit logging/monitoring platform (trusted computing base).",
        "Evaluation conducted on 14 randomly selected hosts from OpTC; broader generalization not empirically shown.",
        "Trade-off between recall and FPR depends on partial-match threshold selection; supervised recall reported as 67–86% depending on operating point.",
        "Focuses on program-path abstraction; may omit finer-grained arguments/attributes that could improve discrimination.",
        "Real-world deployment and end-to-end latency under production load not evaluated."
      ],
      "future_work": [],
      "motivation": "Reduce reliance on expert rules by automatically abstracting host behaviors from audit logs into semantically aggregated process-tree patterns to detect APTs with low analyst workload.",
      "potential_research_ideas": [
        "Integrate graph neural networks or Tree-LSTMs over process-trees to learn similarity scores for partial matching and improve recall at low FPR.",
        "Develop transfer learning/domain adaptation to port baseline behaviors across hosts and organizations while accounting for environment-specific software stacks.",
        "Incorporate concept drift detection and adaptive thresholds for partial matching to stabilize FPR over time in evolving environments.",
        "Combine semantic aggregation with provenance graph attributes (e.g., file hashes, network endpoints) for multi-modal detection.",
        "Design explainability modules that map detected anomalies to human-readable subtrees and temporal motifs with importance scores.",
        "Study adversarial robustness where attackers craft process trees to mimic benign semantics; develop robust matching and anomaly scoring.",
        "Leverage self-supervised pretraining on large unlabeled audit logs to learn behavior embeddings before supervised fine-tuning.",
        "Online streaming implementation with bounded-memory data structures for real-time detection on high-throughput endpoints."
      ],
      "architectural_improvement_recommendations": [
        "Replace heuristic partial-match scoring with a learned similarity model (e.g., Siamese GNN over process-trees) calibrated for recall/FPR trade-offs.",
        "Augment nodes/edges with richer features (arguments, user/session, integrity levels) and use attention-based weighting instead of depth-only weights.",
        "Introduce hierarchical clustering with dynamic time warping (DTW) for sequences extracted from trees to capture timing variability.",
        "Implement adaptive thresholding using Bayesian calibration or ROC-constrained optimization per-host to control FPR.",
        "Parallelize baseline tree growth and matching (e.g., index by subtree hashes/encodings) to scale to enterprise fleets.",
        "Hybrid detector: first-stage fast partial match, second-stage sequence classifier with calibrated probabilities and uncertainty estimates."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": "Tree pattern matching ~47.06 ms per query after semantic aggregation (Table 1)",
      "deployment_challenges": [
        "Requires trusted, comprehensive kernel-level auditing; SIEM integrity assumed.",
        "Threshold tuning for partial-match to balance recall vs FPR across diverse hosts.",
        "Concept drift and evolving software stacks require online updates and validation.",
        "Managing storage and indexing of process-tree baselines at enterprise scale.",
        "High event volumes may require streaming aggregation and resource controls."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "TapTree: first approach (per authors) to host behavioral abstraction that utilizes system process trees to aggregate semantically equivalent patterns.",
      "Noise reduction and optimized tree growth (forward pruning) plus aggregation of similar behaviors to reduce analyst workload.",
      "Validation on DARPA OpTC with tree pattern queries and sequential pattern mining; shows high accuracy for behavior abstraction and APT detection.",
      "Demonstrates online baseline update to adapt to new log patterns over time.",
      "Reports large reduction in behavior patterns (≈98% fewer process-trees after aggregation) and improved matching speed versus non-aggregated baselines.",
      "Quantitative detection results: tree-pattern queries achieve >99% accuracy and FPR < 0.8% at a given threshold; adjusting threshold attains 100% recall with FPR ≈ 2.9%; sequential pattern analysis achieves FPR < 0.1% with >99.9% accuracy and 67% recall, or 86% recall with FPR < 1%."
    ]
  },
  {
    "arxiv_id": "2310.19452v1",
    "title": "Incorporating Zero-Knowledge Succinct Non-interactive Argument of Knowledge for Blockchain-based Identity Management with off-chain computations",
    "authors": "Pranay Kothari; Deepak Chopra; Manjot Singh; Shivam Bhardwaj; Rudresh Dwivedi",
    "abstract": "In today's world, secure and efficient biometric authentication is of keen importance. Traditional authentication methods are no longer considered reliable due to their susceptibility to cyber-attacks. Biometric authentication, particularly fingerprint authentication, has emerged as a promising alternative, but it raises concerns about the storage and use of biometric data, as well as centralized storage, which could make it vulnerable to cyber-attacks. In this paper, a novel blockchain-based fingerprint authentication system is proposed that integrates zk-SNARKs, which are zero-knowledge proofs that enable secure and efficient authentication without revealing sensitive biometric information. A KNN-based approach on the FVC2002, FVC2004 and FVC2006 datasets is used to generate a cancelable template for secure, faster, and robust biometric registration and authentication which is stored using the Interplanetary File System. The proposed approach provides an average accuracy of 99.01%, 98.97% and 98.52% over the FVC2002, FVC2004 and FVC2006 datasets respectively for fingerprint authentication. Incorporation of zk-SNARK facilitates smaller proof size. Overall, the proposed method has the potential to provide a secure and efficient solution for blockchain-based identity management.",
    "published_date": "2023-10-30",
    "pdf_link": "https://arxiv.org/pdf/2310.19452v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Identity and Access Management",
      "subdomain": "Biometric Authentication",
      "specific_problem": "Privacy-preserving fingerprint authentication for blockchain-based identity management with off-chain storage and zk-SNARK verification",
      "attack_types": [
        "identity theft",
        "data leakage/privacy breach"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Nearest Neighbor",
        "specific": "K-NNS (k-Nearest Neighborhood Structure) for cancelable template generation",
        "novel_contribution": "Generates rotation-invariant minutiae neighborhood features; converts to 2D grid -> 1D bitstring -> DFT -> user-specific random matrix projection to produce cancelable templates compatible with zk-SNARK threshold checking"
      },
      {
        "type": "primary",
        "category": "Signal Processing",
        "specific": "Discrete Fourier Transform (DFT) of bitstring followed by random projection",
        "novel_contribution": "Non-invertible transformation of fingerprint-derived bitstrings via DFT and user-specific random matrix to produce cancelable templates"
      },
      {
        "type": "primary",
        "category": "Cryptographic Proof System",
        "specific": "zk-SNARK (Groth16; R1CS/QAP-based)",
        "novel_contribution": "Designs arithmetic circuits (threshold check on global matching score) compiled to R1CS/QAP for off-chain proof generation with on-chain verification; stores only IPFS CID on-chain"
      }
    ],
    "learning_paradigm": [
      "Unsupervised",
      "Template matching",
      "Cryptographic verification"
    ],
    "datasets": [
      {
        "name": "FVC2002",
        "type": "public",
        "domain": "biometric_fingerprints",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "FVC2004",
        "type": "public",
        "domain": "biometric_fingerprints",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "FVC2006",
        "type": "public",
        "domain": "biometric_fingerprints",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Accuracy",
      "Proof size",
      "Verifier time"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can zk-SNARKs enable secure and efficient biometric authentication on blockchain without revealing sensitive biometric information?",
        "Can a KNN-based cancelable template yield accurate and robust fingerprint authentication suitable for decentralized identity management?"
      ],
      "gaps_identified": [
        "Centralized storage of biometric data creates single points of failure and privacy risks in current systems",
        "Lack of transparency and auditability in proprietary fingerprint matching algorithms",
        "Blockchain-based IdM needs reduced on-chain data footprint and privacy-preserving verification"
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Enhance security, privacy, and scalability of biometric identity management by integrating cancelable fingerprint templates with zk-SNARK-based verification and decentralized storage (IPFS) to avoid exposing sensitive data or relying on centralized databases.",
      "potential_research_ideas": [
        "Quantitatively evaluate zk-SNARK performance trade-offs (proof size, proving and verification time, gas costs) across different proving systems (Groth16, PLONK, Halo2) for the threshold circuit",
        "Extend to multimodal biometrics (fingerprint + iris/face) with zk-composable circuits enabling configurable policies (k-of-n modalities)",
        "Incorporate liveness/spoof detection and evaluate robustness to presentation attacks within a privacy-preserving pipeline",
        "Study revocation and re-enrollment strategies for cancelable templates and associated key management, including template update without re-onboarding",
        "Adopt SNARK-friendly primitives (Poseidon/Rescue hashes, field-friendly fixed-point arithmetic) to shrink circuit size",
        "Design a rollup-style aggregator to batch multiple authentication proofs off-chain and reduce on-chain costs",
        "Conduct comprehensive comparisons with other cancelable biometric schemes and deep-learning-based fingerprint representations under the same protocol",
        "Formal security analysis covering linkage attacks, template inversion attempts, and record multiplicity across services"
      ],
      "architectural_improvement_recommendations": [
        "Replace SHA256 and generic comparators with SNARK-friendly hashes and range/compare gadgets to reduce constraints",
        "Parameterize and report the threshold circuit using fixed-point arithmetic optimized for the field modulus to minimize gate count",
        "Use structured reference string (universal SRS) systems (e.g., PLONK-ish) to avoid per-circuit trusted setup",
        "Introduce an off-chain prover marketplace/queue and proof batching to improve throughput",
        "Integrate on-chain verifiers with gas-efficient precompiles and deploy verifiers on an L2 rollup to further cut costs",
        "Adopt learned minutiae extraction or minutiae graph neural encoders, then quantize to SNARK-friendly representations for improved accuracy at similar circuit cost"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Blockchain smart contract storing IPFS CID; off-chain zk-SNARK proving",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": true,
      "fairness_bias": false
    },
    "contributions": [
      "Efficient KNN-S-based algorithm for cancelable fingerprint template generation",
      "Integration of zero-knowledge proofs (zk-SNARKs) for privacy-preserving authentication",
      "Use of IPFS to store cancelable templates with on-chain storage of CID via smart contracts",
      "Reported accuracy: \"The proposed approach provides an average accuracy of 99.01%, 98.97% and 98.52% over the FVC2002, FVC2004 and FVC2006 datasets respectively\"",
      "zk-SNARK design with arithmetic circuits (threshold check and comparators) compiled to R1CS/QAP; mentions Groth16 layers (G1, G2, G3)"
    ]
  },
  {
    "arxiv_id": "2310.11706v1",
    "title": "MalDICT: Benchmark Datasets on Malware Behaviors, Platforms, Exploitation, and Packers",
    "authors": "Robert J. Joyce; Edward Raff; Charles Nicholas; James Holt",
    "abstract": "Existing research on malware classification focuses almost exclusively on two tasks: distinguishing between malicious and benign files and classifying malware by family. However, malware can be categorized according to many other types of attributes, and the ability to identify these attributes in newly-emerging malware using machine learning could provide significant value to analysts. In particular, we have identified four tasks which are under-represented in prior work: classification by behaviors that malware exhibit, platforms that malware run on, vulnerabilities that malware exploit, and packers that malware are packed with. To obtain labels for training and evaluating ML classifiers on these tasks, we created an antivirus (AV) tagging tool called ClarAVy. ClarAVy's sophisticated AV label parser distinguishes itself from prior AV-based taggers, with the ability to accurately parse 882 different AV label formats used by 90 different AV products. We are releasing benchmark datasets for each of these four classification tasks, tagged using ClarAVy and comprising nearly 5.5 million malicious files in total. Our malware behavior dataset includes 75 distinct tags - nearly 7x more than the only prior benchmark dataset with behavioral tags. To our knowledge, we are the first to release datasets with malware platform and packer tags.",
    "published_date": "2023-10-18",
    "pdf_link": "https://arxiv.org/pdf/2310.11706v1",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "benchmark"
    ],
    "security_domain": {
      "primary": "Malware Analysis",
      "subdomain": "Malware Classification",
      "specific_problem": "Multi-attribute malware tagging/classification: behaviors, platforms, exploited vulnerabilities, and packers",
      "attack_types": [
        "ransomware",
        "worm",
        "backdoor",
        "downloader",
        "virus",
        "adware",
        "dropper",
        "pua",
        "redirector"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Rule-based NLP parsing",
        "specific": "Custom AV label parser with delimiter-format–specific regex functions",
        "novel_contribution": "ClarAVy parses 882 AV label delimiter formats across 90 AV products; assigns lexical categories (BEH/PLAT/VULN/PACK/FAM) per token using format-aware parsing."
      },
      {
        "type": "primary",
        "category": "String similarity / Edit distance",
        "specific": "Edit distance with heuristics (substring/anagram caps) and co-occurrence scoring",
        "novel_contribution": "Alias resolution combining co-occurrence percentage and a custom edit-score with thresholds E=0.6 and C=0.5 to find parent–child aliases."
      },
      {
        "type": "primary",
        "category": "Heuristic ensemble weighting",
        "specific": "Correlation-aware voting across AV products",
        "novel_contribution": "Token scoring that merges votes from correlated AV engines and applies category-specific thresholds (T=5 for BEH/PLAT, T=1 for VULN/PACK)."
      },
      {
        "type": "primary",
        "category": "Graph/Traversal",
        "specific": "BFS over parent–child relationships",
        "novel_contribution": "Algorithm to resolve groups of aliases by traversing child relationships and selecting canonical names by frequency."
      }
    ],
    "learning_paradigm": [],
    "datasets": [
      {
        "name": "MalDICT-Behavior",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "MalDICT-Platform",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "MalDICT-Vulnerability",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "MalDICT-Packer",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "SOREL",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Malicia dataset",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "VirusShare (chunks 0–465)",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "VirusTotal AV scan reports (~40M reports)",
        "type": "public",
        "domain": "av_scan_reports",
        "link": null,
        "is_new_contribution": false,
        "availability": "available_on_request"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Existing malware ML focuses on detection and family classification; other attributes are under-studied.",
        "Behavioral tagging datasets are limited: SOREL provides only 11 behavior tags.",
        "No publicly-available benchmark datasets for malware platforms and packers prior to this work.",
        "Malicia’s exploit-tagged dataset is outdated and no longer publicly distributed.",
        "AV label heterogeneity and aliasing hinder consistent non-family tagging without sophisticated parsing."
      ],
      "limitations": [
        "Accurately ranking FAM (family) tokens is much more challenging and deferred to future work.",
        "Some AV labels cannot be fully parsed; tokens may remain ambiguous (PRE/UNK categories).",
        "Reliance on AV outputs introduces vendor correlation and potential propagation of vendor errors.",
        "Generic tokens like 'trojan' and 'win32' are ubiquitous and uninformative, requiring manual exclusions.",
        "Threshold choices trade off noise vs. coverage, especially for rare VULN/PACK tokens."
      ],
      "future_work": [
        "Accurate ranking/selection of FAM (family) tokens.",
        "Deeper modeling and mitigation of AV product correlations beyond currently-identified public relationships."
      ],
      "motivation": "Enable ML classification of under-represented malware attributes (behaviors, platforms, vulnerabilities, packers) by releasing large, labeled benchmark datasets derived from AV scan reports via a sophisticated parser (ClarAVy).",
      "potential_research_ideas": [
        "Develop multi-task/multi-label classifiers that jointly predict behavior, platform, packer, and exploited vulnerability on MalDICT.",
        "Train noise-robust learning methods that account for AV-vendor–induced label noise and correlations.",
        "Leverage self-supervised or representation learning on raw binaries to improve prediction of rare VULN/PACK labels.",
        "Fuse dynamic analysis (sandbox behavior) to validate and refine ClarAVy-derived tags and reduce ambiguity.",
        "Build a probabilistic model of AV vendor reliability and dependency to produce calibrated token probabilities.",
        "Automate alias discovery with weakly supervised or contrastive learning over AV labels plus external knowledge (e.g., CVE/CWE).",
        "Create hierarchical taxonomies and hierarchical loss functions for behaviors/platforms to exploit structure in labels.",
        "Evaluate cross-time generalization and dataset shift (emerging malware) on MalDICT with continual learning."
      ],
      "architectural_improvement_recommendations": [
        "Replace regex-only parsing with a sequence labeling model (e.g., CRF or Transformer tagger) trained to assign lexical categories to AV-label tokens while incorporating delimiter-format priors.",
        "Introduce a Bayesian inference layer that models AV vendor correlations and outputs calibrated posterior probabilities for each token.",
        "Generalize alias resolution via graph clustering with joint optimization of edit-similarity and co-occurrence, using community detection.",
        "Incorporate external knowledge bases (CVE/CWE/CPE, Packer signatures) for canonicalization and disambiguation during parsing.",
        "Adopt adaptive, per-token thresholds using uncertainty estimates instead of fixed T values across categories.",
        "Provide multi-task baselines (e.g., shared encoder with task-specific heads) and publish standardized splits/evaluation scripts."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "Python"
      ],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Correlations and dependencies among AV products complicate independent voting.",
        "Parsing ambiguity leading to PRE/UNK tokens for some labels.",
        "Vendor naming inconsistencies and aliases across 90 AV products.",
        "Potential bias/noise inherited from AV detections and heuristics."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Release of MalDICT: four benchmark datasets totaling nearly 5.5M malicious files for behaviors (75 tags), platforms (43 tags), vulnerabilities (128 tags), and packers (79 tags).",
      "ClarAVy: a sophisticated AV label parser covering 882 delimiter formats across 90 AV products, producing lexical categories and correlation-aware token rankings.",
      "Alias resolution framework combining co-occurrence and edit-distance with BFS-based canonicalization.",
      "Benchmark baselines trained on each MalDICT dataset to enable comparison (two standard malware classifiers).",
      "First public datasets with malware platform and packer tags; behavior dataset has nearly 7× more tags than prior public benchmark (SOREL)."
    ]
  },
  {
    "arxiv_id": "2311.17026v1",
    "title": "When the Few Outweigh the Many: Illicit Content Recognition with Few-Shot Learning",
    "authors": "G. Cascavilla; G. Catolino; M. Conti; D. Mellios; D. A. Tamburri",
    "abstract": "The anonymity and untraceability benefits of the Dark web account for the exponentially-increased potential of its popularity while creating a suitable womb for many illicit activities, to date. Hence, in collaboration with cybersecurity and law enforcement agencies, research has provided approaches for recognizing and classifying illicit activities with most exploiting textual dark web markets' content recognition; few such approaches use images that originated from dark web content. This paper investigates this alternative technique for recognizing illegal activities from images. In particular, we investigate label-agnostic learning techniques like One-Shot and Few-Shot learning featuring the use Siamese neural networks, a state-of-the-art approach in the field. Our solution manages to handle small-scale datasets with promising accuracy. In particular, Siamese neural networks reach 90.9% on 20-Shot experiments over a 10-class dataset; this leads us to conclude that such models are a promising and cheaper alternative to the definition of automated law-enforcing machinery over the dark web.",
    "published_date": "2023-11-28",
    "pdf_link": "https://arxiv.org/pdf/2311.17026v1",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Cyber Threat Intelligence",
      "subdomain": "Dark Web Monitoring",
      "specific_problem": "Illicit content recognition from Dark Web images under low-data (one/few-shot) regimes",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Siamese Network (Metric Learning)",
        "specific": "One-shot Siamese network using Euclidean distance between twin embedding nets",
        "novel_contribution": "Applies label-agnostic one-shot learning with Siamese networks to illicit Dark Web image recognition, a setting not previously studied."
      },
      {
        "type": "primary",
        "category": "Siamese Network (Metric Learning)",
        "specific": "Few-shot Siamese network (e.g., 20-shot) using Euclidean distance",
        "novel_contribution": "Shows that few-shot Siamese networks can achieve high accuracy (reported 90.9% on 20-shot over 10 classes) on small noisy Dark Web image datasets."
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Metric Learning",
      "One-shot",
      "Few-shot"
    ],
    "datasets": [
      {
        "name": "Dark Web Illicit Images (Replication-Package 2023)",
        "type": "proprietary",
        "domain": "web_images",
        "link": null,
        "is_new_contribution": true,
        "availability": "available_on_request"
      },
      {
        "name": "DUTA (Darknet Usage Text Addresses) dataset",
        "type": "public",
        "domain": "web_text",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "TOIC (Dark Web illicit images dataset) (Fidalgo et al., 2018/2019)",
        "type": "public",
        "domain": "web_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Hashemi and Hall Dark Web Images Corpus (~120k train, 1.2M test)",
        "type": "proprietary",
        "domain": "web_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "AT&T (ORL) Face Dataset",
        "type": "public",
        "domain": "face_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "AR Face Database",
        "type": "public",
        "domain": "face_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "eBay + Legal Onion pages corpus (Choshen et al., 2019)",
        "type": "proprietary",
        "domain": "web_text",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Twitter multilingual cybersecurity corpus (Ranade et al., 2018)",
        "type": "proprietary",
        "domain": "social_media_text",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "accuracy"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "RQ. To what extent can illicit Dark Web content be classified through a limited number of images?",
        "SRQ1. To what extent One-Shot technique using Siamese Neural Networks can identify illicit images from the Dark Web?",
        "SRQ2. To what extent Few-Shot technique using Siamese Neural Networks can identify illicit images from the Dark Web?"
      ],
      "gaps_identified": [
        "Most Dark Web content recognition focuses on text; image-based illicit activity recognition is underexplored.",
        "Existing image studies rely on large, well-labeled, and often \"ideal\"/clean datasets, which are unrealistic for Dark Web scraping.",
        "Specific fine-grained illicit categories have not been deeply investigated due to data scarcity and imbalance.",
        "Label-agnostic one/few-shot learning had not been applied to Dark Web illicit image recognition prior to this work."
      ],
      "limitations": [
        "Final dataset remains imbalanced, requiring heavy data augmentation to balance classes.",
        "Severe scarcity for some categories (e.g., passports had fewer than five images; limited weapon images).",
        "Data collection was constrained by marketplace security (CAPTCHAs, anti-scraping measures) and legal/ethical considerations.",
        "Manual relabeling and duplicate removal steps were required; potential label noise remains.",
        "Evaluation emphasizes accuracy; limited set of metrics and no comparisons to alternative architectures or classical baselines.",
        "Reported peak performance (90.9%) is on a 10-class subset (943 unseen test images), not the full 55-class dataset.",
        "Dataset distribution is from a limited time window (Jan–Mar 2020) and a few markets, possibly limiting generalization.",
        "Full raw dataset is available only on request (stored on encrypted drive), hindering out-of-the-box reproducibility."
      ],
      "future_work": [],
      "motivation": "Enable rapid, low-cost identification of illicit Dark Web activities from images in low-data scenarios to support law enforcement and cybersecurity monitoring.",
      "potential_research_ideas": [
        "Develop a standardized benchmark for Dark Web illicit image recognition with multiple difficulty tiers and episodic few-shot splits.",
        "Explore meta-learning approaches (Prototypical Networks, Matching Networks, Relation Networks, MAML) for this domain.",
        "Leverage self-supervised and contrastive pretraining (e.g., SimCLR/MoCo) on large-scale unlabeled web/dark web images, followed by few-shot adaptation.",
        "Cross-modal learning combining product text and images from Dark Web listings for multi-modal illicit content recognition.",
        "Domain adaptation and robust generalization to new markets and evolving visual patterns (unsupervised DA, test-time adaptation).",
        "Active learning and human-in-the-loop strategies to prioritize labeling the most informative dark web images.",
        "Build adversarially robust detectors against obfuscations used by sellers (watermarks, overlays, compression artifacts).",
        "Automate fine-grained illicit taxonomy construction and hierarchical few-shot classification.",
        "Ethical data sharing framework for dark web images enabling privacy-preserving research access."
      ],
      "architectural_improvement_recommendations": [
        "Adopt stronger encoders (e.g., ResNet50/101, ConvNeXt, Vision Transformers) as Siamese backbones with fine-tuning.",
        "Use episodic meta-training with Prototypical Networks or Relation Networks and compare against Siamese-contrastive loss.",
        "Employ triplet/multi-similarity losses with hard/semi-hard negative mining to improve embedding separation.",
        "Integrate modern augmentations (RandAugment, CutMix, MixUp) and test-time augmentation to enhance robustness.",
        "Calibrate embeddings with temperature scaling and use metric learning calibration for open-set detection of novel illicit classes.",
        "Add uncertainty estimation (MC Dropout/Deep Ensembles) to flag low-confidence predictions for analyst review.",
        "Incorporate synthetic data generation (GANs/Diffusion) constrained by forensic realism to alleviate extreme class scarcity.",
        "Evaluate and optimize using additional metrics (F1, per-class recall, AUROC) under class imbalance."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "TensorFlow",
        "scikit-learn",
        "imgaug"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Data collection barriers on Dark Web (CAPTCHAs, anti-scraping defenses, blocking).",
        "Class imbalance and extreme data scarcity for certain illicit categories.",
        "Legal/ethical constraints on acquiring and sharing dark web imagery.",
        "Noisy, low-quality, and heterogeneous images from marketplaces.",
        "Distribution shift across markets and over time; need for continual adaptation.",
        "Potential adversarial obfuscations by sellers (watermarks, overlays)."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Novel Dark Web illicit image dataset (~3,570 images after cleaning) across 55 categories (e.g., drugs, weapons, counterfeit).",
      "New application of One-Shot learning with Siamese Neural Networks for illicit image identification on the Dark Web.",
      "New application of Few-Shot learning with Siamese Neural Networks for illicit image identification on the Dark Web.",
      "An online repository/replication package reporting raw data for further research (data available upon request)."
    ]
  },
  {
    "arxiv_id": "2311.00240v1",
    "title": "Intell-dragonfly: A Cybersecurity Attack Surface Generation Engine Based On Artificial Intelligence-generated Content Technology",
    "authors": "Xingchen Wu; Qin Qiu; Jiaqi Li; Yang Zhao",
    "abstract": "With the rapid development of the Internet, cyber security issues have become increasingly prominent. Traditional cyber security defense methods are limited in the face of ever-changing threats, so it is critical to seek innovative attack surface generation methods. This study proposes Intell-dragonfly, a cyber security attack surface generation engine based on artificial intelligence generation technology, to meet the challenges of cyber security. Based on ChatGPT technology, this paper designs an automated attack surface generation process, which can generate diversified and personalized attack scenarios, targets, elements and schemes. Through experiments in a real network environment, the effect of the engine is verified and compared with traditional methods, which improves the authenticity and applicability of the attack surface. The experimental results show that the ChatGPT-based method has significant advantages in the accuracy, diversity and operability of attack surface generation. Furthermore, we explore the strengths and limitations of the engine and discuss its potential applications in the field of cyber security. This research provides a novel approach to the field of cyber security that is expected to have a positive impact on defense and prevention of cyberthreats.",
    "published_date": "2023-11-01",
    "pdf_link": "https://arxiv.org/pdf/2311.00240v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Threat Modeling and Attack Simulation",
      "subdomain": "Attack Surface Management",
      "specific_problem": "Automated generation of cyber attack surfaces (scenarios, targets, elements, exploit code) using LLMs",
      "attack_types": [
        "Vulnerability exploitation",
        "Local privilege escalation (e.g., CVE-2021-3493)",
        "Attack simulation"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer LLM",
        "specific": "ChatGPT (GPT-3.5/GP T-4 family; proprietary)",
        "novel_contribution": "Uses prompt-engineering with a structured attack playbook template to automatically generate attack scenarios and implementable exploit code; iterative refinement by feeding execution errors back to the LLM"
      },
      {
        "type": "primary",
        "category": "Prompt Engineering",
        "specific": "Bypass mechanisms via scenario framing, imperative prompts, intent disclaimers",
        "novel_contribution": "Demonstrates a practical prompt-bypass workflow to elicit exploit code for a known CVE in a controlled environment"
      },
      {
        "type": "primary",
        "category": "In-context generation",
        "specific": "Template-guided scenario/script generation",
        "novel_contribution": "Template covers background/scene design, targets, elements, and solution, guiding LLM outputs toward actionable attack surfaces"
      }
    ],
    "learning_paradigm": [
      "Zero-shot prompting",
      "In-context learning",
      "Prompt engineering"
    ],
    "datasets": [
      {
        "name": "Cyber security attack surface dataset (internal)",
        "type": "private",
        "domain": "network_topology, operating_systems, services, attack_events, vulnerability_descriptions",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Real network environment data (internal testbed)",
        "type": "private",
        "domain": "system_configurations, deployment_environment, exploit_execution_logs",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "CVE-2021-3493 vulnerability description",
        "type": "public",
        "domain": "vulnerability_descriptions",
        "link": "https://nvd.nist.gov/vuln/detail/CVE-2021-3493",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "accuracy",
      "diversity",
      "operability",
      "authenticity",
      "applicability",
      "flexibility",
      "adaptability"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can a ChatGPT-based engine automatically generate diversified and personalized attack scenarios, targets, elements, and schemes?",
        "Does the LLM-based approach improve the authenticity, applicability, accuracy, diversity, and operability of generated attack surfaces compared to traditional rule-based/manual methods?",
        "Can prompt engineering be used to bypass safety checks to obtain implementable exploit code in a controlled, legal setting?",
        "What are the strengths, limitations, and potential applications of an AIGC-based attack surface generation engine?"
      ],
      "gaps_identified": [
        "Traditional attack surface generation relies on manual rules/expert knowledge and lacks flexibility and operability in dynamic environments.",
        "Limited coverage and timeliness when new vulnerabilities appear; high time/effort to enumerate attack surfaces.",
        "Generated attack surfaces in prior methods can be incomplete/inaccurate; false positives/negatives.",
        "Models can be susceptible to adversarial prompt/attack techniques that yield incorrect attack surfaces."
      ],
      "limitations": [
        "Dependence on a proprietary LLM (ChatGPT) and its moderation policies; reliance on prompt-bypass techniques.",
        "Lack of detailed quantitative evaluation and standardized metrics; comparisons to “traditional methods” are not specified.",
        "Potential for misuse and ethical/legal concerns around generating exploit code.",
        "Robustness/security of the LLM not rigorously evaluated (e.g., adversarial prompts, hallucinations).",
        "Generalization to diverse enterprise environments not demonstrated; details of the real network environment are limited."
      ],
      "future_work": [
        "Broaden evaluation with standardized, quantitative benchmarks and clearer baselines.",
        "Integrate structured knowledge (e.g., MITRE ATT&CK, CVE/NVD) for retrieval-augmented generation.",
        "Automate validation via sandboxed execution, static/dynamic analysis, and feedback-driven refinement.",
        "Assess robustness against adversarial prompts and implement stronger guardrails and alignment layers.",
        "Explore deployment in varied operational environments and study scalability and governance."
      ],
      "motivation": "Address the limitations of manual/rule-based attack surface generation by leveraging AIGC (ChatGPT) to rapidly produce actionable, diverse, and realistic attack scenarios and exploit code for improved defense and prevention.",
      "potential_research_ideas": [
        "Create a public benchmark for attack surface generation with task definitions, gold scenarios, and executable validation suites.",
        "Design a retrieval-augmented attack-surface generator that grounds outputs in ATT&CK techniques, CVE/NVD, and exploit databases with source citations.",
        "Build an agentic system that iteratively plans, generates, executes in sandbox, and self-corrects exploit artifacts using program analysis and feedback.",
        "Develop safety-aligned prompting frameworks and guardrails that allow realistic simulation without enabling misuse (e.g., capability bounding).",
        "Investigate cross-LLM performance and robustness (Open-source LLMs vs proprietary) and ensemble methods for reliability.",
        "Formalize evaluation metrics for authenticity, operability, and diversity; correlate with defender training outcomes.",
        "Use static/dynamic analyzers and fuzzers in the loop to validate or auto-minimize PoCs, producing safer-to-share artifacts."
      ],
      "architectural_improvement_recommendations": [
        "Add retrieval-augmented generation over MITRE ATT&CK, NVD/CVE, ExploitDB, and vendor advisories with citation and confidence scoring.",
        "Introduce a multi-agent pipeline: recon/planning agent, exploit synthesis agent, sandbox executor, and verifier with automated error-feedback loops.",
        "Incorporate static analysis (e.g., CodeQL) and dynamic instrumentation to validate generated code before execution; auto-generate unit/integration tests.",
        "Implement policy and capability guardrails (red-teaming filters, intent verification, output sanitization) to prevent misuse and constrain high-risk outputs.",
        "Track provenance and uncertainty: structured output schema with evidence links, confidence, and risk ratings for each generated element.",
        "Support multiple LLM backends (open-source) for portability and cost control; add fine-tunable adapters for domain-specific jargon."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Real network environment/testbed (details not specified)",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Ethical/legal constraints and risk of misuse when generating exploit code.",
        "Dependence on proprietary LLM availability, cost, and policy changes.",
        "Potential inaccuracies/hallucinations requiring validation via sandboxing and analysis.",
        "Need for secure containment and governance for executing generated code.",
        "Reliance on prompt-bypass techniques may be fragile across LLM versions."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes Intell-dragonfly, an AIGC-based engine for automated attack surface generation using ChatGPT.",
      "Introduces a structured simulation attack script template (background/scene, targets, elements, solution) to guide LLM outputs.",
      "Demonstrates prompt-engineering bypass techniques to elicit implementable exploit code in controlled settings.",
      "Presents a case study reproducing CVE-2021-3493 exploit code via ChatGPT and iterative error-based refinement.",
      "Reports qualitative improvements versus traditional methods in accuracy, diversity, and operability of generated attack surfaces (no detailed quantitative tables)."
    ]
  },
  {
    "arxiv_id": "2310.06656v1",
    "title": "Assessing the Impact of a Supervised Classification Filter on Flow-based Hybrid Network Anomaly Detection",
    "authors": "Dominik Macko; Patrik Goldschmidt; Peter Pištek; Daniela Chudá",
    "abstract": "Constant evolution and the emergence of new cyberattacks require the development of advanced techniques for defense. This paper aims to measure the impact of a supervised filter (classifier) in network anomaly detection. We perform our experiments by employing a hybrid anomaly detection approach in network flow data. For this purpose, we extended a state-of-the-art autoencoder-based anomaly detection method by prepending a binary classifier acting as a prefilter for the anomaly detector. The method was evaluated on the publicly available real-world dataset UGR'16. Our empirical results indicate that the hybrid approach does offer a higher detection rate of known attacks than a standalone anomaly detector while still retaining the ability to detect zero-day attacks. Employing a supervised binary prefilter has increased the AUC metric by over 11%, detecting 30% more attacks while keeping the number of false positives approximately the same.",
    "published_date": "2023-10-10",
    "pdf_link": "https://arxiv.org/pdf/2310.06656v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Flow-based hybrid network anomaly detection with a supervised prefilter in ISP-scale networks",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Ensemble (Tree-based)",
        "specific": "Random Forest",
        "novel_contribution": "Used as a supervised binary (and evaluated multi-class) prefilter in a type-2 hybrid NIDS to remove known attacks before anomaly detection"
      },
      {
        "type": "primary",
        "category": "Autoencoder",
        "specific": "Variational Autoencoder (VAE)",
        "novel_contribution": "Unsupervised anomaly detector operating on 3-minute, per-source-IP aggregated flow features; extends a SOTA AE-based approach (GEE) into a hybrid pipeline"
      },
      {
        "type": "baseline",
        "category": "Autoencoder",
        "specific": "Autoencoder-based anomaly detector (GEE-style)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Ensemble (Tree-based)",
        "specific": "Random Forest (binary and multi-class) without anomaly detector",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "UGR'16",
        "type": "public",
        "domain": "network_flows",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Standalone autoencoder-based anomaly detector (GEE-like VAE)",
        "paper_reference": "Nguyen et al. [54] (GEE) as referenced",
        "metric": "AUC; detection rate; false positives",
        "their_result": "“Employing a supervised binary prefilter has increased the AUC metric by over 11%, detecting 30% more attacks while keeping the number of false positives approximately the same.”",
        "baseline_result": "AUC lower by >11%; detected fewer attacks; approximately same false positives"
      },
      {
        "method_name": "Binary Random Forest classifier only",
        "paper_reference": null,
        "metric": "Zero-day (unknown attack) detection capability",
        "their_result": "Hybrid retains ability to detect zero-day attacks and performs better than binary classification for zero-day detection (RQ3).",
        "baseline_result": "Binary classifier alone is limited to known attacks; inferior zero-day detection."
      },
      {
        "method_name": "Multi-class Random Forest prefilter (hybrid variant)",
        "paper_reference": null,
        "metric": "Overall detection performance vs. binary prefilter",
        "their_result": "Binary classifier prefilter outperforms multi-class prefilter in large-scale setting (RQ1).",
        "baseline_result": "Multi-class prefilter performs worse than binary prefilter."
      }
    ],
    "performance_metrics_used": [
      "AUC",
      "ROC curve",
      "Detection rate",
      "False positives",
      "False positive rate"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "RQ1: When choosing a prefilter for hybrid anomaly detection in real large-scale networks, does a binary classifier outperforms a multi-class one?",
        "RQ2: What is the impact of the classification-based prefilter on the detection and false alarm rates of anomaly detection in real large-scale networks?",
        "RQ3: Does a hybrid anomaly detector perform better than binary classification regarding the detection of zero-day attacks in real large-scale networks?"
      ],
      "gaps_identified": [
        "Lack of rigorous analyses confirming that hybrid NIDS improve performance, especially on modern, large-scale real networks.",
        "Most prior evaluations used smaller, controlled environments or deprecated datasets (e.g., NSL-KDD), limiting real-world relevance.",
        "High false positive costs (base-rate fallacy) in NIDS at ISP scale.",
        "Challenges with per-packet deep inspection due to encryption and high-speed networks; shift toward flow-level detection.",
        "Domain challenges: concept drift and lack of reliable standard datasets.",
        "Potential temporal experimental bias and attack visibility issues in common datasets."
      ],
      "limitations": [
        "Evaluation relies on a single dataset (UGR’16); authors discuss potential temporal experimental bias and attack visibility phenomena in this dataset.",
        "Flow-based aggregation (3-minute windows) and removal of samples with fewer than 10 flows may affect visibility/detection of short-lived events.",
        "Results are not from a live deployment; real-time performance and operational integration are not measured."
      ],
      "future_work": [],
      "motivation": "Measure the impact of adding a supervised classification-based prefilter to an AE-based anomaly detector in flow-based NIDS for ISP-scale networks, to improve known-attack detection while retaining zero-day detection.",
      "potential_research_ideas": [
        "Jointly train the classifier and autoencoder end-to-end with a multi-task or teacher-student objective to align representations and thresholds.",
        "Incorporate online/continual learning to handle concept drift in ISP-scale traffic with model monitoring and automatic recalibration.",
        "Use self-supervised pretraining on large unlabeled flow corpora (e.g., contrastive learning over flow windows) to improve anomaly detection.",
        "Explore sequence/temporal models (e.g., Transformers over flow-window sequences per host) to capture evolving behavior patterns.",
        "Graph-based detection using host-communication graphs and GNNs to augment flow-window features for better context.",
        "Calibrate and cost-sensitive learning to explicitly optimize under base-rate fallacy constraints (optimize precision at fixed alert budgets).",
        "Domain adaptation/transfer to new networks (different ISPs) with limited labels using unsupervised or weakly-supervised techniques.",
        "Evaluate hybrid type-3 (parallel) and type-4 designs for potential gains vs. computational cost on ISP data."
      ],
      "architectural_improvement_recommendations": [
        "Replace static RF prefilter with a calibrated probabilistic classifier (e.g., gradient boosting with Platt/Isotonic calibration) and dynamic thresholding by alert budget.",
        "Introduce a gating/routing module that sends uncertain samples to the VAE while auto-accepting high-confidence benign/attack cases.",
        "Augment VAE with robust/β-VAE or adversarially-trained AE to reduce false positives and improve stability on noisy flows.",
        "Leverage ensemble AEs with diversity (different feature subsets or window sizes) and aggregate anomaly scores.",
        "Incorporate concept-drift detectors (e.g., ADWIN) to trigger retraining or threshold updates for both prefilter and AE.",
        "Use multi-resolution windows (e.g., 1-, 3-, 5-minute) and hierarchical pooling to capture short- and long-term behaviors.",
        "Add explainability via feature attribution on RF (per-window SHAP) and reconstruction-contribution analysis for the VAE."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/kinit-sk/hybrid-anomaly-detection",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "High-speed traffic makes per-packet deep inspection impractical; favors flow-level processing.",
        "Widespread encryption limits payload-based detection.",
        "Base-rate fallacy: even small FPR yields many alerts in ISP-scale traffic.",
        "Concept drift in network behavior necessitates model updates.",
        "Potential computational cost when serializing multiple detectors in hybrid systems."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Measured the impact of supervised traffic filtering on anomaly detection using a real large-scale network dataset (UGR’16).",
      "Extended a SOTA autoencoder-based anomaly detector (GEE) into a hybrid approach with a supervised prefilter, achieving higher detection rate of known attacks.",
      "Provided a more comprehensive evaluation of GEE by exploring attack visibility and potential temporal experimental bias in the dataset.",
      "Empirically showed that a supervised binary prefilter increased AUC by over 11% and detected ~30% more attacks with approximately the same number of false positives, while retaining zero-day detection capability."
    ]
  },
  {
    "arxiv_id": "2310.06046v1",
    "title": "LLM for SoC Security: A Paradigm Shift",
    "authors": "Dipayan Saha; Shams Tarek; Katayoon Yahyaei; Sujan Kumar Saha; Jingbo Zhou; Mark Tehranipoor; Farimah Farahmandi",
    "abstract": "As the ubiquity and complexity of system-on-chip (SoC) designs increase across electronic devices, the task of incorporating security into an SoC design flow poses significant challenges. Existing security solutions are inadequate to provide effective verification of modern SoC designs due to their limitations in scalability, comprehensiveness, and adaptability. On the other hand, Large Language Models (LLMs) are celebrated for their remarkable success in natural language understanding, advanced reasoning, and program synthesis tasks. Recognizing an opportunity, our research delves into leveraging the emergent capabilities of Generative Pre-trained Transformers (GPTs) to address the existing gaps in SoC security, aiming for a more efficient, scalable, and adaptable methodology. By integrating LLMs into the SoC security verification paradigm, we open a new frontier of possibilities and challenges to ensure the security of increasingly complex SoCs. This paper offers an in-depth analysis of existing works, showcases practical case studies, demonstrates comprehensive experiments, and provides useful promoting guidelines. We also present the achievements, prospects, and challenges of employing LLM in different SoC security verification tasks.",
    "published_date": "2023-10-09",
    "pdf_link": "https://arxiv.org/pdf/2310.06046v1",
    "paper_types": [
      "position",
      "empirical_analysis",
      "survey"
    ],
    "security_domain": {
      "primary": "Hardware Security",
      "subdomain": "SoC Security Verification",
      "specific_problem": "Assessing how Large Language Models can assist with RTL-level SoC security tasks: vulnerability insertion (as a testbed), security assessment, security verification against rules/policies and metrics, and countermeasure development",
      "attack_types": [
        "Information leakage",
        "Side-channel leakage (power, EM, timing)",
        "Access control violations",
        "Hardware Trojans",
        "Improper memory overlap (e.g., CWE-1260)",
        "Privilege escalation",
        "Insider threats/malicious modifications",
        "Test/Debug infrastructure abuse"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "GPT-4",
        "novel_contribution": "Applied to multiple SoC security verification tasks with task-specific prompt engineering and practical guidelines; large-scale investigation of capabilities and challenges"
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "GPT-3.5",
        "novel_contribution": "Compared on same SoC security tasks to assess capability gaps and limitations of mid-size vs larger LLMs"
      }
    ],
    "learning_paradigm": [
      "Zero-shot",
      "Few-shot",
      "Prompt engineering",
      "In-context learning"
    ],
    "datasets": [
      {
        "name": "MITRE CWE (Hardware CWE)",
        "type": "public",
        "domain": "hardware_vulnerabilities",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Trust-Hub Property Database",
        "type": "public",
        "domain": "hardware_vulnerabilities",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Hack@DAC (hackathon artifacts)",
        "type": "public",
        "domain": "hardware_vulnerabilities",
        "link": null,
        "is_new_contribution": false,
        "availability": "available_on_request"
      },
      {
        "name": "HOST Microelectronics Challenge (artifacts)",
        "type": "public",
        "domain": "hardware_vulnerabilities",
        "link": null,
        "is_new_contribution": false,
        "availability": "available_on_request"
      },
      {
        "name": "SoC Vulnerability Benchmarks (per [84])",
        "type": "public",
        "domain": "hardware_vulnerabilities",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "What are the prospects of applying LLMs to SoC security verification?",
        "Can LLMs insert or emulate RTL vulnerabilities from natural-language descriptions?",
        "Can LLMs perform security assessment of RTL to identify weaknesses and coding issues?",
        "Can LLMs verify security policies, compute security metrics, and generate functional testbenches?",
        "Can LLMs propose effective countermeasures to mitigate identified RTL vulnerabilities?",
        "What prompt-engineering strategies are effective for various SoC security tasks?",
        "How do capabilities differ between GPT-3.5 and GPT-4 on these tasks?",
        "What are the key challenges, limitations, and guidelines for employing LLMs in SoC security?"
      ],
      "gaps_identified": [
        "Existing SoC security solutions are not scalable, comprehensive, or adaptable to modern designs and evolving threats",
        "Security verification consumes 70–80% of project effort/time and remains a bottleneck",
        "Lack of standardized, comprehensive benchmarks and vulnerability databases for hardware security verification",
        "Many documented vulnerabilities are not open-source or lack adequate documentation",
        "Security analysis and mitigation are largely manual and ad-hoc in pre-silicon stages",
        "Limited prior research on using LLMs for SoC security; potential remains untapped"
      ],
      "limitations": [],
      "future_work": [
        "Further exploration of LLM usage across additional SoC security tasks",
        "Development of standardized benchmarks and databases for evaluating LLM-based security workflows",
        "Refinement of prompt engineering and fidelity-check practices for hardware security verification"
      ],
      "motivation": "Leverage emergent capabilities of GPT-style LLMs to address scalability, coverage, and adaptability gaps in SoC security verification and provide a comprehensive analysis, case studies, experiments, and guidelines.",
      "potential_research_ideas": [
        "Create a standardized RTL security benchmark suite tailored for LLM evaluation (insertion, detection, verification, patching) with ground-truth labels and metrics",
        "Develop a hardware-security-tuned LLM via continued pretraining and instruction tuning on HDL, CWE, Trust-Hub, and verified patches",
        "Integrate LLMs with formal engines (SVA/PSL checking, SMT/SAT) for automatic assertion synthesis and proof-guided refinement (LLM-in-the-loop formal verification)",
        "Design retrieval-augmented LLMs that ground on CWE/Trust-Hub and internal design docs to reduce hallucinations and improve policy compliance",
        "Use grammar-constrained decoding and HDL-aware linters/simulators during generation to ensure syntactic/semantic correctness of RTL and properties",
        "Build multi-agent pipelines: one agent for threat modeling, one for assertion/property synthesis, one for testbench generation, plus a verifier agent",
        "Explore multimodal LLMs combining RTL/netlists, waveforms, and documentation for richer security reasoning",
        "Evaluate adversarial robustness of LLM-generated fixes and prompts; study prompt-injection/poisoning risks in design flows",
        "Develop privacy-preserving workflows (on-prem inference, redaction, synthetic sharing) for proprietary RTL during LLM-assisted verification"
      ],
      "architectural_improvement_recommendations": [
        "Adopt retrieval-augmented generation over curated CWE/benchmark corpora and design specs",
        "Employ toolformer-style function calling to connect LLMs with HDL linters, simulators, formal checkers, and IFT tools for automatic fidelity checks",
        "Use self-reflection/self-consistency and program-of-thought to improve structured security reasoning and reduce errors",
        "Constrain decoding with HDL grammars and property templates; enforce schema-validated outputs",
        "Fine-tune or LoRA-adapt code-capable LLMs on RTL/property corpora; include RLHF with verification feedback",
        "Compose LLMs with classical program analysis (e.g., IFT, concolic) and EDA passes to cross-validate findings and triage false positives"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "OpenAI API (GPT-3.5, GPT-4)"
      ],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Need for robust fidelity checks to validate LLM outputs in security-critical flows",
        "Prompt sensitivity and requirement for task-specific prompt engineering",
        "Limited availability/quality of open hardware vulnerability corpora for evaluation and grounding"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "First comprehensive investigation of LLM potential across multiple SoC security tasks",
      "Comprehensive survey of existing LLMs and related hardware security works",
      "Formulation of key research questions and systematic analysis via practical case studies and large-scale investigations",
      "Identification of specific prompt guidelines for SoC security tasks",
      "Discussion of achievements, prospects, and challenges of employing LLMs in SoC security"
    ]
  },
  {
    "arxiv_id": "2310.17430v1",
    "title": "A near-autonomous and incremental intrusion detection system through active learning of known and unknown attacks",
    "authors": "Lynda Boukela; Gongxuan Zhang; Meziane Yacoub; Samia Bouzefrane",
    "abstract": "Intrusion detection is a traditional practice of security experts, however, there are several issues which still need to be tackled. Therefore, in this paper, after highlighting these issues, we present an architecture for a hybrid Intrusion Detection System (IDS) for an adaptive and incremental detection of both known and unknown attacks. The IDS is composed of supervised and unsupervised modules, namely, a Deep Neural Network (DNN) and the K-Nearest Neighbors (KNN) algorithm, respectively. The proposed system is near-autonomous since the intervention of the expert is minimized through the active learning (AL) approach. A query strategy for the labeling process is presented, it aims at teaching the supervised module to detect unknown attacks and improve the detection of the already-known attacks. This teaching is achieved through sliding windows (SW) in an incremental fashion where the DNN is retrained when the data is available over time, thus rendering the IDS adaptive to cope with the evolutionary aspect of the network traffic. A set of experiments was conducted on the CICIDS2017 dataset in order to evaluate the performance of the IDS, promising results were obtained.",
    "published_date": "2023-10-26",
    "pdf_link": "https://arxiv.org/pdf/2310.17430v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Incremental and near-autonomous detection of known and unknown network attacks via active learning",
      "attack_types": [
        "DoS: Hulk",
        "DoS: GoldenEye",
        "DoS: Slowloris",
        "DoS: Slowhttptest",
        "Port Scan",
        "Heartbleed",
        "Web attacks: Brute Force",
        "Web attacks: SQL Injection",
        "Web attacks: XSS"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "MLP/Feedforward DNN",
        "specific": "4-layer fully connected DNN (256-128-64-32 ReLU) + 2-unit Softmax output; weighted cross-entropy; Adam optimizer",
        "novel_contribution": "Used as the supervised component in an incremental active learning IDS that is periodically retrained on labels queried via a hybrid uncertainty + outlier sampling strategy"
      },
      {
        "type": "primary",
        "category": "KNN (outlier detection)",
        "specific": "KNN outlierness via distance to Kth neighbor (K=100, PyOD)",
        "novel_contribution": "Used as an unsupervised detector to surface likely unknown attacks for labeling and integration into the supervised model via active learning"
      },
      {
        "type": "primary",
        "category": "Active Learning",
        "specific": "Hybrid query strategy: top n/2 highest DNN uncertainty + top n/2 highest KNN outlierness",
        "novel_contribution": "Query function explicitly targets both low-confidence known-class instances and likely unknown attacks to teach the DNN new attacks incrementally"
      },
      {
        "type": "primary",
        "category": "Incremental/Online Learning",
        "specific": "Sliding windows (60 windows of 5,000 samples) with periodic retraining",
        "novel_contribution": "Windowed, incremental retraining enables adaptation to evolving traffic and newly appearing attack classes"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised",
      "Active Learning",
      "Incremental Learning"
    ],
    "datasets": [
      {
        "name": "CICIDS2017",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Full training set (no active learning query budget limit)",
        "paper_reference": null,
        "metric": "AUC",
        "their_result": "\"labeling only 600 samples from each window, i.e. 13% of the full training data allows us to achieve a performance as good as when using the full training set\"",
        "baseline_result": "Full training set performance (AUC) on test set; matched by 13% labeling regime (n=600/window)."
      }
    ],
    "performance_metrics_used": [
      "AUC"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Signature-based IDS cannot cope with previously unseen attacks.",
        "Supervised ML-based IDS rely heavily on labeled data and often fail to detect unknown attacks.",
        "Unsupervised IDS approaches can detect new anomalies but suffer from low detection accuracy.",
        "Active learning in prior IDS work primarily reduces labeling effort but \"almost none of them have attempted to 'teach' the classifier how to detect new attacks\".",
        "Need for scalable, adaptive, and near-autonomous IDS with minimal expert labeling."
      ],
      "limitations": [
        "Evaluation conducted on a single dataset (CICIDS2017).",
        "Comparisons with external state-of-the-art methods are not provided.",
        "Binary classification setup; multiclass attack-type discrimination not implemented.",
        "Performance drops sharply when a new attack first appears before adaptation (e.g., Port Scan)."
      ],
      "future_work": [
        "Compare the framework with state-of-the-art solutions.",
        "Explore more complex deep learning architectures.",
        "Use more data and conduct more experiments to study parameter effects, especially sliding window size.",
        "Extend the model to multiclass classification to distinguish different attacks."
      ],
      "motivation": "Minimize human labeling effort while enabling an IDS to adapt incrementally to evolving traffic and detect both known and previously unseen attacks.",
      "potential_research_ideas": [
        "Incorporate open-set recognition and dedicated OOD detectors (e.g., deep ensembles, energy-based methods) to improve first-encounter detection of unknown attacks before retraining.",
        "Combine active learning with diversity and class-balance aware sampling (e.g., core-set, BADGE) to reduce querying benign/majority samples under class imbalance.",
        "Add continual learning techniques (experience replay, EWC/LwF) to retain performance on earlier attacks and mitigate forgetting across windows.",
        "Pretrain with self-supervised learning on large unlabeled traffic, then fine-tune with the AL loop to reduce label needs further.",
        "Integrate drift detection to adapt window sizes and trigger retraining based on detected distribution shifts.",
        "Evaluate on multiple modern datasets (e.g., newer CIC/CSE-CIC, UNSW-NB15 variants, ToN-IoT) and cross-dataset transfer to assess generalization.",
        "Design a human-in-the-loop interface with explanations (e.g., SHAP) to speed up oracle labeling and trust calibration.",
        "Harden against adversarial evasion with adversarial training on flow features and testing with attack-generation frameworks."
      ],
      "architectural_improvement_recommendations": [
        "Replace or augment KNN outlier scoring with scalable OOD detectors (e.g., Mahalanobis distance in feature space, Isolation Forest, deep one-class models).",
        "Use uncertainty estimates from Bayesian NNs or MC-Dropout/Deep Ensembles to drive the uncertainty half of the query more reliably.",
        "Adopt a multiclass head with hierarchical labels (benign -> attack family -> attack type) to improve specificity and AL targeting.",
        "Employ approximate nearest neighbor search or sketching to scale the outlier component to higher throughput.",
        "Introduce a replay buffer and regularization (EWC/LwF) during incremental retraining to reduce catastrophic forgetting.",
        "Calibrate the classifier (e.g., temperature scaling) to improve uncertainty quality for AL."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "Keras",
        "modAL",
        "PyOD"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Labeling budget and expert involvement remain necessary, though reduced.",
        "Initial performance drops when new attacks first appear before adaptation.",
        "Imbalanced data can skew query selection toward benign samples early on.",
        "KNN outlier detection may face scalability challenges on high-throughput streams."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Hybrid IDS framework combining supervised DNN and unsupervised KNN for known and unknown attack detection.",
      "Active learning to minimize expert labeling and move toward near-autonomous operation.",
      "Hybrid query function (uncertainty-based + KNN outlierness) to improve known-attack detection and teach the model new attacks.",
      "Incremental, sliding-window processing with periodic retraining to adapt to evolving traffic.",
      "Empirical evaluation on CICIDS2017 demonstrating that with only ~13% labeled data per window (n=600) performance matches using the full training set; analysis of incremental adaptation to newly introduced attacks."
    ]
  },
  {
    "arxiv_id": "2311.12420v3",
    "title": "How Far Have We Gone in Vulnerability Detection Using Large Language Models",
    "authors": "Zeyu Gao; Hao Wang; Yuchen Zhou; Wenyu Zhu; Chao Zhang",
    "abstract": "As software becomes increasingly complex and prone to vulnerabilities, automated vulnerability detection is critically important, yet challenging. Given the significant successes of large language models (LLMs) in various tasks, there is growing anticipation of their efficacy in vulnerability detection. However, a quantitative understanding of their potential in vulnerability detection is still missing. To bridge this gap, we introduce a comprehensive vulnerability benchmark VulBench. This benchmark aggregates high-quality data from a wide range of CTF (Capture-the-Flag) challenges and real-world applications, with annotations for each vulnerable function detailing the vulnerability type and its root cause. Through our experiments encompassing 16 LLMs and 6 state-of-the-art (SOTA) deep learning-based models and static analyzers, we find that several LLMs outperform traditional deep learning approaches in vulnerability detection, revealing an untapped potential in LLMs. This work contributes to the understanding and utilization of LLMs for enhanced software security.",
    "published_date": "2023-11-21",
    "pdf_link": "https://arxiv.org/pdf/2311.12420v3",
    "paper_types": [
      "benchmark",
      "empirical_analysis",
      "new_dataset"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Vulnerability Detection",
      "specific_problem": "Function-level vulnerability detection and vulnerability type classification in C/C++ source and decompiled code (CTF and real-world CVEs)",
      "attack_types": [
        "stack-based buffer overflow (stack overflow)",
        "memory leak",
        "memory-related vulnerabilities (general)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Prompting",
        "specific": "Few-shot in-context learning (2-shot and 5-shot chat prompts with standardized output template)",
        "novel_contribution": "Standardized few-shot chat prompts to enforce parseable outputs (binary and multi-class classification) across diverse LLMs on vulnerability detection"
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "GPT-4",
        "novel_contribution": "Evaluated on VulBench for binary and multi-class vulnerability detection; compared against deep learning and static analyzers"
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "GPT-3.5",
        "novel_contribution": "Evaluated on VulBench for binary and multi-class vulnerability detection; compared against deep learning and static analyzers"
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "Llama-2 (7B/13B/70B, RLHF variants)",
        "novel_contribution": "Evaluated scaling and alignment effects on vulnerability detection under few-shot prompting"
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "Vicuna (7B/7B-16k/13B/13B-16k/33B)",
        "novel_contribution": "Evaluated few-shot performance; context-length constraints affected 5-shot results for 33B"
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "CodeLlama-34B",
        "novel_contribution": "Evaluated code-specialized LLM on source and decompiled inputs"
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "Falcon-40B-instruct",
        "novel_contribution": "Evaluated; 5-shot excluded due to context-length limits"
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "Baichuan2-13B",
        "novel_contribution": "Evaluated under few-shot prompting"
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "InternLM-20B",
        "novel_contribution": "Evaluated under few-shot prompting"
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "Platypus2-70B",
        "novel_contribution": "Evaluated; supervised fine-tuned variant showed improved performance over base Llama 2"
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "ChatGLM2-6B",
        "novel_contribution": "Evaluated under few-shot prompting"
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "VulBERTa (RoBERTa for code)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "CodeBERT (CodeXGLUE vulnerability classification)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "LineVul (CodeBERT-based)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Rule-based static analysis",
        "specific": "flawfinder",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Rule-based static analysis",
        "specific": "cppcheck",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Static analysis (binary)",
        "specific": "BinAbsInspector",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "In-context Learning (few-shot prompting)",
      "Supervised",
      "Rule-based (static analysis)"
    ],
    "datasets": [
      {
        "name": "VulBench",
        "type": "public",
        "domain": "source_code,decompiled_code",
        "link": "https://github.com/Hustcw/VulBench",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "BUUOJ CTF PWN (decompiled and manually reversed code)",
        "type": "public",
        "domain": "decompiled_code,binaries",
        "link": "https://buuoj.cn",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "MAGMA",
        "type": "public",
        "domain": "source_code,decompiled_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Devign",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Big-Vul",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "D2A",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "VulBERTa",
        "paper_reference": "Hanif & Maffeis, 2022",
        "metric": "F1/Precision/Recall (binary classification)",
        "their_result": null,
        "baseline_result": "CTF: F1 0.354, P 0.350, R 0.391; Real-world: F1 0.406, P 0.456, R 0.388"
      },
      {
        "method_name": "LineVul (CodeBERT-based)",
        "paper_reference": "Fu & Tantithamthavorn, 2022",
        "metric": "F1/Precision/Recall (binary classification)",
        "their_result": null,
        "baseline_result": "CTF: F1 0.155, P 0.619, R 0.187; Real-world: F1 0.166, P 0.419, R 0.193"
      },
      {
        "method_name": "CodeXGLUE (CodeBERT on Devign)",
        "paper_reference": "Lu et al., 2021",
        "metric": "F1/Precision/Recall (binary classification)",
        "their_result": null,
        "baseline_result": "CTF: F1 0.375, P 0.341, R 0.617; Real-world: F1 0.429, P 0.437, R 0.462"
      },
      {
        "method_name": "flawfinder (static analyzer)",
        "paper_reference": null,
        "metric": "F1/Precision/Recall (multi-class classification on CTF)",
        "their_result": null,
        "baseline_result": "Raw decompiled: F1 0.174, P 0.229, R 0.662; Reversed decompiled: F1 0.136, P 0.178, R 0.324"
      },
      {
        "method_name": "cppcheck (static analyzer)",
        "paper_reference": null,
        "metric": "F1/Precision/Recall (multi-class classification on CTF)",
        "their_result": null,
        "baseline_result": "Raw decompiled: F1 0.020, P 0.029, R 0.015; Reversed decompiled: F1 0.016, P 0.010, R 0.170"
      },
      {
        "method_name": "BinAbsInspector (binary static analyzer)",
        "paper_reference": null,
        "metric": "F1/Precision/Recall (multi-class classification on CTF, binary input)",
        "their_result": null,
        "baseline_result": "F1 0.604, P 0.652, R 0.563"
      }
    ],
    "performance_metrics_used": [
      "F1",
      "Precision",
      "Recall"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How effective are LLMs at detecting software vulnerabilities at the function level?",
        "Do LLMs outperform prior deep learning-based models and static analyzers in vulnerability detection?",
        "How do different few-shot settings (2-shot vs 5-shot) impact LLM performance in binary and multi-class tasks?",
        "How well do LLMs handle different input modalities (source vs decompiled code) and varying amounts of contextual information?",
        "Can a high-quality benchmark (VulBench) provide a more accurate quantitative assessment of LLMs for vulnerability detection?"
      ],
      "gaps_identified": [
        "Lack of high-quality, accurate vulnerability datasets for evaluating LLMs; prior datasets show detection correctness as low as 30%–50%.",
        "Limited quantitative understanding of LLMs' potential in vulnerability detection, especially across open-access LLMs.",
        "Existing datasets often fail to represent real-world complexity where vulnerabilities span multiple functions.",
        "Static analysis generated datasets suffer high false positive rates."
      ],
      "limitations": [
        "Dataset size is too limited to retrain deep learning baselines for multi-class classification; thus, they are evaluated only on binary classification.",
        "Context length constraints prevented 5-shot evaluation for some large models (e.g., Vicuna-33B, Falcon-40B).",
        "Decompiled code contains distributional patterns uncommon in LLM pretraining corpora, potentially hurting performance.",
        "Reliance on manual reverse engineering to improve decompiled code readability.",
        "Need to enforce output formatting; non-conforming outputs are treated as invalid."
      ],
      "future_work": [
        "Further improve dataset quality and breadth, adding richer cross-function context and more real-world cases.",
        "Explore fine-tuning or alignment strategies tailored to vulnerability detection.",
        "Investigate providing broader contextual information (e.g., related functions, macros) and measuring robustness to extraneous information.",
        "Extend evaluations to more models and languages, and to more complex vulnerability types."
      ],
      "motivation": "Quantitatively assess LLMs' capability for vulnerability detection in the presence of limited high-quality datasets by introducing a curated benchmark and comparing LLMs with state-of-the-art deep learning models and static analyzers.",
      "potential_research_ideas": [
        "Fine-tune open-source LLMs on VulBench with instruction-style objectives to jointly predict vulnerability presence, type, and natural-language root cause explanations.",
        "Retrieval-augmented code analysis that pulls relevant functions, call graphs, and macro definitions to provide richer context to the LLM.",
        "Hybrid neuro-symbolic systems that integrate static analyzers or fuzzers as tools (Toolformer-style) invoked by the LLM during reasoning.",
        "Constrained decoding and structured-output learning (regex/JSON schemas) to eliminate invalid outputs and improve evaluation reliability.",
        "Domain-adaptive pretraining on decompiled-code corpora to reduce distribution mismatch and improve performance on binary-only scenarios.",
        "Self-consistency and multi-turn reasoning protocols for complex, cross-function vulnerabilities.",
        "Adversarial robustness studies against code obfuscation, dead-code insertion, and variable renaming to assess and harden LLM detectors.",
        "Cross-language and cross-representation transfer (source ↔ decompiled) via multi-task learning."
      ],
      "architectural_improvement_recommendations": [
        "Combine LLMs with program graphs (AST/CFG/DFG) via a graph-augmented encoder and cross-attention to code tokens.",
        "Implement retrieval over repository-level code (function linking, macros, headers) to supply structured context to the LLM.",
        "Adopt constrained decoding with validation (regex/grammar) to ensure parseable outputs for both binary and multi-class tasks.",
        "Apply lightweight LoRA/QLoRA fine-tuning on decompiled-code style data to adapt to binaries.",
        "Use tool-use APIs to call static analyzers or symbolic executors when the LLM's uncertainty is high.",
        "Employ few-shot example selection by semantic retrieval (embedding-based) rather than random selection."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/Hustcw/VulBench",
      "frameworks": [
        "vLLM",
        "Hugging Face text-generation-inference"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Evaluations run on 48 A800 GPUs across 6 nodes; inference accelerated with vLLM and text-generation-inference; each task repeated 5 times."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "LLM context length limitations affecting few-shot prompts for large models",
        "Ensuring standardized, parseable outputs to avoid invalid predictions",
        "Distribution shift between decompiled code and LLM pretraining data",
        "Over-alignment potentially harming detection performance",
        "Dataset quality and availability of full-program context"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "First large-scale quantitative study of 16 LLMs for vulnerability detection against SOTA deep learning models and static analyzers.",
      "Introduction of VulBench, a curated benchmark aggregating CTF, MAGMA, Devign, D2A, and Big-Vul with improved quality control and natural-language root-cause annotations.",
      "Evidence that several LLMs (e.g., GPT-3.5/4) outperform traditional deep learning approaches in vulnerability detection.",
      "Public release of the benchmark dataset to facilitate future research (https://github.com/Hustcw/VulBench)."
    ]
  },
  {
    "arxiv_id": "2310.16406v2",
    "title": "Radio Frequency Fingerprinting via Deep Learning: Challenges and Opportunities",
    "authors": "Saeif Al-Hazbi; Ahmed Hussain; Savio Sciancalepore; Gabriele Oligeri; Panos Papadimitratos",
    "abstract": "Radio Frequency Fingerprinting (RFF) techniques promise to authenticate wireless devices at the physical layer based on inherent hardware imperfections introduced during manufacturing. Such RF transmitter imperfections are reflected into over-the-air signals, allowing receivers to accurately identify the RF transmitting source. Recent advances in Machine Learning, particularly in Deep Learning (DL), have improved the ability of RFF systems to extract and learn complex features that make up the device-specific fingerprint. However, integrating DL techniques with RFF and operating the system in real-world scenarios presents numerous challenges, originating from the embedded systems and the DL research domains. This paper systematically identifies and analyzes the essential considerations and challenges encountered in the creation of DL-based RFF systems across their typical development life-cycle, which include (i) data collection and preprocessing, (ii) training, and finally, (iii) deployment. Our investigation provides a comprehensive overview of the current open problems that prevent real deployment of DL-based RFF systems while also discussing promising research opportunities to enhance the overall accuracy, robustness, and privacy of these systems.",
    "published_date": "2023-10-25",
    "pdf_link": "https://arxiv.org/pdf/2310.16406v2",
    "paper_types": [
      "position",
      "empirical_analysis",
      "survey"
    ],
    "security_domain": {
      "primary": "Wireless/Physical Layer Security",
      "subdomain": "Specific Emitter Identification (SEI) / Radio Frequency Fingerprinting (RFF)",
      "specific_problem": "Challenges and opportunities for deploying deep-learning-based RF device authentication across data collection, training, and deployment",
      "attack_types": [
        "spoofing",
        "replay/impersonation",
        "jamming",
        "adversarial attacks (general risk, not empirically evaluated)"
      ]
    },
    "ml_techniques": [
      {
        "type": "baseline",
        "category": "CNN",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "DNN",
        "specific": "Fully connected feed-forward network",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "Transformer without positional encoder",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "RNN/LSTM",
        "specific": "LSTM",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Open-set recognition (discussion)",
      "Zero-shot learning (discussion)",
      "Fine-tuning/transfer learning (discussion)"
    ],
    "datasets": [
      {
        "name": "RFF dataset referenced as [5] (preamble raw IQ; 143 transmitters; BPSK examples)",
        "type": "public",
        "domain": "wireless_IQ_samples",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "CNN",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "DNN (fully-connected)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Transformer (without positional encoder)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "LSTM",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "Cosine similarity (feature similarity analysis)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "What essential considerations and challenges arise when creating DL-based RFF systems across data collection, preprocessing, training, and deployment?",
        "Which factors currently prevent real-world deployment of DL-based RFF systems?",
        "What research opportunities can enhance accuracy, robustness, scalability, and privacy of DL-based RFF?"
      ],
      "gaps_identified": [
        "Lack of standardized benchmark datasets for RFF across protocols, rates, and channel conditions",
        "Receiver hardware bias affecting learned fingerprints and portability",
        "Scalability degradation as the number of devices increases and feature spaces overlap",
        "Protocol dependency and sensitivity to protocol configuration changes",
        "Closed-set assumption; inability to detect unseen/unauthorized devices",
        "Handling simultaneous transmissions and interference",
        "Deployment on resource-constrained devices is impractical with current heavy DL models",
        "Variable input lengths across technologies challenge fixed-size DL models",
        "Unclear best practices for RF-specific data augmentation",
        "Ambiguity in optimal signal representations (raw IQ, spectral, time-frequency)",
        "Need for robust feature normalization for scale-sensitive models",
        "Validation/testing methodologies that reflect temporal and spatial variability",
        "Model interpretability for security-critical decisions",
        "Adversarial robustness remains underexplored for DL-based RFF",
        "Privacy considerations in data collection and model training"
      ],
      "limitations": [
        "Tutorial/challenges paper; no introduction of a new model or dataset",
        "Illustrative analyses rely on one external dataset [5]; no comprehensive quantitative benchmarking",
        "No real-world field deployment evaluation",
        "Adversarial robustness, privacy, and fairness are discussed conceptually without experiments"
      ],
      "future_work": [
        "Create comprehensive, standardized RFF benchmark datasets spanning protocols, rates, and channel conditions",
        "Develop receiver-agnostic and channel-robust models to mitigate receiver bias",
        "Design scalable architectures and features that separate highly similar devices",
        "Develop protocol-agnostic or multi-protocol models robust to configuration changes",
        "Introduce open-set/zero-shot mechanisms to detect and handle unseen devices",
        "Address concurrent transmissions and channel profile shifts",
        "Optimize models for constrained devices via compression, pruning, quantization, and distillation",
        "Architectures handling variable-length inputs natively",
        "Enhance interpretability and trust in DL-based RFF decisions",
        "Explore adversarial robustness and privacy-preserving training for RFF"
      ],
      "motivation": "Bridge the gap between research and practice by systematically classifying challenges across the DL-based RFF pipeline to enable real-world deployment and highlight research opportunities.",
      "potential_research_ideas": [
        "Receiver/domain-invariant representation learning using domain-adversarial training to remove receiver/channel imprinting",
        "Self-supervised or contrastive pretraining on large unlabeled RF IQ corpora to improve generalization and reduce labeled data needs",
        "Open-set RFF with energy-based/OOD detection and prototype or metric-learning embeddings enabling rejection and zero-shot classification",
        "Generative augmentation (e.g., conditional diffusion/GANs) with differentiable channel models to synthesize protocol- and channel-diverse RF samples",
        "Multi-receiver/antenna fusion using attention or graph neural networks to disambiguate overlapping features and handle simultaneous transmissions",
        "Protocol-agnostic encoders with mixture-of-experts heads that adapt to protocol configurations at inference time",
        "Complex-valued and equivariant neural networks tailored to IQ structure and RF symmetries",
        "Neural channel equalization-in-the-loop to disentangle device fingerprint from channel effects",
        "Federated or split learning for privacy-preserving cross-site RFF model training",
        "Any-time and variable-length architectures (e.g., causal Transformers) accommodating variable symbol durations without truncation/padding heuristics"
      ],
      "architectural_improvement_recommendations": [
        "Adopt complex-valued CNN/Transformer layers and spectral-temporal dual-branch networks to leverage IQ structure",
        "Use Transformers without positional encoders or with learned invariant positional schemes to reduce sequence-order sensitivity observed in RFF preambles",
        "Hybrid CNN-Transformer backbones with attention over time-frequency representations (STFT, wavelets)",
        "Metric learning (triplet/prototypical) to create well-separated embeddings improving scalability to many devices",
        "Domain-adversarial and CORAL/MMD-based adaptation to reduce receiver and channel bias",
        "Mixture-of-experts or conditional adapters to handle protocol/configuration shifts",
        "Lightweight deployment via pruning, low-bit quantization (e.g., 4–8 bit), and knowledge distillation to microcontrollers/edge NPUs",
        "Neural OOD heads (energy score, Mahalanobis) and threshold calibration for open-set device rejection",
        "Differentiable channel layers for sim2real training and robustification"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Receiver hardware bias requiring retraining on receiver change",
        "Scalability degradation with many similar devices",
        "Protocol dependency and configuration sensitivity",
        "Closed-set limitation; inability to detect unseen devices",
        "Simultaneous transmissions and channel profile shifts",
        "Resource constraints on edge/IoT devices",
        "Variable input lengths across technologies",
        "Model interpretability needs in security contexts"
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Systematic identification and classification of challenges for DL-based RFF across data collection, training, and deployment",
      "Tutorial-style synthesis bridging DL and embedded/wireless system considerations for RFF",
      "Highlighting research opportunities to enhance accuracy, robustness, scalability, and privacy",
      "Illustrative analyses using an existing RFF dataset [5] showing architecture sensitivity (e.g., Transformer without positional encoder generalization) and hyper-parameter effects (batch size vs. convergence)",
      "Discussion of the need for standardized benchmarks and rigorous validation methodologies reflecting temporal and spatial variability"
    ]
  },
  {
    "arxiv_id": "2310.03605v3",
    "title": "FASER: Binary Code Similarity Search through the use of Intermediate Representations",
    "authors": "Josh Collyer; Tim Watson; Iain Phillips",
    "abstract": "Being able to identify functions of interest in cross-architecture software is useful whether you are analysing for malware, securing the software supply chain or conducting vulnerability research. Cross-Architecture Binary Code Similarity Search has been explored in numerous studies and has used a wide range of different data sources to achieve its goals. The data sources typically used draw on common structures derived from binaries such as function control flow graphs or binary level call graphs, the output of the disassembly process or the outputs of a dynamic analysis approach. One data source which has received less attention is binary intermediate representations. Binary Intermediate representations possess two interesting properties: they are cross architecture by their very nature and encode the semantics of a function explicitly to support downstream usage. Within this paper we propose Function as a String Encoded Representation (FASER) which combines long document transformers with the use of intermediate representations to create a model capable of cross architecture function search without the need for manual feature engineering, pre-training or a dynamic analysis step. We compare our approach against a series of baseline approaches for two tasks; A general function search task and a targeted vulnerability search task. Our approach demonstrates strong performance across both tasks, performing better than all baseline approaches.",
    "published_date": "2023-10-05",
    "pdf_link": "https://arxiv.org/pdf/2310.03605v3",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Binary Analysis",
      "specific_problem": "Cross-architecture binary code similarity search and vulnerability function search in firmware",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "Longformer",
        "novel_contribution": "Uses ESIL (radare2 IR) function-as-string inputs with a Longformer trunk to learn cross-architecture function embeddings without pre-training or dynamic analysis."
      },
      {
        "type": "primary",
        "category": "Metric Learning",
        "specific": "Siamese network with Circle Loss",
        "novel_contribution": "Pair-based training with online batch-hard mining over function labels to directly optimize binary function similarity."
      },
      {
        "type": "primary",
        "category": "Similarity Function",
        "specific": "Cosine similarity",
        "novel_contribution": "Used as the distance function for online hard pair mining and retrieval."
      },
      {
        "type": "baseline",
        "category": "GNN",
        "specific": "GMN (Graph Matching Networks)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "GNN",
        "specific": "GNN (Li et al., 2019)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "GNN",
        "specific": "Gemini / GNN(s2v)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "Trex",
        "novel_contribution": "Pre-trained on emulator-generated micro-traces before static fine-tuning."
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Metric Learning",
      "Siamese"
    ],
    "datasets": [
      {
        "name": "Dataset-1 (Marcelli, 2022)",
        "type": "public",
        "domain": "binary_functions",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Dataset-Vulnerability (Marcelli, 2022) – Firmware + compiled libcrypto",
        "type": "public",
        "domain": "firmware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Dataset-Vulnerability (RISC-V augmentation of libcrypto)",
        "type": "synthetic",
        "domain": "firmware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "GMN (Graph Matching Networks)",
        "paper_reference": "[13]",
        "metric": "XM task: Recall@1 / MRR@10",
        "their_result": "FASER NRM: 0.51 / 0.57",
        "baseline_result": "GMN: 0.45 / 0.53"
      },
      {
        "method_name": "GNN (Li et al., 2019)",
        "paper_reference": "[13]",
        "metric": "XM task: Recall@1 / MRR@10",
        "their_result": "FASER NRM: 0.51 / 0.57",
        "baseline_result": "GNN: 0.44 / 0.52"
      },
      {
        "method_name": "GNN (s2v) / Gemini",
        "paper_reference": "[23]",
        "metric": "XM task: Recall@1 / MRR@10",
        "their_result": "FASER NRM: 0.51 / 0.57",
        "baseline_result": "GNN (s2v): 0.26 / 0.36"
      },
      {
        "method_name": "GMN (Graph Matching Networks)",
        "paper_reference": "[13]",
        "metric": "Vulnerability search (Netgear R7000 ARM32 firmware): Mean Rank / Median Rank",
        "their_result": "FASER RN: 2 / 1",
        "baseline_result": "GMN: 3.625 / 1"
      },
      {
        "method_name": "Trex",
        "paper_reference": "[16]",
        "metric": "Vulnerability search (Netgear R7000 ARM32 firmware): Mean Rank / Median Rank",
        "their_result": "FASER RN: 2 / 1",
        "baseline_result": "Trex: 9.125 / 3"
      },
      {
        "method_name": "GNN (Li et al., 2019)",
        "paper_reference": "[13]",
        "metric": "Vulnerability search (Netgear R7000 ARM32 firmware): Mean Rank / Median Rank",
        "their_result": "FASER RN: 2 / 1",
        "baseline_result": "GNN: 25 / 5.5"
      },
      {
        "method_name": "GNN (s2v) / Gemini",
        "paper_reference": "[23]",
        "metric": "Vulnerability search (Netgear R7000 ARM32 firmware): Mean Rank / Median Rank",
        "their_result": "FASER RN: 2 / 1",
        "baseline_result": "GNN (s2v): 8.125 / 5.5"
      }
    ],
    "performance_metrics_used": [
      "Recall@1",
      "MRR@10",
      "Rank",
      "Mean Rank",
      "Median Rank"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "RQ1 - How does FASER perform when compared against other baseline approaches for the binary function search task?",
        "RQ2 - How effective is FASER at searching real firmware images for known vulnerabilities?",
        "RQ3 - Does using intermediate representations as the input data enable the model to zero shot architectures not previously seen as part of the training data?"
      ],
      "gaps_identified": [
        "NLP-based assembly token inputs face severe out-of-vocabulary (OOV) issues across architectures.",
        "Existing work often requires architecture-specific normalization and manual feature engineering.",
        "IRs for direct binary function search are under-explored compared to assembly, CFG/call-graph, or dynamic traces.",
        "Many prior transformer approaches rely on costly pre-training and/or dynamic micro-traces.",
        "Deduplication of semantically identical functions across builds is often overlooked.",
        "Lack of cross-architecture evaluation including newer ISAs like RISC-V."
      ],
      "limitations": [
        "Zero-shot transfer to unseen ISA (RISC-V) performs significantly worse.",
        "Long functions exceeding the input limit are truncated, which could potentially cause a loss of key information.",
        "Relies on availability and correctness of disassembly and ESIL lifting (radare2)."
      ],
      "future_work": [
        "Include RISC-V binaries in training to improve cross-ISA generalization and address data imbalance.",
        "Explore effects of register normalization strategies across tasks.",
        "Further investigate scaling sequence length to reduce truncation impacts."
      ],
      "motivation": "Enable accurate cross-architecture binary function similarity search and known-vulnerability detection using an input representation (IR/ESIL) that is inherently cross-architecture and avoids OOV and heavy pre-training/dynamic analysis requirements.",
      "potential_research_ideas": [
        "Pre-train on large IR corpora (ESIL/PCode/LLVM IR) with self-supervised objectives to improve generalization while retaining metric-learning fine-tuning.",
        "Multi-modal fusion of IR sequences with structural signals (CFG/call-graph) via late fusion or cross-attention.",
        "Cross-encoder or bi-encoder + re-ranker pipeline for improved retrieval precision in large pools.",
        "Adversarial robustness study against obfuscations and IR-lifting perturbations; develop augmentation strategies at the ESIL level.",
        "Domain adaptation and continual learning for new ISAs (e.g., RISC-V, 1750A) using few-shot metric learning.",
        "Explainability methods to highlight ESIL tokens/instructions driving similarity scores for analyst trust."
      ],
      "architectural_improvement_recommendations": [
        "Hierarchical encoder that segments long ESIL sequences (basic block-level encodings aggregated to function-level).",
        "Dynamic token pruning or learned pooling to reduce truncation while keeping salient semantics.",
        "Contrastive hard-negative mining across architectures with curriculum focusing on historically weak ISA pairs.",
        "Experiment with alternative IRs (PCode, VEX, LLVM IR) or hybrid IRs to test representation compactness vs. semantics.",
        "Knowledge distillation from a larger Longformer/Transformer to a lighter model for faster inference.",
        "Incorporate lightweight structural features (e.g., CFG summary embeddings) concatenated with IR embeddings."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/br0kej/FASER",
      "frameworks": [
        "PyTorch",
        "HuggingFace Transformers",
        "radare2",
        "bin2ml"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Trained for ~18 epochs sampling 100k functions/epoch (~3 days). Gradient accumulation to effective batch size 512, Adam lr=5e-4. Longformer with input 4096 tokens, 8 blocks, local attention window 512; two dense layers to 128-dim embedding."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Offline firmware analysis (Netgear R7000 ARM32 and TP-Link Deco-M4 MIPS32 firmware images)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Requires reliable disassembly and ESIL lifting of binaries.",
        "Long functions may exceed input length, leading to truncation.",
        "Generalization to unseen ISAs (zero-shot) is weak without additional training data."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "A binary function representation as IR Functions as Strings requiring only normalization (no manual feature engineering).",
      "A cross-architecture model combining IRs with a long-context transformer for function search and vulnerability detection.",
      "Demonstration of strong cross-architecture performance without pre-training, using deep metric learning for the search objective.",
      "First experiment including RISC-V architecture in cross-architecture function search methodology."
    ]
  },
  {
    "arxiv_id": "2311.14501v1",
    "title": "Malware Analysis on AI Technique",
    "authors": "Amjani Gupta; Karan Singh",
    "abstract": "In today's world, we are performing our maximum work through the Internet, i.e., online payment, data transfer, etc., per day. More than thousands of users are connecting. So, it's essential to provide security to the user. It is necessary to detect and prevent malicious object from gaining persistence and causing destruction within the organization. Therefore, Malware analysis is needed in order to secure the system. This necessitates the use of effective and efficient approaches for detecting OS malware. Due to the cheap cost of technology, artificial intelligence has also become less difficult to implement in projects to analyse malware. The categorization and analysis of malware on OS using various AI-based analysis techniques are covered in detail in this paper.",
    "published_date": "2023-11-24",
    "pdf_link": "https://arxiv.org/pdf/2311.14501v1",
    "paper_types": [
      "survey"
    ],
    "security_domain": {
      "primary": "Malware Analysis",
      "subdomain": "Malware Detection and Classification",
      "specific_problem": "Survey of AI-based techniques for analyzing and detecting OS malware using static, dynamic, and hybrid analysis",
      "attack_types": [
        "ransomware",
        "virus",
        "worm",
        "trojan",
        "rootkit",
        "adware",
        "keylogger"
      ]
    },
    "ml_techniques": [
      {
        "type": "baseline",
        "category": "Decision Tree",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Ensemble (Tree-based)",
        "specific": "Random Forest",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "SVM",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Naive Bayes",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "K-NN",
        "specific": "K-Nearest Neighbors",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Linear Models",
        "specific": "SGD classifier",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Linear Models",
        "specific": "Logistic Regression",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "MobileNetV2 (transfer learning)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Clustering",
        "specific": "K-means",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Meta-learning",
        "specific": "MAML (Model-Agnostic Meta-Learning)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Reinforcement Learning",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Feature selection",
        "specific": "ANOVA F-Test",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised",
      "Reinforcement"
    ],
    "datasets": [
      {
        "name": "EMBER (Elastic Malware Benchmark for Empowering Researchers)",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "SOREL-20M (Sophos/ReversingLabs 20M dataset) / SoReL",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Microsoft Malware Classification Challenge (Kaggle, 9 families)",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "PE Malware Ontology / Knowledge-Based Dataset for Training PE Malware Detection Models",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Avast-CTU Public CAPE Dataset",
        "type": "public",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Ijaz et al. (2019) Windows malware/benign dataset (800 benign, 2200 malicious)",
        "type": "private",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Mosli et al. (2016) forensic memory images dataset (400 malware, 100 benign)",
        "type": "private",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Hybrid static+dynamic features with Cuckoo sandbox + PEFILES (Ijaz et al.)",
        "paper_reference": "[6]",
        "metric": "Accuracy",
        "their_result": "94.4% accuracy on dataset (800 benign, 2200 malicious)",
        "baseline_result": null
      },
      {
        "method_name": "SGD classifier on artifacts (registry, imported libraries, API calls) (Mosli et al.)",
        "paper_reference": "[8]",
        "metric": "Accuracy",
        "their_result": "96% (highest among SVM, SGD, DT, RF, KNN)",
        "baseline_result": null
      },
      {
        "method_name": "Ransomware family classification with XGBoost and Linear Regression (Rani et al.)",
        "paper_reference": "[13]",
        "metric": "Accuracy",
        "their_result": "98.21% (highest performance)",
        "baseline_result": null
      },
      {
        "method_name": "Few-shot malware classification with MAML variants (Rani et al.)",
        "paper_reference": "[15]",
        "metric": "Accuracy",
        "their_result": "98.71% (limited-dataset setting) and 72.06% (generalized malware classification)",
        "baseline_result": null
      },
      {
        "method_name": "SVM-based API-call vectorization for malware detection",
        "paper_reference": null,
        "metric": null,
        "their_result": "Increases prediction accuracy and lowers missed rate (no numeric values provided)",
        "baseline_result": null
      },
      {
        "method_name": "Logistic Regression + ANOVA F-Test with Snort IDS for polymorphic malware",
        "paper_reference": "[12]",
        "metric": null,
        "their_result": "Polymorphic malware successfully thwarted; ANOVA F-Test significantly improved detection (no numeric values provided)",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "Precision",
      "Recall"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "No single technique is 100% effective; methods often classify some malware families well but fall short for others.",
        "Strong dependence on optimal feature engineering choices for ML effectiveness.",
        "Generalization to unknown/polymorphic malware remains challenging.",
        "Static and dynamic methods each have shortcomings; combining modalities can yield better results.",
        "Need for large-scale datasets and automatic feature learning to reduce manual feature engineering."
      ],
      "limitations": [
        "The paper is a survey without new empirical evaluation.",
        "Cited results are on heterogeneous datasets and setups, making direct comparison difficult."
      ],
      "future_work": [
        "Multi-layered deep learning models with automatic feature engineering for massive datasets can analyze and classify malware.",
        "Identify uncommon behavioral elements (e.g., crypto-related function calls, mass directory access, heavy file modifications) to distinguish benign vs. malicious behavior."
      ],
      "motivation": "Growing prevalence and impact of malware on OS, need for effective detection and analysis methods; AI has become more accessible for malware analysis due to reduced technology cost.",
      "potential_research_ideas": [
        "Develop a unified multimodal benchmark and evaluation protocol that jointly uses static PE features (e.g., EMBER/SOREL) and dynamic sandbox traces (e.g., CAPE) with temporal splits to measure generalization and concept drift.",
        "Pretrain representations with self-supervised learning on large-scale SOREL-20M and CAPE logs (e.g., contrastive learning on API-call sequences vs. PE metadata) to reduce manual feature engineering.",
        "Design few-shot/meta-learning approaches for novel malware families and evaluate on Microsoft Malware Classification Challenge with family hold-out and time-based splits.",
        "Study robustness to polymorphic and obfuscation transformations via adversarial training and data augmentation on API-call sequences and PE headers.",
        "Create explainable malware detectors that attribute decisions to specific API calls, registry actions, or PE sections, enabling analyst feedback loops.",
        "Investigate privacy-preserving collaborative learning (e.g., federated learning) across organizations using feature-level sharing derived from SOREL/EMBER."
      ],
      "architectural_improvement_recommendations": [
        "Build a multi-branch network: transformer encoder for API-call sequences, gradient-boosted trees (or MLP) for tabular PE features, and CNN for memory artifacts, with attention-based late fusion.",
        "Use graph neural networks on function call graphs/import graphs extracted from PE files to capture structural relationships beyond flat features.",
        "Incorporate drift detection and continual learning pipelines with time-aware validation to mitigate dataset shift.",
        "Automate feature selection with hybrid filters (ANOVA F-Test, mutual information) followed by model-based selection and calibration.",
        "Leverage disarmed binaries in SOREL-20M to train byte-level CNNs while aligning with feature-based detectors via knowledge distillation."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Polymorphism and obfuscation hinder static signatures and model generalization.",
        "High dependence on high-quality, representative datasets and feature engineering.",
        "Potential false positives and family coverage gaps across techniques.",
        "Need to detect unknown/novel malware in real time.",
        "Integration of static and dynamic analysis at scale."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Survey of malware analysis techniques (static, dynamic, hybrid) and AI methods for OS malware.",
      "Overview of widely used datasets (e.g., EMBER, SOREL-20M, Microsoft Malware Classification Challenge) and their characteristics.",
      "Summary of classical ML algorithms (DT, RF, SVM, KNN, NB, SGD, Logistic Regression) and deep learning applications (CNN/MobileNetV2) for malware detection/classification.",
      "Discussion of feature types (API calls, registry keys, DLLs, PE features) and performance metrics (accuracy, precision, recall).",
      "Identification of limitations and the need for multi-layer deep learning with automatic feature engineering on large datasets."
    ]
  },
  {
    "arxiv_id": "2312.00009v1",
    "title": "Risk-Aware and Explainable Framework for Ensuring Guaranteed Coverage in Evolving Hardware Trojan Detection",
    "authors": "Rahul Vishwakarma; Amin Rezaei",
    "abstract": "As the semiconductor industry has shifted to a fabless paradigm, the risk of hardware Trojans being inserted at various stages of production has also increased. Recently, there has been a growing trend toward the use of machine learning solutions to detect hardware Trojans more effectively, with a focus on the accuracy of the model as an evaluation metric. However, in a high-risk and sensitive domain, we cannot accept even a small misclassification. Additionally, it is unrealistic to expect an ideal model, especially when Trojans evolve over time. Therefore, we need metrics to assess the trustworthiness of detected Trojans and a mechanism to simulate unseen ones. In this paper, we generate evolving hardware Trojans using our proposed novel conformalized generative adversarial networks and offer an efficient approach to detecting them based on a non-invasive algorithm-agnostic statistical inference framework that leverages the Mondrian conformal predictor. The method acts like a wrapper over any of the machine learning models and produces set predictions along with uncertainty quantification for each new detected Trojan for more robust decision-making. In the case of a NULL set, a novel method to reject the decision by providing a calibrated explainability is discussed. The proposed approach has been validated on both synthetic and real chip-level benchmarks and proven to pave the way for researchers looking to find informed machine learning solutions to hardware security problems.",
    "published_date": "2023-10-14",
    "pdf_link": "https://arxiv.org/pdf/2312.00009v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Hardware Security",
      "subdomain": "Hardware Trojan Detection",
      "specific_problem": "Risk-aware detection of evolving hardware Trojans with guaranteed coverage and calibrated explainability using conformal prediction and conformalized GAN-generated data",
      "attack_types": [
        "Hardware Trojans",
        "Evolving/Concept-drift Trojans"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Conformal Prediction",
        "specific": "Mondrian Inductive Conformal Prediction (ICP)",
        "novel_contribution": "Algorithm-agnostic wrapper that outputs set predictions with label-conditional validity; tunable significance level alpha; guaranteed coverage even under covariate shift; reject option with calibrated explainability."
      },
      {
        "type": "primary",
        "category": "GAN",
        "specific": "Conformalized GAN (conformalized generator and discriminator)",
        "novel_contribution": "Generates evolving hardware Trojans to simulate unseen distributional shifts and address class imbalance; uses conformalization to calibrate GAN outputs for risk-sensitive synthesis."
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Generative (adversarial)",
      "Post-hoc calibration/uncertainty quantification"
    ],
    "datasets": [
      {
        "name": "Evolving hardware Trojan synthetic dataset (via conformalized GAN)",
        "type": "synthetic",
        "domain": "hardware_design_features",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "Real chip-level HT benchmarks (unspecified)",
        "type": "public",
        "domain": "hardware_design_features",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Guaranteed coverage (1 - alpha)",
      "Significance level (alpha)",
      "Label-conditional validity",
      "Prediction set size",
      "Inconclusive rate (both-label set)",
      "Empty set (NULL) rate",
      "p-values",
      "False negative rate (discussed qualitatively)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can we quantify the uncertainty associated with the HT prediction outcome and guarantee the validity of the predicted label with a handful of highly imbalanced data points (e.g., 95% provable guaranteed coverage)?",
        "Can we rank the detected “Trojan-Infected” circuit for more informed treatment?"
      ],
      "gaps_identified": [
        "Existing ML-based HT detection focuses on accuracy and lacks risk-aware metrics and guaranteed coverage.",
        "Evaluation under concept drift/evolution of Trojans is largely missing in HT literature.",
        "Datasets are highly imbalanced and often insufficiently documented in prior work.",
        "Calibration and trustworthiness of predictions are rarely addressed.",
        "Explainability methods like SHAP have limitations (ignore causality, susceptible to human bias).",
        "Unclear how graph-based representations capture both structural and behavioral circuit attributes.",
        "No prior work considers evolutionary aspects while designing HT detection approaches (prediction of evolution)."
      ],
      "limitations": [
        "Conformal prediction may produce uncertain predictions where the prediction set contains both labels.",
        "Possibility of a NULL (empty) prediction set, requiring a reject decision and human investigation."
      ],
      "future_work": [],
      "motivation": "In high-risk, sensitive hardware security domains, even small misclassifications are unacceptable. Trojans evolve over time, so models must provide trustworthy, calibrated decisions with guaranteed coverage and simulate unseen/evolving attacks.",
      "potential_research_ideas": [
        "Online/streaming conformal prediction for HT detection with adaptive recalibration under concept drift.",
        "Conditional/Mondrian grouping beyond labels (e.g., circuit family, design node, IP type) to tighten prediction sets.",
        "Hybrid generators (CTGAN/TVAE/Score-based diffusion) for richer evolving Trojan synthesis and comparison against conformalized GAN.",
        "Graph-based representations (e.g., GNN on netlists/AST) combined with conformal wrappers to capture structural and behavioral attributes.",
        "Active learning with reject-option triage to optimally query human experts and minimize NULL or ambiguous prediction rates.",
        "Causal explainability for HT features to overcome SHAP limitations; integrate causal discovery with conformal risk control.",
        "Drift detection modules (e.g., MMD, energy distance) to trigger recalibration or generator retraining.",
        "Evaluation and defenses against adaptive adversaries who target the conformal procedure (e.g., manipulate p-values)."
      ],
      "architectural_improvement_recommendations": [
        "Use cross-conformal or jackknife+ methods to reduce variance and potentially shrink prediction sets.",
        "Adopt class-conditional and feature-conditional Mondrian partitions to better handle severe imbalance.",
        "Incorporate calibration techniques such as Venn-Abers or temperature scaling alongside CP for improved point-prediction calibration.",
        "Leverage conditional generation (e.g., cGAN/CTGAN) by Trojan type, trigger mechanism, or circuit family to better simulate evolution modes.",
        "Combine CP with cost-sensitive learning to prioritize minimizing false negatives in risk-sensitive settings.",
        "Integrate formal drift detectors to schedule recalibration of CP and retraining of the generator/discriminator.",
        "Benchmark multiple base classifiers (e.g., RF, XGB, SVM, GNN) under the same CP wrapper to assess trade-offs in set size vs. accuracy.",
        "Assess and optimize computational efficiency (e.g., batched p-value computation, approximate CP) for large-scale deployment."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Concept drift and evolving Trojans after deployment.",
        "Severe class imbalance in real-world HT detection.",
        "Need for human-in-the-loop when NULL or ambiguous prediction sets occur.",
        "Handling Non-IID data and covariate shift."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Suggest the notion of HT evolution and provide a method to create evolving HTs with high precision using a conformalized GAN.",
      "Discuss guaranteed coverage of prediction sets with a tunable significance level via conformal prediction for HT detection.",
      "Define an algorithm-agnostic, explainability-aware reject option when the model is uncertain (NULL set), enabling calibrated explainability and human review.",
      "Propose a ranking mechanism for evolved Trojans using confidence scores; validate on synthetic and real chip-level benchmarks.",
      "Provide risk-aware theoretical guaranteed coverage of predictions valid under covariate shift via a non-invasive wrapper over ML models."
    ]
  },
  {
    "arxiv_id": "2310.13247v1",
    "title": "Anomaly Detection of Command Shell Sessions based on DistilBERT: Unsupervised and Supervised Approaches",
    "authors": "Zefang Liu; John Buford",
    "abstract": "Anomaly detection in command shell sessions is a critical aspect of computer security. Recent advances in deep learning and natural language processing, particularly transformer-based models, have shown great promise for addressing complex security challenges. In this paper, we implement a comprehensive approach to detect anomalies in Unix shell sessions using a pretrained DistilBERT model, leveraging both unsupervised and supervised learning techniques to identify anomalous activity while minimizing data labeling. The unsupervised method captures the underlying structure and syntax of Unix shell commands, enabling the detection of session deviations from normal behavior. Experiments on a large-scale enterprise dataset collected from production systems demonstrate the effectiveness of our approach in detecting anomalous behavior in Unix shell sessions. This work highlights the potential of leveraging recent advances in transformers to address important computer security challenges.",
    "published_date": "2023-10-20",
    "pdf_link": "https://arxiv.org/pdf/2310.13247v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Endpoint Security",
      "subdomain": "Host-based Anomaly Detection",
      "specific_problem": "Anomaly detection in Unix command shell sessions (session-level)",
      "attack_types": [
        "Remote System Discovery",
        "System Owner/User Discovery",
        "System Network Connections Discovery",
        "System Network Configuration Discovery",
        "System Information Discovery",
        "Account Discovery",
        "Permission Groups Discovery",
        "Network Sniffing",
        "Hijack Execution Flow: Dynamic Linker Hijacking",
        "Boot or Logon Autostart Execution: Kernel Modules and Extensions",
        "Create Account",
        "Scheduled Task/Job: Cron",
        "Service Stop",
        "Impair Defenses: Disable or Modify Tools",
        "Ingress Tool Transfer",
        "File and Directory Permissions Modification (Linux/Mac)",
        "OS Credential Dumping: /etc/passwd and /etc/shadow",
        "Indicator Removal: Clear Command History",
        "Abuse Elevation Control Mechanism: Sudo and Sudo Caching",
        "Event Triggered Execution: Unix Shell Configuration Modification"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer (Encoder)",
        "specific": "DistilBERT (pretrained from scratch with MLM)",
        "novel_contribution": "Domain-specific pretraining of DistilBERT on Unix shell commands to produce session embeddings for anomaly detection"
      },
      {
        "type": "primary",
        "category": "Ensemble Outlier Detection",
        "specific": "PCA, Isolation Forest, COPOD, Autoencoder (PyOD)",
        "novel_contribution": "Ensembling multiple detectors over DistilBERT session embeddings; averaged normalized decision scores as anomaly score"
      },
      {
        "type": "primary",
        "category": "Contrastive Fine-tuning",
        "specific": "SetFit (Sentence Transformer Fine-tuning)",
        "novel_contribution": "Few-shot supervised fine-tuning of the pretrained DistilBERT for session-level anomaly classification using keyword-derived labels"
      },
      {
        "type": "baseline",
        "category": "Linear Model",
        "specific": "Logistic Regression",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer (Encoder)",
        "specific": "Standard fine-tuned DistilBERT classifier (CLS + linear head)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Self-supervised",
      "Unsupervised",
      "Supervised",
      "Few-shot"
    ],
    "datasets": [
      {
        "name": "Enterprise Unix keystroke/shell session dataset (90 days, >15k users)",
        "type": "proprietary",
        "domain": "shell_command_sessions",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "SEA dataset (Schonlau et al.)",
        "type": "public",
        "domain": "shell_commands",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Greenberg dataset",
        "type": "public",
        "domain": "shell_commands",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "PU dataset (Purdue command histories)",
        "type": "public",
        "domain": "shell_commands",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "NL2Bash",
        "type": "public",
        "domain": "shell_commands",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Logistic Regression on fixed session embeddings",
        "paper_reference": null,
        "metric": "precision/recall/F1",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Standard fine-tuned DistilBERT classifier",
        "paper_reference": null,
        "metric": "precision/recall/F1",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "precision",
      "recall",
      "F1"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Existing log anomaly detection models (e.g., LSTM/transformer log methods) target system logs resembling human language; Unix shell commands have distinct structures requiring specialized methods.",
        "Masquerade detection focuses on user impersonation patterns and supervised features; it is ill-suited for discovering suspicious/exploitable command patterns without predefined rules.",
        "Prior Unix shell datasets are outdated, truncated, lack options/subshells and diversity, and have limited real exploit coverage, hindering modern model development.",
        "Heavy reliance on labeled data and expert-crafted features limits flexibility and adaptability to novel threats."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Automate detection of anomalous/suspicious Unix shell sessions at enterprise scale while minimizing manual labeling; complement rule-based detections by flagging outlier session patterns for triage.",
      "potential_research_ideas": [
        "Extend domain-specific pretraining to Windows PowerShell and cross-platform command lines with multi-domain adapters.",
        "Online/streaming anomaly detection with concept drift handling and user-specific baselines.",
        "Incorporate command output/context (stdin/stdout, exit codes) for multimodal anomaly scoring.",
        "Explainable anomaly detection: highlight tokens/commands/options contributing to high anomaly scores and map to ATT&CK techniques.",
        "Adversarially robust detection against obfuscation (encoding, quoting, environment var tricks, aliases).",
        "Hierarchical modeling of sessions (line-level transformer + session-level aggregator) and segment-level anomaly localization.",
        "Open-set recognition and calibrated anomaly scoring with EVT or energy-based models.",
        "Weak supervision using ATT&CK regex tags and distant supervision to scale labeled data.",
        "Combine embedding-based kNN/LOF with learned density models (e.g., Deep SVDD) for hybrid scoring.",
        "User-conditional modeling to separate rare-but-benign power-user behavior from malicious outliers."
      ],
      "architectural_improvement_recommendations": [
        "Replace uniform score averaging with learned stacking/weighting of outlier detectors via a small calibration set.",
        "Use next-token perplexity from the domain-pretrained model as an additional anomaly feature; fuse with embedding-based detectors.",
        "Adopt hierarchical transformer (per-line encoder) with attention pooling and token-level saliency for explainability.",
        "Leverage contrastive self-supervised objectives beyond MLM (e.g., SimCSE/InfoNCE over augmented sessions) prior to anomaly modeling.",
        "Introduce retrieval/kNN in embedding space (FAISS/HNSW) for density estimation and fast nearest-neighbor explanations.",
        "Train a smaller, domain-specific tokenizer (BPE/Unigram) optimized for shell tokens, flags, and paths to reduce OOV/token fragmentation.",
        "Incorporate temporal/contextual features (time-of-day, tty, host, user role) via feature fusion or conditional adapters.",
        "Use Deep SVDD or normalizing flows for one-class modeling over session embeddings; compare to COPOD/IF."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "Hugging Face Transformers",
        "PyTorch",
        "SetFit",
        "PyOD",
        "scikit-learn"
      ],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Enterprise Unix hosts (production systems)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Extracting prompts and commands from noisy keystroke logs with heterogeneous prompts and subshells.",
        "Handling truncated/wrapped lines, aliases, and mixed background process outputs.",
        "Cleaning errors, editor buffers, completions, and cyclic/script-generated commands.",
        "Case sensitivity and tokenizer vocabulary design for shell-specific tokens."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Comprehensive anomaly detection framework for Unix shell sessions using a pretrained DistilBERT and an ensemble of anomaly detectors.",
      "Unsupervised approach that computes anomaly scores from DistilBERT session embeddings with four PyOD detectors on a large-scale enterprise dataset, enabling discovery without extensive labels.",
      "Supervised few-shot approach via SetFit fine-tuning on keyword-derived labels; evaluates precision/recall/F1 to demonstrate adaptability and accuracy."
    ]
  },
  {
    "arxiv_id": "2310.04171v3",
    "title": "Dynamic Relation-Attentive Graph Neural Networks for Fraud Detection",
    "authors": "Heehyeon Kim; Jinhyeok Choi; Joyce Jiyoung Whang",
    "abstract": "Fraud detection aims to discover fraudsters deceiving other users by, for example, leaving fake reviews or making abnormal transactions. Graph-based fraud detection methods consider this task as a classification problem with two classes: frauds or normal. We address this problem using Graph Neural Networks (GNNs) by proposing a dynamic relation-attentive aggregation mechanism. Based on the observation that many real-world graphs include different types of relations, we propose to learn a node representation per relation and aggregate the node representations using a learnable attention function that assigns a different attention coefficient to each relation. Furthermore, we combine the node representations from different layers to consider both the local and global structures of a target node, which is beneficial to improving the performance of fraud detection on graphs with heterophily. By employing dynamic graph attention in all the aggregation processes, our method adaptively computes the attention coefficients for each node. Experimental results show that our method, DRAG, outperforms state-of-the-art fraud detection methods on real-world benchmark datasets.",
    "published_date": "2023-10-06",
    "pdf_link": "https://arxiv.org/pdf/2310.04171v3",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Fraud Detection",
      "subdomain": "Graph-based Fraud/Anomaly Detection",
      "specific_problem": "Binary node classification to detect fraudulent users/reviews on multi-relation graphs under heterophily",
      "attack_types": [
        "fake reviews (opinion spam)",
        "abnormal transactions"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "GNN (Graph Attention)",
        "specific": "Dynamic relation- and layer-attentive GATv2-style aggregation (DRAG)",
        "novel_contribution": "Learns per-relation node representations and aggregates them with dynamic attention; adds dynamic layer-wise attention to combine local/global structures; uses dynamic attention in all aggregation steps"
      },
      {
        "type": "baseline",
        "category": "MLP",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "GNN (GraphSAGE)",
        "specific": "GraphSAGE [26]",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "GNN (Graph Attention)",
        "specific": "GAT [17]",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "GNN (Graph Attention)",
        "specific": "GATv2 [16]",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "GNN (Relation-aware)",
        "specific": "FRAUDRE [13]",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "GNN (Relation-aware)",
        "specific": "CARE-GNN [11]",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "GNN (Spectral/Propagation rethinking)",
        "specific": "BWGNN [12] (Homo/Hetero variants)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "GNN (Propagation and confidence)",
        "specific": "PC-GNN [27]",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "YelpChi",
        "type": "public",
        "domain": "review_graphs",
        "link": "https://github.com/bdi-lab/DRAG",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Amazon (review fraud graph)",
        "type": "public",
        "domain": "ecommerce_review_graphs",
        "link": "https://github.com/bdi-lab/DRAG",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "BWGNN-Hetero",
        "paper_reference": "[12]",
        "metric": "YelpChi AUC (40% labels)",
        "their_result": "0.9233 ± 0.0053",
        "baseline_result": "0.9026 ± 0.0105"
      },
      {
        "method_name": "BWGNN-Hetero",
        "paper_reference": "[12]",
        "metric": "YelpChi F1-macro (40% labels)",
        "their_result": "0.7988 ± 0.0067",
        "baseline_result": "0.7176 ± 0.0705"
      },
      {
        "method_name": "BWGNN-Hetero",
        "paper_reference": "[12]",
        "metric": "YelpChi AUC (10% labels)",
        "their_result": "0.8833 ± 0.0056",
        "baseline_result": "0.8455 ± 0.0146"
      },
      {
        "method_name": "BWGNN-Hetero",
        "paper_reference": "[12]",
        "metric": "YelpChi F1-macro (10% labels)",
        "their_result": "0.7462 ± 0.0076",
        "baseline_result": "0.7137 ± 0.0197"
      },
      {
        "method_name": "BWGNN-Hetero",
        "paper_reference": "[12]",
        "metric": "YelpChi AUC (1% labels)",
        "their_result": "0.8279 ± 0.0100",
        "baseline_result": "0.7764 ± 0.0196"
      },
      {
        "method_name": "BWGNN-Hetero",
        "paper_reference": "[12]",
        "metric": "YelpChi F1-macro (1% labels)",
        "their_result": "0.6884 ± 0.0094",
        "baseline_result": "0.6558 ± 0.0118"
      },
      {
        "method_name": "BWGNN-Homo",
        "paper_reference": "[12]",
        "metric": "Amazon AUC (40% labels)",
        "their_result": "0.9701 ± 0.0031",
        "baseline_result": "0.9700 ± 0.0046"
      },
      {
        "method_name": "GraphSAGE",
        "paper_reference": "[26]",
        "metric": "Amazon F1-macro (40% labels)",
        "their_result": "0.9130 ± 0.0082",
        "baseline_result": "0.9123 ± 0.0065"
      },
      {
        "method_name": "GraphSAGE",
        "paper_reference": "[26]",
        "metric": "Amazon AUC (10% labels)",
        "their_result": "0.9450 ± 0.0068",
        "baseline_result": "0.9549 ± 0.0092"
      },
      {
        "method_name": "CARE-GNN",
        "paper_reference": "[11]",
        "metric": "Amazon AUC (1% labels)",
        "their_result": "0.9172 ± 0.0216",
        "baseline_result": "0.9235 ± 0.0245"
      }
    ],
    "performance_metrics_used": [
      "F1-macro",
      "AUC"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Static attention in GAT leads to fixed ranking of attention coefficients; dynamic attention is needed to adapt per node.",
        "Many real-world graphs include different relation types; ignoring these harms fraud detection performance.",
        "Heterophily makes fraud detection challenging as nodes connect across classes; need mechanisms to weigh neighbors/relations/layers adaptively."
      ],
      "limitations": [
        "Evaluated on static graphs; evolving graphs with new nodes/edges are not handled.",
        "On Amazon, DRAG is only comparable (not superior) to best baselines in several settings.",
        "No theoretical analysis of learned attention under heterophily is provided.",
        "No explicit consideration of class imbalance strategies beyond standard training."
      ],
      "future_work": [
        "Investigate attention mechanisms from a theoretical point of view under heterophily.",
        "Extend the approach to more complex relational graphs.",
        "Extend DRAG to handle evolving graphs where new nodes/edges appear over time."
      ],
      "motivation": "Leverage multi-relation structures and dynamic attention to improve graph-based fraud detection, particularly under heterophily.",
      "potential_research_ideas": [
        "Develop a theoretically grounded analysis of dynamic relation and layer attention under varying heterophily levels, including generalization bounds.",
        "Design a temporal DRAG variant for dynamic/evolving graphs with time-aware relation and layer attention.",
        "Integrate cost-sensitive or AUC-oriented objectives with DRAG to better handle label imbalance prevalent in fraud detection.",
        "Incorporate contrastive/self-supervised pretraining across relations to improve performance with very low label rates.",
        "Add causal or counterfactual attention regularization to increase interpretability and mitigate attention overfitting.",
        "Extend DRAG to heterogeneous node/edge types beyond simple multi-relation settings (e.g., users, items, reviews as distinct node types)."
      ],
      "architectural_improvement_recommendations": [
        "Introduce decoupled propagation and feature transformation (e.g., APPNP/GCNII-style residuals) within DRAG to stabilize deeper layers under heterophily.",
        "Add temporal attention heads and edge-time encodings to support streaming/evolving graphs.",
        "Apply relation-specific parameter sharing with low-rank adapters to reduce memory footprint when the number of relations grows.",
        "Combine DRAG with label propagation or consistency regularization to better exploit unlabeled nodes.",
        "Incorporate class-imbalance handling (focal loss, re-weighting, or AUC-maximization) directly into the DRAG loss.",
        "Expose calibrated attention outputs with sparsity or entropy regularization to improve explanation quality."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/bdi-lab/DRAG",
      "frameworks": [],
      "reproducibility_score": "high",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Handling heterophily and multi-relation structures at scale",
        "Label sparsity and class imbalance in operational fraud settings",
        "Adapting to evolving/dynamic graphs not covered by current model"
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes DRAG: a dynamic relation-attentive GNN that learns per-relation node representations and aggregates them with learnable attention weights.",
      "Introduces layer-wise dynamic attention to combine intermediate representations, capturing both local and global structures and mitigating heterophily effects.",
      "Employs dynamic graph attention in all aggregation processes so attention coefficients adapt per node and per layer.",
      "Demonstrates state-of-the-art or superior performance on real-world benchmarks (YelpChi) and comparable performance on Amazon; provides ablation studies showing importance of relation- and layer-attentive aggregations.",
      "Releases implementation and datasets via GitHub."
    ]
  },
  {
    "arxiv_id": "2310.00516v2",
    "title": "Enhancing Efficiency and Privacy in Memory-Based Malware Classification through Feature Selection",
    "authors": "Salim Sazzed; Sharif Ullah",
    "abstract": "Malware poses a significant security risk to individuals, organizations, and critical infrastructure by compromising systems and data. Leveraging memory dumps that offer snapshots of computer memory can aid the analysis and detection of malicious content, including malware. To improve the efficacy and address privacy concerns in malware classification systems, feature selection can play a critical role as it is capable of identifying the most relevant features, thus, minimizing the amount of data fed to classifiers. In this study, we employ three feature selection approaches to identify significant features from memory content and use them with a diverse set of classifiers to enhance the performance and privacy of the classification task. Comprehensive experiments are conducted across three levels of malware classification tasks: i) binary-level benign or malware classification, ii) malware type classification (including Trojan horse, ransomware, and spyware), and iii) malware family classification within each family (with varying numbers of classes). Results demonstrate that the feature selection strategy, incorporating mutual information and other methods, enhances classifier performance for all tasks. Notably, selecting only 25\\% and 50\\% of input features using Mutual Information and then employing the Random Forest classifier yields the best results. Our findings reinforce the importance of feature selection for malware classification and provide valuable insights for identifying appropriate approaches. By advancing the effectiveness and privacy of malware classification systems, this research contributes to safeguarding against security threats posed by malicious software.",
    "published_date": "2023-09-30",
    "pdf_link": "https://arxiv.org/pdf/2310.00516v2",
    "paper_types": [
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Malware Analysis",
      "subdomain": "Memory Forensics and Malware Classification",
      "specific_problem": "Feature selection for memory dump-based malware classification at binary, type, and family levels",
      "attack_types": [
        "Trojan horse",
        "Spyware",
        "Ransomware",
        "Zeus",
        "Emotet",
        "Refroso",
        "scar",
        "Reconyc",
        "180Solutions",
        "Coolwebsearch",
        "Gator",
        "Transponder",
        "TIBS",
        "Conti",
        "MAZE",
        "Pysa",
        "ako",
        "Shade"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Filter-based Feature Selection",
        "specific": "Mutual Information (scikit-learn)",
        "novel_contribution": "Systematic evaluation on memory-dump malware data showing that selecting 25–50% of features via MI combined with Random Forest yields similar or better performance than using all features, improving efficiency and privacy."
      },
      {
        "type": "primary",
        "category": "Filter-based Feature Selection",
        "specific": "Chi-square (χ2)",
        "novel_contribution": "Compared as a filter-based selector for reducing memory features prior to classification."
      },
      {
        "type": "primary",
        "category": "Filter-based Feature Selection",
        "specific": "ANOVA F-test",
        "novel_contribution": "Compared as a filter-based selector for reducing memory features prior to classification."
      },
      {
        "type": "baseline",
        "category": "Ensemble Trees",
        "specific": "Random Forest",
        "novel_contribution": "Used as a classifier; reported as the strongest performer when combined with MI feature selection."
      },
      {
        "type": "baseline",
        "category": "Probabilistic Classifier",
        "specific": "Naive Bayes",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Instance-based",
        "specific": "K-Nearest Neighbors (KNN)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Ensemble",
        "specific": "AdaBoost",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Linear/Generative",
        "specific": "Linear Discriminant Analysis (LDA)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Ensemble Trees",
        "specific": "Extra Trees (Extremely Randomized Trees)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "MalMemAnalysis-2022",
        "type": "public",
        "domain": "memory_dumps",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can filter-based feature selection (MI, Chi-square, ANOVA) identify a small subset of memory features that maintains or improves malware classification performance?",
        "What fraction of features (e.g., 25% or 50%) is sufficient to achieve competitive performance across binary, type, and family-level malware classification?",
        "Which classifier(s) benefit most from reduced feature sets in memory-based malware classification?"
      ],
      "gaps_identified": [
        "Feature selection remains mostly unexplored in malware detection, particularly for memory-based analysis.",
        "Wrapper-based feature selection methods can be computationally expensive; efficient filter-based alternatives are desirable in cybersecurity contexts where time is critical.",
        "Privacy concerns in cybersecurity can be mitigated by limiting the amount of data used; there is a need to study how feature selection supports privacy."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Improve efficacy and address privacy concerns in memory-based malware classification by selecting only the most relevant features to reduce data usage and computational cost.",
      "potential_research_ideas": [
        "Evaluate embedded and wrapper-based feature selection (e.g., L1-regularized models, recursive feature elimination) under strict compute budgets to compare against filter methods for memory-based malware tasks.",
        "Quantify privacy benefits from feature reduction using formal privacy risk metrics (e.g., membership inference, model inversion risk) on memory-dump features.",
        "Cross-dataset and cross-environment generalization: test selected features across different memory forensics datasets or OS versions to assess robustness to domain shift.",
        "Adaptive or streaming feature selection for real-time memory telemetry to handle concept drift in evolving malware.",
        "Joint multi-task learning that shares a compact feature subset across binary/type/family tasks, optimizing global utility under feature budget constraints.",
        "Robust feature selection under adversarial manipulation (evasion attacks) on memory-derived features.",
        "Interpretability analysis of selected memory features to derive forensic insights and actionable rules for analysts."
      ],
      "architectural_improvement_recommendations": [
        "Introduce a model selection and hyperparameter tuning pipeline (e.g., cross-validated grid or Bayesian search) for classifiers and feature fractions instead of default settings.",
        "Use embedded feature selection via sparsity-inducing regularization (L1/L0) or tree-based importance with permutation importance to validate filter selections.",
        "Calibrate and ensemble top-performing classifiers (e.g., RF + ExtraTrees) on selected features to improve stability and calibration.",
        "Automate selection of feature fraction via nested cross-validation optimizing accuracy/F1 under latency or feature budget constraints.",
        "Apply class imbalance handling (e.g., class weights, focal loss surrogates) for skewed family distributions.",
        "Add model explainability (e.g., SHAP on selected features) to validate forensic relevance and support analyst trust."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "scikit-learn"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Data privacy concerns when using comprehensive memory dumps",
        "Need for computational efficiency in cybersecurity settings"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": true,
      "fairness_bias": false
    },
    "contributions": [
      "Investigated the effectiveness of diverse filter-based feature selection approaches (Mutual Information, Chi-square, ANOVA) for memory-based malware identification and classification.",
      "Demonstrated that MI-based feature selection can select 25%–50% of features and achieve similar or better performance than using all features, particularly with Random Forest.",
      "Conducted comprehensive experiments across three levels: binary (benign vs malware), malware type (Trojan, ransomware, spyware), and malware family classification."
    ]
  },
  {
    "arxiv_id": "2310.17127v1",
    "title": "A Method for Network Intrusion Detection Using Flow Sequence and BERT Framework",
    "authors": "Loc Gia Nguyen; Kohei Watabe",
    "abstract": "A Network Intrusion Detection System (NIDS) is a tool that identifies potential threats to a network. Recently, different flow-based NIDS designs utilizing Machine Learning (ML) algorithms have been proposed as solutions to detect intrusions efficiently. However, conventional ML-based classifiers have not seen widespread adoption in the real world due to their poor domain adaptation capability. In this research, our goal is to explore the possibility of using sequences of flows to improve the domain adaptation capability of network intrusion detection systems. Our proposal employs natural language processing techniques and Bidirectional Encoder Representations from Transformers framework, which is an effective technique for modeling data with respect to its context. Early empirical results show that our approach has improved domain adaptation capability compared to previous approaches. The proposed approach provides a new research method for building a robust intrusion detection system.",
    "published_date": "2023-10-26",
    "pdf_link": "https://arxiv.org/pdf/2310.17127v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Flow-based network intrusion detection with improved cross-domain (domain adaptation) performance",
      "attack_types": [
        "dos",
        "scan",
        "portScan",
        "pingScan",
        "bruteForce"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "BERT",
        "novel_contribution": "Uses sequences of network flows as 'sentences' and individual flows as 'tokens'; pre-trains BERT with MLM on only benign flows; omits positional encoding and special tokens; fine-tunes for binary classification to improve domain adaptation."
      },
      {
        "type": "primary",
        "category": "MLP",
        "specific": null,
        "novel_contribution": "Simple linear layer with softmax on top of BERT embeddings for benign vs malicious classification."
      },
      {
        "type": "baseline",
        "category": "Energy-based/Statistical",
        "specific": "Energy-based Flow Classifier (EFC)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Decision Tree",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "K-Nearest Neighbors",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "SVM",
        "specific": "Linear SVM",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "MLP",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Naive Bayes",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Ensemble",
        "specific": "AdaBoost",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Ensemble",
        "specific": "Random Forest",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Self-supervised (MLM pretraining on benign flows)",
      "Supervised (fine-tuning for classification)"
    ],
    "datasets": [
      {
        "name": "CIDDS-001 (OpenStack)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CIDDS-001 (External Server)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CIDDS-002 (OpenStack)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Energy-based Flow Classifier (EFC)",
        "paper_reference": "[5] (as cited in the paper)",
        "metric": "Accuracy/F1/Recall/Precision (Train: CIDDS-001 internal → Test: CIDDS-001 external)",
        "their_result": "0.9078 / 0.9311 / 0.9120 / 0.9511",
        "baseline_result": "0.8659 / 0.9044 / 0.9278 / 0.8822"
      },
      {
        "method_name": "Decision Tree",
        "paper_reference": null,
        "metric": "Accuracy/F1/Recall/Precision (Train: CIDDS-001 internal → Test: CIDDS-001 external)",
        "their_result": "0.9078 / 0.9311 / 0.9120 / 0.9511",
        "baseline_result": "0.8491 / 0.8800 / 0.8088 / 0.9649"
      },
      {
        "method_name": "KNN",
        "paper_reference": null,
        "metric": "Accuracy/F1/Recall/Precision (Train: CIDDS-001 internal → Test: CIDDS-001 external)",
        "their_result": "0.9078 / 0.9311 / 0.9120 / 0.9511",
        "baseline_result": "0.7978 / 0.8270 / 0.7067 / 0.9967"
      },
      {
        "method_name": "Linear SVM",
        "paper_reference": null,
        "metric": "Accuracy/F1/Recall/Precision (Train: CIDDS-001 internal → Test: CIDDS-001 external)",
        "their_result": "0.9078 / 0.9311 / 0.9120 / 0.9511",
        "baseline_result": "0.6784 / 0.6940 / 0.5333 / 0.9933"
      },
      {
        "method_name": "MLP (baseline)",
        "paper_reference": null,
        "metric": "Accuracy/F1/Recall/Precision (Train: CIDDS-001 internal → Test: CIDDS-001 external)",
        "their_result": "0.9078 / 0.9311 / 0.9120 / 0.9511",
        "baseline_result": "0.4380 / 0.3254 / 0.1982 / 0.9087"
      },
      {
        "method_name": "Naive Bayes",
        "paper_reference": null,
        "metric": "Accuracy/F1/Recall/Precision (Train: CIDDS-001 internal → Test: CIDDS-001 external)",
        "their_result": "0.9078 / 0.9311 / 0.9120 / 0.9511",
        "baseline_result": "0.3161 / 0.0000 / 0.0000 / 0.9937"
      },
      {
        "method_name": "AdaBoost",
        "paper_reference": null,
        "metric": "Accuracy/F1/Recall/Precision (Train: CIDDS-001 internal → Test: CIDDS-001 external)",
        "their_result": "0.9078 / 0.9311 / 0.9120 / 0.9511",
        "baseline_result": "0.4606 / 0.3812 / 0.2430 / 0.8845"
      },
      {
        "method_name": "Random Forest",
        "paper_reference": null,
        "metric": "Accuracy/F1/Recall/Precision (Train: CIDDS-001 internal → Test: CIDDS-001 external)",
        "their_result": "0.9078 / 0.9311 / 0.9120 / 0.9511",
        "baseline_result": "0.8177 / 0.8510 / 0.7613 / 0.9647"
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "F1-score",
      "Recall",
      "Precision"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can modeling sequences of flows (context) using BERT improve the domain adaptation capability of flow-based NIDS compared to single-flow classifiers?"
      ],
      "gaps_identified": [
        "Conventional ML-based NIDS classifiers adapt poorly to changing data distributions in real-world deployment.",
        "Existing approaches largely operate on single flows, limiting contextual understanding across flows.",
        "EFC improves domain adaptability but still shows limitations on certain data distributions."
      ],
      "limitations": [
        "Evaluation limited to CIDDS-001/-002 environments; no production deployment results reported.",
        "Uses only six flow features (duration, proto, src port, dst port, packets, bytes, flags) and excludes IPs and timestamps; may limit expressiveness.",
        "No exploration of positional information or alternative context encodings beyond fixed-size sequences.",
        "No analysis of computational cost or latency for real-time deployment.",
        "Adopts discretization thresholds from prior work; effects of discretization design not ablated."
      ],
      "future_work": [],
      "motivation": "Improve robustness/domain adaptation of flow-based NIDS by leveraging contextual information from sequences of flows via BERT.",
      "potential_research_ideas": [
        "Pre-train on multi-environment unlabeled flows (cross-network corpora) to further enhance domain invariance.",
        "Incorporate contrastive learning (e.g., SimCLR/InfoNCE) across sessions/hosts to learn domain-invariant representations.",
        "Use hierarchical transformers with session- and host-level aggregation to capture multi-scale context.",
        "Introduce learnable or relative positional encodings reflecting temporal gaps and directionality to capture ordering cues.",
        "Augment with side-channel metadata (e.g., subnet roles, service banners) via multi-modal fusion while preserving privacy.",
        "Active domain adaptation: online unsupervised adaptation using test-time entropy minimization or pseudo-labeling.",
        "Uncertainty estimation and open-set detection to better handle unknown attack types under domain shift.",
        "Evaluate on additional public datasets with distribution shifts (e.g., UNSW-NB15, CIC-IDS2017) and real enterprise traces."
      ],
      "architectural_improvement_recommendations": [
        "Add relative/rotary positional encodings tailored to irregular flow timing (time-delta embeddings).",
        "Use adapters or LoRA modules for parameter-efficient fine-tuning across domains.",
        "Replace discretization with learned vector quantization or embedding of continuous features (e.g., feature-wise embedding MLPs).",
        "Adopt hierarchical transformer: token=flow, sentence=session/host window, document=day to capture longer context with pooling.",
        "Incorporate contrastive pretraining objectives (flow-to-context, host-to-host) alongside MLM.",
        "Calibrate outputs with temperature scaling and add uncertainty heads for open-set detection.",
        "Memory-efficient attention (Longformer/Performer) to scale to 1000+ flow sequences in real time."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "PyTorch",
        "scikit-learn"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "BERT with 1 layer, 1 attention head, hidden size 768; batch size 512; training sequence length 128 (testing 1024); Adam optimizer, lr=1e-5; 2000 total iterations with staged training (400 MLM pretrain, 1100 classifier with BERT frozen, 400 joint fine-tune)."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Assembling temporal sequences online without introducing latency.",
        "Memory/compute overhead for long-sequence transformer inference in high-throughput networks.",
        "Domain shift over time requires continual or test-time adaptation.",
        "Selecting/maintaining discretization or embedding strategies for evolving feature distributions."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Investigates using sequences of flows to improve domain adaptation of NIDS.",
      "Proposes a BERT-based feature extractor over flow sequences with an MLP classifier; omits positional encoding and uses MLM pretraining on benign flows.",
      "Claims to be the first to use BERT to improve domain adaptation capability of NIDS.",
      "Demonstrates improved cross-domain performance on CIDDS-001/-002 compared to EFC and standard ML baselines (e.g., 0.9078 accuracy and 0.9311 F1 on CIDDS-001 external when trained on CIDDS-001 internal)."
    ]
  },
  {
    "arxiv_id": "2310.11640v1",
    "title": "Free-text Keystroke Authentication using Transformers: A Comparative Study of Architectures and Loss Functions",
    "authors": "Saleh Momeni; Bagher BabaAli",
    "abstract": "Keystroke biometrics is a promising approach for user identification and verification, leveraging the unique patterns in individuals' typing behavior. In this paper, we propose a Transformer-based network that employs self-attention to extract informative features from keystroke sequences, surpassing the performance of traditional Recurrent Neural Networks. We explore two distinct architectures, namely bi-encoder and cross-encoder, and compare their effectiveness in keystroke authentication. Furthermore, we investigate different loss functions, including triplet, batch-all triplet, and WDCL loss, along with various distance metrics such as Euclidean, Manhattan, and cosine distances. These experiments allow us to optimize the training process and enhance the performance of our model. To evaluate our proposed model, we employ the Aalto desktop keystroke dataset. The results demonstrate that the bi-encoder architecture with batch-all triplet loss and cosine distance achieves the best performance, yielding an exceptional Equal Error Rate of 0.0186%. Furthermore, alternative algorithms for calculating similarity scores are explored to enhance accuracy. Notably, the utilization of a one-class Support Vector Machine reduces the Equal Error Rate to an impressive 0.0163%. The outcomes of this study indicate that our model surpasses the previous state-of-the-art in free-text keystroke authentication. These findings contribute to advancing the field of keystroke authentication and offer practical implications for secure user verification systems.",
    "published_date": "2023-10-18",
    "pdf_link": "https://arxiv.org/pdf/2310.11640v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Authentication and Access Control",
      "subdomain": "Behavioral Biometrics",
      "specific_problem": "Free-text keystroke-based user verification/authentication",
      "attack_types": [
        "impostor/impersonation attempts",
        "replay attack"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "Transformer encoder (self-attention) in bi-encoder architecture",
        "novel_contribution": "Transformer-based bi-encoder for free-text keystroke authentication with feature fusion of temporal and spatial (key embedding) channels"
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "Transformer encoder in cross-encoder architecture",
        "novel_contribution": "Joint encoding of pairwise keystroke sequences for similarity scoring"
      },
      {
        "type": "primary",
        "category": "Metric Learning / Contrastive Learning",
        "specific": "Triplet loss, Batch-all triplet loss, Weighted Decoupled Contrastive Learning (WDCL)",
        "novel_contribution": "Comparison of multiple contrastive losses; application of WDCL with a von Mises–Fisher-inspired weighting for keystroke biometrics"
      },
      {
        "type": "baseline",
        "category": "Distance Metrics",
        "specific": "Euclidean, Manhattan, Cosine",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "SVM",
        "specific": "One-Class SVM for scoring",
        "novel_contribution": "Used as an alternative similarity/scoring algorithm on top of learned embeddings to further reduce EER"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Contrastive Learning",
      "One-class Anomaly Detection"
    ],
    "datasets": [
      {
        "name": "Aalto desktop keystroke dataset",
        "type": "public",
        "domain": "keystroke_dynamics",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Killourhy and Maxion keystroke-dynamics dataset",
        "type": "public",
        "domain": "keystroke_dynamics",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "TypeNet (Siamese LSTM) — Acien et al.",
        "paper_reference": "Acien et al. [1]",
        "metric": "Equal Error Rate (EER)",
        "their_result": "0.0186% EER with bi-encoder + batch-all triplet + cosine; improved to 0.0163% EER with one-class SVM scoring",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Equal Error Rate (EER)"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Which architecture (bi-encoder vs. cross-encoder) is most effective for free-text keystroke authentication?",
        "Which contrastive loss (triplet, batch-all triplet, WDCL) and distance metric (Euclidean, Manhattan, cosine) optimize verification performance?",
        "Can alternative similarity/scoring algorithms (e.g., one-class SVM) improve accuracy over distance-based scoring?"
      ],
      "gaps_identified": [
        "Many prior works focus on fixed-text authentication, which does not reflect real free-text typing behavior.",
        "Prior approaches often rely on small datasets and overlook scalability.",
        "Need for models that handle variability in typing patterns due to factors like stress, fatigue, injury, or keyboard design."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Improve free-text keystroke authentication by overcoming fixed-text limitations and scalability issues using Transformer architectures and contrastive learning to achieve lower EER on a public benchmark.",
      "potential_research_ideas": [
        "Pretrain transformers on large unlabeled keystroke streams with self-supervised objectives (e.g., masked time-step prediction) and fine-tune for verification.",
        "Domain adaptation and personalization to handle cross-device, cross-session, and cross-keyboard shifts (e.g., meta-learning or test-time adaptation).",
        "Privacy-preserving training such as federated learning or differential privacy to mitigate biometric data exposure risks.",
        "Adversarial robustness and spoofing resistance studies (including replay and synthetic keystroke generation attacks) with corresponding defenses.",
        "Multimodal behavioral fusion (keystroke + mouse dynamics, or inertial sensors on mobile) for improved robustness.",
        "Online/continuous authentication with drift detection and adaptive enrollment updates.",
        "Calibration and open-set verification scoring (e.g., PLDA, score calibration, or Mahalanobis with covariance estimation).",
        "Lightweight/efficient transformer variants (Performer/Linformer) and quantization/pruning for on-device inference."
      ],
      "architectural_improvement_recommendations": [
        "Introduce a temporal convolutional front-end and channel/spatial attention to better fuse temporal and key-embedding features.",
        "Employ supervised metric-learning losses like ArcFace/CosFace/Circle Loss to enforce angular margins with cosine scoring.",
        "Adopt hard/semi-hard negative mining strategies or memory-bank/queue of negatives for more effective contrastive learning.",
        "Use prototypical networks or class-center losses to stabilize per-user representations and improve scoring.",
        "Evaluate additional one-class/anomaly scorers (SVDD/Deep SVDD, Isolation Forest, Mahalanobis distance) and perform score calibration.",
        "Leverage self-supervised pretraining objectives (e.g., contrastive predictive coding) before supervised fine-tuning.",
        "Assess robust positional encodings (relative/rotary) tailored to irregular keystroke timing.",
        "Incorporate uncertainty estimation (e.g., Monte Carlo dropout) to support decision thresholds for security-critical deployment."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Variability in typing due to stress, fatigue, injury, or keyboard differences.",
        "Privacy concerns from collecting and storing biometric keystroke data.",
        "Vulnerability to replay attacks without liveness or anti-spoofing measures.",
        "Enrollment and template update strategies for continuous authentication."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Comparison of bi-encoder and cross-encoder Transformer architectures for free-text keystroke authentication.",
      "Systematic evaluation of contrastive learning setups (triplet, batch-all triplet, WDCL) and distance metrics (Euclidean, Manhattan, cosine).",
      "Exploration of alternative similarity scoring via anomaly detection, notably one-class SVM.",
      "New state-of-the-art performance on the public Aalto desktop keystroke dataset with EER of 0.0186%, improved to 0.0163% using one-class SVM."
    ]
  },
  {
    "arxiv_id": "2310.03119v1",
    "title": "Crossed-IoT device portability of Electromagnetic Side Channel Analysis: Challenges and Dataset",
    "authors": "Tharindu Lakshan Yasarathna; Lojenaa Navanesan; Simon Barque; Assanka Sayakkara; Nhien-An Le-Khac",
    "abstract": "IoT (Internet of Things) refers to the network of interconnected physical devices, vehicles, home appliances, and other items embedded with sensors, software, and connectivity, enabling them to collect and exchange data. IoT Forensics is collecting and analyzing digital evidence from IoT devices to investigate cybercrimes, security breaches, and other malicious activities that may have taken place on these connected devices. In particular, EM-SCA has become an essential tool for IoT forensics due to its ability to reveal confidential information about the internal workings of IoT devices without interfering these devices or wiretapping their networks. However, the accuracy and reliability of EM-SCA results can be limited by device variability, environmental factors, and data collection and processing methods. Besides, there is very few research on these limitations that affects significantly the accuracy of EM-SCA approaches for the crossed-IoT device portability as well as limited research on the possible solutions to address such challenge. Therefore, this empirical study examines the impact of device variability on the accuracy and reliability of EM-SCA approaches, in particular machine-learning (ML) based approaches for EM-SCA. We firstly presents the background, basic concepts and techniques used to evaluate the limitations of current EM-SCA approaches and datasets. Our study then addresses one of the most important limitation, which is caused by the multi-core architecture of the processors (SoC). We present an approach to collect the EM-SCA datasets and demonstrate the feasibility of using transfer learning to obtain more meaningful and reliable results from EM-SCA in IoT forensics of crossed-IoT devices. Our study moreover contributes a new dataset for using deep learning models in analysing Electromagnetic Side-Channel data with regards to the cross-device portability matter.",
    "published_date": "2023-10-04",
    "pdf_link": "https://arxiv.org/pdf/2310.03119v1",
    "paper_types": [
      "new_dataset",
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Digital Forensics",
      "subdomain": "IoT Forensics",
      "specific_problem": "Cross-device portability of ML-based Electromagnetic Side-Channel Analysis (EM-SCA) for activity classification on IoT devices",
      "attack_types": [
        "Electromagnetic side-channel leakage"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Deep Neural Network (MLP)",
        "specific": "Keras Sequential DNN with 6 hidden layers",
        "novel_contribution": "Used as the main classifier on EM-SCA spectrogram features for within-device and cross-device experiments"
      },
      {
        "type": "primary",
        "category": "Transfer Learning",
        "specific": null,
        "novel_contribution": "Proposed and empirically examined to improve cross-device portability of EM-SCA models"
      },
      {
        "type": "baseline",
        "category": "Deep Neural Network (unspecified architecture)",
        "specific": null,
        "novel_contribution": "Prior work [14] achieving 99.66% accuracy on Echo Show EM-SCA used as baseline reference"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Transfer Learning"
    ],
    "datasets": [
      {
        "name": "EMSCA-2023-Latest (Crossed-IoT EM-SCA dataset)",
        "type": "public",
        "domain": "electromagnetic_emanations from IoT devices (DragonBoard 410c, Amazon Echo Show 5)",
        "link": "https://aseados.ucd.ie/datasets/EMSCA-2023-Latest/",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "DragonBoard 410c EM-SCA traces (this paper)",
        "type": "public",
        "domain": "electromagnetic_emanations (CPU EM traces under synthetic tasks)",
        "link": "https://aseados.ucd.ie/datasets/EMSCA-2023-Latest/",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "Amazon Echo Show 5 EM-SCA traces (this paper)",
        "type": "public",
        "domain": "electromagnetic_emanations (voice-command activities)",
        "link": "https://aseados.ucd.ie/datasets/EMSCA-2023-Latest/",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "EM-SCA datasets referenced in prior work [13]",
        "type": "proprietary",
        "domain": "electromagnetic_emanations (mobile/IoT application activities)",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "EM-SCA datasets referenced in prior work [14] (Echo Show, Google Home, SmartThings Hub, smartphones)",
        "type": "proprietary",
        "domain": "electromagnetic_emanations (various smart devices)",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Prior EM-SCA DNN on Echo Show (study [14])",
        "paper_reference": "[14] (as cited in this paper)",
        "metric": "Accuracy",
        "their_result": "Echo Show 1: 89.0% test accuracy (same-device split) ; Echo Show 2 (cross-device): 25.0% test accuracy",
        "baseline_result": "Echo Show: 99.66% accuracy; other devices: >98.0% accuracy in [14]"
      }
    ],
    "performance_metrics_used": [
      "accuracy",
      "precision",
      "recall",
      "F1-score"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "R1: How the multi-core architecture of SoC integrated on these devices affects the ML/DL models’ accuracy?",
        "R2: How the number of activities running on the IoT device affect the ML/DL models’ accuracy?",
        "R3: Can transfer learning techniques be applied to improve the ML/DL models’ accuracy?"
      ],
      "gaps_identified": [
        "Very limited research on cross-device portability for EM-SCA ML/DL models",
        "Device variability (even among same model devices) significantly degrades accuracy",
        "Impact of multi-core SoC architecture on EM emissions confounds model learning",
        "Environmental factors and data collection/processing methods affect reliability",
        "Lack of suitable public datasets designed for cross-device EM-SCA portability"
      ],
      "limitations": [
        "Only two devices per type (DragonBoard 410c x2, Amazon Echo Show 5 x2), limiting generalizability",
        "Kept each raw data file under 3GB due to processing limitations",
        "Transfer learning approach is described at high level; detailed algorithmic settings and ablations are not fully reported in the provided text",
        "Laboratory setup; environmental variability control not extensively documented"
      ],
      "future_work": [],
      "motivation": "Assess and mitigate the challenges of applying ML/DL EM-SCA across similar IoT devices (cross-device portability) and provide a dedicated dataset enabling such research.",
      "potential_research_ideas": [
        "Unsupervised or semi-supervised domain adaptation for cross-device EM-SCA (e.g., DANN, MMD-based, CORAL)",
        "Self-supervised pretraining on large unlabeled EM traces (contrastive learning on spectrograms) to improve transfer",
        "Multi-core/source disentanglement: learning to separate per-core EM signatures via source separation or multi-instance learning",
        "Multi-antenna/array sensing to spatially isolate EM sources and improve robustness",
        "Physics-informed or hybrid models that incorporate SoC scheduling/core-affinity metadata into the ML pipeline",
        "Robust data augmentation for EM spectrograms (frequency shifts, time warping, noise injection) to simulate device variability",
        "Cross-frequency and cross-SDR generalization studies to decouple hardware acquisition bias",
        "Explainable EM-SCA: saliency on time-frequency regions to understand decision bases and guide sensor placement"
      ],
      "architectural_improvement_recommendations": [
        "Replace MLP with 2D CNNs or hybrid CNN-Transformer on spectrograms for stronger inductive biases",
        "Domain-adversarial training (gradient reversal) to learn device-invariant features",
        "Feature alignment losses (CORAL/MMD) between source and target device distributions",
        "Layer-wise fine-tuning with small target-device labeled set and discriminative learning rates",
        "Meta-learning (MAML/ANIL) to rapidly adapt to unseen devices",
        "Incorporate per-core/core-affinity signals (when available) as auxiliary labels or multitask heads",
        "Adaptive STFT configurations or wavelet transforms; learnable front-end (Sinc or Conv front-ends)",
        "Calibration step at deployment: few-shot calibration on new device with active learning"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "Keras",
        "TensorFlow (implied backend)",
        "GNU Radio",
        "scikit-learn (MinMaxScaler)"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Trained for 30 epochs with 6-layer DNN; GPU/CPU requirements not specified; EM capture at 20 MHz I/Q via HackRF One."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Laboratory setup with SDR (HackRF One), GNU Radio, and close-proximity H-loop antenna capturing EM from real devices",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Device variability across identical models causing severe accuracy drop",
        "Multi-core SoC scheduling/core usage alters EM emissions and confounds models",
        "Environmental EM noise and sensor placement sensitivity",
        "High data volume from I/Q sampling and storage/processing overhead",
        "Need for close-proximity antenna placement and careful frequency tuning"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Empirical examination of device variability, environmental, and collection/processing impacts on EM-SCA ML accuracy",
      "Analysis and demonstration of transfer learning feasibility to improve cross-device EM-SCA",
      "Methodology and setup to collect EM-SCA datasets considering multi-core SoC effects",
      "Release of a new public EM-SCA dataset tailored for cross-device portability research"
    ]
  },
  {
    "arxiv_id": "2310.12880v2",
    "title": "TwinPot: Digital Twin-assisted Honeypot for Cyber-Secure Smart Seaports",
    "authors": "Yagmur Yigit; Omer Kemal Kinaci; Trung Q. Duong; Berk Canberk",
    "abstract": "The idea of next-generation ports has become more apparent in the last ten years in response to the challenge posed by the rising demand for efficiency and the ever-increasing volume of goods. In this new era of intelligent infrastructure and facilities, it is evident that cyber-security has recently received the most significant attention from the seaport and maritime authorities, and it is a primary concern on the agenda of most ports. Traditional security solutions can be applied to safeguard IoT and Cyber-Physical Systems (CPS) from harmful entities. Nevertheless, security researchers can only watch, examine, and learn about the behaviors of attackers if these solutions operate more transparently. Herein, honeypots are potential solutions since they offer valuable information about the attackers. It can be virtual or physical. Virtual honeypots must be more realistic to entice attackers, necessitating better high-fidelity. To this end, Digital Twin (DT) technology can be employed to increase the complexity and simulation fidelity of the honeypots. Seaports can be attacked from both their existing devices and external devices at the same time. Existing mechanisms are insufficient to detect external attacks; therefore, the current systems cannot handle attacks at the desired level. DT and honeypot technologies can be used together to tackle them. Consequently, we suggest a DT-assisted honeypot, called TwinPot, for external attacks in smart seaports. Moreover, we propose an intelligent attack detection mechanism to handle different attack types using DT for internal attacks. Finally, we build an extensive smart seaport dataset for internal and external attacks using the MANSIM tool and two existing datasets to test the performance of our system. We show that under simultaneous internal and external attacks on the system, our solution successfully detects internal and external attacks.",
    "published_date": "2023-10-19",
    "pdf_link": "https://arxiv.org/pdf/2310.12880v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Cyber-Physical Systems Security",
      "subdomain": "IoT/IIoT and Maritime Infrastructure Security",
      "specific_problem": "Digital Twin-assisted honeypot and intelligent attack detection for smart seaports under simultaneous internal (on-port assets) and external (arriving devices/ships) attacks",
      "attack_types": [
        "Internal attacks from IIoT connectivity protocols (as in Edge-IIoTset; 12 attack traffic types used)",
        "External DDoS UDP attacks (9 variants from SDSN)",
        "General external probing/attacks against honeypot entities"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "AutoML / Model Selection",
        "specific": "AutoCM (Automated Classification Method) with Final Method Algorithm",
        "novel_contribution": "Selects the best classifier among 10 methods using an optimization objective combining weighted precision/recall (λi) and determination time (νi); monitors reliability γ=1−FN/(TP+FN) to trigger re-selection and feature changes"
      },
      {
        "type": "primary",
        "category": "Naive Bayes",
        "specific": null,
        "novel_contribution": "Included as one of the selectable classifiers in AutoCM"
      },
      {
        "type": "primary",
        "category": "K-Nearest Neighbors",
        "specific": null,
        "novel_contribution": "Included as one of the selectable classifiers in AutoCM"
      },
      {
        "type": "primary",
        "category": "CNN",
        "specific": null,
        "novel_contribution": "Included as one of the selectable classifiers in AutoCM"
      },
      {
        "type": "primary",
        "category": "Decision Tree",
        "specific": null,
        "novel_contribution": "Included as one of the selectable classifiers in AutoCM"
      },
      {
        "type": "primary",
        "category": "RNN",
        "specific": null,
        "novel_contribution": "Included as one of the selectable classifiers in AutoCM"
      },
      {
        "type": "primary",
        "category": "Logistic Regression",
        "specific": null,
        "novel_contribution": "Included as one of the selectable classifiers in AutoCM"
      },
      {
        "type": "primary",
        "category": "LSTM",
        "specific": null,
        "novel_contribution": "Included as one of the selectable classifiers in AutoCM"
      },
      {
        "type": "primary",
        "category": "SVM",
        "specific": null,
        "novel_contribution": "Included as one of the selectable classifiers in AutoCM"
      },
      {
        "type": "primary",
        "category": "Multilayer Perceptron",
        "specific": null,
        "novel_contribution": "Included as one of the selectable classifiers in AutoCM"
      },
      {
        "type": "primary",
        "category": "Random Forest",
        "specific": null,
        "novel_contribution": "Included as one of the selectable classifiers in AutoCM"
      },
      {
        "type": "baseline",
        "category": "Deep Neural Network",
        "specific": "DNN (as per Ferrag et al.)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Semi-supervised"
    ],
    "datasets": [
      {
        "name": "MANSIM-generated maritime motion/asset telemetry",
        "type": "synthetic",
        "domain": "maritime_cps_telemetry",
        "link": "http://maneuvering.mansim.org (tool; dataset generated in this paper)",
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Edge-IIoTset",
        "type": "public",
        "domain": "iiot_network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "SDSN dataset (from authors' previous study)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Comprehensive Smart Seaport Dataset (internal and external) built for this study",
        "type": "synthetic",
        "domain": "maritime_iot_and_network_traffic",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Deep Neural Network (DNN)",
        "paper_reference": "Ferrag et al. [22]",
        "metric": "Sensitivity (internal and external attacks), Detection rate for simultaneous attacks",
        "their_result": "“The results showed that our solution performed better than DNN for internal and external attacks.” and “The result shows that the detection performance of our solution is better and more robust than DNN.”",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "sensitivity (recall)",
      "precision",
      "recall",
      "false positive (FP)",
      "false negative (FN)",
      "detection rate (0–1)",
      "determination time (inference time)",
      "reliability metric γ=1−FN/(TP+FN)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can a Digital Twin-assisted honeypot (TwinPot) attract and analyze external attacks on smart seaports while protecting real assets?",
        "Can an intelligent, network-aware detection mechanism using DT data accurately detect internal attacks across varying attack types?",
        "Does combining DT-driven honeypots with adaptive classifier selection (AutoCM) improve detection under simultaneous internal and external attacks?"
      ],
      "gaps_identified": [
        "“Existing intrusion detection mechanisms are insufficient to detect external attacks.”",
        "Virtual honeypots lack sufficient fidelity to entice attackers; need higher realism.",
        "Limited prior work combining honeypot and Digital Twin technologies for intelligent seaports: “None of the above studies specifically addressed attack detection and behavioral analysis in combination with honeypot and DT technologies for the intelligent seaports domain.”",
        "IoT/IIoT honeypots for innovative environments (e.g., ports) are underexplored, including ransomware and protocol-specific threats."
      ],
      "limitations": [
        "Evaluation relies on simulated maritime data (MANSIM) and existing datasets; no real-world port deployment reported.",
        "No publicly released code or combined smart seaport dataset is indicated.",
        "Performance comparisons to baselines are qualitative in the text (figures referenced) without tabulated quantitative values."
      ],
      "future_work": [],
      "motivation": "Smart seaports face increasing cyber threats from both internal assets and external entities. DT can boost honeypot realism and support adaptive detection to address insufficient external attack detection and enable behavior analysis.",
      "potential_research_ideas": [
        "Release a standardized Smart Seaport DT-Honeypot benchmark dataset with labeled internal/external multi-modal data (network, telemetry, logs).",
        "Extend TwinPot to maritime/ICS protocols (e.g., NMEA 2000, AIS, Modbus, OPC UA) and evaluate protocol-specific attack behaviors.",
        "Federated DT-honeypot learning across multiple ports for privacy-preserving collaborative detection.",
        "Incorporate online/continual learning with concept drift detection for evolving maritime operations and attacker behavior.",
        "Design adversarially robust detection via adversarial training and attack simulation tailored to IIoT traffic.",
        "Add explainable ML for operator trust and forensics (e.g., SHAP on selected models per entity).",
        "Graph-based detection using GNNs over the DT graph topology and entity relations.",
        "Closed-loop deception optimization: use attacker behavior feedback to automatically reconfigure TwinPot lures."
      ],
      "architectural_improvement_recommendations": [
        "Replace winner-takes-all selection with stacked generalization or Bayesian model averaging to combine strengths of multiple classifiers selected by AutoCM.",
        "Use time-series deep models (Temporal CNNs, Transformers) for sequential telemetry and network flows to reduce FN while controlling νi.",
        "Adopt uncertainty-aware selection (e.g., predictive entropy) alongside γ to trigger re-training and data acquisition.",
        "Integrate active learning to label difficult samples and reduce reliance on the prior baseline dataset.",
        "Employ GNNs leveraging DT graph structure for entity-level and relation-level anomaly detection.",
        "Introduce drift detection (e.g., ADWIN, DDM) to adapt models when maritime patterns change.",
        "Instrument TwinPot with high-interaction components and sandboxing to capture richer TTPs while isolating risk."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "Microsoft Azure Digital Twins (ADT)",
        "YANG models",
        "MANSIM (data generation tool)"
      ],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Smart seaport with DT and TwinPot networks (Azure Digital Twins-based DT, TwinPot honeypot network and service layer)",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Maintaining high-fidelity synchronization between DT and TwinPot to sustain attacker engagement.",
        "Labeling and curating diverse maritime/IIoT data; reliance on labeling algorithm and baseline data.",
        "Handling simultaneous internal and external attacks with low FN while meeting operational time constraints (νi).",
        "Secure isolation of honeypot traffic from production systems."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposed TwinPot: a Digital Twin-assisted honeypot to analyze behaviors of external attackers in smart seaports and improve security mechanisms.",
      "Proposed a network-aware intelligent attack detection mechanism for internal seaport entities using DT, with AutoCM that selects the best classifier among 10 algorithms based on a weighted objective and reliability.",
      "Built an extensive smart seaport dataset for internal and external attacks by combining MANSIM-generated maritime data with Edge-IIoTset and SDSN datasets; demonstrated improved detection over a DNN baseline and robustness under simultaneous attacks."
    ]
  },
  {
    "arxiv_id": "2310.09298v3",
    "title": "ByteStack-ID: Integrated Stacked Model Leveraging Payload Byte Frequency for Grayscale Image-based Network Intrusion Detection",
    "authors": "Irfan Khan; Yasir Ali Farrukh; Syed Wali",
    "abstract": "In the ever-evolving realm of network security, the swift and accurate identification of diverse attack classes within network traffic is of paramount importance. This paper introduces \"ByteStack-ID,\" a pioneering approach tailored for packet-level intrusion detection. At its core, ByteStack-ID leverages grayscale images generated from the frequency distributions of payload data, a groundbreaking technique that greatly enhances the model's ability to discern intricate data patterns. Notably, our approach is exclusively grounded in packet-level information, a departure from conventional Network Intrusion Detection Systems (NIDS) that predominantly rely on flow-based data. While building upon the fundamental concept of stacking methodology, ByteStack-ID diverges from traditional stacking approaches. It seamlessly integrates additional meta learner layers into the concatenated base learners, creating a highly optimized, unified model. Empirical results unequivocally confirm the outstanding effectiveness of the ByteStack-ID framework, consistently outperforming baseline models and state-of-the-art approaches across pivotal performance metrics, including precision, recall, and F1-score. Impressively, our proposed approach achieves an exceptional 81\\% macro F1-score in multiclass classification tasks. In a landscape marked by the continuous evolution of network threats, ByteStack-ID emerges as a robust and versatile security solution, relying solely on packet-level information extracted from network traffic data.",
    "published_date": "2023-10-06",
    "pdf_link": "https://arxiv.org/pdf/2310.09298v3",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Packet-level network intrusion detection using payload-only features transformed into grayscale images with an integrated stacked CNN ensemble",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "CNN",
        "specific": "Deep 2D-CNN with input/output concatenation across blocks",
        "novel_contribution": "Used as base learners on 16x16 grayscale images derived from payload byte frequency; trained one-vs-all per attack class"
      },
      {
        "type": "primary",
        "category": "Ensemble (Stacking)",
        "specific": "Integrated stacking with frozen base CNNs and additional dense meta-learner layers",
        "novel_contribution": "Non-traditional stacking where meta-learner layers are integrated directly with the penultimate layers of 15 base CNNs; base CNN weights frozen during meta-training"
      },
      {
        "type": "baseline",
        "category": "Random Forest",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Logistic Regression",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Boosting",
        "specific": "AdaBoostClassifier",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "DNN/MLP",
        "specific": "3-layer DNN",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Hybrid CNN+RNN",
        "specific": "1D-CNN + LSTM (five layers)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "Base model (same CNN architecture without integrated stacking; single training stage)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "CIC-IDS2017",
        "type": "public",
        "domain": "network_traffic_payload",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Random Forest",
        "paper_reference": null,
        "metric": "Macro Precision/Recall/F1",
        "their_result": "Precision 84%, Recall 79%, F1 81% (ByteStack-ID)",
        "baseline_result": "Precision 62%, Recall 56%, F1 58%"
      },
      {
        "method_name": "Logistic Regression",
        "paper_reference": null,
        "metric": "Macro Precision/Recall/F1",
        "their_result": "Precision 84%, Recall 79%, F1 81%",
        "baseline_result": "Precision 48%, Recall 42%, F1 42%"
      },
      {
        "method_name": "AdaBoostClassifier",
        "paper_reference": null,
        "metric": "Macro Precision/Recall/F1",
        "their_result": "Precision 84%, Recall 79%, F1 81%",
        "baseline_result": "Precision 37%, Recall 54%, F1 40%"
      },
      {
        "method_name": "DNN (3-layer)",
        "paper_reference": null,
        "metric": "Macro Precision/Recall/F1",
        "their_result": "Precision 84%, Recall 79%, F1 81%",
        "baseline_result": "Precision 56%, Recall 51%, F1 52%"
      },
      {
        "method_name": "1D-CNN + LSTM (five layers)",
        "paper_reference": null,
        "metric": "Macro Precision/Recall/F1",
        "their_result": "Precision 84%, Recall 79%, F1 81%",
        "baseline_result": "Precision 61%, Recall 56%, F1 56%"
      },
      {
        "method_name": "Base Model (single-stage CNN similar to ByteStack-ID)",
        "paper_reference": null,
        "metric": "Macro Precision/Recall/F1",
        "their_result": "Precision 84%, Recall 79%, F1 81%",
        "baseline_result": "Precision 81%, Recall 75%, F1 78%"
      }
    ],
    "performance_metrics_used": [
      "Precision (macro)",
      "Recall (macro)",
      "F1-score (macro)"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Packet-level NIDS performance remains in need of further enhancement, particularly in methods to transform packet-level information into effective features.",
        "Most NIDS research focuses on flow-based analysis; packet-level approaches are underexplored and may better capture higher-level threats.",
        "Datasets with actual payloads are sparse due to privacy concerns; many available datasets are unlabeled in raw packet form."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Improve packet-level intrusion detection by leveraging payload content only, avoiding flow-based limitations, and handling high-dimensional payloads via a learned image representation and integrated stacking.",
      "potential_research_ideas": [
        "Evaluate ByteStack-ID across multiple payload-bearing datasets and realistic traffic traces to assess generalization.",
        "Extend payload imaging beyond frequency to preserve positional context (e.g., joint 2D histograms of byte value vs. position, multi-channel images by payload segments).",
        "Incorporate self-supervised pretraining on raw packet bytes or images for feature learning before supervised fine-tuning.",
        "Adapt the approach for encrypted traffic using side-channel features (sizes, timings) combined with partial payload representations when available.",
        "Add open-set/unknown attack detection and calibration to handle novel threats.",
        "Study adversarial robustness of payload-image models and develop defenses (e.g., randomized smoothing, adversarial training).",
        "Integrate explainability (saliency on byte-value bins or segments) to interpret detections.",
        "Develop real-time, streaming variants with optimized inference and batching on NIC/DPU accelerators."
      ],
      "architectural_improvement_recommendations": [
        "Replace or augment CNN backbone with modern lightweight CNNs (e.g., EfficientNet-lite, MobileNetV3) or Vision Transformers for small images with positional encodings.",
        "Introduce residual/skip connections and attention modules (SE/CBAM) within the base CNN blocks for richer features.",
        "Use multi-channel inputs: channels for normalized frequency, log-frequency, and tf-idf-like weighting across packets.",
        "Allow partial fine-tuning of base learners during meta-training (e.g., unfreeze last block) with low LR to improve end-to-end optimization.",
        "Use a learned fusion layer (attention-based concatenation) instead of plain concatenation across base learners.",
        "Calibrate outputs with temperature scaling and class-balanced loss functions (focal loss) to handle imbalance.",
        "Ensemble diverse base architectures (CNN variants, small ViTs) to increase diversity in stacking.",
        "Employ k-fold cross-validated stacking to reduce overfitting in the meta-learner."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Scarcity of labeled, payload-bearing datasets due to privacy concerns.",
        "Need to process high-dimensional packet payloads efficiently at line rate.",
        "Labeling raw packets requires mapping to flows (5-tuple), which may be non-trivial in operational settings.",
        "Class imbalance (benign vs. attacks) necessitates careful sampling/weighting."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces an integrated stacking approach that directly integrates meta-learner dense layers with frozen penultimate layers of multiple base CNNs, forming a unified model.",
      "Proposes a novel transformation of packet payloads into 16x16 grayscale images using per-packet normalized byte frequency distributions.",
      "Provides comprehensive empirical evaluation on CIC-IDS2017 packet data, outperforming baseline models and reported state-of-the-art; achieves 81% macro F1 for multiclass classification."
    ]
  },
  {
    "arxiv_id": "2310.08036v2",
    "title": "ZEST: Attention-based Zero-Shot Learning for Unseen IoT Device Classification",
    "authors": "Binghui Wu; Philipp Gysel; Dinil Mon Divakaran; Mohan Gurusamy",
    "abstract": "Recent research works have proposed machine learning models for classifying IoT devices connected to a network. However, there is still a practical challenge of not having all devices (and hence their traffic) available during the training of a model. This essentially means, during the operational phase, we need to classify new devices not seen in the training phase. To address this challenge, we propose ZEST -- a ZSL (zero-shot learning) framework based on self-attention for classifying both seen and unseen devices. ZEST consists of i) a self-attention based network feature extractor, termed SANE, for extracting latent space representations of IoT traffic, ii) a generative model that trains a decoder using latent features to generate pseudo data, and iii) a supervised model that is trained on the generated pseudo data for classifying devices. We carry out extensive experiments on real IoT traffic data; our experiments demonstrate i) ZEST achieves significant improvement (in terms of accuracy) over the baselines; ii) SANE is able to better extract meaningful representations than LSTM which has been commonly used for modeling network traffic.",
    "published_date": "2023-10-12",
    "pdf_link": "https://arxiv.org/pdf/2310.08036v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "IoT Security",
      "subdomain": "Device Fingerprinting",
      "specific_problem": "Zero-shot classification of seen and unseen IoT device types from network traffic",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "Self-attention encoder (ViT-inspired) feature extractor named SANE",
        "novel_contribution": "Design of SANE: a transformer-encoder with a learnable Sequence Level Aggregation (SLA) token and positional embeddings to extract packet- and sequence-level latent features (L and Λ) from per-packet network traffic sequences"
      },
      {
        "type": "primary",
        "category": "Generative Model",
        "specific": "Conditional Variational Autoencoder (CVAE)",
        "novel_contribution": "Generative zero-shot pipeline that conditions on learned attribute vectors (averaged Λ) to synthesize pseudo data for unseen device classes and convert ZSL into supervised learning"
      },
      {
        "type": "primary",
        "category": "MLP/Feedforward",
        "specific": "Supervised classifier head trained on generated pseudo data",
        "novel_contribution": "Final supervised classifier trained solely on synthetic samples for unseen classes plus seen-class data via ZSL workflow"
      },
      {
        "type": "baseline",
        "category": "RNN",
        "specific": "LSTM / Bi-LSTM",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Autoencoder",
        "specific": "VAE + k-means clustering",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Clustering + Tree",
        "specific": "DEFT (seeded k-means + Random Forest)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "CNN with multi-task learning (semi-supervised)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Generative",
      "Zero-shot"
    ],
    "datasets": [
      {
        "name": "Real IoT traffic data (unnamed)",
        "type": "proprietary",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Bi-LSTM/LSTM supervised sequence classifier for IoT device traffic",
        "paper_reference": "Referenced as [7] in the paper; prior work applying Bi-LSTM to IoT device classification",
        "metric": "accuracy",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "VAE + k-means (unsupervised)",
        "paper_reference": "[9]",
        "metric": "accuracy",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "DEFT (seeded k-means + Random Forest)",
        "paper_reference": "[10], [31]",
        "metric": "accuracy",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "CNN multi-task semi-supervised method",
        "paper_reference": "[14]",
        "metric": "accuracy",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "accuracy",
      "inference time"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can zero-shot learning be leveraged to classify both seen and unseen IoT devices by mapping network traffic data to an attribute space?",
        "How to define meaningful, information-dense attributes for IoT device traffic where textual descriptions do not translate?",
        "Does a self-attention based sequence model (SANE) extract more meaningful traffic representations and outperform LSTMs for IoT fingerprinting?",
        "What is an appropriate dimensionality for attribute vectors to optimize performance?"
      ],
      "gaps_identified": [
        "Supervised IoT fingerprinting struggles with unseen devices due to continual emergence of new device types.",
        "Unsupervised/semi-supervised clustering-based methods (e.g., seeded k-means) have limitations: need number of clusters, sensitivity to initialization/outliers, and difficulty with high-dimensional, non-linear data.",
        "Textual device descriptions (manuals/webpages) do not translate into useful attributes for network traffic.",
        "Absence of strong pre-trained models for network traffic analogous to vision models (e.g., ResNet) complicates feature extraction.",
        "Network traffic presents long sequences and high-dimensional packet-level features, challenging traditional ML models."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Practical IoT deployments cannot assume availability of traffic for all device types at training time; need methods that classify unseen devices while handling long, high-dimensional network sequences.",
      "potential_research_ideas": [
        "Develop contrastive or self-supervised pretraining on large unlabeled network traffic corpora to improve attribute extraction and reduce reliance on labeled seen classes.",
        "Explore alternative generative models (e.g., diffusion models or VAE-GAN hybrids) conditioned on attributes to improve synthetic data fidelity for unseen classes.",
        "Incorporate lightweight textual/metadata attributes (e.g., protocol support lists, vendor info) via multimodal fusion to enrich attribute space without manual annotation.",
        "Investigate domain generalization and cross-network transfer to handle deployment across different sites with varying traffic distributions.",
        "Add continual/online learning to update attribute vectors and classifiers as new device types and firmware updates appear.",
        "Use hierarchical classification (vendor → device family → model) to improve zero-shot generalization with structured priors.",
        "Design uncertainty estimation and open-set detection on top of ZSL outputs to flag truly novel or ambiguous devices."
      ],
      "architectural_improvement_recommendations": [
        "Augment SANE with supervised contrastive loss on Λ to improve attribute separability and class-condition quality for CVAE.",
        "Replace or complement CVAE with conditional diffusion models for robust generation in high-dimensional sequence spaces.",
        "Introduce cross-attention between SLA token and packet tokens or add a CLS-like token to strengthen sequence summarization.",
        "Adopt hierarchical prototypes in attribute space and prototype-based classification for calibrated zero-shot predictions.",
        "Apply domain-adversarial training or adaptive normalization to mitigate site/domain shifts in network traffic.",
        "Leverage efficient attention (e.g., Performer/FlashAttention) and sequence chunking to scale to longer sequences with lower latency.",
        "Incorporate calibration and confidence scoring (e.g., temperature scaling) for more reliable deployment decisions."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/Binghui99/ZEST",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Rapidly evolving device landscape leading to unseen device classes at inference time",
        "Long, high-dimensional packet sequences increasing computational complexity",
        "Defining meaningful attributes for network traffic without textual descriptions",
        "Potential domain shift across different network environments"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduce ZEST, a generative zero-shot learning framework for IoT fingerprinting based on self-attention and attribute-conditioned generation.",
      "Design SANE, a transformer-based self-attention feature extractor for network traffic; demonstrate it outperforms LSTM and improves baseline models.",
      "Propose a new method to derive attribute vectors for IoT devices from traffic using averaged latent features Λ produced by SANE; study attribute dimension selection.",
      "Open-source the implementation of ZEST, LSTM baselines, and data processing methods."
    ]
  },
  {
    "arxiv_id": "2310.00673v2",
    "title": "Learning Type Inference for Enhanced Dataflow Analysis",
    "authors": "Lukas Seidel; Sedick David Baker Effendi; Xavier Pinho; Konrad Rieck; Brink van der Merwe; Fabian Yamaguchi",
    "abstract": "Statically analyzing dynamically-typed code is a challenging endeavor, as even seemingly trivial tasks such as determining the targets of procedure calls are non-trivial without knowing the types of objects at compile time. Addressing this challenge, gradual typing is increasingly added to dynamically-typed languages, a prominent example being TypeScript that introduces static typing to JavaScript. Gradual typing improves the developer's ability to verify program behavior, contributing to robust, secure and debuggable programs. In practice, however, users only sparsely annotate types directly. At the same time, conventional type inference faces performance-related challenges as program size grows. Statistical techniques based on machine learning offer faster inference, but although recent approaches demonstrate overall improved accuracy, they still perform significantly worse on user-defined types than on the most common built-in types. Limiting their real-world usefulness even more, they rarely integrate with user-facing applications. We propose CodeTIDAL5, a Transformer-based model trained to reliably predict type annotations. For effective result retrieval and re-integration, we extract usage slices from a program's code property graph. Comparing our approach against recent neural type inference systems, our model outperforms the current state-of-the-art by 7.85% on the ManyTypes4TypeScript benchmark, achieving 71.27% accuracy overall. Furthermore, we present JoernTI, an integration of our approach into Joern, an open source static analysis tool, and demonstrate that the analysis benefits from the additional type information. As our model allows for fast inference times even on commodity CPUs, making our system available through Joern leads to high accessibility and facilitates security research.",
    "published_date": "2023-10-01",
    "pdf_link": "https://arxiv.org/pdf/2310.00673v2",
    "paper_types": [
      "empirical_analysis",
      "benchmark",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Static Application Security Testing (SAST) / Static Analysis",
      "specific_problem": "Statistical type inference for dynamically-typed JavaScript/TypeScript to enhance interprocedural dataflow/taint analysis",
      "attack_types": [
        "Cross-Site Scripting (XSS)",
        "Sensitive data leakage"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer (Encoder-Decoder)",
        "specific": "CodeT5+ (220M parameters)",
        "novel_contribution": "Generative seq2seq type prediction with open type vocabulary; integrates CPG-based usage slices for precise retrieval and re-integration into static analysis"
      },
      {
        "type": "primary",
        "category": "Program Representation / Preprocessing",
        "specific": "CPG Usage Slicing",
        "novel_contribution": "Intraprocedural usage slices extracted from Code Property Graphs to create usage vectors that enable efficient localization and re-integration of inferred types"
      },
      {
        "type": "baseline",
        "category": "BERT-style Transformer",
        "specific": "TypeBert",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Sequence Model",
        "specific": "DeepTyper",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Constraint-aware sequence model",
        "specific": "OptTyper",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Constraint-aware sequence model",
        "specific": "TypeWriter",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Graph Neural Network (GNN)",
        "specific": "LambdaNet",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Relational GNN family",
        "specific": "R-GNN",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer (Encoder-Decoder)",
        "specific": "TypeT5",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised fine-tuning",
      "Transfer Learning",
      "Self-supervised pretraining (for backbone)",
      "Sequence-to-Sequence"
    ],
    "datasets": [
      {
        "name": "ManyTypes4TypeScript",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Usage slices of 300,000 open-source projects (Joern usage slicing dataset)",
        "type": "public",
        "domain": "source_code",
        "link": "https://doi.org/10.5281/zenodo.8321614",
        "is_new_contribution": true,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Previous SOTA on ManyTypes4TypeScript (unspecified)",
        "paper_reference": null,
        "metric": "accuracy",
        "their_result": "71.27% accuracy overall on ManyTypes4TypeScript",
        "baseline_result": "Outperformed prior SOTA by 7.85% (exact prior value and method name not specified)"
      },
      {
        "method_name": "TypeBert",
        "paper_reference": "[20]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "DeepTyper",
        "paper_reference": "[13]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "OptTyper",
        "paper_reference": "[25]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "TypeWriter",
        "paper_reference": "[29]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "LambdaNet",
        "paper_reference": "[42]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "R-GNN family",
        "paper_reference": "[46]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "TypeT5",
        "paper_reference": "[40]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "accuracy"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can a generative Transformer with CPG-based usage slices improve type inference accuracy, especially for user-defined types, and thereby enhance downstream dataflow/taint analysis?",
        "How to integrate probabilistic type inference seamlessly into a production static analysis platform (Joern) for real-world usability?"
      ],
      "gaps_identified": [
        "R1: Limited performance on user-defined types in prior ML-based type inference approaches.",
        "R2: High-effort setups and specialized IO formats hinder adoption.",
        "R3: Lack of integration with existing developer and analysis platforms (e.g., IDEs, SAST), limiting real-world usefulness."
      ],
      "limitations": [
        "Usage slicing and inference are intraprocedural (they explicitly remain intraprocedurally bound).",
        "They omit 'potential usages' in usage graphs (compared to related work).",
        "Training relies on annotated TypeScript; inference applied to JavaScript via compatibility (possible domain shift)."
      ],
      "future_work": [],
      "motivation": "Improve static analysis of dynamically-typed code by providing reliable, fast, and integrable type inference that particularly helps with user-defined types, reducing barriers to adoption and enhancing dataflow/taint analysis in SAST.",
      "potential_research_ideas": [
        "Extend CodeTIDAL5 to additional languages (Python, Ruby) and multi-language projects with shared libraries.",
        "Hybrid model that fuses CPG graph structure (GNN) with Transformer text encodings via graph-aware attention for improved user-defined type accuracy.",
        "Uncertainty-aware inference with calibrated confidence to guide where to trust predictions and when to fall back to rule-based inference or request user input.",
        "Active learning in JoernTI: interactively query analysts for ambiguous variables to iteratively improve the model on specific codebases.",
        "Retrieval-augmented typing: retrieve candidate type signatures from library typings/DefinitelyTyped during decoding to constrain or bias generation.",
        "Interprocedural usage slicing with selective expansion to capture cross-procedure evidence while controlling cost.",
        "Apply the inferred types to improve other security analyses (API misuse, access control checks, deserialization sinks) and measure end-to-end vulnerability detection gains.",
        "Robustness study against code obfuscation/minification and adversarial perturbations of identifiers; design defenses (identifier normalization, adversarial training)."
      ],
      "architectural_improvement_recommendations": [
        "Incorporate a constrained decoding/copy mechanism that leverages known type vocabularies from project typings and dependency metadata while retaining open-vocabulary generation.",
        "Fuse CPG structural features via a lightweight GNN or relational attention and concatenate with Transformer embeddings (graph-text multimodal encoder).",
        "Introduce hierarchical type modeling that explicitly represents subtype/supertype relations and allows structured decoding of complex union/generic types.",
        "Add uncertainty calibration (temperature scaling, evidential deep learning) and output top-k with calibrated probabilities for safer integration into SAST.",
        "Use retrieval-augmented decoding: index library/type definition files and attend over retrieved snippets during generation.",
        "Curriculum learning from primitives to user-defined types and from intra- to limited interprocedural slices to stabilize training."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "Hugging Face Transformers",
        "PyTorch"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Model: CodeT5+ small (220M parameters). Optimizations include FlashAttention. Reported as fast inference on commodity CPUs; no specific GPU or training time reported."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Integrated into Joern (open-source static analysis/SAST tool) for JavaScript/TypeScript analysis; suitable for developer workstations/CI.",
      "scalability_discussed": true,
      "inference_time": "Described as fast on commodity CPUs (no exact numbers provided).",
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "CodeTIDAL5: A Transformer-based (CodeT5+) generative model for type inference with open type vocabulary and larger context, achieving 71.27% accuracy on ManyTypes4TypeScript and +7.85% over prior SOTA.",
      "JoernTI: Integration of the model into the Joern static analysis platform, demonstrating that additional inferred types improve dataflow analysis.",
      "Implementation of CPG-based usage slicing in Joern to enable efficient result retrieval and re-integration; released a dataset of usage slices from 300,000 open-source projects (https://doi.org/10.5281/zenodo.8321614)."
    ]
  },
  {
    "arxiv_id": "2311.04109v1",
    "title": "Do Language Models Learn Semantics of Code? A Case Study in Vulnerability Detection",
    "authors": "Benjamin Steenhoek; Md Mahbubur Rahman; Shaila Sharmin; Wei Le",
    "abstract": "Recently, pretrained language models have shown state-of-the-art performance on the vulnerability detection task. These models are pretrained on a large corpus of source code, then fine-tuned on a smaller supervised vulnerability dataset. Due to the different training objectives and the performance of the models, it is interesting to consider whether the models have learned the semantics of code relevant to vulnerability detection, namely bug semantics, and if so, how the alignment to bug semantics relates to model performance. In this paper, we analyze the models using three distinct methods: interpretability tools, attention analysis, and interaction matrix analysis. We compare the models' influential feature sets with the bug semantic features which define the causes of bugs, including buggy paths and Potentially Vulnerable Statements (PVS). We find that (1) better-performing models also aligned better with PVS, (2) the models failed to align strongly to PVS, and (3) the models failed to align at all to buggy paths. Based on our analysis, we developed two annotation methods which highlight the bug semantics inside the model's inputs. We evaluated our approach on four distinct transformer models and four vulnerability datasets and found that our annotations improved the models' performance in the majority of settings - 11 out of 16, with up to 9.57 points improvement in F1 score compared to conventional fine-tuning. We further found that with our annotations, the models aligned up to 232% better to potentially vulnerable statements. Our findings indicate that it is helpful to provide the model with information of the bug semantics, that the model can attend to it, and motivate future work in learning more complex path-based bug semantics. Our code and data are available at https://figshare.com/s/4a16a528d6874aad51a0.",
    "published_date": "2023-11-07",
    "pdf_link": "https://arxiv.org/pdf/2311.04109v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Vulnerability Detection",
      "specific_problem": "Function-level source code vulnerability classification for C/C++",
      "attack_types": [
        "Buffer overflow",
        "Double free",
        "Memory leak",
        "Use-after-free",
        "NULL pointer dereference",
        "Integer overflow/underflow",
        "Divide-by-zero"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "Pretrained code LMs fine-tuned for classification with novel bug-semantics input annotations",
        "novel_contribution": "Two input annotation methods that highlight bug semantics (e.g., Potentially Vulnerable Statements) inside the model inputs to improve alignment and F1."
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "CodeBERT",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "LineVul (CodeBERT-based)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "UniXcoder",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "CodeT5",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Attribution/Interpretability",
        "specific": "Saliency, InputXGradient, DeepLIFT, SHAP (via Captum)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Attention Analysis",
        "specific": "Self-attention head alignment analysis (Wan et al., Vig et al.)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Interaction Matrix Analysis",
        "specific": "Interaction matrix (Paltenghi et al.)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Transfer Learning",
      "Self-supervised pretraining (MLM)"
    ],
    "datasets": [
      {
        "name": "D2A",
        "type": "public",
        "domain": "source_code",
        "link": "https://developer.ibm.com/exchanges/data/all/d2a/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Devign (CodeXGLUE split)",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Big-Vul",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "ReVeal",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "CodeBERT on D2A",
        "paper_reference": null,
        "metric": "F1",
        "their_result": "66.76",
        "baseline_result": null
      },
      {
        "method_name": "UniXcoder on D2A",
        "paper_reference": null,
        "metric": "F1",
        "their_result": "57.19",
        "baseline_result": null
      },
      {
        "method_name": "CodeT5 on D2A",
        "paper_reference": null,
        "metric": "F1",
        "their_result": "57.33",
        "baseline_result": null
      },
      {
        "method_name": "LineVul on D2A",
        "paper_reference": null,
        "metric": "F1",
        "their_result": "68.22",
        "baseline_result": null
      },
      {
        "method_name": "CodeBERT on Devign",
        "paper_reference": null,
        "metric": "F1",
        "their_result": "56.90",
        "baseline_result": null
      },
      {
        "method_name": "UniXcoder on Devign",
        "paper_reference": null,
        "metric": "F1",
        "their_result": "56.81",
        "baseline_result": null
      },
      {
        "method_name": "CodeT5 on Devign",
        "paper_reference": null,
        "metric": "F1",
        "their_result": "58.79",
        "baseline_result": null
      },
      {
        "method_name": "LineVul on Devign",
        "paper_reference": null,
        "metric": "F1",
        "their_result": "54.15",
        "baseline_result": null
      },
      {
        "method_name": "CodeBERT on Big-Vul",
        "paper_reference": null,
        "metric": "F1",
        "their_result": "40.65",
        "baseline_result": null
      },
      {
        "method_name": "UniXcoder on Big-Vul",
        "paper_reference": null,
        "metric": "F1",
        "their_result": "39.55",
        "baseline_result": null
      },
      {
        "method_name": "CodeT5 on Big-Vul",
        "paper_reference": null,
        "metric": "F1",
        "their_result": "40.20",
        "baseline_result": null
      },
      {
        "method_name": "LineVul on Big-Vul",
        "paper_reference": null,
        "metric": "F1",
        "their_result": "39.46",
        "baseline_result": null
      },
      {
        "method_name": "CodeBERT on ReVeal",
        "paper_reference": null,
        "metric": "F1",
        "their_result": "42.69",
        "baseline_result": null
      },
      {
        "method_name": "UniXcoder on ReVeal",
        "paper_reference": null,
        "metric": "F1",
        "their_result": "40.53",
        "baseline_result": null
      },
      {
        "method_name": "CodeT5 on ReVeal",
        "paper_reference": null,
        "metric": "F1",
        "their_result": "40.56",
        "baseline_result": null
      },
      {
        "method_name": "LineVul on ReVeal",
        "paper_reference": null,
        "metric": "F1",
        "their_result": "42.92",
        "baseline_result": null
      },
      {
        "method_name": "Annotated inputs vs conventional fine-tuning",
        "paper_reference": null,
        "metric": "F1",
        "their_result": "\"improved in 11/16 settings, with up to 9.57 points improvement in F1 score compared to conventional fine-tuning\"",
        "baseline_result": "Conventional fine-tuning without annotations"
      }
    ],
    "performance_metrics_used": [
      "F1 score",
      "Intersection-over-Union (IoU) alignment between model-important features and bug semantic features",
      "Attention alignment metrics",
      "Interaction matrix alignment"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "To what extent do the models use bug root causes to make predictions?",
        "Do models perform better when using more causal lines?",
        "How does alignment to bug semantics relate to model performance?"
      ],
      "gaps_identified": [
        "Pretrained LMs on code lack explicit integration of program/bug semantics.",
        "Deep learning-based vulnerability detectors often focus on spurious features (e.g., variable names) unrelated to bugs.",
        "Models showed weak alignment to Potentially Vulnerable Statements (often <50%).",
        "Models failed to align at all (or substantially) to buggy paths (median alignment below 7%).",
        "Difficulty generalizing to new projects and bug types; unstable performance reported in prior work."
      ],
      "limitations": [
        "Buggy paths derived from Infer are an approximation; obtaining ground-truth buggy paths is undecidable.",
        "PVS identification is heuristic and necessary-but-not-sufficient; may include non-causal statements and miss causal ones.",
        "Study limited to four transformer models and four C/C++ datasets at function level.",
        "Context length fixed at 512 tokens for all models; longer contexts may be truncated.",
        "Whitespace normalization may affect rare preprocessor/macros cases."
      ],
      "future_work": [
        "Consider other approaches for obtaining buggy paths beyond Infer.",
        "Learn and integrate more complex path-based bug semantics into models.",
        "Explore methods to more directly encode bug semantics into model training/objectives."
      ],
      "motivation": "Understand whether state-of-the-art code LMs fine-tuned for vulnerability detection learn and use bug semantics, diagnose alignment gaps, and improve performance by exposing bug semantics to the model.",
      "potential_research_ideas": [
        "Supervise models with auxiliary tasks to predict PVS and buggy paths (multi-task learning with rationale supervision).",
        "Integrate control/data-flow graphs (CFG/PDG) with transformers via graph encoders or relational attention to capture path semantics.",
        "Pretrain with code-semantic objectives (e.g., dataflow edge prediction, memory-safety invariants, bounds checking) rather than pure MLM.",
        "Construct a benchmark and metrics for bug-semantics alignment (PVS/path alignment) across models and datasets.",
        "Use static analysis outputs (Infer, Clang-Tidy, CodeQL) as weak labels to scale supervision for semantics-aware training.",
        "Design a path reasoning module that encodes candidate execution paths and performs differentiable path scoring.",
        "Contrastive learning between vulnerable vs fixed code versions focused on causal lines to reduce spurious correlations.",
        "Span-level tagging models (e.g., token-CRF) to jointly detect vulnerability presence and highlight PVS/buggy lines."
      ],
      "architectural_improvement_recommendations": [
        "Add cross-attention between token embeddings and AST/CFG/PDG node embeddings with relative position biases for data/control dependencies.",
        "Introduce auxiliary alignment losses to maximize IoU between model attributions/attentions and PVS/buggy path spans.",
        "Incorporate graph neural networks over program graphs fused with transformer layers (GraphCodeBERT-style but extended to path semantics).",
        "Implement a rationale-aware classifier that consumes annotated inputs and predicts both label and rationale consistency.",
        "Use hierarchical encoders (function -> basic block -> statement) to better capture long-range path interactions.",
        "Adopt constrained decoding/inference or neuro-symbolic modules to enforce semantic constraints (e.g., bounds checks)."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://figshare.com/s/4a16a528d6874aad51a0",
      "frameworks": [
        "PyTorch",
        "Captum",
        "Transformers"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Generalization to unseen projects and bug types is challenging.",
        "Models may rely on spurious textual features rather than causal bug semantics.",
        "Lack of explicit program semantic signals (paths, dataflow) in standard LM inputs limits precision."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Adapted and harmonized three distinct analysis approaches (interpretation tools, attention analysis, interaction matrix) for models on code.",
      "Investigated whether models encode two types of bug semantics: PVS and buggy paths.",
      "Showed that better-performing models aligned better to bug semantics, but alignment to PVS was weak and to buggy paths negligible.",
      "Developed two bug-semantics annotation methods that improved F1 in 11/16 settings (up to +9.57) and increased alignment to PVS by up to 232%."
    ]
  },
  {
    "arxiv_id": "2310.02059v4",
    "title": "Security Weaknesses of Copilot-Generated Code in GitHub Projects: An Empirical Study",
    "authors": "Yujia Fu; Peng Liang; Amjed Tahir; Zengyang Li; Mojtaba Shahin; Jiaxin Yu; Jinfu Chen",
    "abstract": "Modern code generation tools utilizing AI models like Large Language Models (LLMs) have gained increased popularity due to their ability to produce functional code. However, their usage presents security challenges, often resulting in insecure code merging into the code base. Thus, evaluating the quality of generated code, especially its security, is crucial. While prior research explored various aspects of code generation, the focus on security has been limited, mostly examining code produced in controlled environments rather than open source development scenarios. To address this gap, we conducted an empirical study, analyzing code snippets generated by GitHub Copilot and two other AI code generation tools (i.e., CodeWhisperer and Codeium) from GitHub projects. Our analysis identified 733 snippets, revealing a high likelihood of security weaknesses, with 29.5% of Python and 24.2% of JavaScript snippets affected. These issues span 43 Common Weakness Enumeration (CWE) categories, including significant ones like CWE-330: Use of Insufficiently Random Values, CWE-94: Improper Control of Generation of Code, and CWE-79: Cross-site Scripting. Notably, eight of those CWEs are among the 2023 CWE Top-25, highlighting their severity. We further examined using Copilot Chat to fix security issues in Copilot-generated code by providing Copilot Chat with warning messages from the static analysis tools, and up to 55.5% of the security issues can be fixed. We finally provide the suggestions for mitigating security issues in generated code.",
    "published_date": "2023-10-03",
    "pdf_link": "https://arxiv.org/pdf/2310.02059v4",
    "paper_types": [
      "empirical_analysis",
      "new_dataset"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Secure Coding and Vulnerability Analysis",
      "specific_problem": "Empirical evaluation of security weaknesses in AI-generated code (from GitHub Copilot and other LLM-based code assistants) used in open-source GitHub projects; assessment with static analysis and testing AI-assisted fixes",
      "attack_types": [
        "Cross-site Scripting (XSS)",
        "Code Injection",
        "Insufficiently Random Values (Predictable RNG)"
      ]
    },
    "ml_techniques": [
      {
        "type": "baseline",
        "category": "LLM / Transformer",
        "specific": "GitHub Copilot (Codex-based)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "LLM / Transformer",
        "specific": "Amazon CodeWhisperer",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "LLM / Transformer",
        "specific": "Codeium",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "LLM / Transformer",
        "specific": "Copilot Chat (GPT-4 based)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [],
    "datasets": [
      {
        "name": "Curated dataset of Copilot/AI-generated code snippets used in GitHub projects",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "LLMSecEval",
        "type": "public",
        "domain": "source_code_prompts_for_security_evaluation",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "SecurityEval",
        "type": "public",
        "domain": "source_code (Python)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Dataset by Natella et al. for evaluating security of LLMs (Copilot, CodeWhisperer, CodeBERT)",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Copilot Chat with enhanced prompt (static-analysis warning message) vs. Copilot Chat 'fix' command",
        "paper_reference": null,
        "metric": "Percent of security issues fixed",
        "their_result": "55.5% (enhanced prompt with warning message)",
        "baseline_result": "19.3% (fix command)"
      }
    ],
    "performance_metrics_used": [
      "Percentage of insecure snippets",
      "CWE coverage/count and distribution",
      "Overlap with 2023 CWE Top-25",
      "Percent of security issues fixed by AI assistant"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "What is the prevalence of security weaknesses in AI-generated code snippets found in open-source GitHub projects?",
        "Which CWE categories are associated with these weaknesses and how severe are they (e.g., overlap with 2023 CWE Top-25)?",
        "Can Copilot Chat fix identified security weaknesses and how does providing static-analysis warning messages affect fix rates?"
      ],
      "gaps_identified": [
        "Prior work largely evaluated security in controlled settings or on synthetic prompts rather than in real open-source development scenarios.",
        "Limited understanding of the diversity of CWEs arising in practical usage of AI code generation within GitHub projects.",
        "Lack of comprehensive coverage across CWEs in earlier studies (often limited to specific targeted CWEs)."
      ],
      "limitations": [
        "It is not possible to access all AI-generated code in GitHub projects due to lack of signatures indicating code origin.",
        "Reliance on static analysis tools requires manual filtering to remove false positives; remaining inaccuracies are possible.",
        "Success of Copilot Chat fixes varies across CWE types.",
        "Generalizability may be constrained by languages and tools analyzed."
      ],
      "future_work": [],
      "motivation": "Evaluate and understand security weaknesses introduced by AI code generation tools (especially GitHub Copilot) when used in real-world open-source development, and explore mitigation via AI-assisted fixes.",
      "potential_research_ideas": [
        "Automatic detection of LLM-generated code segments in repositories to enable broader, unbiased corpus construction for security auditing.",
        "Hybrid static-dynamic or fuzzing-augmented pipelines to validate and prioritize true vulnerable instances suggested by SAST tools.",
        "A standardized, extensible benchmark for security evaluation of code LLMs grounded in real OSS artifacts, with CWE labeling and severity metadata.",
        "Prompt-engineering and tool-augmentation strategies (e.g., auto-feeding SAST findings) to systematically maximize AI fix success across CWE families.",
        "Learning security-aware preference models or guardrails (e.g., RLHF with security feedback) to bias code assistants toward secure patterns.",
        "Longitudinal studies on how AI-generated insecure code propagates in OSS (e.g., commits, forks) and its remediation lifecycle.",
        "IDE-integrated co-pilots that chain CodeQL/Bandit/ESLint scans, suggest secure alternatives, and explain CWE-specific risks to developers."
      ],
      "architectural_improvement_recommendations": [
        "Integrate SAST feedback loops directly into code assistant inference (e.g., chain-of-tools: generation -> SAST -> critique -> regeneration).",
        "Maintain CWE-specific secure coding templates/pattern libraries that steer the model via retrieval-augmented generation.",
        "Use program analysis–aware prompts that include type/context summaries and taint-flow hints to reduce injection/XSS issues.",
        "Implement risk-aware acceptance gating in IDEs (block/flag suggestions mapped to Top-25 CWEs unless overridden with justification).",
        "Adopt multi-scan consensus (CodeQL + language-specific tools) with cross-tool agreement to reduce false positives/negatives before fixes."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": null,
      "frameworks": [
        "CodeQL",
        "Bandit",
        "Cppcheck"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Open-source GitHub repositories; developer IDE environments using Copilot",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Identifying whether code in repositories was generated by AI assistants (lack of provenance signatures).",
        "Managing static analysis false positives/negatives across languages and tools.",
        "Ensuring developer oversight when applying AI-suggested fixes; varying success across CWE categories."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Curated and publicly released a dataset of code snippets generated by Copilot (and other tools) that are used in GitHub projects, with categorization of application areas.",
      "Performed extensive static security analysis across all possible CWE categories on collected snippets to characterize common weaknesses (43 CWE categories; 8 in 2023 CWE Top-25).",
      "Evaluated Copilot Chat’s ability to fix security issues, showing up to 55.5% fix rate when provided with SAST warning messages (vs. 19.3% with fix command alone).",
      "Reported empirical prevalence of insecure AI-generated code in-the-wild (e.g., 29.5% Python, 24.2% JavaScript snippets affected)."
    ]
  },
  {
    "arxiv_id": "2310.03831v1",
    "title": "SIFT -- File Fragment Classification Without Metadata",
    "authors": "Shahid Alam",
    "abstract": "A vital issue of file carving in digital forensics is type classification of file fragments when the filesystem metadata is missing. Over the past decades, there have been several efforts for developing methods to classify file fragments. In this research, a novel sifting approach, named SIFT (Sifting File Types), is proposed. SIFT outperforms the other state-of-the-art techniques by at least 8%. (1) One of the significant differences between SIFT and others is that SIFT uses a single byte as a separate feature, i.e., a total of 256 (0x00 - 0xFF) features. We also call this a lossless feature (information) extraction, i.e., there is no loss of information. (2) The other significant difference is the technique used to estimate inter-Classes and intra-Classes information gain of a feature. Unlike others, SIFT adapts TF-IDF for this purpose, and computes and assigns weight to each byte (feature) in a fragment (sample). With these significant differences and approaches, SIFT produces promising (better) results compared to other works.",
    "published_date": "2023-10-05",
    "pdf_link": "https://arxiv.org/pdf/2310.03831v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Digital Forensics",
      "subdomain": "File Carving",
      "specific_problem": "File fragment type classification without filesystem metadata",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Feature engineering / TF-IDF",
        "specific": "Adapted TF-IDF weighting on byte (0x00–0xFF) features",
        "novel_contribution": "Adapts TF-IDF to estimate inter-class and intra-class information gain per byte and assigns weights to each byte in a fragment"
      },
      {
        "type": "primary",
        "category": "Ensemble / Random Forest",
        "specific": "Random Forest classifier",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Public digital forensics dataset [3] (exact name not stated)",
        "type": "public",
        "domain": "file_fragments",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Byte2Vec (Haque et al.) + k-NN classifier",
        "paper_reference": "Haque et al. [5]",
        "metric": "TPR",
        "their_result": "0.80 (Weighted Avg. TPR)",
        "baseline_result": "0.72"
      },
      {
        "method_name": "Hierarchical ML with SVM (Bhatt et al.)",
        "paper_reference": "Bhatt et al. [6]",
        "metric": "TPR",
        "their_result": "0.80 (Weighted Avg. TPR)",
        "baseline_result": "0.67"
      },
      {
        "method_name": "Sparse coding on byte n-grams + Linear SVM (Wang et al.)",
        "paper_reference": "Wang et al. [8]",
        "metric": "TPR",
        "their_result": "0.80 (Weighted Avg. TPR)",
        "baseline_result": "0.61"
      }
    ],
    "performance_metrics_used": [
      "True Positive Rate (TPR)",
      "False Positive Rate (FPR)",
      "Precision",
      "F-Measure",
      "Confusion Matrix"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Many prior works focus on only specific/graphic file types (e.g., JPG, GIF, PNG).",
        "Methods perform poorly on high-entropy/compressed fragments because they lack discernible patterns to exploit.",
        "Feature extraction in prior AI/statistical approaches often loses information (e.g., using aggregated statistics rather than per-byte features).",
        "Fragment size choice varies in literature (512 vs 4096 bytes) with trade-offs for devices and sector sizes."
      ],
      "limitations": [
        "Poor performance for certain classes: “The results are poor for the Classes PDF, HTML, PPT, and SWF, because of the images embedded in these files.”",
        "Many fragments misclassified as GZ (compressed) due to similar byte-frequency weights: “Most of the fragments from PDF, PPT, SWF, and TEXT Classes are wrongly classified as GZ.”",
        "Evaluation limited to 20 file types and 512-byte fragments.",
        "Only Random Forest classifier explored; hyperparameters and alternative classifiers not reported."
      ],
      "future_work": [],
      "motivation": "Enable accurate classification of file fragments without filesystem metadata to support file carving in digital forensics; also potentially useful for early identification of malicious code in network payloads.",
      "potential_research_ideas": [
        "Design a compression-awareness pre-classifier to detect and handle compressed/high-entropy fragments prior to type classification.",
        "Incorporate multi-fragment/contextual modeling (e.g., sequence models over adjacent fragments) to disambiguate classes like PDF/PPT/SWF vs GZ.",
        "Augment byte-level TF-IDF with byte n-grams or learned embeddings (e.g., Byte2Vec/transformers) to capture local structure without losing information.",
        "Develop a hierarchical classifier: first detect broad families (compressed, image, document), then fine-grained types.",
        "Explore variable fragment sizes and multi-scale features (512 and 4096 bytes) to assess robustness across storage sector sizes and IoT constraints.",
        "Integrate decompression-signature features (e.g., DEFLATE markers) or train a model to estimate compressibility/entropy as auxiliary tasks.",
        "Apply calibration and cost-sensitive learning to mitigate systematic confusion with GZ.",
        "Evaluate few-shot/continual learning to add new file types without retraining from scratch."
      ],
      "architectural_improvement_recommendations": [
        "Combine per-byte TF-IDF with byte n-gram TF-IDF and character-level CNN/Transformer encoders; fuse features before the classifier.",
        "Replace or complement Random Forest with calibrated linear models or gradient-boosted trees; tune hyperparameters and use class weighting.",
        "Add an auxiliary head to predict entropy/compressibility, using multi-task learning to reduce GZ confusions.",
        "Implement a hierarchical pipeline: compression detector -> family classifier -> fine-grained type classifier.",
        "Use stratified group k-fold by file (not just fragments) to avoid leakage and better estimate generalization.",
        "Perform feature selection/regularization to drop non-informative bytes for specific classes and improve precision for hard classes."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Experiments ran on Windows 8.1, Intel Core i7 @ 2.00 GHz with 8 GB RAM; 10-fold cross-validation on 47,482 fragments of 512 bytes."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Systematic confusion with compressed (GZ) fragments due to similar byte-weight patterns.",
        "High-entropy and embedded-image content in document formats (PDF, PPT, SWF, HTML) degrades classification.",
        "Choice of fragment size affects applicability across devices (e.g., IoT vs HDD sector sizes)."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces SIFT, an AI-based file fragment classifier using lossless per-byte features (0x00–0xFF) and adapted TF-IDF weighting.",
      "Adapts TF-IDF to compute inter- and intra-class information gain per byte and uses these weights for classification.",
      "Achieves “Weighted Avg. [TPR] 0.80 … Precision 0.84 … F-Measure 0.79” on 20 file types and 47,482 fragments (512 bytes).",
      "Outperforms prior state-of-the-art by 8%–19% TPR: “SIFT outperforms the other state-of-the-art techniques by at least 8%.”",
      "Provides detailed per-class results and confusion matrix; identifies challenging classes (PDF, HTML, PPT, SWF) and sources of confusion (compression)."
    ]
  },
  {
    "arxiv_id": "2312.07110v1",
    "title": "LLMs Perform Poorly at Concept Extraction in Cyber-security Research Literature",
    "authors": "Maxime Würsch; Andrei Kucharavy; Dimitri Percia David; Alain Mermoud",
    "abstract": "The cybersecurity landscape evolves rapidly and poses threats to organizations. To enhance resilience, one needs to track the latest developments and trends in the domain. It has been demonstrated that standard bibliometrics approaches show their limits in such a fast-evolving domain. For this purpose, we use large language models (LLMs) to extract relevant knowledge entities from cybersecurity-related texts. We use a subset of arXiv preprints on cybersecurity as our data and compare different LLMs in terms of entity recognition (ER) and relevance. The results suggest that LLMs do not produce good knowledge entities that reflect the cybersecurity context, but our results show some potential for noun extractors. For this reason, we developed a noun extractor boosted with some statistical analysis to extract specific and relevant compound nouns from the domain. Later, we tested our model to identify trends in the LLM domain. We observe some limitations, but it offers promising results to monitor the evolution of emergent trends.",
    "published_date": "2023-12-12",
    "pdf_link": "https://arxiv.org/pdf/2312.07110v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Security Operations & Analytics",
      "subdomain": "Technology Foresight / Threat Intelligence",
      "specific_problem": "Concept/entity extraction from cybersecurity research literature to monitor and forecast emerging technology trends",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Rule-based NLP",
        "specific": "spaCy compound noun chunking",
        "novel_contribution": "Custom pipeline that extracts compound nouns and filters them via statistical significance (volcano plot) to retain cybersecurity-specific terms"
      },
      {
        "type": "primary",
        "category": "Statistical Testing",
        "specific": "t-test + fold change (volcano plot)",
        "novel_contribution": "Compares term frequencies in arXiv vs BookCorpus to select domain-specific terms with small p-value and significant fold-change"
      },
      {
        "type": "primary",
        "category": "Embedding Model",
        "specific": "spaCy embeddings",
        "novel_contribution": "Used to embed extracted entities and analyze similarity/UMAP projections for trend exploration"
      },
      {
        "type": "primary",
        "category": "Dimensionality Reduction",
        "specific": "UMAP (and mentions of t-SNE)",
        "novel_contribution": "2D projection of entity embeddings to explore structure of extracted concepts"
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "BERT, RoBERTa, DistilBERT, ELECTRA, XLM-RoBERTa",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Keyphrase Extraction",
        "specific": "KeyBERT, KBIR (kpcrowd, inspec), YAKE (non-LLM)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "NER / Token Classification",
        "specific": "Pretrained NER models on CoNLL-2003, OntoNotes 5; Token Classification on COCA-docu",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Unsupervised",
      "Transfer Learning (pretrained LMs used zero-shot)"
    ],
    "datasets": [
      {
        "name": "arXiv CS listings (cs.CR, cs.NI; plus cs.CC, cs.LO, cs.DS, cs.IT, cs.CL, cs.AI) up to Sep 2021",
        "type": "public",
        "domain": "research_publications",
        "link": "https://arxiv.org",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "BookCorpus",
        "type": "public",
        "domain": "general_english_text",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Google Trends",
        "type": "public",
        "domain": "public_attention",
        "link": "https://trends.google.com",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "OpenAlex",
        "type": "public",
        "domain": "bibliometrics",
        "link": "https://openalex.org",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "OntoNotes 5 (used by referenced NER models)",
        "type": "proprietary",
        "domain": "scientific_ner_benchmark",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "CoNLL-2003 (used by referenced NER models)",
        "type": "public",
        "domain": "ner_benchmark",
        "link": null,
        "is_new_contribution": false,
        "availability": "available_on_request"
      },
      {
        "name": "KBIR kpcrowd (keyphrase extraction dataset)",
        "type": "public",
        "domain": "keyphrase_benchmark",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Inspec (keyphrase extraction dataset)",
        "type": "public",
        "domain": "keyphrase_benchmark",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "COCA (Corpus of Contemporary American English) / docusco",
        "type": "proprietary",
        "domain": "linguistic_corpus",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "CVE (Common Vulnerabilities and Exposures) database (mentioned in related work context)",
        "type": "public",
        "domain": "vulnerability_database",
        "link": "https://cve.mitre.org",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "spaCy Large (Noun Extractor)",
        "paper_reference": "[23]",
        "metric": "Entities per document (mean ± std)",
        "their_result": "99.3 ± 6.93",
        "baseline_result": null
      },
      {
        "method_name": "spaCy Transformer (Noun Extractor)",
        "paper_reference": "[23]",
        "metric": "Entities per document (mean ± std)",
        "their_result": "99.3 ± 6.97",
        "baseline_result": null
      },
      {
        "method_name": "YAKE (Keyphrase Extractor)",
        "paper_reference": "[8]",
        "metric": "Entities per document (mean ± std)",
        "their_result": "19.9 ± 1.97",
        "baseline_result": null
      },
      {
        "method_name": "KeyBERT",
        "paper_reference": "[21]",
        "metric": "Entities per document (mean ± std)",
        "their_result": "99.3 ± 7.25",
        "baseline_result": null
      },
      {
        "method_name": "KBIR (kpcrowd)",
        "paper_reference": "[33, 36]",
        "metric": "Entities per document (mean ± std)",
        "their_result": "96.9 ± 14.6",
        "baseline_result": null
      },
      {
        "method_name": "KBIR (inspec)",
        "paper_reference": "[33, 51]",
        "metric": "Entities per document (mean ± std)",
        "their_result": "76.4 ± 27.7",
        "baseline_result": null
      },
      {
        "method_name": "BERT-base-uncased (NER)",
        "paper_reference": "[15]",
        "metric": "Entities per document (mean ± std)",
        "their_result": "44.7 ± 24.0",
        "baseline_result": null
      },
      {
        "method_name": "BERT-base-uncased (NER + Concept Recognition)",
        "paper_reference": "[15]",
        "metric": "Entities per document (mean ± std)",
        "their_result": "43.3 ± 23.3",
        "baseline_result": null
      },
      {
        "method_name": "XLM-RoBERTa-base (OntoNotes 5; NER + Number Recognition)",
        "paper_reference": "[56, 24]",
        "metric": "Entities per document (mean ± std)",
        "their_result": "36.4 ± 23.4",
        "baseline_result": null
      },
      {
        "method_name": "ELECTRA-base (CoNLL-2003 NER)",
        "paper_reference": "[12, 52]",
        "metric": "Entities per document (mean ± std)",
        "their_result": "39.9 ± 25.0",
        "baseline_result": null
      },
      {
        "method_name": "BERT-large-cased (CoNLL-2003 NER)",
        "paper_reference": "[15, 52]",
        "metric": "Entities per document (mean ± std)",
        "their_result": "41.7 ± 24.9",
        "baseline_result": null
      },
      {
        "method_name": "BERT-large-uncased (CoNLL-2003 NER)",
        "paper_reference": "[15, 52]",
        "metric": "Entities per document (mean ± std)",
        "their_result": "33.5 ± 23.3",
        "baseline_result": null
      },
      {
        "method_name": "DistilBERT-base-uncased (CoNLL-2003 NER)",
        "paper_reference": "[53, 52]",
        "metric": "Entities per document (mean ± std)",
        "their_result": "37.7 ± 24.8",
        "baseline_result": null
      },
      {
        "method_name": "RoBERTa-large (CoNLL-2003 NER)",
        "paper_reference": "[35, 52]",
        "metric": "Entities per document (mean ± std)",
        "their_result": "28.7 ± 21.1",
        "baseline_result": null
      },
      {
        "method_name": "XLM-RoBERTa-large (CoNLL-2003 NER)",
        "paper_reference": "[20, 52]",
        "metric": "Entities per document (mean ± std)",
        "their_result": "26.0 ± 19.5",
        "baseline_result": null
      },
      {
        "method_name": "BERT COCA-docusco (Token Classification)",
        "paper_reference": "[15, 26]",
        "metric": "Entities per document (mean ± std)",
        "their_result": "99.6 ± 6.11",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "entities per document (mean ± std)",
      "p-value (t-test) for frequency difference vs BookCorpus",
      "fold change of term frequency vs BookCorpus",
      "cosine similarity between spaCy embeddings of extracted entities"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Do off-the-shelf LLM-based entity extractors produce relevant knowledge entities that reflect cybersecurity technological context in scientific literature?",
        "How similar are commonly used LLM-based entity extractors due to base model and fine-tuning dataset reuse?",
        "Are embeddings of extracted entities useful for downstream document understanding tasks, e.g., classifying arXiv documents as relevant to cybersecurity?",
        "Can noun/compound noun extraction with statistical filtering yield more specific and relevant domain terms for trend monitoring?"
      ],
      "gaps_identified": [
        "LLM-based entity extractors trained on generic datasets are ill-suited for scientific bibliometrics in cybersecurity and computer science.",
        "Strong re-use of base models and fine-tuning datasets leads to very similar behavior across extractors, limiting diversity.",
        "Downstream processing of extracted terms is highly sensitive to the choice of embeddings.",
        "Standard proxies like Google Trends and OpenAlex suffer from temporal lag, contamination, ontology granularity issues, and seasonal effects.",
        "Benchmark datasets and document structure (multi-topic, length) complicate fair evaluation of entity extractors."
      ],
      "limitations": [
        "The proposed noun-extractor + statistical filter still shows flaws; results, while promising, are not perfect for trend forecasting.",
        "Reliance on PDF-to-text extraction introduces noise; tables/formulas and line breaks harm extractor accuracy.",
        "A frequency threshold (≥3 occurrences) may filter out genuinely emerging but rare terms.",
        "Comparisons and downstream analyses remain sensitive to the embedding model used.",
        "Evaluation focuses on smaller LLMs (110M–350M), which may not fully represent capabilities of the latest frontier models."
      ],
      "future_work": [
        "“Due to the delay for a new technology to emerge and the time for it to get a consistent name, it should not depend on a single ontology.”",
        "“Considerate the delay for data recording to prevent biases against recent results.”",
        "“Take into account for the semantic drift, such as ‘Transformer-Like’ and ‘Self-Attention’ are correctly designate ‘Neural Language Models’.”",
        "“Topics need to be accurately separated. For example, Transformer Neural Language Models need to be isolated from current Transformers or other types of Transformers from unrelated fields.”"
      ],
      "motivation": "Conventional bibliometrics and attention proxies lag and are noisy for rapidly evolving cybersecurity technologies; explore whether LLM-based entity extraction can better track emerging trends.",
      "potential_research_ideas": [
        "Develop a cybersecurity-scientific NER/concept extraction model using weak supervision and distant supervision (e.g., leveraging arXiv categories, citation contexts, and curated glossaries) to reduce annotation cost.",
        "Contrastive learning of concept embeddings using document–keyword pairs from scientific corpora (e.g., SPECTER-style) to improve downstream similarity and clustering.",
        "Dynamic concept normalization with alias/abbreviation resolution to handle semantic drift and evolving terminology (e.g., ‘self-attention’, ‘transformer-like’).",
        "Hybrid pipeline that fuses noun-compound extraction with keyphrase extraction and domain-specific embeddings (SciBERT/SciSpaCy/SPECTER2) for improved specificity.",
        "Temporal modeling of term trajectories (state-space models, change point detection) atop statistically filtered terms to detect emergent trends earlier.",
        "Human-in-the-loop evaluation framework for domain relevance and novelty of extracted concepts to create a lightweight, continuously updated gold set."
      ],
      "architectural_improvement_recommendations": [
        "Replace generic embeddings with domain/science-specific encoders (SciBERT, SPECTER2, SciSpaCy) for entity embedding and similarity.",
        "Augment noun-compound extraction with dependency parsing + collocation statistics (e.g., PMI) to capture multi-word technical terms robustly.",
        "Add alias/abbreviation detection and canonicalization using heuristic rules plus retrieval-augmented LLM prompts.",
        "Use robust PDF parsing (Grobid/ScienceParse) to reduce noise from formulas/tables and to segment sections for targeted extraction.",
        "Adopt a multi-extractor ensemble with agreement weighting and diversity encouragement to mitigate extractor similarity.",
        "Incorporate temporal smoothing and debiasing to counteract recording delays and seasonality in external signals (OpenAlex/Trends)."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://anonymous.4open.science/r/joi-28CE/",
      "frameworks": [
        "spaCy",
        "HuggingFace Transformers",
        "KeyBERT",
        "YAKE",
        "PyMuPDF (fitz)",
        "UMAP (umap-learn)"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Pretrained small LMs (110M–350M) used for extraction; documents segmented to fit attention windows; standard workstation sufficient."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Domain/temporal drift of terminology and semantic contamination across fields",
        "Need for continuous updates or adaptation without expensive fine-tuning",
        "Sensitivity to choice of embeddings for downstream analysis",
        "PDF-to-text extraction noise affecting entity quality"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Empirical analysis showing that off-the-shelf LLM-based entity extractors do not yield relevant cybersecurity knowledge entities for scientific bibliometrics.",
      "Development of a noun/compound-noun extractor augmented with statistical filtering (t-test + fold change vs BookCorpus) to select domain-specific terms.",
      "Demonstration of trend identification in the LLM domain using the proposed pipeline, acknowledging limitations but showing promise.",
      "Comparison across multiple extractor classes (noun, keyphrase, NER, token classification) and highlighting similarity due to shared base models/fine-tuning datasets.",
      "Observation that downstream automated processing is highly sensitive to the embedding choice, questioning common bibliometrics pipelines."
    ]
  },
  {
    "arxiv_id": "2310.10666v1",
    "title": "Extracting Physical Causality from Measurements to Detect and Localize False Data Injection Attacks",
    "authors": "Shengyang Wu; Jingyu Wang; Dongyuan Shi",
    "abstract": "False Data Injection Attack (FDIA) has become a growing concern in modern cyber-physical power systems. Most existing FDIA detection techniques project the raw measurement data into a high-dimensional latent space to separate normal and attacked samples. These approaches focus more on the statistical correlations of data values and are therefore susceptible to data distribution drifts induced by changes in system operating points or changes in FDIA types and strengths, especially for FDIA localization tasks. Causal inference, on the other hand, extracts the causality behind the coordinated fluctuations of different measurements. The causality patterns are determined by fundamental physical laws such as Ohm's Law and Kirchhoff's Law. They are sensitive to the violation of physical laws caused by FDIA, but tend to remain stable with the drift of system operating points. Leveraging this advantage, this paper proposes a joint FDIA detection and localization framework based on causal inference and the Graph Attention Network (GAT) to identify the attacked system nodes. The proposed framework consists of two levels. The lower level uses the X-learner algorithm to estimate the causality strength between measurements and generate Measurement Causality Graphs (MCGs). The upper level then applies a GAT to identify the anomaly patterns in the MCGs. Since the extracted causality patterns are intrinsically related to the measurements, it is easier for the upper level to figure out the attacked nodes than the existing FDIA localization approaches. The performance of the proposed framework is evaluated on the IEEE 39-bus system. Experimental results show that the causality-based FDIA detection and localization mechanism is highly interpretable and robust.",
    "published_date": "2023-09-21",
    "pdf_link": "https://arxiv.org/pdf/2310.10666v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Smart Grid Security",
      "subdomain": "Data Integrity Attack Detection and Localization",
      "specific_problem": "Joint detection and localization of False Data Injection Attacks (FDIAs) in power system state estimation",
      "attack_types": [
        "False Data Injection Attack (FDIA)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Causal Inference",
        "specific": "X-learner (CATE estimation)",
        "novel_contribution": "Defines a causal model over measurement trends to estimate pairwise causality strengths between physically neighboring measurements and constructs Measurement Causality Graphs (MCGs)."
      },
      {
        "type": "primary",
        "category": "Graph Neural Network",
        "specific": "Graph Attention Network (GAT)",
        "novel_contribution": "Applies GAT to MCGs to detect abnormal causal patterns and output measurement-wise attack probabilities, then aggregates to node-wise localization."
      },
      {
        "type": "primary",
        "category": "Linear Models",
        "specific": "Linear regression",
        "novel_contribution": "Used as base learners within X-learner for counterfactual outcome and CATE estimation to reduce computational cost."
      },
      {
        "type": "primary",
        "category": "Linear Models",
        "specific": "Logistic regression with elastic net regularization",
        "novel_contribution": "Used to estimate propensity scores within X-learner, enabling variable selection and mitigating overfitting."
      },
      {
        "type": "primary",
        "category": "Feedforward Neural Network",
        "specific": "Fully-connected network (classifier)",
        "novel_contribution": "Maps measurement-wise attack probabilities from the GAT to node-wise (bus-level) multi-label outputs."
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Causal inference"
    ],
    "datasets": [
      {
        "name": "IEEE 39-bus test system (simulated measurements)",
        "type": "synthetic",
        "domain": "power_system_measurements",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Correlation-based FDIA methods rely on i.i.d. assumptions and are brittle to data distribution drifts from operating point changes and varying FDIA types/strengths.",
        "Embedding-based methods lack physical meaning, hindering interpretability and precise localization back to measurements/nodes.",
        "Model-based localization requires accurate system models and faces scalability challenges."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Address the generalization and interpretability limitations of correlation-based FDIA detection/localization by leveraging physically grounded causal patterns that are stable across operating point drifts.",
      "potential_research_ideas": [
        "Extend the causal-graph framework to real SCADA/PMU datasets and heterogeneous sensor mixes to assess domain shift robustness in the wild.",
        "Incorporate time-lagged causality (e.g., Granger-causal or temporal CATE) to capture dynamic dependencies beyond trend-based up/down definitions.",
        "Develop end-to-end differentiable pipelines that jointly learn causal edge weights and GNN parameters with physics-informed regularizers.",
        "Explore domain adaptation and continual learning on MCGs to handle evolving grid conditions and attacker strategies.",
        "Integrate uncertainty quantification for CATE and GAT outputs to enable risk-aware decision-making and alarm calibration.",
        "Evaluate resilience against adaptive attackers that target causality estimation (e.g., poisoning of trend windows) and design robust estimators.",
        "Scale to large grids via hierarchical or subgraph-level causal graph construction and pooling mechanisms."
      ],
      "architectural_improvement_recommendations": [
        "Replace static GAT with temporal graph attention (e.g., TGAT/T-GAT) operating on sequences of MCGs to model evolving causal patterns.",
        "Use nonlinear base learners for X-learner (e.g., gradient boosting, random forests, or shallow neural nets) to capture complex counterfactual relationships while calibrating for overfitting.",
        "Jointly learn edge confidence with Bayesian or ensemble CATE estimators and propagate uncertainty through the GAT.",
        "Introduce graph transformers with global attention for long-range dependencies while constraining with topology-aware masks.",
        "Apply physics-informed loss terms that penalize violations of Ohm’s and Kirchhoff’s laws within learned representations.",
        "Incorporate multi-task learning to simultaneously predict measurement- and node-level anomalies with shared encoders and task-specific heads."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "A systematic approach to detect and localize FDIAs based on physical causality between power system measurements.",
      "A causal inference model using the X-learner meta-algorithm to quantify causality strengths between physically neighboring measurements and embed them into Measurement Causality Graphs (MCGs).",
      "A Graph Attention Network to identify abnormal MCG patterns and output measurement-wise attack probabilities; a fully connected network converts these to node-wise (bus-level) multi-label predictions.",
      "Demonstration on the IEEE 39-bus system showing interpretable and robust performance of the causality-based detection and localization mechanism."
    ]
  },
  {
    "arxiv_id": "2311.00304v1",
    "title": "Stacking an autoencoder for feature selection of zero-day threats",
    "authors": "Mahmut Tokmak; Mike Nkongolo",
    "abstract": "Zero-day attack detection plays a critical role in mitigating risks, protecting assets, and staying ahead in the evolving threat landscape. This study explores the application of stacked autoencoder (SAE), a type of artificial neural network, for feature selection and zero-day threat classification using a Long Short-Term Memory (LSTM) scheme. The process involves preprocessing the UGRansome dataset and training an unsupervised SAE for feature extraction. Finetuning with supervised learning is then performed to enhance the discriminative capabilities of this model. The learned weights and activations of the autoencoder are analyzed to identify the most important features for discriminating between zero-day threats and normal system behavior. These selected features form a reduced feature set that enables accurate classification. The results indicate that the SAE-LSTM performs well across all three attack categories by showcasing high precision, recall, and F1 score values, emphasizing the model's strong predictive capabilities in identifying various types of zero-day attacks. Additionally, the balanced average scores of the SAE-LSTM suggest that the model generalizes effectively and consistently across different attack categories.",
    "published_date": "2023-11-01",
    "pdf_link": "https://arxiv.org/pdf/2311.00304v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Zero-day threat detection in network traffic using SAE-based feature selection followed by LSTM classification",
      "attack_types": [
        "Zero-day attacks (unknown/novel anomalies)",
        "Ransomware (Locky, CryptoLocker, SamSam, WannaCry, Razy, JigSAW, Globe, TowerWeb)",
        "Advanced Persistent Threats (APT)",
        "Botnet (Neris)",
        "Denial of Service (DoS)",
        "UDP scan",
        "SSH-related attacks",
        "Port scanning"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Autoencoder",
        "specific": "Stacked Autoencoder (SAE)",
        "novel_contribution": "Unsupervised SAE for feature extraction with supervised fine-tuning; analysis of learned weights/activations to select the most important features for zero-day discrimination"
      },
      {
        "type": "primary",
        "category": "RNN/LSTM",
        "specific": "LSTM",
        "novel_contribution": "Temporal classifier operating on SAE-selected features; reported configuration includes 168 hidden units and 400 training epochs for multi-class (A, S, SS) detection"
      }
    ],
    "learning_paradigm": [
      "Unsupervised",
      "Supervised"
    ],
    "datasets": [
      {
        "name": "UGRansome",
        "type": "",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      }
    ],
    "baselines": [
      {
        "method_name": "Fuzzification-based IDS on UGRansome",
        "paper_reference": "Nkongolo & Tokmak, 2023",
        "metric": "Accuracy",
        "their_result": "0.9849",
        "baseline_result": "0.99"
      },
      {
        "method_name": "Deep learning on UGRansome",
        "paper_reference": "Tokmak, 2022",
        "metric": "Accuracy",
        "their_result": "0.9849",
        "baseline_result": "0.97"
      },
      {
        "method_name": "Signal autoencoder on signal data",
        "paper_reference": "Kim et al., 2020",
        "metric": "Accuracy",
        "their_result": "0.9849",
        "baseline_result": "0.97"
      },
      {
        "method_name": "LSTM on malware dataset",
        "paper_reference": "Zhang, 2020",
        "metric": "Accuracy",
        "their_result": "0.9849",
        "baseline_result": "0.81"
      },
      {
        "method_name": "SAE with data weighting across datasets",
        "paper_reference": "Sun et al., 2021",
        "metric": "Accuracy",
        "their_result": "0.9849",
        "baseline_result": "0.90"
      },
      {
        "method_name": "Split-and-Merge IDS on MAWI and UCSD",
        "paper_reference": "Blaise et al., 2020",
        "metric": "Accuracy",
        "their_result": "0.9849",
        "baseline_result": "0.85"
      }
    ],
    "performance_metrics_used": [
      "accuracy",
      "precision",
      "recall",
      "F1 score",
      "confusion matrix"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can a feature-selection-driven SAE combined with an LSTM improve zero-day threat detection on the UGRansome dataset?",
        "Which features learned by an SAE are most discriminative for distinguishing zero-day threats from normal behavior?"
      ],
      "gaps_identified": [
        "Many IDS studies lack robust feature selection mechanisms.",
        "Zero-day/anomaly attacks evade signature-based systems, making them harder to detect.",
        "Computational efficiency challenges when working with the UGRansome dataset and deep feature learning models."
      ],
      "limitations": [
        "Lower performance on synthetic signature (SS) and anomaly (A) attacks compared to signature (S) attacks.",
        "No real-world deployment or live traffic evaluation reported.",
        "No assessment of robustness against adversarial manipulation.",
        "Computational efficiency concerns mentioned but not quantified."
      ],
      "future_work": [
        "Refine the proposed feature selection model.",
        "Address computational efficiency of processing UGRansome.",
        "Integrate hybrid methodologies and ensemble approaches to enhance intrusion detection.",
        "Adapt the model for modified signatures and improve anomaly detection.",
        "Implement continuous learning for emerging threats.",
        "Develop interpretable techniques."
      ],
      "motivation": "Zero-day attacks exploit unknown vulnerabilities and often evade signature-based defenses; effective feature selection and temporal modeling can improve proactive detection.",
      "potential_research_ideas": [
        "Self-supervised or contrastive pretraining on large unlabeled network traffic to enhance representation learning before SAE fine-tuning.",
        "Continual/online learning for zero-day adaptation on streaming traffic with concept drift detection.",
        "Domain adaptation and cross-dataset generalization from UGRansome to other network datasets.",
        "Explainable feature attribution for SAE-selected features using SHAP/Integrated Gradients to guide SOC analysts.",
        "Robust training with adversarial examples or randomized smoothing to improve resilience to evasion.",
        "Cost-sensitive or focal loss optimization to prioritize rare zero-day anomalies.",
        "Transformer-based temporal models (e.g., Temporal Transformers) on selected features for long-range dependencies."
      ],
      "architectural_improvement_recommendations": [
        "Add sparsity or L1/L2 penalties to SAE encoders for crisper feature selection and interpretability.",
        "Explore variational or contractive autoencoders to improve disentanglement and robustness.",
        "Introduce attention mechanisms on top of LSTM (or replace with Temporal Transformers) to focus on salient temporal cues.",
        "Use joint training (end-to-end) of SAE and LSTM with multi-task losses (reconstruction + classification) for better alignment.",
        "Incorporate an auxiliary one-class or energy-based head for anomaly scoring alongside multi-class classification.",
        "Hyperparameter tuning with Bayesian optimization and early stopping; report variance over multiple seeds.",
        "Quantify computational performance and prune/quantize the model for faster inference."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "TensorFlow",
        "Keras",
        "NumPy",
        "pandas",
        "scikit-learn",
        "matplotlib",
        "seaborn",
        "Google Colab"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Training performed on Google Colab with GPU (CUDA). SAE: relu/Adam/mse, 50 epochs; total SAE params ≈11,026. LSTM: 168 units (reported), dense output (3 classes), Adam with sparse_categorical_crossentropy, 400 epochs; total LSTM params ≈122,811."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Computational efficiency concerns on large network datasets.",
        "Harder detection for anomalies and synthetic signatures compared to known signature attacks.",
        "Need for continuous learning to track evolving threats."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes a feature-selection-driven SAE + LSTM pipeline for zero-day threat detection.",
      "Uses unsupervised SAE pretraining and supervised fine-tuning; analyzes learned weights/activations to select important features.",
      "Demonstrates high performance on UGRansome with overall accuracy ≈0.9849 and strong per-class precision/recall/F1.",
      "Discusses generalization across attack categories and highlights challenges for anomaly and synthetic signature detection.",
      "Provides architectural and training details (layers, parameters, epochs) and a comparative discussion against prior IDS studies."
    ]
  },
  {
    "arxiv_id": "2310.09831v1",
    "title": "MAGIC: Detecting Advanced Persistent Threats via Masked Graph Representation Learning",
    "authors": "Zian Jia; Yun Xiong; Yuhong Nan; Yao Zhang; Jinjing Zhao; Mi Wen",
    "abstract": "Advance Persistent Threats (APTs), adopted by most delicate attackers, are becoming increasing common and pose great threat to various enterprises and institutions. Data provenance analysis on provenance graphs has emerged as a common approach in APT detection. However, previous works have exhibited several shortcomings: (1) requiring attack-containing data and a priori knowledge of APTs, (2) failing in extracting the rich contextual information buried within provenance graphs and (3) becoming impracticable due to their prohibitive computation overhead and memory consumption.   In this paper, we introduce MAGIC, a novel and flexible self-supervised APT detection approach capable of performing multi-granularity detection under different level of supervision. MAGIC leverages masked graph representation learning to model benign system entities and behaviors, performing efficient deep feature extraction and structure abstraction on provenance graphs. By ferreting out anomalous system behaviors via outlier detection methods, MAGIC is able to perform both system entity level and batched log level APT detection. MAGIC is specially designed to handle concept drift with a model adaption mechanism and successfully applies to universal conditions and detection scenarios. We evaluate MAGIC on three widely-used datasets, including both real-world and simulated attacks. Evaluation results indicate that MAGIC achieves promising detection results in all scenarios and shows enormous advantage over state-of-the-art APT detection approaches in performance overhead.",
    "published_date": "2023-10-15",
    "pdf_link": "https://arxiv.org/pdf/2310.09831v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "System/Host Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Advanced Persistent Threat (APT) detection from provenance graphs (host audit logs)",
      "attack_types": [
        "Advanced Persistent Threats (APTs)",
        "Port scanning (as part of APT kill-chain)",
        "Exfiltration/C2 (implied by silent connection)",
        "Zero-day exploitation (general APT context)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "GNN / Masked Autoencoder",
        "specific": "Graph Masked Auto-Encoder (GMAE)",
        "novel_contribution": "Extends graph masked auto-encoders with joint supervision of masked feature reconstruction and sample-based structure reconstruction to learn node and system-state embeddings from provenance graphs"
      },
      {
        "type": "primary",
        "category": "Anomaly Detection",
        "specific": "Unsupervised outlier detection on learned embeddings (method not specified in excerpt)",
        "novel_contribution": "Unified use of node embeddings for entity-level and pooled system-state embeddings for batched-log level detection; supports self-/semi-/fully-supervised settings via feedback-based model adaptation"
      },
      {
        "type": "baseline",
        "category": "Sequence Models",
        "specific": "Sequence-based DL (e.g., RNN-style) used in prior work (e.g., ATLAS)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Graph Autoencoders / GNNs",
        "specific": "Graph-based DL approaches (e.g., ShadeWatcher, other GAE-based)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Statistical Anomaly Detection",
        "specific": "Rarity and label-propagation/causal-analysis-based methods",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Rule-based Systems",
        "specific": "Heuristic rule matching against APT patterns",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Self-supervised",
      "Unsupervised",
      "Semi-supervised"
    ],
    "datasets": [
      {
        "name": "DARPA Transparent Computing Engagement 3 (E3) Trace",
        "type": "public",
        "domain": "provenance_graphs (host audit logs)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "StreamSpot dataset",
        "type": "public",
        "domain": "provenance_graphs / system call streams",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Unicorn Wget dataset",
        "type": "public",
        "domain": "provenance_graphs (simulated APT on wget)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "ShadeWatcher",
        "paper_reference": "[18]",
        "metric": "Training time / performance overhead",
        "their_result": "\"51 times faster than ShadeWatcher [18]\"; completes training on DARPA E3 in minutes",
        "baseline_result": "Reported to take \"1 day to train on the DARPA E3 Trace dataset with GPU available\""
      },
      {
        "method_name": "ATLAS",
        "paper_reference": "[11]",
        "metric": "Training time",
        "their_result": "MAGIC trains in minutes on DARPA E3 (qualitative, different dataset scale)",
        "baseline_result": "\"ATLAS takes an average 1 hour to train on 676MB of audit logs\""
      },
      {
        "method_name": "Unicorn",
        "paper_reference": "[10]",
        "metric": "Task granularity (system-level alarms vs multi-granularity)",
        "their_result": "Provides multi-granularity (batched-log and entity-level) detection",
        "baseline_result": "Unicorn yields system-level alarms from streaming logs (no fine-grained entity localization)"
      },
      {
        "method_name": "Statistics-based provenance methods",
        "paper_reference": "[7–9]",
        "metric": "Feature richness / false positives (qualitative)",
        "their_result": "Deep graph representation reduces false positives by capturing context",
        "baseline_result": "Rarity/propagation-based scoring lacks deep semantics, leading to higher FPs"
      }
    ],
    "performance_metrics_used": [
      "Precision",
      "Recall",
      "Training time",
      "Performance overhead",
      "Memory consumption"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How to detect APTs without requiring attack-containing data or a priori knowledge of APT patterns?",
        "How to capture rich contextual and structural information in provenance graphs to reduce false positives?",
        "How to achieve practical computation and memory efficiency for large-scale audit logs?",
        "How to support multi-granularity (batched-log and entity-level) detection and handle concept drift?"
      ],
      "gaps_identified": [
        "Supervised methods require attack-containing data and are vulnerable to unseen APT patterns (lack-of-data problem)",
        "Statistics-based methods fail to capture deep semantics and correlations in provenance graphs, causing high false positives",
        "DL-based sequence/graph approaches often incur prohibitive computation and memory overheads, limiting practicality",
        "Many recent approaches are end-to-end for a single detection task, lacking universality/multi-granularity"
      ],
      "limitations": [
        "Threat model excludes poisoning and evasion attacks",
        "Lookup embeddings operate in a transductive setting tied to the data source’s label space; generalizing to unseen label vocabularies or new data sources may require re-initialization",
        "Batched log-level detection does not localize malicious entities (coarse-grained)",
        "Relies on benign-only training distribution; outlier thresholding and drift handling depend on quality/timeliness of analyst feedback"
      ],
      "future_work": [
        "Apply and evaluate MAGIC in real-world online APT detection pipelines with continuous model adaptation",
        "Further improve performance through semi-supervised adaptation with analyst feedback"
      ],
      "motivation": "Address the need for a label-efficient, context-aware, and computationally practical APT detector over provenance graphs that operates at multiple granularities and adapts to concept drift.",
      "potential_research_ideas": [
        "Temporal- and streaming-aware masked graph representation learning to natively handle evolving provenance graphs and sliding windows",
        "Adversarial robustness for provenance-graph detectors (evasion/poison-resilient objectives and training)",
        "Cross-host/cross-process correlation using multi-graph or graph-of-graphs models for lateral movement detection",
        "Contrastive multi-view learning combining structural, temporal, and semantic views (e.g., command-line, file path, network context)",
        "Few-shot and continual learning for rapid adaptation to new environments and software updates",
        "Uncertainty-aware outlier detection and calibrated scoring to reduce false positives",
        "Federated or privacy-preserving training across organizations without sharing raw logs",
        "Explainability modules that attribute anomaly scores to subgraphs, edge types, or event motifs for analyst triage"
      ],
      "architectural_improvement_recommendations": [
        "Adopt heterogeneous/relational GNNs or graph transformers with edge-type attention and time encodings to better capture provenance heterogeneity",
        "Add temporal positional encodings and masked temporal prediction objectives alongside masked feature/structure reconstruction",
        "Use contrastive learning (e.g., graph-level and node-level InfoNCE) jointly with masked reconstruction to improve representation quality",
        "Design inductive label encoders (e.g., hashing with sub-tokenization or attribute encoders) to handle unseen entity/edge labels across data sources",
        "Integrate streaming mini-batch training and memory modules for long-horizon dependency capture with bounded state",
        "Evaluate multiple outlier detectors (e.g., one-class SVM, Isolation Forest, deep SVDD) and ensemble them with calibration",
        "Subgraph anomaly localization via score decomposition (attributing system-state anomalies back to nodes/edges)"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/FDUDSDE/MAGIC",
      "frameworks": [],
      "reproducibility_score": "high",
      "computational_requirements": "Trains on DARPA E3 in minutes; significantly lower memory usage; reported 51× faster than ShadeWatcher; no explicit GPU requirement stated."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Host-based audit logging environment (enterprise endpoints; DARPA TC E3 traces); batch-wise processing of streaming logs",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Concept drift in benign behaviors requiring periodic adaptation",
        "High data volume of audit logs necessitating efficient preprocessing and graph construction",
        "Dependence on audit log formats and label vocabularies (transductive lookup embeddings)",
        "Need for analyst feedback loops to fine-tune thresholds and reduce false positives"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces MAGIC, a universal APT detection approach based on masked graph representation learning and outlier detection, supporting multi-granularity detection.",
      "Minimizes computation overhead with extended graph masked auto-encoders, enabling training and detection within acceptable time and memory limits.",
      "Ensures universality across supervision levels (self-/semi-/fully-supervised), detection granularities, and diverse audit log sources.",
      "Evaluates MAGIC on three widely-used datasets (DARPA E3, StreamSpot, Unicorn Wget) with both real-world and simulated attacks, showing promising detection and minimal overhead.",
      "Releases open-source implementation and pre-processed datasets."
    ]
  },
  {
    "arxiv_id": "2309.15518v1",
    "title": "Raijū: Reinforcement Learning-Guided Post-Exploitation for Automating Security Assessment of Network Systems",
    "authors": "Van-Hau Pham; Hien Do Hoang; Phan Thanh Trung; Van Dinh Quoc; Trong-Nghia To; Phan The Duy",
    "abstract": "In order to assess the risks of a network system, it is important to investigate the behaviors of attackers after successful exploitation, which is called post-exploitation. Although there are various efficient tools supporting post-exploitation implementation, no application can automate this process. Most of the steps of this process are completed by experts who have profound knowledge of security, known as penetration testers or pen-testers. To this end, our study proposes the Raijū framework, a Reinforcement Learning (RL)-driven automation approach that assists pen-testers in quickly implementing the process of post-exploitation for security-level evaluation in network systems. We implement two RL algorithms, Advantage Actor-Critic (A2C) and Proximal Policy Optimization (PPO), to train specialized agents capable of making intelligent actions, which are Metasploit modules to automatically launch attacks of privileges escalation, gathering hashdump, and lateral movement. By leveraging RL, we aim to empower these agents with the ability to autonomously select and execute actions that can exploit vulnerabilities in target systems. This approach allows us to automate certain aspects of the penetration testing workflow, making it more efficient and responsive to emerging threats and vulnerabilities. The experiments are performed in four real environments with agents trained in thousands of episodes. The agents automatically select actions and launch attacks on the environments and achieve over 84\\% of successful attacks with under 55 attack steps given. Moreover, the A2C algorithm has proved extremely effective in the selection of proper actions for automation of post-exploitation.",
    "published_date": "2023-09-27",
    "pdf_link": "https://arxiv.org/pdf/2309.15518v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Penetration Testing",
      "subdomain": "Post-Exploitation Automation",
      "specific_problem": "Reinforcement learning agent to autonomously select and execute Metasploit modules for privilege escalation, credential dumping (hashdump), and lateral movement in networked Windows/Linux systems",
      "attack_types": [
        "Privilege escalation",
        "Credential dumping (hashdump)",
        "Lateral movement"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Actor-Critic",
        "specific": "A2C (Advantage Actor-Critic)",
        "novel_contribution": "Applied as the primary RL policy to select Metasploit post-exploitation modules; shown to be highly effective in this domain"
      },
      {
        "type": "primary",
        "category": "Policy Optimization",
        "specific": "PPO (Proximal Policy Optimization)",
        "novel_contribution": "Compared as an alternative RL policy for the same action-selection task"
      }
    ],
    "learning_paradigm": [
      "Reinforcement Learning"
    ],
    "datasets": [
      {
        "name": "Windows 7 vulnerable machine(s)",
        "type": "proprietary",
        "domain": "host_system",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Metasploitable 2",
        "type": "public",
        "domain": "host_system",
        "link": "https://information.rapid7.com/download-metasploitable-2017.html",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Four real lab environments (Windows/Linux network scenarios)",
        "type": "proprietary",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "CSV logs of environment states and exploit outcomes (from Metasploit/Nmap runs)",
        "type": "proprietary",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "PPO (compared against A2C)",
        "paper_reference": null,
        "metric": "Attack success rate; number of attack steps",
        "their_result": "\"achieve over 84% of successful attacks with under 55 attack steps given. Moreover, the A2C algorithm has proved extremely effective\"",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Attack success rate",
      "Number of attack steps",
      "Training episodes"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How can reinforcement learning automate post-exploitation by selecting appropriate Metasploit modules to achieve privilege escalation, hashdump collection, and lateral movement?",
        "What state features, action space, and reward mechanism enable effective RL for post-exploitation in variable target environments?",
        "Can A2C and PPO effectively learn policies that generalize across different real network environments?"
      ],
      "gaps_identified": [
        "No application can fully automate post-exploitation; current tools require expert-driven configuration and decision-making.",
        "Supervised/unsupervised learning are ill-suited for continuous, real-time post-exploitation environments due to data preparation challenges.",
        "Prior work focuses on isolated aspects of automated penetration testing, neglecting interconnections that could improve flexibility."
      ],
      "limitations": [
        "Direct online training caused time/resource overhead and connection errors; authors resorted to training from saved CSV logs instead of continuous real-time interaction.",
        "Evaluation limited to Windows 7 and Metasploitable 2-based scenarios in four lab environments.",
        "Action space limited to selected Metasploit modules for privilege escalation, hashdump, and lateral movement.",
        "No head-to-head quantitative comparison with existing automated tools (e.g., DeepExploit, Empire-based systems) beyond internal A2C vs PPO."
      ],
      "future_work": [],
      "motivation": "Automate post-exploitation to reduce pen-tester effort and increase responsiveness by enabling an RL agent to select and execute actions that exploit vulnerabilities in target systems.",
      "potential_research_ideas": [
        "Online RL in live networks with safety constraints and rollback to handle instability and connection errors.",
        "Integrate attack graphs (e.g., MulVAL) or knowledge graphs for prior-informed policy learning and action pruning.",
        "Hierarchical or multi-agent RL separating host-level privilege escalation from network-level lateral movement.",
        "Generalize to modern OS (Windows 10/11, Server versions, contemporary Linux distros) and cloud environments (AWS/Azure/containers).",
        "Offline RL from large historical red team/blue team logs to pretrain policies before fine-tuning online.",
        "Incorporate stealth-aware rewards (evasion of EDR/IDS) and cost-sensitive objectives (noise, dwell time).",
        "Combine symbolic planning with RL (neuro-symbolic PT) for constraint satisfaction (credentials, reachability).",
        "Leverage LLMs to generate/parameterize Metasploit module options guided by RL policy for complex chaining.",
        "Use graph neural networks over host/service graphs to encode environment states for better generalization."
      ],
      "architectural_improvement_recommendations": [
        "Adopt hierarchical RL (manager-worker) with subgoals for privilege escalation, credential collection, and lateral movement.",
        "Incorporate attack-graph-informed rewards and action masking to reduce exploration of infeasible exploits.",
        "Use curriculum learning: start with single-host tasks and progressively add network depth and defenses.",
        "Apply constrained/safe RL with penalty terms or Lagrangian methods to control stealth/noise and operational risk.",
        "Switch to model-based RL or learned environment models to simulate exploit outcomes and reduce lab reliance.",
        "Represent environment as a graph and use a GNN-based policy/value network for state encoding.",
        "Parameter optimization of Metasploit modules via continuous control heads alongside discrete module selection.",
        "Uncertainty-aware policies (ensembles, bootstrapped actors) to handle stochastic exploit success and noisy states."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Lab network with vulnerable Windows 7 and Metasploitable 2 hosts integrated with Metasploit/Nmap",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Connection errors and instability during long training episodes when interacting with real environments.",
        "Fragility and parameterization of exploits; success depends on exact environment state.",
        "Legal/ethical constraints and risk of unintended impact when automating offensive actions.",
        "Generalization across heterogeneous systems and services."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposed Raijū framework: an RL-driven agent that selects Metasploit modules as actions to automate post-exploitation.",
      "Implemented and compared A2C and PPO to learn action policies for privilege escalation, hashdump collection, and lateral movement.",
      "Conducted experiments across four real environments (Windows and Linux) and reported high success with limited attack steps."
    ]
  },
  {
    "arxiv_id": "2309.12421v1",
    "title": "Change Management using Generative Modeling on Digital Twins",
    "authors": "Nilanjana Das; Anantaa Kotal; Daniel Roseberry; Anupam Joshi",
    "abstract": "A key challenge faced by small and medium-sized business entities is securely managing software updates and changes. Specifically, with rapidly evolving cybersecurity threats, changes/updates/patches to software systems are necessary to stay ahead of emerging threats and are often mandated by regulators or statutory authorities to counter these. However, security patches/updates require stress testing before they can be released in the production system. Stress testing in production environments is risky and poses security threats. Large businesses usually have a non-production environment where such changes can be made and tested before being released into production. Smaller businesses do not have such facilities. In this work, we show how \"digital twins\", especially for a mix of IT and IoT environments, can be created on the cloud. These digital twins act as a non-production environment where changes can be applied, and the system can be securely tested before patch release. Additionally, the non-production digital twin can be used to collect system data and run stress tests on the environment, both manually and automatically. In this paper, we show how using a small sample of real data/interactions, Generative Artificial Intelligence (AI) models can be used to generate testing scenarios to check for points of failure.",
    "published_date": "2023-09-21",
    "pdf_link": "https://arxiv.org/pdf/2309.12421v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Security Operations",
      "subdomain": "Patch and Change Management",
      "specific_problem": "Pre-release testing and validation of security patches/updates using cloud-hosted digital twins and AI-generated test scenarios",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "GAN",
        "specific": "PriveTab-style tabular GAN (mode-specific normalization, conditional generator, training-by-sampling)",
        "novel_contribution": "Applies a tabular GAN to system/device telemetry (process/CPU/memory usage) collected from digital twins to synthesize realistic test data for stress testing patches."
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "GPT-2 (decoder-only) fine-tuned for command/script generation",
        "novel_contribution": "Fine-tunes GPT-2 on small sets of recorded macro (.ahk) scripts to generate additional automated test scripts for digital-twin testing."
      }
    ],
    "learning_paradigm": [
      "Unsupervised",
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Active Processes and CPU Usage from IoT Digital Twin (Linux top)",
        "type": "private",
        "domain": "system_metrics",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "User Interaction Macro Scripts (.ahk) from Windows Digital Twin",
        "type": "private",
        "domain": "user_interaction_logs",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Earth Mover’s Distance (distributional closeness threshold during GAN sampling)",
      "Average CPU usage (%)",
      "Average memory usage (%)",
      "Average process runtime (ms)",
      "Cosine similarity (between generated and original command sequences)",
      "BLEU score (between generated and original command sequences)",
      "Replay success rate of generated scripts"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Small and medium businesses often lack non-production environments for safe patch/change testing.",
        "Manual stress-testing and scenario design are time-consuming and resource-intensive.",
        "Limited real-world system data for ML-driven testing; need to augment with synthetic data.",
        "Standard generative models target images/text; system/device data are tabular with mixed types and constrained distributions, requiring specialized generators.",
        "Need a safe way to observe post-patch behavior without risking production environments."
      ],
      "limitations": [
        "Prototype demonstrated on limited platforms (Windows 10 VM on Azure; Ubuntu 22.10 on Raspberry Pi 4 via Docker).",
        "Small datasets for training (e.g., 560 process datapoints; 6 macro scenarios).",
        "Evaluation focuses on distributional similarity and text similarity; lacks extensive functional/utility metrics or comparisons to alternative generators.",
        "No quantitative scalability evaluation or cost analysis.",
        "No discussion of privacy, security hardening, or adversarial robustness of the generative components.",
        "Digital-twin fidelity to production not rigorously validated; some filesystem differences (e.g., excluded tmp) may affect equivalence."
      ],
      "future_work": [],
      "motivation": "Enable SMEs to securely test patches/changes using cloud-hosted digital twins and to automate stress-testing via generative AI when only small samples of real interactions are available.",
      "potential_research_ideas": [
        "Create a public benchmark of digital-twin system telemetry and test scripts for patch-testing to standardize evaluation.",
        "Closed-loop test generation: use failure signals/logs to iteratively fine-tune generators for maximal fault discovery.",
        "Utility-centric evaluation: measure defect/failure discovery rates of synthetic data vs. real data across diverse patches.",
        "Extend to networked, multi-host twins (IT+IoT+OT) to test distributed updates and configuration drifts.",
        "Integrate reinforcement learning to synthesize adversarial user/system actions that maximize likelihood of failure.",
        "Add privacy-preserving synthetic data generation (e.g., DP for tabular GANs) to enable data sharing across SMEs.",
        "Twin fidelity assessment methods (e.g., causal invariance tests) to quantify similarity to production under patches.",
        "Leverage newer instruction-tuned code LLMs to generate richer, reliable test scripts with constraints (guardrails).",
        "Develop multi-agent test orchestration where one agent plans scenarios, another executes, and a third validates outcomes.",
        "Automated environment provisioning and snapshotting pipelines for rapid A/B testing of patch variants."
      ],
      "architectural_improvement_recommendations": [
        "Adopt state-of-the-art tabular generators (e.g., CTGAN, TVAE, TabDDPM) and compare against PriveTab under utility metrics.",
        "Introduce differential privacy or PATE-style training for the tabular generator to mitigate leakage of sensitive telemetry.",
        "Use constraint-aware decoding for LLM test-script generation (schema/DSL constraints, unit tests) to improve replay success.",
        "Implement twin-production synchronization (config drift detection, golden image baselining) with integrity checks.",
        "Add a utility evaluation suite: downstream anomaly/issue detection performance, failure-inducing test coverage, and fault localization metrics.",
        "Harden the pipeline with security controls (signing of images, SBOM, vulnerability scanning as gates) and audit logging.",
        "Scalable data collection agents with sampling/aggregation to reduce overhead and support larger fleets.",
        "Incorporate online monitoring of generated tests with rollback and sandboxing to ensure safety during automated stress tests."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "Azure",
        "Docker",
        "Pulover’s Macro Creator",
        "Microsoft Disk2vhd",
        "Vulmap",
        "CIS-CAT Lite"
      ],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Azure cloud VMs (Windows 10) and Docker container on Azure VM (Ubuntu 22.10, Raspberry Pi 4 image)",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Accurate and secure image capture and upload of devices to the cloud.",
        "Potential fidelity gaps between physical devices and digital twins (filesystem/environment differences).",
        "Limited data availability for training reliable generators.",
        "Operational overhead for SMEs to manage cloud resources and twin lifecycle.",
        "Safety of automated generated tests; need for sandboxing and rollback.",
        "Security of stored images and telemetry in cloud storage."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Framework to create cloud-hosted digital twins for IT (Windows 10 on Azure VM) and IoT (Ubuntu 22.10 on Raspberry Pi via Docker) devices to serve as non-production testbeds.",
      "Background services to capture system images (VHD for Windows; tar for IoT) and upload to cloud; automated twin provisioning.",
      "Automated testing pipeline integrating vulnerability scanning (Vulmap), configuration scanning (CIS-CAT Lite), Windows event logs, and process monitoring.",
      "Use of a tabular GAN (PriveTab-style) to generate synthetic system telemetry (process/CPU/memory) closely matching real distributions for stress testing.",
      "Fine-tuning GPT-2 on recorded macro (.ahk) scripts to generate additional test scripts for automated testing.",
      "Prototype demonstration and preliminary validation: distributional similarity (CPU/mem/runtime), cosine similarity 0.73 and BLEU 0.72 for generated command sequences; 2/3 generated scenarios replayed successfully."
    ]
  },
  {
    "arxiv_id": "2310.03202v1",
    "title": "ResolverFuzz: Automated Discovery of DNS Resolver Vulnerabilities with Query-Response Fuzzing",
    "authors": "Qifan Zhang; Xuesong Bai; Xiang Li; Haixin Duan; Qi Li; Zhou Li",
    "abstract": "Domain Name System (DNS) is a critical component of the Internet. DNS resolvers, which act as the cache between DNS clients and DNS nameservers, are the central piece of the DNS infrastructure, essential to the scalability of DNS. However, finding the resolver vulnerabilities is non-trivial, and this problem is not well addressed by the existing tools. To list a few reasons, first, most of the known resolver vulnerabilities are non-crash bugs that cannot be directly detected by the existing oracles (or sanitizers). Second, there lacks rigorous specifications to be used as references to classify a test case as a resolver bug. Third, DNS resolvers are stateful, and stateful fuzzing is still challenging due to the large input space.   In this paper, we present a new fuzzing system termed ResolverFuzz to address the aforementioned challenges related to DNS resolvers, with a suite of new techniques being developed. First, ResolverFuzz performs constrained stateful fuzzing by focusing on the short query-response sequence, which has been demonstrated as the most effective way to find resolver bugs, based on our study of the published DNS CVEs. Second, to generate test cases that are more likely to trigger resolver bugs, we combine probabilistic context-free grammar (PCFG) based input generation with byte-level mutation for both queries and responses. Third, we leverage differential testing and clustering to identify non-crash bugs like cache poisoning bugs. We evaluated ResolverFuzz against 6 mainstream DNS software under 4 resolver modes. Overall, we identify 23 vulnerabilities that can result in cache poisoning, resource consumption, and crash attacks. After responsible disclosure, 19 of them have been confirmed or fixed, and 15 CVE numbers have been assigned.",
    "published_date": "2023-10-04",
    "pdf_link": "https://arxiv.org/pdf/2310.03202v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "DNS Security",
      "specific_problem": "Automated discovery of DNS resolver vulnerabilities via constrained query-response fuzzing and differential testing",
      "attack_types": [
        "cache poisoning",
        "resource consumption / denial-of-service",
        "non-memory crash",
        "memory corruption / crash"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Probabilistic Grammar",
        "specific": "Probabilistic Context-Free Grammar (PCFG)",
        "novel_contribution": "PCFG-based generation combined with byte-level mutation to produce resolver-acceptable DNS queries and responses tailored to bug discovery"
      },
      {
        "type": "primary",
        "category": "Clustering",
        "specific": "Bisecting K-means",
        "novel_contribution": "Clusters differential-testing inconsistencies to triage non-crash bugs and reduce manual investigation effort"
      }
    ],
    "learning_paradigm": [
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "CVE reports for 6 mainstream DNS resolver software (1999–2023)",
        "type": "public",
        "domain": "vulnerability_reports",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Generated DNS query-response test cases (~700K sequences)",
        "type": "synthetic",
        "domain": "network_traffic_dns_messages",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "dns-fuzz-server (grammar + byte-level mutation for DNS)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "SnapFuzz (greybox network fuzzer, tested on Dnsmasq)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "DNS Fuzzer (byte-level mutation)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "number_of_vulnerabilities_discovered",
      "number_confirmed_fixed_and_CVEs_assigned",
      "fuzzing_throughput",
      "coverage_of_valid_DNS_messages",
      "detection_of_cache_poisoning_via_differential_testing",
      "reduction_in_manual_triage_via_clustering"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How to efficiently uncover DNS resolver vulnerabilities, especially non-crash semantic bugs, at high throughput?",
        "How to design effective test generation for stateful resolvers with a very large input space?",
        "How to build oracles for non-crash bugs (e.g., cache poisoning, resource consumption) given the lack of rigorous DNS resolver specifications?"
      ],
      "gaps_identified": [
        "Most known DNS resolver vulnerabilities are non-crash bugs that are not detected by existing oracles/sanitizers",
        "Lack of rigorous resolver specifications to classify test cases as bugs",
        "Stateful fuzzing for resolvers is challenging due to large input/state space",
        "Generic greybox mutation provides insufficient guidance for complex DNS message fields",
        "Resolver inconsistencies are pervasive, and many are not vulnerabilities, complicating differential-testing oracles",
        "CVE-based understanding may suffer from survivorship bias"
      ],
      "limitations": [
        "Did not test all resolver configurations; DNSSEC was disabled due to many resolver inconsistencies irrelevant to bugs",
        "Potential survivorship bias in the CVE study",
        "Blackbox approach may miss bugs requiring internal instrumentation or longer interaction sequences",
        "Focus on short query-response sequences may miss bugs requiring longer, more complex state interactions",
        "Nameserver hierarchy was localized; behavior may differ from the public Internet in some edge cases"
      ],
      "future_work": [],
      "motivation": "Develop effective tools to uncover resolver vulnerabilities and secure the DNS infrastructure, addressing shortcomings of existing fuzzers and oracles for non-crash semantic bugs.",
      "potential_research_ideas": [
        "Integrate formal specifications and SMT-based checking to strengthen non-crash oracles beyond differential testing (e.g., RFC-conformant invariants for caching and bailiwick rules).",
        "Learn probabilistic grammars from real DNS traffic to adapt PCFG productions toward realistic-but-adversarial messages.",
        "Reinforcement learning or bandit-based guidance to select mutations and message fields that maximize inconsistency or cache impact.",
        "Extend beyond single-step sequences to multi-turn state exploration using sequence-generation policies (e.g., MCTS) while keeping throughput high.",
        "Online shadow-fuzzing in production resolvers with isolated shadow caches to discover deployment-specific bugs safely.",
        "Generalize the framework to DoT/DoH and DNSSEC, including cryptographic corner cases and validation-state transitions.",
        "Automated invariant mining over resolver traces to propose candidate semantic rules and detect violations.",
        "Cross-implementation differential semantics learning to prioritize inputs likely to cause divergent caching behavior.",
        "Expand the approach to other stateful core Internet protocols (e.g., BGP update handling, DHCP) with protocol-specific grammars.",
        "Automated root-cause analysis linking clustered inconsistencies to code paths via lightweight tracing or eBPF probes."
      ],
      "architectural_improvement_recommendations": [
        "Adopt density-based clustering (DBSCAN/HDBSCAN) or ensemble clustering for inconsistency triage to better handle noise and non-spherical clusters than bisecting K-means.",
        "Grammar-aware mutation operators that respect cross-field constraints (e.g., name compression pointers, TTL coherency) while deliberately violating selected invariants.",
        "Lightweight greybox feedback via eBPF or coverage sampling to guide mutation hot spots without recompilation.",
        "Active learning loop where analyst feedback on clusters refines anomaly scoring and test-case prioritization.",
        "Multi-objective fuzzing scheduler optimizing for inconsistency magnitude, cache-delta size, and resolver resource usage.",
        "Delta-debugging and automated minimization integrated with the clustering pipeline to yield compact PoCs.",
        "DNSSEC-aware modules covering validation states, key rollover, and signature corner cases to broaden semantic bug coverage.",
        "Support for TLS-based DNS (DoT/DoH) transport variations and TCP-specific edge cases (e.g., truncation handling).",
        "Traffic-replay seeding from real-world DNS traces to bias PCFG probabilities and mutation distributions.",
        "Per-implementation learned mutation policies that adapt to resolver-specific parsing quirks uncovered during fuzzing."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Docker-based local testbed with localized nameserver hierarchy and parallel resolver instances",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Achieving high throughput for stateful resolver fuzzing due to network and caching latencies",
        "Resetting and isolating resolver state across tests",
        "Handling pervasive cross-implementation inconsistencies without overwhelming analysts",
        "Testing DNSSEC introduces many inconsistencies, complicating oracle design",
        "Avoiding unintended traffic to remote nameservers (necessitating localization)"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Comprehensive study of 291 DNS CVEs (245 resolver-related) across 6 mainstream DNS software, characterizing non-crash prevalence and short-sequence triggers.",
      "Design and implementation of ResolverFuzz, a blackbox fuzzing system performing constrained query-response fuzzing for resolvers.",
      "PCFG-based DNS message generation augmented with byte-level mutation to improve acceptance and bug-triggering likelihood.",
      "Differential-testing oracle and bisecting K-means clustering to detect and triage non-crash semantic bugs (e.g., cache poisoning).",
      "New test infrastructure that localizes the nameserver hierarchy and enables concurrent resolver testing with Docker-based isolation.",
      "Evaluation on 6 resolvers across 4 modes; discovery of 23 vulnerabilities leading to cache poisoning, resource consumption, and crashes; 19 confirmed/fixed with 15 CVEs assigned.",
      "Open-sourcing of ResolverFuzz."
    ]
  },
  {
    "arxiv_id": "2310.16569v1",
    "title": "Flow-Attention-based Spatio-Temporal Aggregation Network for 3D Mask Detection",
    "authors": "Yuxin Cao; Yian Li; Yumeng Zhu; Derui Wang; Minhui Xue",
    "abstract": "Anti-spoofing detection has become a necessity for face recognition systems due to the security threat posed by spoofing attacks. Despite great success in traditional attacks, most deep-learning-based methods perform poorly in 3D masks, which can highly simulate real faces in appearance and structure, suffering generalizability insufficiency while focusing only on the spatial domain with single frame input. This has been mitigated by the recent introduction of a biomedical technology called rPPG (remote photoplethysmography). However, rPPG-based methods are sensitive to noisy interference and require at least one second (> 25 frames) of observation time, which induces high computational overhead. To address these challenges, we propose a novel 3D mask detection framework, called FASTEN (Flow-Attention-based Spatio-Temporal aggrEgation Network). We tailor the network for focusing more on fine-grained details in large movements, which can eliminate redundant spatio-temporal feature interference and quickly capture splicing traces of 3D masks in fewer frames. Our proposed network contains three key modules: 1) a facial optical flow network to obtain non-RGB inter-frame flow information; 2) flow attention to assign different significance to each frame; 3) spatio-temporal aggregation to aggregate high-level spatial features and temporal transition features. Through extensive experiments, FASTEN only requires five frames of input and outperforms eight competitors for both intra-dataset and cross-dataset evaluations in terms of multiple detection metrics. Moreover, FASTEN has been deployed in real-world mobile devices for practical 3D mask detection.",
    "published_date": "2023-10-25",
    "pdf_link": "https://arxiv.org/pdf/2310.16569v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Biometric Security",
      "subdomain": "Face Presentation Attack Detection (PAD)",
      "specific_problem": "3D mask detection (high-fidelity mask anti-spoofing) from short RGB video clips",
      "attack_types": [
        "3D mask presentation attacks"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "CNN",
        "specific": "MobileNetV3-Small (0.75x)",
        "novel_contribution": "Backbone for spatial feature extraction with early-stage multi-frame feature fusion and later-stage classification"
      },
      {
        "type": "primary",
        "category": "Optical Flow Estimator (CNN)",
        "specific": "FlowNetFace (evolved from FlowNetS, 2-layer encoder/decoder)",
        "novel_contribution": "Facial optical flow network trained with pseudo ground truth from FlowNet2.0 to capture inter-frame motion and provide temporal transitional features at low computational cost"
      },
      {
        "type": "primary",
        "category": "Attention Mechanism",
        "specific": "Flow attention (3x3 conv over optical flow to compute frame weights)",
        "novel_contribution": "Uses facial optical flow to assign frame-wise attention weights to emphasize frames with large, informative movements"
      },
      {
        "type": "primary",
        "category": "Spatio-Temporal Aggregation",
        "specific": "Concatenation of deep spatial features and temporal transitional features",
        "novel_contribution": "Aggregates weighted spatial features with temporal features for robust 3D mask detection from only five frames"
      },
      {
        "type": "primary",
        "category": "Loss Function",
        "specific": "Weighted Binary Cross-Entropy",
        "novel_contribution": "Balances class imbalance between real faces and 3D masks"
      },
      {
        "type": "baseline",
        "category": "Handcrafted Features",
        "specific": "MS-LBP",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Handcrafted Features",
        "specific": "CTA (color texture analysis)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "CDCN++ (Central Difference Convolution + multi-scale attention)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "HRFP (High-Resolution Face Parts)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "ViTranZFAS (ViT-based transfer learning)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Domain Adaptation",
        "specific": "MD-FAS",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Physiological Signal (rPPG)",
        "specific": "CFrPPG",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Physiological Signal (rPPG)",
        "specific": "LeTSrPPG",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "3DMAD (3D Mask Attack Dataset)",
        "type": "public",
        "domain": "face_videos",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "HKBU-MARs V1+ (Hong Kong Baptist University 3D Mask Attack with Real-World Variations)",
        "type": "public",
        "domain": "face_videos",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "HiFiMask (High-Fidelity Mask dataset)",
        "type": "public",
        "domain": "face_videos",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "MS-LBP",
        "paper_reference": "[32]",
        "metric": "AUC; HTER; EER; APCER; BPCER",
        "their_result": "FASTEN intra-dataset: 3DMAD AUC 99.8%, HTER 1.17%; HKBU-MARs V1+ AUC 99.7%, HTER 2.13%; HiFiMask AUC 99.8%, HTER 2.11%",
        "baseline_result": null
      },
      {
        "method_name": "CTA (color texture analysis)",
        "paper_reference": "[9]",
        "metric": "AUC; HTER; EER; APCER; BPCER",
        "their_result": "FASTEN intra-dataset: 3DMAD AUC 99.8%, HTER 1.17%; HKBU-MARs V1+ AUC 99.7%, HTER 2.13%; HiFiMask AUC 99.8%, HTER 2.11%",
        "baseline_result": null
      },
      {
        "method_name": "CDCN++",
        "paper_reference": "[15]",
        "metric": "AUC; HTER; EER; APCER; BPCER",
        "their_result": "FASTEN intra-dataset: 3DMAD AUC 99.8%, HTER 1.17%; HKBU-MARs V1+ AUC 99.7%, HTER 2.13%; HiFiMask AUC 99.8%, HTER 2.11%",
        "baseline_result": null
      },
      {
        "method_name": "HRFP",
        "paper_reference": "[37]",
        "metric": "AUC; HTER; EER; APCER; BPCER",
        "their_result": "FASTEN intra-dataset: 3DMAD AUC 99.8%, HTER 1.17%; HKBU-MARs V1+ AUC 99.7%, HTER 2.13%; HiFiMask AUC 99.8%, HTER 2.11%",
        "baseline_result": null
      },
      {
        "method_name": "ViTranZFAS (Vision Transformer transfer)",
        "paper_reference": "[38]",
        "metric": "AUC; HTER; EER; APCER; BPCER",
        "their_result": "FASTEN intra-dataset: 3DMAD AUC 99.8%, HTER 1.17%; HKBU-MARs V1+ AUC 99.7%, HTER 2.13%; HiFiMask AUC 99.8%, HTER 2.11%",
        "baseline_result": null
      },
      {
        "method_name": "MD-FAS",
        "paper_reference": "[39]",
        "metric": "AUC; HTER; EER; APCER; BPCER",
        "their_result": "FASTEN intra-dataset: 3DMAD AUC 99.8%, HTER 1.17%; HKBU-MARs V1+ AUC 99.7%, HTER 2.13%; HiFiMask AUC 99.8%, HTER 2.11%",
        "baseline_result": null
      },
      {
        "method_name": "CFrPPG",
        "paper_reference": "[44]",
        "metric": "AUC; HTER; EER; APCER; BPCER",
        "their_result": "FASTEN requires only 5 frames and outperforms rPPG-based methods on intra-/cross-dataset evaluations (numerical rPPG results cited from their papers for 3DMAD and HKBU-MARs V1+).",
        "baseline_result": null
      },
      {
        "method_name": "LeTSrPPG",
        "paper_reference": "[21]",
        "metric": "AUC; HTER; EER; APCER; BPCER",
        "their_result": "FASTEN requires only 5 frames and outperforms rPPG-based methods on intra-/cross-dataset evaluations (numerical rPPG results cited from their papers for 3DMAD and HKBU-MARs V1+).",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "HTER",
      "EER",
      "AUC",
      "APCER",
      "BPCER",
      "BPCER@APCER=0.1",
      "BPCER@APCER=0.01",
      "AEPE (for optical flow training)"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can 3D mask PAD be achieved robustly using only a few RGB frames by leveraging spatio-temporal features without rPPG or depth?",
        "Can frame-wise attention derived from facial optical flow improve generalizability and reduce computation compared to existing methods?"
      ],
      "gaps_identified": [
        "Data availability: Most methods rely on devices hard to deploy in practice (e.g., RGBD for depth, specialized lighting for rPPG).",
        "Temporal information involvement: Few works exploit temporal features that capture facial variations.",
        "Computational cost: rPPG methods require ≥1 second (>25 frames) of observation, incurring high computation."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "3D masks highly simulate real faces, degrading existing PAD performance, while rPPG methods are noise-sensitive and require long observations. The authors aim to create an efficient, robust, RGB-only method that uses few frames and exploits temporal motion cues.",
      "potential_research_ideas": [
        "Domain generalization strategies (e.g., meta-learning or style augmentation) to further improve cross-dataset robustness to unseen mask materials and lighting.",
        "Self-supervised pretraining on large-scale in-the-wild face videos to enhance motion and structure representations for optical-flow and temporal modules.",
        "Hybrid physiological-motion models that fuse weak rPPG cues with optical-flow features when reliable, with uncertainty weighting.",
        "Few-shot adaptation to new mask types or camera sensors using lightweight calibration clips.",
        "Temporal transformer or ConvNeXt-based temporal heads to replace/augment aggregation for longer context when available without increasing minimum frames.",
        "Explainability methods (e.g., spatiotemporal saliency maps) to highlight splicing trace regions and improve trust and debugging.",
        "Adversarial robustness evaluation and defenses against adaptive presentation attacks and adversarial pre-processing.",
        "On-device personalization and federated learning to adapt thresholds/models without sharing raw video."
      ],
      "architectural_improvement_recommendations": [
        "Replace or augment MobileNetV3 backbone with lightweight ConvNeXt or MobileViT and compare latency-accuracy on mobile.",
        "Introduce a temporal transformer or lightweight gated temporal convolution on top of the flow-attended features to better capture medium-range dependencies while keeping 5-frame minimum.",
        "Multi-scale optical flow pyramid to capture both micro and macro motion; fuse with spatial features via cross-attention instead of concatenation.",
        "Uncertainty-aware frame weighting: estimate flow confidence and use it to modulate attention weights.",
        "Joint end-to-end training of FlowNetFace with the classifier using multi-task loss (optical flow AEPE + PAD loss) once the flow stabilizes, to tailor flow features for PAD.",
        "Knowledge distillation to an even smaller student for ultra-low-power devices."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/JosephCao0327/FASTEN",
      "frameworks": [
        "PyTorch"
      ],
      "reproducibility_score": "high",
      "computational_requirements": "Trained with two Tesla V100 GPUs; batch size 120; 150 epochs; AdamW optimizer; cosine annealing with warm restarts; requires only 5 input frames at inference."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Mobile devices (resource-constrained platforms)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "We propose a flow-attention-based spatio-temporal aggregation network, called FASTEN, for 3D mask PAD. FASTEN outperforms existing works in intra-/cross-dataset evaluations.",
      "We train a facial optical flow model, FlowNetFace, to learn the small variations in facial features between frames. The facial features are applied simultaneously to the calculation of frame weights and temporal transitional features.",
      "We address the long-time issue that the state-of-the-art rPPGM still needs about one second (> 25 frames) for observation, and propose a frame-based method that only needs any five frames of a 3D mask video, by adding frame weights to multi-frame spatial features. We also release a lightweight application for FASTEN to mobile devices."
    ]
  },
  {
    "arxiv_id": "2309.15203v1",
    "title": "Eve Said Yes: AirBone Authentication for Head-Wearable Smart Voice Assistant",
    "authors": "Chenpei Huang; Hui Zhong; Jie Lian; Pavana Prakash; Dian Shi; Yuan Xu; Miao Pan",
    "abstract": "Recent advances in machine learning and natural language processing have fostered the enormous prosperity of smart voice assistants and their services, e.g., Alexa, Google Home, Siri, etc. However, voice spoofing attacks are deemed to be one of the major challenges of voice control security, and never stop evolving such as deep-learning-based voice conversion and speech synthesis techniques. To solve this problem outside the acoustic domain, we focus on head-wearable devices, such as earbuds and virtual reality (VR) headsets, which are feasible to continuously monitor the bone-conducted voice in the vibration domain. Specifically, we identify that air and bone conduction (AC/BC) from the same vocalization are coupled (or concurrent) and user-level unique, which makes them suitable behavior and biometric factors for multi-factor authentication (MFA). The legitimate user can defeat acoustic domain and even cross-domain spoofing samples with the proposed two-stage AirBone authentication. The first stage answers \\textit{whether air and bone conduction utterances are time domain consistent (TC)} and the second stage runs \\textit{bone conduction speaker recognition (BC-SR)}. The security level is hence increased for two reasons: (1) current acoustic attacks on smart voice assistants cannot affect bone conduction, which is in the vibration domain; (2) even for advanced cross-domain attacks, the unique bone conduction features can detect adversary's impersonation and machine-induced vibration. Finally, AirBone authentication has good usability (the same level as voice authentication) compared with traditional MFA and those specially designed to enhance smart voice security. Our experimental results show that the proposed AirBone authentication is usable and secure, and can be easily equipped by commercial off-the-shelf head wearables with good user experience.",
    "published_date": "2023-09-26",
    "pdf_link": "https://arxiv.org/pdf/2309.15203v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Authentication and Access Control",
      "subdomain": "Voice Assistant Security / Anti-spoofing",
      "specific_problem": "Multi-factor authentication for smart voice assistants using concurrent air- and bone-conduction signals to resist spoofing and cross-domain attacks",
      "attack_types": [
        "impersonation",
        "synthetic speech (SS)",
        "voice conversion (VC)",
        "replay",
        "audio injection",
        "cross-domain impersonation",
        "cross-domain SS/VC/replay",
        "noisy environment/background speakers"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Signal processing / cross-modal matching",
        "specific": "Temporal Consistency Scoring (TCS)",
        "novel_contribution": "Time-domain consistency scoring between concurrently captured air- and bone-conduction signals at representative frequencies to verify that both originate from the same vocalization"
      },
      {
        "type": "primary",
        "category": "DNN with adversarial learning",
        "specific": "Domain-adversarial learning (DANN-style) for speaker recognition",
        "novel_contribution": "Learning-based bone-conduction speaker recognition (BC-SR) with pretraining and domain-adversarial learning to distinguish legitimate users from spoofing/machine-induced vibration under BC-domain noise"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Adversarial"
    ],
    "datasets": [
      {
        "name": "Self-collected Air–Bone Conduction Voice Dataset",
        "type": "private",
        "domain": "voice_audio + bone_conducted_vibration (accelerometer)",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "VAuth (Feng et al.)",
        "paper_reference": "[9]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "WearID (Shi et al.)",
        "paper_reference": "[10]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Earprint (Gao et al.)",
        "paper_reference": "[11]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Are air- and bone-conduction utterances from the same vocalization time-domain consistent enough to authenticate a user?",
        "Can bone-conduction speaker recognition uniquely verify the legitimate user as a biometric factor even under cross-domain attacks?",
        "Can a head-wearable–based, voice-only workflow achieve MFA-level security with usability comparable to standard voice authentication?"
      ],
      "gaps_identified": [
        "Prior cross-domain defenses often assume attackers cannot obtain consistent cross-domain signals; this assumption can be broken when the user is absent and the attacker controls the device.",
        "Matching-based methods may require long inputs and degrade under weak signals or noise, harming usability.",
        "Some prior approaches lack a biometric factor and are vulnerable to spoofing.",
        "In-ear voice–based authentication can be contaminated by audio playback from other applications.",
        "Acoustic-domain SR/ASR is in an arms race with increasingly powerful generative attacks (SS/VC)."
      ],
      "limitations": [
        "Assumes hardware (sensors, compute, memory) and wireless channels (e.g., WPA3, TLS, Bluetooth SL) are secure and inviolable.",
        "Assumes the attacker will not physically compromise the system.",
        "Requires head-wearable devices equipped with microphones and motion sensors (accelerometers) and user contact for BC capture.",
        "BC signals can be affected by body motion, gravity, and MEMS noise; relies on preprocessing and adversarial learning to mitigate.",
        "Potential remote sensing (e.g., mmWave) could detect tiny vibrations but requires LOS and is assumed noticeable to the user.",
        "Relies on AC denoising/diarization/source separation in noisy or multi-speaker environments."
      ],
      "future_work": [],
      "motivation": "Defend smart voice assistants against evolving spoofing and cross-domain attacks by leveraging concurrent, user-unique bone-conduction signals as an additional, unobtrusive authentication factor without sacrificing the usability of voice interaction.",
      "potential_research_ideas": [
        "Large-scale, diverse data collection for air–bone pairs across varied demographics, devices, and environments to quantify generalization and robustness.",
        "Contrastive/multi-modal representation learning to jointly embed AC and BC for stronger cross-modal consistency and liveness verification.",
        "Automated detection of machine-induced vibrations and device-surface coupling artifacts using specialized BC spoofing detectors.",
        "Self-supervised pretraining on unlabeled BC vibration data to improve data efficiency and robustness to motion/noise.",
        "Personalized on-device adaptation/federated learning to capture individual BC channel characteristics while preserving privacy.",
        "Evaluation and defenses against emerging cross-domain attack tools (e.g., vibro-acoustic actuators, bone transducers) and remote sensing attacks.",
        "Formal security model and provable bounds for cross-domain consistency under realistic attackers and device variability."
      ],
      "architectural_improvement_recommendations": [
        "Replace hand-crafted TCS with a learnable cross-modal synchronization/consistency network trained with contrastive losses (e.g., InfoNCE) and hard negative mining.",
        "Use multi-branch CNN/Transformer backbones on BC spectrograms with attention-based pooling and domain-adversarial heads for noise/device invariance.",
        "Fuse accelerometer and gyroscope (6-DoF) signals and perform sensor-axis attention to better capture bone vibration patterns.",
        "Incorporate explicit spoofing/machine-vibration classifiers alongside BC-SR (multi-task learning) to detect device-coupled or actuator-induced signals.",
        "Leverage data augmentation specific to BC (e.g., motion contamination, contact pressure variation) and adversarial training to harden against realistic perturbations.",
        "Calibrate decision thresholds with score-level fusion (AC-SV + TCS + BC-SR) using logistic regression or Bayesian fusion for better operating points."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Head-wearable devices (earbuds, smart glasses, VR headsets) with microphones and accelerometers; authentication server integrated with voice services",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Requirement for concurrent AC and BC capture and reliable time alignment/synchronization.",
        "Dependence on good skin/device contact quality for BC sensing; variability across users and devices.",
        "Sensitivity to body motion and MEMS noise in BC; need for robust preprocessing and modeling.",
        "Handling background speakers and environmental noise on AC via diarization/source separation.",
        "Mandatory availability of motion sensors on wearables; heterogeneity across COTS devices.",
        "Secure channel and device integrity assumptions must hold in deployment."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": true,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "A two-stage AirBone authentication scheme for head-wearable smart voice assistants using concurrent air- and bone-conduction signals, requiring only speech from the user.",
      "Stage I: Temporal Consistency Scoring (TCS) to verify that AC and BC signals originate from the same vocalization; Stage II: Bone-Conduction Speaker Recognition (BC-SR) with pretraining and domain-adversarial learning to verify the legitimate user.",
      "Extensive experiments under noisy and spoofing scenarios demonstrating improved security and usability compared to recent peer designs, and feasibility on commercial off-the-shelf wearables."
    ]
  },
  {
    "arxiv_id": "2309.12720v1",
    "title": "Towards a Near-real-time Protocol Tunneling Detector based on Machine Learning Techniques",
    "authors": "Filippo Sobrero; Beatrice Clavarezza; Daniele Ucci; Federica Bisio",
    "abstract": "In the very last years, cybersecurity attacks have increased at an unprecedented pace, becoming ever more sophisticated and costly. Their impact has involved both private/public companies and critical infrastructures. At the same time, due to the COVID-19 pandemic, the security perimeters of many organizations expanded, causing an increase of the attack surface exploitable by threat actors through malware and phishing attacks. Given these factors, it is of primary importance to monitor the security perimeter and the events occurring in the monitored network, according to a tested security strategy of detection and response. In this paper, we present a protocol tunneling detector prototype which inspects, in near real time, a company's network traffic using machine learning techniques. Indeed, tunneling attacks allow malicious actors to maximize the time in which their activity remains undetected. The detector monitors unencrypted network flows and extracts features to detect possible occurring attacks and anomalies, by combining machine learning and deep learning. The proposed module can be embedded in any network security monitoring platform able to provide network flow information along with its metadata. The detection capabilities of the implemented prototype have been tested both on benign and malicious datasets. Results show 97.1% overall accuracy and an F1-score equals to 95.6%.",
    "published_date": "2023-09-22",
    "pdf_link": "https://arxiv.org/pdf/2309.12720v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Detection of protocol tunneling in cleartext traffic (with emphasis on DNS tunneling) and identification of compressed/encrypted payloads within ostensibly unencrypted connections",
      "attack_types": [
        "Protocol tunneling",
        "DNS tunneling",
        "Covert data exfiltration",
        "Command-and-control communications"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Feedforward Neural Network (MLP)",
        "specific": null,
        "novel_contribution": "Packet-level byte-sequence classification of cleartext protocols to detect interleaving/mixed protocols indicative of tunneling; modified input byte sequence design and sizes compared to [21]."
      },
      {
        "type": "primary",
        "category": "SVM",
        "specific": "One-class SVM",
        "novel_contribution": "Per-protocol one-class SVMs used for input sanitization to remove outliers/mislabeled packets from training data."
      },
      {
        "type": "primary",
        "category": "SVM",
        "specific": "Supervised SVM (one-vs-all)",
        "novel_contribution": "Classifier to detect compressed/encrypted packets within cleartext connections for downstream anomaly signaling."
      },
      {
        "type": "primary",
        "category": "Resampling/Data Augmentation",
        "specific": "SMOTE",
        "novel_contribution": "Applied to address class imbalance after sanitization when training the supervised models."
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "Benign enterprise network traffic captures",
        "type": "proprietary",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Simulated DNS tunneling traffic",
        "type": "synthetic",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "accuracy",
      "F1-score"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can a near-real-time detector using ANN on packet byte sequences reliably identify protocol tunneling by spotting interleaving/mixed cleartext protocols?",
        "How effectively can one-class SVM-based input sanitization remove mislabeled/outlier packets to improve downstream model accuracy?",
        "Can a supervised SVM detect compressed/encrypted payloads within nominally cleartext connections to flag anomalies?"
      ],
      "gaps_identified": [
        "Need for near-real-time detection of tunneling in cleartext protocols beyond rule-based approaches.",
        "Prior works often focus on DNS tunneling using CNN/RNN; fewer approaches examine simple ANNs on packet byte sequences and combined ML+DL pipelines.",
        "Labeling of traffic via ports/metadata is error-prone (custom ports, proxies), necessitating data sanitization.",
        "Cleartext protocol analysis is still needed while most traffic is encrypted; bridging analysis between cleartext and encrypted flows remains a gap."
      ],
      "limitations": [
        "Module monitors only cleartext protocols; encrypted traffic handled by a separate analytics outside this paper.",
        "Initial labeling may be noisy; requires sanitization via one-class SVMs.",
        "Only 5 NIST randomness tests used due to sequence length constraints.",
        "Training set is unbalanced; requires SMOTE augmentation.",
        "Proxy environments may introduce encrypted packets in HTTP-labeled flows, complicating labeling and detection.",
        "Limited disclosure of datasets and no public availability; external reproducibility is constrained.",
        "No quantitative comparison against competing methods; baseline benchmarking absent."
      ],
      "future_work": [],
      "motivation": "Provide an automated, near-real-time detector for protocol tunneling that can integrate into network security monitoring platforms to reduce dwell time of covert communications.",
      "potential_research_ideas": [
        "Model flow-level context: augment per-packet ANN with sequence models (1D CNNs, Temporal CNNs, Transformers) over packet sequences to detect subtle tunneling behavior.",
        "Self-supervised pretraining on raw packet bytes (contrastive or masked-byte modeling) to improve feature learning and reduce labeled data needs.",
        "Unified multi-view model combining byte-level features with flow/session metadata (timing, sizes, directionality) for robustness.",
        "Online/continual learning to adapt to evolving benign protocol patterns and concept drift.",
        "Robust tunneling detection for additional protocols (e.g., ICMP, HTTP/2, QUIC) and multi-layer tunneling scenarios.",
        "Adversarial robustness evaluation (evasion/poisoning) and defenses (adversarial training, robust statistics) for ML-driven NIDS.",
        "Active learning loop for analyst-in-the-loop labeling to improve sanitization and reduce false positives.",
        "Create and release standardized tunneling datasets with rich labels to enable fair benchmarking."
      ],
      "architectural_improvement_recommendations": [
        "Replace/augment the MLP with lightweight temporal models (Temporal CNN or small Transformer) on sequences of packet bytes per flow to capture ordering/context.",
        "Two-stage pipeline: (1) byte-level feature extractor (CNN/Transformer) pretrained self-supervised; (2) protocol/tunneling head fine-tuned supervised.",
        "Feature fusion: concatenate NIST/entropy features with learned embeddings from byte models for improved separability.",
        "Calibrated uncertainty (e.g., temperature scaling) to better flag low-confidence packets/flows for analyst review.",
        "Hard negative mining from proxied/mislabeled traffic to strengthen the compression/encryption detector.",
        "Automate sanitization with isolation forests or deep SVDD as alternatives to one-class SVM for scalability.",
        "Evaluate and, if helpful, switch SMOTE to more suitable imbalanced-learn strategies (SMOTE-ENN, ADASYN) to reduce overfitting."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Commercial network security monitoring platform context (enterprise network); designed for near-real-time packet inspection of cleartext protocols",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "High prevalence of encrypted traffic reduces coverage of cleartext-only detector.",
        "Port-based/mislabeling due to proxies and custom configurations complicates ground truth and may trigger false positives.",
        "Imbalanced datasets require careful resampling and validation.",
        "Per-packet analysis may miss flow-level temporal patterns; needs aggregation.",
        "NIST test subset and short byte sequences may limit discrimination between compressed and encrypted payloads."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Implemented a near-real-time protocol tunneling detector that analyzes packet-level byte sequences of cleartext protocols.",
      "Combined an ANN for cleartext protocol classification with an SVM-based detector for compressed/encrypted payloads within nominally cleartext connections.",
      "Designed an input sanitization module using per-protocol one-class SVMs and a supervised SVM to remove outliers and mislabeled/compressed/encrypted packets from training data, improving accuracy (\"7% for the ANN and 20% for compression/encryption detector\").",
      "Modified the input byte sequences and sizes relative to prior work [21].",
      "Demonstrated performance: \"Results show 97.1% overall accuracy and an F1-score equals to 95.6%.\"",
      "Prototype integrable into network security monitoring platforms and complementary to encrypted-traffic analytics."
    ]
  },
  {
    "arxiv_id": "2310.01878v1",
    "title": "Enhancing Workflow Security in Multi-Cloud Environments through Monitoring and Adaptation upon Cloud Service and Network Security Violations",
    "authors": "Nafiseh Soveizi; Dimka Karastoyanova",
    "abstract": "Cloud computing has emerged as a crucial solution for handling data- and compute-intensive workflows, offering scalability to address dynamic demands. However, ensuring the secure execution of workflows in the untrusted multi-cloud environment poses significant challenges, given the sensitive nature of the involved data and tasks. The lack of comprehensive approaches for detecting attacks during workflow execution, coupled with inadequate measures for reacting to security and privacy breaches has been identified in the literature. To close this gap, in this work, we propose an approach that focuses on monitoring cloud services and networks to detect security violations during workflow executions. Upon detection, our approach selects the optimal adaptation action to minimize the impact on the workflow. To mitigate the uncertain cost associated with such adaptations and their potential impact on other tasks in the workflow, we employ adaptive learning to determine the most suitable adaptation action. Our approach is evaluated based on the performance of the detection procedure and the impact of the selected adaptations on the workflows.",
    "published_date": "2023-10-03",
    "pdf_link": "https://arxiv.org/pdf/2310.01878v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Cloud Security",
      "subdomain": "Workflow Security in Multi-Cloud",
      "specific_problem": "Monitoring and adaptive response to cloud service and network security violations during workflow execution",
      "attack_types": [
        "network attacks",
        "service-level attacks",
        "VM-based attacks",
        "CIA violations (confidentiality, integrity, availability)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Ensemble/Tree-based",
        "specific": "Random Forest",
        "novel_contribution": "Used to train an attack detection model over network traffic and cloud log data"
      },
      {
        "type": "primary",
        "category": "Linear Model",
        "specific": "Linear Regression",
        "novel_contribution": "Used alongside Random Forest in training the attack detection model"
      },
      {
        "type": "primary",
        "category": "Clustering",
        "specific": "K-means",
        "novel_contribution": "Used to learn an attack severity model for multiple attack types"
      },
      {
        "type": "primary",
        "category": "Other",
        "specific": null,
        "novel_contribution": "Adaptive learning model to recommend adaptation actions using past adaptations, current workflow state, and dependencies"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised",
      "Online/Adaptive"
    ],
    "datasets": [
      {
        "name": "Network Traffic Data (NTD)",
        "type": "private",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Cloud Log File (CLF)",
        "type": "private",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Accuracy"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How can cloud services and network infrastructure be monitored to detect security violations during workflow execution in untrusted multi-cloud environments?",
        "How to select the optimal adaptation action upon detection to minimize workflow impact given uncertain costs and cascading effects?",
        "Can adaptive learning from past adaptations improve adaptation decisions under uncertainty?"
      ],
      "gaps_identified": [
        "Lack of comprehensive approaches for detecting attacks during workflow execution in multi-cloud workflows",
        "Inadequate measures for reacting to security and privacy breaches in existing WfMSs",
        "Existing works often rely solely on cloud-side or engine-side monitoring and focus on limited violation types or only task failures",
        "Only a few prior works cover the full CIA triad; none comprehensively tackle all potential attacks against outsourced workflow tasks",
        "Absence of structured, diverse adaptation options; typical works implement only a single reaction strategy"
      ],
      "limitations": [
        "Paper scope limits focus to cloud service and network violations; tenant/user-originated detection modules are not detailed",
        "Uncertain overhead costs for adaptation actions cannot be predetermined and vary by workflow and state",
        "Details on datasets, concrete attack types, and quantitative evaluation are limited in the provided text"
      ],
      "future_work": [],
      "motivation": "Enable secure execution of data- and compute-intensive workflows in untrusted multi-cloud environments by detecting service/network violations and adaptively mitigating their impact under uncertainty.",
      "potential_research_ideas": [
        "Formulate adaptation decision as a contextual bandit or reinforcement learning problem with delayed and cascading costs",
        "Integrate causal inference to estimate counterfactual impact of adaptation actions on workflow KPIs",
        "Leverage graph-based models (e.g., GNNs) over workflow DAGs to capture dependencies for both detection and adaptation selection",
        "Incorporate federated or privacy-preserving learning across tenants to improve detection models without sharing raw data",
        "Augment detection with modern time-series/sequence models (e.g., Temporal CNNs, Transformers) on logs and traffic for richer context",
        "Develop standardized, open multi-cloud workflow security benchmarks with labeled violations and adaptation outcomes",
        "Add probabilistic risk models (e.g., Bayesian networks) to quantify uncertainty and propagate risk through workflow states"
      ],
      "architectural_improvement_recommendations": [
        "Replace/augment Linear Regression with gradient boosting or calibrated probabilistic classifiers; use model stacking with RF and boosting",
        "Adopt an ensemble of supervised detectors with unsupervised anomaly detectors (e.g., Isolation Forest, Autoencoders) for zero-day attacks",
        "Design a policy network for adaptation using actor-critic RL with reward shaping from price, time, value, and mitigation impact",
        "Model workflow state as a graph and use GNN-based policy/value functions for adaptation decisions",
        "Introduce confidence calibration and thresholding for detection to reduce false positives/negatives feeding adaptation",
        "Implement online learning with drift detection to handle provider/service behavior changes",
        "Create a simulation/digital twin of multi-cloud workflows to pre-train adaptation policies and stress-test scenarios"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Multi-cloud environment with centralized middleware and per-tenant kernels",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Trustworthiness and completeness of cloud-side logs and telemetry",
        "Heterogeneity across cloud providers and services complicating unified monitoring",
        "Uncertain and cascading costs of adaptation actions affecting workflow KPIs",
        "Overhead of monitoring and adaptation on workflow performance",
        "Maintaining tenant isolation while leveraging shared middleware intelligence"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": true,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes SecFlow-based architecture for monitoring and adapting to cloud service and network security violations during workflow execution",
      "Dual-level adaptation (tenant-level: Skip/Switch/Insert; middleware-level: Reconfiguration/Rework/Redundancy) with provider trust updates",
      "Attack detection trained on network traffic and cloud logs using Random Forest and Linear Regression",
      "Attack severity modeling using K-means clustering",
      "Adaptive learning-based adaptation recommendation that accounts for uncertain costs, workflow state, and dependencies",
      "Integration of monitoring outputs into a tenant Adaptation Decision Engine with two strategies: heuristic cost/impact minimization and adaptive model selection"
    ]
  },
  {
    "arxiv_id": "2310.03554v1",
    "title": "Digital Twin-Empowered Smart Attack Detection System for 6G Edge of Things Networks",
    "authors": "Yagmur Yigit; Christos Chrysoulas; Gokhan Yurdakul; Leandros Maglaras; Berk Canberk",
    "abstract": "As global Internet of Things (IoT) devices connectivity surges, a significant portion gravitates towards the Edge of Things (EoT) network. This shift prompts businesses to deploy infrastructure closer to end-users, enhancing accessibility. However, the growing EoT network expands the attack surface, necessitating robust and proactive security measures. Traditional solutions fall short against dynamic EoT threats, highlighting the need for proactive and intelligent systems. We introduce a digital twin-empowered smart attack detection system for 6G EoT networks. Leveraging digital twin and edge computing, it monitors and simulates physical assets in real time, enhancing security. An online learning module in the proposed system optimizes the network performance. Our system excels in proactive threat detection, ensuring 6G EoT network security. The performance evaluations demonstrate its effectiveness, robustness, and adaptability using real datasets.",
    "published_date": "2023-10-05",
    "pdf_link": "https://arxiv.org/pdf/2310.03554v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "IoT/IIoT Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Proactive attack/anomaly detection at the 6G Edge of Things using a digital twin-enabled, online-learning system that adaptively selects features and classifiers",
      "attack_types": [
        "Backdoor",
        "DDoS HTTP",
        "DDoS UDP",
        "Fingerprinting",
        "Man-in-the-Middle (MITM)",
        "Password attack",
        "Port scanning",
        "Ransomware",
        "SQL injection",
        "Cross-site scripting (XSS)",
        "Injection",
        "Scanning",
        "DDoS"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "AutoML / Model Selection",
        "specific": "AutoCM",
        "novel_contribution": "Online-learning driven automatic selection of the best classifier from 10 algorithms in near real-time to adapt to changing EoT traffic"
      },
      {
        "type": "primary",
        "category": "Feature Selection",
        "specific": "AutoFS",
        "novel_contribution": "Online-learning driven automatic selection of the best feature selection method from 5 techniques with dynamic feature subset updates"
      },
      {
        "type": "primary",
        "category": "Online Learning",
        "specific": "Near real-time update loop (Detection Performance Module -> Online Learning Module -> AutoCM -> AutoFS)",
        "novel_contribution": "Adaptive reliability-thresholding triggers on-the-fly re-selection of FS and classifier using a labeling procedure that blends unlabeled live data with a baseline dataset"
      },
      {
        "type": "baseline",
        "category": "RNN / Autoencoder",
        "specific": "LSTM Autoencoder (LSTM-AE)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Online Learning",
      "Semi-supervised"
    ],
    "datasets": [
      {
        "name": "Edge-IIoTset",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      },
      {
        "name": "ToN-IoT",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      }
    ],
    "baselines": [
      {
        "method_name": "LSTM-AE",
        "paper_reference": null,
        "metric": "Detection Rate (%) (Train: ToN-IoT, Test: Edge-IIoT) - Password Attack",
        "their_result": "99.46%",
        "baseline_result": "91.94%"
      },
      {
        "method_name": "LSTM-AE",
        "paper_reference": null,
        "metric": "Detection Rate (%) (Train: ToN-IoT, Test: Edge-IIoT) - Port Scanning Attack",
        "their_result": "96.89%",
        "baseline_result": "81.79%"
      },
      {
        "method_name": "LSTM-AE",
        "paper_reference": null,
        "metric": "Detection Rate (%) (Train: ToN-IoT, Test: Edge-IIoT) - DDoS UDP Attack",
        "their_result": "99.24%",
        "baseline_result": "92.64%"
      },
      {
        "method_name": "LSTM-AE",
        "paper_reference": null,
        "metric": "Detection Rate (%) (Train: ToN-IoT, Test: Edge-IIoT) - XSS Attack",
        "their_result": "99.82%",
        "baseline_result": "93.27%"
      },
      {
        "method_name": "LSTM-AE",
        "paper_reference": null,
        "metric": "Detection Rate (%) (Train: ToN-IoT, Test: Edge-IIoT) - MITM Attack",
        "their_result": "99.36%",
        "baseline_result": "91.08%"
      },
      {
        "method_name": "LSTM-AE",
        "paper_reference": null,
        "metric": "Detection Rate (%) (Train: ToN-IoT, Test: Edge-IIoT) - Backdoor Attack",
        "their_result": "99.18%",
        "baseline_result": "90.39%"
      },
      {
        "method_name": "LSTM-AE",
        "paper_reference": null,
        "metric": "Detection Rate (%) (Train: ToN-IoT, Test: Edge-IIoT) - Fingerprinting Attack",
        "their_result": "97.02%",
        "baseline_result": "87.38%"
      },
      {
        "method_name": "LSTM-AE",
        "paper_reference": null,
        "metric": "Detection Rate (%) (Train: ToN-IoT, Test: Edge-IIoT) - SQL Injection Attack",
        "their_result": "97.34%",
        "baseline_result": "91.86%"
      },
      {
        "method_name": "LSTM-AE",
        "paper_reference": null,
        "metric": "Detection Rate (%) (Train: ToN-IoT, Test: Edge-IIoT) - Ransomware Attack",
        "their_result": "98.62%",
        "baseline_result": "87.26%"
      },
      {
        "method_name": "LSTM-AE",
        "paper_reference": null,
        "metric": "Detection Rate (%) (Train: ToN-IoT, Test: Edge-IIoT) - DDoS HTTP Attack",
        "their_result": "99.36%",
        "baseline_result": "94.15%"
      },
      {
        "method_name": "LSTM-AE",
        "paper_reference": null,
        "metric": "Detection Rate (%) (Train: Edge-IIoT, Test: ToN-IoT) - DDoS Attack",
        "their_result": "99.75%",
        "baseline_result": "90.75%"
      },
      {
        "method_name": "LSTM-AE",
        "paper_reference": null,
        "metric": "Detection Rate (%) (Train: Edge-IIoT, Test: ToN-IoT) - Injection Attack",
        "their_result": "98.92%",
        "baseline_result": "88.92%"
      },
      {
        "method_name": "LSTM-AE",
        "paper_reference": null,
        "metric": "Detection Rate (%) (Train: Edge-IIoT, Test: ToN-IoT) - Ransomware Attack",
        "their_result": "98.25%",
        "baseline_result": "79.82%"
      },
      {
        "method_name": "LSTM-AE",
        "paper_reference": null,
        "metric": "Detection Rate (%) (Train: Edge-IIoT, Test: ToN-IoT) - XSS Attack",
        "their_result": "98.04%",
        "baseline_result": "89.79%"
      },
      {
        "method_name": "LSTM-AE",
        "paper_reference": null,
        "metric": "Detection Rate (%) (Train: Edge-IIoT, Test: ToN-IoT) - Backdoor Attack",
        "their_result": "97.86%",
        "baseline_result": "82.96%"
      },
      {
        "method_name": "LSTM-AE",
        "paper_reference": null,
        "metric": "Detection Rate (%) (Train: Edge-IIoT, Test: ToN-IoT) - Scanning Attack",
        "their_result": "97.35%",
        "baseline_result": "85.24%"
      },
      {
        "method_name": "LSTM-AE",
        "paper_reference": null,
        "metric": "Detection Rate (%) (Train: Edge-IIoT, Test: ToN-IoT) - Password Attack",
        "their_result": "96.80%",
        "baseline_result": "92.58%"
      }
    ],
    "performance_metrics_used": [
      "Sensitivity (Recall)",
      "Detection Rate (%)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Traditional solutions fall short against dynamic EoT threats; need for proactive and intelligent systems.",
        "6G edge introduces new attack vectors on local infrastructure and users, requiring comprehensive defense strategies.",
        "Ground-truth scarcity in real-world EoT traffic complicates accurate estimation of TP/FN."
      ],
      "limitations": [
        "Determination of FN and TP in real-world scenarios without ground truth is difficult; the paper estimates these via a labeling method combining unlabeled data with a baseline dataset.",
        "Evaluation is performed on subsets of two public datasets and a simulated edge network (NS-3) rather than a live deployment.",
        "Exact list of classifiers and feature selection algorithms within AutoCM/AutoFS is not disclosed, limiting reproducibility."
      ],
      "future_work": [],
      "motivation": "EoT growth increases the attack surface; the paper aims to provide a proactive, adaptive, digital twin-empowered detection system for 6G EoT networks with online learning to maintain performance under dynamic threats.",
      "potential_research_ideas": [
        "Integrate federated/edge collaborative learning with the digital twin to enable privacy-preserving cross-edge model updates while retaining the online re-selection capability.",
        "Augment the detection with multimodal DT signals (e.g., system logs, device telemetry, control-plane data) and perform cross-twin correlation for attack attribution.",
        "Develop a robust labeling mechanism using weak supervision or positive-unlabeled learning to reduce reliance on a baseline dataset composition.",
        "Investigate adversarial robustness of the online selection loop (e.g., poisoning of the reliability metric or labeling algorithm) and design defenses.",
        "Extend to continual learning with concept drift detection explicitly modeled within the DT to trigger targeted retraining rather than full reselection."
      ],
      "architectural_improvement_recommendations": [
        "Expose and standardize the pool of candidate classifiers/FS methods and add lightweight deep models (e.g., 1D CNN, temporal transformers) optimized for edge inference.",
        "Incorporate drift detection (e.g., ADWIN, DDM) to inform the reliability threshold adjustment and reduce unnecessary model switches.",
        "Cache multiple top-performing model/FS pairs per traffic context in the DT and hot-swap based on context detection to minimize re-training latency.",
        "Add explainability components (e.g., SHAP for tabular features) to support operator trust and faster mitigation decisions.",
        "Use streaming feature computation and incremental learners where possible (e.g., online SVM, Hoeffding trees) to reduce update time on resource-constrained edge nodes."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "NS-3",
        "Microsoft Azure Digital Twins"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Simulated 6G Edge of Things network (NS-3) with Azure Digital Twins representing the edge layer",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Ground-truth scarcity in operational networks requires estimating TP/FN via labeling.",
        "Balancing false positives and false negatives through adaptive thresholding.",
        "Operating under edge resource constraints while performing near real-time updates.",
        "Integration and standardization using YANG models for data ingestion."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Digital twin-empowered smart attack detection system integrated into the edge network for proactive 6G EoT security.",
      "Dynamic and adaptive approach that continuously updates feature selection and classification methods via an online learning module.",
      "Reliability-based trigger and adaptive thresholding to balance FP and FN, enabling near real-time adaptation.",
      "Empirical evaluation on Edge-IIoTset and ToN-IoT datasets showing improved detection rates over an LSTM-AE baseline.",
      "Attack mitigation module with automated blocking and isolation policies guided by detected risk level."
    ]
  },
  {
    "arxiv_id": "2310.01850v1",
    "title": "Multi-class Network Intrusion Detection with Class Imbalance via LSTM & SMOTE",
    "authors": "Muhammad Wasim Nawaz; Rashid Munawar; Ahsan Mehmood; Muhammad Mahboob Ur Rahman; Qammer H. Abbasi",
    "abstract": "Monitoring network traffic to maintain the quality of service (QoS) and to detect network intrusions in a timely and efficient manner is essential. As network traffic is sequential, recurrent neural networks (RNNs) such as long short-term memory (LSTM) are suitable for building network intrusion detection systems. However, in the case of a few dataset examples of the rare attack types, even these networks perform poorly. This paper proposes to use oversampling techniques along with appropriate loss functions to handle class imbalance for the detection of various types of network intrusions. Our deep learning model employs LSTM with fully connected layers to perform multi-class classification of network attacks. We enhance the representation of minority classes: i) through the application of the Synthetic Minority Over-sampling Technique (SMOTE), and ii) by employing categorical focal cross-entropy loss to apply a focal factor to down-weight examples of the majority classes and focus more on hard examples of the minority classes. Extensive experiments on KDD99 and CICIDS2017 datasets show promising results in detecting network intrusions (with many rare attack types, e.g., Probe, R2L, DDoS, PortScan, etc.).",
    "published_date": "2023-10-03",
    "pdf_link": "https://arxiv.org/pdf/2310.01850v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Multi-class network intrusion detection under severe class imbalance",
      "attack_types": [
        "DoS",
        "DDoS",
        "Probe",
        "PortScan",
        "R2L",
        "U2R",
        "Brute Force",
        "Botnet",
        "Heartbleed",
        "Web Attack",
        "Normal"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "RNN",
        "specific": "LSTM",
        "novel_contribution": "Hybrid architecture integrating LSTM with fully connected and dropout layers for multi-class NIDS"
      },
      {
        "type": "primary",
        "category": "Oversampling",
        "specific": "SMOTE",
        "novel_contribution": "Applied to minority attack classes to balance training distribution"
      },
      {
        "type": "primary",
        "category": "Loss Function",
        "specific": "Categorical Focal Cross-Entropy",
        "novel_contribution": "Class-weighted focal factor to down-weight majority classes and focus on hard minority examples"
      },
      {
        "type": "primary",
        "category": "Optimizer",
        "specific": "Adam",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "SVM",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Decision Tree",
        "specific": "J48 (C4.5)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Naive Bayes",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Ensemble",
        "specific": "Random Forest",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "RNN",
        "specific": "LSTM (conventional)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "RNN",
        "specific": "Bidirectional LSTM",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "KDD99 (KDD Cup 1999)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CICIDS2017",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "SVM",
        "paper_reference": null,
        "metric": "ACC",
        "their_result": "98.83%",
        "baseline_result": "77.12%"
      },
      {
        "method_name": "SVM",
        "paper_reference": null,
        "metric": "TPR (Recall)",
        "their_result": "95.4%",
        "baseline_result": "77.12%"
      },
      {
        "method_name": "SVM",
        "paper_reference": null,
        "metric": "F1",
        "their_result": "89.17%",
        "baseline_result": "73.90%"
      },
      {
        "method_name": "J48 (C4.5)",
        "paper_reference": null,
        "metric": "ACC",
        "their_result": "98.83%",
        "baseline_result": "75.23%"
      },
      {
        "method_name": "J48 (C4.5)",
        "paper_reference": null,
        "metric": "TPR (Recall)",
        "their_result": "95.4%",
        "baseline_result": "75.23%"
      },
      {
        "method_name": "J48 (C4.5)",
        "paper_reference": null,
        "metric": "F1",
        "their_result": "89.17%",
        "baseline_result": "71.3%"
      },
      {
        "method_name": "Naive Bayes",
        "paper_reference": null,
        "metric": "ACC",
        "their_result": "98.83%",
        "baseline_result": "71.48%"
      },
      {
        "method_name": "Naive Bayes",
        "paper_reference": null,
        "metric": "TPR (Recall)",
        "their_result": "95.4%",
        "baseline_result": "71.50%"
      },
      {
        "method_name": "Naive Bayes",
        "paper_reference": null,
        "metric": "F1",
        "their_result": "89.17%",
        "baseline_result": "71.40%"
      },
      {
        "method_name": "Conventional LSTM",
        "paper_reference": null,
        "metric": "ACC",
        "their_result": "98.83%",
        "baseline_result": "87.3%"
      },
      {
        "method_name": "Bidirectional LSTM [35]",
        "paper_reference": "[35]",
        "metric": "ACC",
        "their_result": "98.83%",
        "baseline_result": "91.36%"
      },
      {
        "method_name": "Bidirectional LSTM [35]",
        "paper_reference": "[35]",
        "metric": "PPV (Precision)",
        "their_result": null,
        "baseline_result": "85.81%"
      }
    ],
    "performance_metrics_used": [
      "Accuracy (ACC)",
      "Precision (PPV)",
      "Recall (TPR)",
      "F1-score",
      "Confusion Matrix"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "KDD99 and CICIDS2017 datasets are highly imbalanced with many rare attack types, challenging minority class detection.",
        "Even RNNs like LSTM perform poorly when few dataset examples exist for rare attack types."
      ],
      "limitations": [
        "Performance drop on minority classes in KDD99 test: R2L (PPV 0.74, TPR 0.98, F1 0.84) and U2R (PPV 0.52, TPR 0.80, F1 0.63).",
        "Results and evaluation focus on two benchmarks; no cross-network or online deployment evaluation is reported."
      ],
      "future_work": [],
      "motivation": "Improve multi-class network intrusion detection by addressing severe class imbalance using LSTM with SMOTE oversampling and focal loss to better detect rare attacks.",
      "potential_research_ideas": [
        "Use class-conditional generative models (e.g., GANs/VAEs for tabular data) to synthesize rare attack flows as an alternative to SMOTE.",
        "Incorporate attention mechanisms or transformer encoders to capture long-range temporal dependencies in flow sequences.",
        "Apply class-balanced loss based on effective number of samples or cost-sensitive learning in combination with focal loss.",
        "Calibrate per-class decision thresholds and optimize macro-averaged metrics to further boost minority-class F1.",
        "Leverage self-supervised or semi-supervised pretraining on large unlabeled traffic to improve minority-class representations.",
        "Evaluate domain adaptation and transfer learning across datasets (e.g., KDD99 → CICIDS2017) and across time to assess generalization.",
        "Combine anomaly detection front-end with supervised multiclass classifier to reduce false negatives on rare classes.",
        "Investigate online/streaming training with dynamic class imbalance and concept drift handling."
      ],
      "architectural_improvement_recommendations": [
        "Adopt stacked or bidirectional LSTMs with residual connections and attention layers for better sequence modeling.",
        "Evaluate Temporal Convolutional Networks (TCNs) or lightweight transformers for improved throughput and latency.",
        "Learn embeddings for categorical features and apply feature-wise attention to enhance interpretability and performance.",
        "Tune focal loss hyperparameters (gamma, class-wise alpha) via Bayesian optimization with macro-F1 as objective.",
        "Replace or complement SMOTE with class-balanced sampling or mixup/cutmix for tabular data.",
        "Ensemble the LSTM with tree-based models (e.g., XGBoost) to capture nonlinear tabular interactions.",
        "Introduce hierarchical classification (normal vs. attack → attack family) to decompose the task and aid rare subclass detection.",
        "Apply probability calibration (temperature scaling) and per-class thresholding to control precision-recall trade-offs."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Batch size 1024; 30 epochs; Adam optimizer; datasets: KDD99 and CICIDS2017. No GPU/compute details reported."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Handling severe class imbalance and reliably detecting rare attack types (e.g., R2L, U2R) in production traffic.",
        "Potential generalization gaps across networks/datasets not evaluated."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Apply SMOTE to improve minority-class representation in intrusion detection datasets.",
      "Propose a hybrid LSTM architecture with fully connected and dropout layers for multi-class NIDS.",
      "Use Categorical Focal Cross-Entropy loss to emphasize minority classes and hard examples.",
      "Conduct extensive experiments on CICIDS2017 and KDD99 demonstrating adaptability across contexts.",
      "Outperform compared techniques in accuracy, precision, recall, and F1-score on KDD99.",
      "Demonstrate generalization across multiple intrusion types (DoS, DDoS, Probe, Web Attack, Bot, PortScan, R2L, U2R)."
    ]
  },
  {
    "arxiv_id": "2310.00843v1",
    "title": "Prov2vec: Learning Provenance Graph Representation for Unsupervised APT Detection",
    "authors": "Bibek Bhattarai; H. Howie Huang",
    "abstract": "Modern cyber attackers use advanced zero-day exploits, highly targeted spear phishing, and other social engineering techniques to gain access and also use evasion techniques to maintain a prolonged presence within the victim network while working gradually towards the objective. To minimize the damage, it is necessary to detect these Advanced Persistent Threats as early in the campaign as possible. This paper proposes, Prov2Vec, a system for the continuous monitoring of enterprise host's behavior to detect attackers' activities. It leverages the data provenance graph built using system event logs to get complete visibility into the execution state of an enterprise host and the causal relationship between system entities. It proposes a novel provenance graph kernel to obtain the canonical representation of the system behavior, which is compared against its historical behaviors and that of other hosts to detect the deviation from the normality. These representations are used in several machine learning models to evaluate their ability to capture the underlying behavior of an endpoint host. We have empirically demonstrated that the provenance graph kernel produces a much more compact representation compared to existing methods while improving prediction ability.",
    "published_date": "2023-10-02",
    "pdf_link": "https://arxiv.org/pdf/2310.00843v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Host/Endpoint Security",
      "subdomain": "Advanced Persistent Threat (APT) Detection",
      "specific_problem": "Unsupervised anomaly detection on host provenance graphs to identify compromised hosts",
      "attack_types": [
        "APT",
        "Internal Reconnaissance",
        "Privilege Escalation",
        "Lateral Movement",
        "Data Exfiltration"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Graph Kernel",
        "specific": "Provenance graph kernel using label-aware backward walks with set aggregation",
        "novel_contribution": "Canonical representation of provenance graph snapshots by stacking set-aggregated labels over 0..h-length backward walks; produces compact node-label histograms that generalize similar behaviors"
      },
      {
        "type": "primary",
        "category": "Sketching/Hashing",
        "specific": "Histosketch (Consistent Weighted Hashing/Sampling)",
        "novel_contribution": "Converts variable-sized node-label histograms to fixed-size vectors for streaming similarity while preserving similarity structure"
      },
      {
        "type": "baseline",
        "category": "Graph Kernel",
        "specific": "Weisfeiler–Lehman (WL) Subtree kernel",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Graph Kernel",
        "specific": "Unicorn: temporally ordered WL subtree kernel",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Unsupervised",
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Windows host provenance graphs (from system event logs)",
        "type": "private",
        "domain": "provenance_graphs",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Linux host provenance graphs (from system event logs)",
        "type": "private",
        "domain": "provenance_graphs",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "WL Subtree kernel",
        "paper_reference": "[60]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Unicorn (temporally ordered WL subtree kernel)",
        "paper_reference": "[20]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can a compact, canonical representation of provenance graph snapshots enable accurate unsupervised detection of compromised hosts?",
        "Does set-aggregation over label-aware backward walks improve generalization of similar behaviors compared to WL and temporal WL kernels?",
        "Can streaming histosketches support scalable, continuous monitoring across enterprise hosts?"
      ],
      "gaps_identified": [
        "Rule-based EDR alerts generate high false positives and are limited by rule coverage and platform-specific semantics.",
        "Fine-grained alert correlation over long-running APT campaigns is computationally prohibitive for real-time, network-wide deployment.",
        "Sequence-only anomaly detection fails to capture causal relationships among system entities and can miss 'low-and-slow' attacks."
      ],
      "limitations": [
        "Assumes correctness of log collection frameworks and audit data.",
        "Assumes sufficient historical benign data to profile host behavior.",
        "Assumes that similar provenance graph structures indicate comparable operational behavior.",
        "Runtime scales as O(h^2 |ΔE|), though h is typically small (≤4)."
      ],
      "future_work": [],
      "motivation": "Detect APTs early by continuously modeling host behavior via provenance graphs to overcome limitations of rule-based alerts and computationally heavy universal alert correlation.",
      "potential_research_ideas": [
        "Integrate temporal dynamics explicitly (e.g., time-decayed or time-bucketed provenance labels) to capture evolving behaviors without overfitting to exact order.",
        "Cross-host correlation using multi-graph or hypergraph representations to jointly detect coordinated activities and lateral movement.",
        "Adversarially robust provenance representations resilient to log manipulation or missing events.",
        "Online concept drift detection to adapt profiles as benign software updates or workload changes occur.",
        "Semi-supervised or weakly supervised learning that leverages occasional analyst feedback to refine anomaly thresholds and reduce false positives."
      ],
      "architectural_improvement_recommendations": [
        "Augment provenance kernel with lightweight temporal features (e.g., inter-arrival times, burstiness) as additional histogram channels before sketching.",
        "Hierarchical sketching: per-entity-type histosketches combined via learned weighting to improve discrimination while keeping footprint low.",
        "Hybrid model: use Prov2vec features with an incremental one-class classifier (e.g., streaming One-Class SVM/Isolation Forest) and drift-aware thresholding.",
        "GPU-accelerated or parallel incremental kernel computation with batched edge updates to further reduce latency.",
        "Add explainability module that maps anomalous sketch dimensions back to top-contributing provenance labels and exemplar subgraphs."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": "Provenance kernel incremental update complexity O(h^2 |ΔE|) per batch; initial snapshot O(h^2 |E0|). h typically ≤ 4. Node versioning reduces edges by 3.38× on their data (avg 1.2 versions per node). Histosketch produces fixed-size vectors for streaming similarity."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Enterprise endpoints (Windows and Linux hosts)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Maintaining and processing large, long-duration provenance graphs in memory is impractical; database traversal is costly.",
        "High alert volumes and false positives from EDR systems strain analysts (alert fatigue).",
        "Dependence on correctness and coverage of audit logging frameworks.",
        "Universal fine-grained alert correlation across all hosts is computationally excessive; need host prioritization.",
        "Evolving benign behaviors require continuous profile updates to avoid false positives."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "End-to-end system (Prov2vec) for unsupervised APT detection using provenance graphs to build host behavior profiles and identify anomalies.",
      "Novel provenance graph kernel that generalizes similar structures using compact node-label histograms; achieves superior or comparable accuracy with histogram size an order of magnitude smaller than WLSubtree and Unicorn kernels.",
      "Demonstration on Windows and Linux provenance graphs across graph classification, clustering, and anomaly detection tasks.",
      "Streaming-friendly featurization via histosketch to obtain fixed-size vectors preserving similarity."
    ]
  },
  {
    "arxiv_id": "2310.09193v1",
    "title": "Tikuna: An Ethereum Blockchain Network Security Monitoring System",
    "authors": "Andres Gomez Ramirez; Loui Al Sardy; Francis Gomez Ramirez",
    "abstract": "Blockchain security is becoming increasingly relevant in today's cyberspace as it extends its influence in many industries. This paper focuses on protecting the lowest level layer in the blockchain, particularly the P2P network that allows the nodes to communicate and share information. The P2P network layer may be vulnerable to several families of attacks, such as Distributed Denial of Service (DDoS), eclipse attacks, or Sybil attacks. This layer is prone to threats inherited from traditional P2P networks, and it must be analyzed and understood by collecting data and extracting insights from the network behavior to reduce those risks. We introduce Tikuna, an open-source tool for monitoring and detecting potential attacks on the Ethereum blockchain P2P network, at an early stage. Tikuna employs an unsupervised Long Short-Term Memory (LSTM) method based on Recurrent Neural Network (RNN) to detect attacks and alert users. Empirical results indicate that the proposed approach significantly improves detection performance, with the ability to detect and classify attacks, including eclipse attacks, Covert Flash attacks, and others that target the Ethereum blockchain P2P network layer, with high accuracy. Our research findings demonstrate that Tikuna is a valuable security tool for assisting operators to efficiently monitor and safeguard the status of Ethereum validators and the wider P2P network",
    "published_date": "2023-10-13",
    "pdf_link": "https://arxiv.org/pdf/2310.09193v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Blockchain Security",
      "subdomain": "P2P Network Security",
      "specific_problem": "Early anomaly detection of attacks in the Ethereum P2P (DevP2P/libp2p/GossipSub) network using peer message traces",
      "attack_types": [
        "Eclipse attack",
        "Sybil attack",
        "DDoS/DoS",
        "Censorship attack",
        "Cold Boot attack",
        "Flash attack",
        "Covert Flash attack",
        "Deanonymization (mentioned as a risk)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "RNN",
        "specific": "LSTM",
        "novel_contribution": "Unsupervised LSTM anomaly detection trained on normal Ethereum P2P communication to detect deviations indicating attacks; applied to gossip message traces from simulation and Prysm mainnet logs with forecasting-loss based anomaly scoring."
      },
      {
        "type": "baseline",
        "category": "Tree-based ensemble",
        "specific": "Random Forest (ETH-EDS by Xu et al. [40])",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "Testground/libp2p GossipSub attack simulation traces",
        "type": "synthetic",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Prysm Ethereum mainnet discovery/connection logs (custom-generated)",
        "type": "private",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "ETH-EDS eclipse-attack detection",
        "paper_reference": "Xu et al. [40]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "GossipSub hardening (mesh/score + countermeasures)",
        "paper_reference": "Vyzovitis et al. [37]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Accuracy"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Ethereum DevP2P/Kademlia-based peer discovery has known security limitations enabling trivial eclipse attacks in practice.",
        "Existing hardening (e.g., GossipSub score/mesh) relies on fixed, manually tuned rules, limiting adoption; need ML-based automatic parameter/feature selection.",
        "Lack of open tooling for continuous monitoring and early detection at the Ethereum P2P layer to improve network visibility."
      ],
      "limitations": [
        "Evaluation used only the Prysm client on Ethereum mainnet; other clients left for future work.",
        "Attack simulations and mainnet evaluation rely on custom-generated logs; public datasets are not provided."
      ],
      "future_work": [
        "Explore and evaluate additional popular Ethereum clients beyond Prysm.",
        "Identify potential future research directions for detection and mitigation (stated in conclusion)."
      ],
      "motivation": "Protect the Ethereum blockchain at the network layer by monitoring P2P behavior and detecting early-stage attacks such as eclipse and Sybil variants using ML.",
      "potential_research_ideas": [
        "Graph-based anomaly detection over dynamic peer graphs (e.g., temporal GNNs) to capture topological attack patterns beyond sequence timing.",
        "Self-supervised/contrastive pretraining on large volumes of unlabeled P2P traces to improve generalization and reduce threshold tuning.",
        "Federated/privacy-preserving learning across validators/operators to learn global normal patterns without sharing raw logs.",
        "Adversarial evaluation and robust training against stealthy Covert Flash or slow-evolving eclipse strategies.",
        "Multi-task framework that jointly detects anomalies and categorizes likely attack type with uncertainty estimates for operator triage.",
        "Concept-drift detection and online adaptation to evolving network conditions and client updates.",
        "Cross-client generalization study to assess portability across Prysm, Lighthouse, Teku, Nimbus, and Geth/Erigon discovery stacks."
      ],
      "architectural_improvement_recommendations": [
        "Augment LSTM with attention or Transformer-based temporal encoders for long-range dependencies and better feature salience.",
        "Fuse sequential features with peer-graph features (hybrid seq+graph model) for richer context on neighbor behavior.",
        "Use probabilistic autoencoders (VAE) or normalizing flows for calibrated anomaly scores and better tail modeling.",
        "Employ sequence-to-sequence forecasting with prediction intervals; trigger anomalies on interval breaches to control false positives.",
        "Implement online learning with drift-aware thresholding (e.g., ADWIN/KS tests) and periodic recalibration.",
        "Build an ensemble combining unsupervised deep models with lightweight tree models on engineered features for robustness.",
        "Add explainability via temporal saliency maps or SHAP-on-sequences to aid operator trust and response."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Experiments used five dedicated Hetzner root servers (64 GB DDR4 RAM, 2×512 GB NVMe SSDs, AMD Ryzen CPUs). Evaluation across a libp2p Testground simulation environment and Ethereum mainnet using the Prysm client."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Ethereum mainnet validator nodes (Prysm) and Testground-based simulation environment",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces Tikuna, an open-source security monitoring and early-attack detection tool for the Ethereum P2P network layer.",
      "Proposes an unsupervised LSTM-based anomaly detection approach trained on normal P2P communication to detect deviations indicative of attacks.",
      "Demonstrates detection of eclipse attacks on mainnet by extracting custom discovery/connection logs from the Prysm client and applying LSTM.",
      "Develops a custom eclipse attack exploit that fills peer-table buckets from a single machine using virtual addresses and Docker to bypass per-IP limits, enabling evaluation of Tikuna.",
      "Uses Testground/libp2p simulations to generate multiple P2P attack scenarios (eclipse, Sybil variants, Flash/Covert Flash, etc.) for evaluation and feature extraction."
    ]
  },
  {
    "arxiv_id": "2310.18165v1",
    "title": "Enhancing Enterprise Network Security: Comparing Machine-Level and Process-Level Analysis for Dynamic Malware Detection",
    "authors": "Baskoro Adi Pratomo; Toby Jackson; Pete Burnap; Andrew Hood; Eirini Anthi",
    "abstract": "Analysing malware is important to understand how malicious software works and to develop appropriate detection and prevention methods. Dynamic analysis can overcome evasion techniques commonly used to bypass static analysis and provide insights into malware runtime activities. Much research on dynamic analysis focused on investigating machine-level information (e.g., CPU, memory, network usage) to identify whether a machine is running malicious activities. A malicious machine does not necessarily mean all running processes on the machine are also malicious. If we can isolate the malicious process instead of isolating the whole machine, we could kill the malicious process, and the machine can keep doing its job. Another challenge dynamic malware detection research faces is that the samples are executed in one machine without any background applications running. It is unrealistic as a computer typically runs many benign (background) applications when a malware incident happens. Our experiment with machine-level data shows that the existence of background applications decreases previous state-of-the-art accuracy by about 20.12% on average. We also proposed a process-level Recurrent Neural Network (RNN)-based detection model. Our proposed model performs better than the machine-level detection model; 0.049 increase in detection rate and a false-positive rate below 0.1.",
    "published_date": "2023-10-27",
    "pdf_link": "https://arxiv.org/pdf/2310.18165v1",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Malware Analysis",
      "subdomain": "Dynamic Malware Detection",
      "specific_problem": "Per-process malware detection on Windows using Sysmon event sequences; comparison of machine-level versus process-level dynamic analysis under realistic background applications",
      "attack_types": [
        "ransomware",
        "trojan",
        "botnet",
        "exploit",
        "miner"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "RNN",
        "specific": "LSTM/GRU",
        "novel_contribution": "First ML-based malware detection model to determine whether a specific process is malicious using process-level Windows/Sysmon event sequences"
      },
      {
        "type": "baseline",
        "category": "RNN",
        "specific": "RNN on machine-level system utilization (per Rhode et al. [4])",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Proposed dynamic malware dataset (machine-level + process-level Sysmon events, second-by-second)",
        "type": "proprietary",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "VirusShare",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Benignware set from Rhode et al. [4]",
        "type": "",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      },
      {
        "name": "EMBER [5]",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "SoRel-20M [6]",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "MOTIF [7]",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Rhode et al. dynamic dataset [4] (machine-level CPU/memory/network/num_processes)",
        "type": "",
        "domain": "system_utilization",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      },
      {
        "name": "Sihwail et al. dynamic dataset [10] (API calls)",
        "type": "",
        "domain": "api_calls",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      }
    ],
    "baselines": [
      {
        "method_name": "Machine-level RNN (Rhode et al. [4]) evaluated with background applications",
        "paper_reference": "[4]",
        "metric": "accuracy",
        "their_result": "\"the existence of background applications decreases previous state-of-the-art accuracy by about 20.12% on average\"",
        "baseline_result": null
      },
      {
        "method_name": "Machine-level RNN vs. proposed process-level RNN",
        "paper_reference": null,
        "metric": "detection rate / false positive rate",
        "their_result": "\"0.049 increase in detection rate and a false-positive rate below 0.1\"",
        "baseline_result": "Baseline machine-level detection rate lower by 0.049 (FPR not specified)"
      }
    ],
    "performance_metrics_used": [
      "accuracy",
      "detection rate",
      "false positive rate"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How do background applications affect machine-level dynamic malware detection performance?",
        "Can process-level Sysmon event sequences enable more accurate detection of malicious processes than machine-level signals?",
        "Can malicious processes be detected early from their event sequences?"
      ],
      "gaps_identified": [
        "Dynamic malware detection datasets commonly execute samples on a single VM without background applications, which is unrealistic for enterprise settings.",
        "Machine-level aggregation mixes benign background activity with malicious behavior, hindering precise detection.",
        "No prior ML work determines whether a specific process is malicious using process-level event data.",
        "Popular malware datasets (e.g., EMBER, SoRel-20M, MOTIF) are static and do not capture runtime behavior."
      ],
      "limitations": [
        "Network traffic was not captured: \"as of now, we did not capture any network traffic.\"",
        "Operational issues with the cyber range caused some samples to be desynchronized; mitigated by setting synchronized start times.",
        "All outgoing traffic was blocked in the testbed, potentially limiting realism.",
        "Windows Defender and Firewall were disabled during data generation.",
        "Experiments used Windows 7 SP1 endpoints only."
      ],
      "future_work": [],
      "motivation": "Improve realism and granularity in dynamic malware detection by modeling per-process behavior so malicious processes can be isolated without quarantining entire machines, and assess the impact of realistic background applications.",
      "potential_research_ideas": [
        "Pretrain event-sequence embeddings (self-supervised) on large unlabeled Sysmon corpora, then fine-tune for per-process detection.",
        "Model process trees and inter-process relationships with graph neural networks, combining sequence signals with parent-child context.",
        "Fuse process-level event sequences with machine-level utilization and optional static PE features for multimodal detection.",
        "Cross-host correlation for campaign-level detection by aggregating similar suspicious process behaviors across endpoints.",
        "Online early-detection framework with calibrated thresholds optimizing time-to-detect versus FPR under background noise.",
        "Adversarial robustness: detect/mitigate log tampering and event manipulation (e.g., via redundancy across event types or integrity checks).",
        "Domain adaptation across OS versions and enterprise configurations to improve generalization beyond Windows 7.",
        "Leverage network telemetry (DNS/flow) with process attribution when available to improve precision and attribution."
      ],
      "architectural_improvement_recommendations": [
        "Replace/augment RNNs with transformer encoders using time-aware positional encodings for irregular event timing.",
        "Hierarchical architecture: per-process event encoder + process-tree/context encoder with attention over parent/sibling processes.",
        "Temporal point-process modeling (e.g., Neural Hawkes) to capture event occurrence times in addition to event types.",
        "Class-imbalance handling and calibrated decision thresholds to maintain FPR < 0.1 under variable background loads.",
        "Event-type specific embeddings and feature hashing for high-cardinality fields (e.g., image path, registry key, DNS).",
        "Multi-task learning to jointly predict process maliciousness and likely malware family/behaviour class."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Virtualised small–medium enterprise network: five Windows 7 SP1 clients, Windows Server 2016 LME/Sysmon event collector, Ubuntu 20.04 Cuckoo server; outgoing traffic blocked",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Background applications significantly affect machine-level detection accuracy (~20.12% average drop).",
        "Requires Sysmon and Windows Event Forwarding (LME) deployment across endpoints.",
        "Event collection cadence tuned (every 15 seconds) to avoid loss before VM resets in the testbed.",
        "Windows Defender/Firewall disabled during data generation (not feasible in production).",
        "Desynchronization issues in virtualized scheduling needed mitigation."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "First ML-based malware detection model to determine whether a specific process is malicious using process-level data.",
      "A new malware dataset containing both machine-level and process-level data (second-by-second) from benign and malicious samples in a realistic, multi-endpoint network with background applications.",
      "Empirical finding: \"the existence of background applications decreases previous state-of-the-art accuracy by about 20.12% on average\" for machine-level models.",
      "Proposed process-level RNN model outperforms machine-level model with \"0.049 increase in detection rate and a false-positive rate below 0.1.\""
    ]
  },
  {
    "arxiv_id": "2310.19845v1",
    "title": "Modified Genetic Algorithm for Feature Selection and Hyper Parameter Optimization: Case of XGBoost in Spam Prediction",
    "authors": "Nazeeh Ghatasheh; Ismail Altaharwa; Khaled Aldebei",
    "abstract": "Recently, spam on online social networks has attracted attention in the research and business world. Twitter has become the preferred medium to spread spam content. Many research efforts attempted to encounter social networks spam. Twitter brought extra challenges represented by the feature space size, and imbalanced data distributions. Usually, the related research works focus on part of these main challenges or produce black-box models. In this paper, we propose a modified genetic algorithm for simultaneous dimensionality reduction and hyper parameter optimization over imbalanced datasets. The algorithm initialized an eXtreme Gradient Boosting classifier and reduced the features space of tweets dataset; to generate a spam prediction model. The model is validated using a 50 times repeated 10-fold stratified cross-validation, and analyzed using nonparametric statistical tests. The resulted prediction model attains on average 82.32\\% and 92.67\\% in terms of geometric mean and accuracy respectively, utilizing less than 10\\% of the total feature space. The empirical results show that the modified genetic algorithm outperforms $Chi^2$ and $PCA$ feature selection methods. In addition, eXtreme Gradient Boosting outperforms many machine learning algorithms, including BERT-based deep learning model, in spam prediction. Furthermore, the proposed approach is applied to SMS spam modeling and compared to related works.",
    "published_date": "2023-10-30",
    "pdf_link": "https://arxiv.org/pdf/2310.19845v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Social Network Security",
      "subdomain": "Spam Detection",
      "specific_problem": "Content-based spam detection on Twitter; extended to SMS spam classification",
      "attack_types": [
        "social spam",
        "malicious links",
        "fraudulent reviews",
        "profanity",
        "insulting",
        "hate speech"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Genetic Algorithm (Evolutionary Algorithm)",
        "specific": "Modified GA for simultaneous feature selection and XGBoost hyperparameter optimization",
        "novel_contribution": "Joint optimization of XGBoost hyperparameters and sparse feature subset selection on imbalanced data; GA modifications include constraints such as preventing duplicate genes and limiting selected features"
      },
      {
        "type": "primary",
        "category": "Gradient Boosted Trees",
        "specific": "XGBoost",
        "novel_contribution": "Used as the base classifier initialized/tuned by the modified GA for spam prediction"
      },
      {
        "type": "baseline",
        "category": "Filter Feature Selection",
        "specific": "Chi-square (Chi2)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Dimensionality Reduction",
        "specific": "Principal Component Analysis (PCA)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "BERT-based model (unspecified variant)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Evolutionary optimization (wrapper-based feature selection)"
    ],
    "datasets": [
      {
        "name": "Twitter spam tweets dataset (5096 tweets) from [22]",
        "type": "public",
        "domain": "social_media_text (tweets)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "SMS spam dataset",
        "type": "public",
        "domain": "sms_text_messages",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Chi-square feature selection (Chi2)",
        "paper_reference": null,
        "metric": "accuracy, geometric mean",
        "their_result": "Accuracy 92.67%, G-mean 82.32% (proposed GA+XGBoost)",
        "baseline_result": null
      },
      {
        "method_name": "PCA feature selection",
        "paper_reference": null,
        "metric": "accuracy, geometric mean",
        "their_result": "Accuracy 92.67%, G-mean 82.32% (proposed GA+XGBoost)",
        "baseline_result": null
      },
      {
        "method_name": "BERT-based deep learning model",
        "paper_reference": null,
        "metric": "accuracy",
        "their_result": "Accuracy 92.67% (proposed GA+XGBoost) reported to outperform BERT-based baseline",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "accuracy",
      "geometric mean"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can a modified genetic algorithm jointly select a compact, informative feature subset and optimize XGBoost hyperparameters to handle imbalanced Twitter spam datasets?",
        "Does the resulting GA-initialized XGBoost model achieve high performance while retaining interpretability via a small feature set?",
        "How does the proposed approach compare against standard feature selection methods (Chi-square, PCA) and deep learning (BERT-based) models?"
      ],
      "gaps_identified": [
        "Large feature space and imbalanced distributions in social spam datasets complicate modeling and often lead to black-box models",
        "Limited availability and quality of labeled text datasets; many are language-specific, imbalanced, biased, or have unverified labels",
        "Few works explicitly determine the appropriate number of features to select for tweets classification, often using arbitrary feature counts",
        "Deep learning solutions, while strong, raise interpretability and scalability concerns for social spam detection",
        "Emerging multi-modal content (text, images, video) poses future challenges not widely addressed"
      ],
      "limitations": [
        "Work focuses on content-based features; graph/account-based features are not incorporated (noted as alternative stream in literature)",
        "Evaluation primarily on one Twitter dataset (5096 tweets) and an additional SMS spam dataset; broader generalization not demonstrated in the provided text",
        "Adversarial robustness, privacy, and fairness are not addressed",
        "Multi-modal content is acknowledged as a challenge but not modeled"
      ],
      "future_work": [
        "Incorporating mixed content types (text, images, video, etc.) into model building for social media spam",
        "Exploring broader datasets and settings beyond the studied Twitter and SMS corpora (implied by discussion of dataset limitations)"
      ],
      "motivation": "Address Twitter spam detection challenges of high-dimensional feature spaces and class imbalance with an interpretable, scalable approach by jointly performing feature selection and hyperparameter optimization.",
      "potential_research_ideas": [
        "Develop a multi-objective GA (e.g., NSGA-II) that optimizes performance, feature sparsity, and model stability simultaneously for spam detection",
        "Integrate imbalance-aware objectives (cost-sensitive fitness, focal loss proxies) directly into the GA fitness function",
        "Combine transformer embeddings (e.g., BERT sentence embeddings) with tree-based models; use GA to select embedding dimensions and textual n-grams jointly",
        "Investigate cross-domain transfer: train on Twitter, adapt to SMS or other platforms with domain adaptation guided by GA-selected stable features",
        "Online/streaming variant handling concept drift in social platforms, with incremental GA updates and drift-aware fitness",
        "Robustness-driven feature selection using adversarial text augmentation and GA penalties for sensitivity to perturbations"
      ],
      "architectural_improvement_recommendations": [
        "Use a multi-objective GA (e.g., NSGA-II) to explicitly balance accuracy, geometric mean, and number of features",
        "Embed class-imbalance strategies into XGBoost (e.g., scale_pos_weight) and the GA fitness; augment with SMOTE or class-weighted sampling during fitness evaluation",
        "Adopt nested cross-validation inside GA to avoid optimistic bias from a single train/validation split during fitness calculation",
        "Leverage early stopping and cross-validated fitness for XGBoost within GA to reduce overfitting and accelerate search",
        "Explore alternative gradient boosting libraries (LightGBM, CatBoost) within the same GA framework for categorical/text features",
        "Use SHAP-informed mutation/crossover to bias GA toward features with consistently high contributions; add stability selection constraints",
        "Parallelize GA evaluations and exploit GPU-accelerated XGBoost to scale the search"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Class imbalance in real-world social media spam data",
        "High-dimensional, sparse text feature spaces requiring dimensionality reduction",
        "Need to balance interpretability and performance for operational use",
        "Limited availability/quality of labeled datasets for training and validation"
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes a content-based social spam detection approach covering contemporary spam behaviors",
      "Develops a modified genetic algorithm to simultaneously initialize/tune an XGBoost classifier and select features",
      "Validates the approach on a publicly released real-world Twitter dataset with extensive repeated stratified cross-validation and nonparametric statistical tests; also demonstrates application to SMS spam"
    ]
  },
  {
    "arxiv_id": "2312.00006v1",
    "title": "Enhancing ML-Based DoS Attack Detection Through Combinatorial Fusion Analysis",
    "authors": "Evans Owusu; Mohamed Rahouti; D. Frank Hsu; Kaiqi Xiong; Yufeng Xin",
    "abstract": "Mitigating Denial-of-Service (DoS) attacks is vital for online service security and availability. While machine learning (ML) models are used for DoS attack detection, new strategies are needed to enhance their performance. We suggest an innovative method, combinatorial fusion, which combines multiple ML models using advanced algorithms. This includes score and rank combinations, weighted techniques, and diversity strength of scoring systems. Through rigorous evaluations, we demonstrate the effectiveness of this fusion approach, considering metrics like precision, recall, and F1-score. We address the challenge of low-profiled attack classification by fusing models to create a comprehensive solution. Our findings emphasize the potential of this approach to improve DoS attack detection and contribute to stronger defense mechanisms.",
    "published_date": "2023-10-02",
    "pdf_link": "https://arxiv.org/pdf/2312.00006v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Detecting and classifying Denial-of-Service (DoS/DDoS) attacks in network traffic using combinatorial fusion of multiple ML models, with emphasis on improving low-profile attack detection.",
      "attack_types": [
        "DoS",
        "DDoS",
        "DoS Goldeneye",
        "DoS Hulk",
        "DoS Slowhttptest",
        "DoS Slowloris",
        "Bot",
        "FTP-Patator (brute force)",
        "SSH-Patator (brute force)",
        "Heartbleed",
        "Portscan",
        "Webattack Bruteforce",
        "Webattack SQL Injection",
        "Webattack XSS"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Ensemble / Fusion (Rank and Score Aggregation)",
        "specific": "Combinatorial Fusion Analysis (CFA) with Rank-Score Characteristic (RSC) and Cognitive Diversity (CD)",
        "novel_contribution": "Applies CFA to DoS detection; proposes weighted combinations by diversity strength and by model performance (recall), exploring rank vs score combinations and two-model fusion across six base models."
      },
      {
        "type": "baseline",
        "category": "Discriminant Analysis",
        "specific": "Linear Discriminant Analysis (LDA)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Naive Bayes",
        "specific": "Gaussian Naive Bayes (GNB)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Linear Model",
        "specific": "Logistic Regression",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "KNN",
        "specific": "k-Nearest Neighbors",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Decision Tree",
        "specific": "Decision Tree",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Random Forest",
        "specific": "Random Forest",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Ensemble"
    ],
    "datasets": [
      {
        "name": "LYCOS-IDS2017",
        "type": "unknown",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "unknown"
      }
    ],
    "baselines": [
      {
        "method_name": "Linear Discriminant Analysis (A)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Gaussian Naive Bayes (B)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Logistic Regression (C)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "k-Nearest Neighbors (D)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Decision Tree (E)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Random Forest (F)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "accuracy",
      "precision",
      "recall",
      "F1-score",
      "false positive rate",
      "rank-based analysis (RSC/CD)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can combinatorial fusion of multiple ML models improve DoS attack detection performance and interpretability compared to individual models?",
        "Which rank- and score-based fusion strategies (including weighting by diversity strength or by recall performance) are most effective, especially for low-profile attack classes?",
        "Does increasing cognitive diversity (CD) between models improve fusion performance?"
      ],
      "gaps_identified": [
        "ML models for DoS detection face interpretability and explainability limitations, especially with deep models.",
        "Existing approaches struggle with evolving and low-profile DoS attacks.",
        "Many IDS works rely on older datasets and traditional methods; there is a need for advanced fusion that leverages model diversity.",
        "Individual models show uneven per-class performance; those good at low-profile attacks often underperform on other classes."
      ],
      "limitations": [
        "On top-ranked data items, models can exhibit low diversity (ties in highest probabilities), limiting the benefits of diversity-based fusion.",
        "Results rely on a single dataset split (75/25) from LYCOS-IDS2017; generalization to other datasets/environments is not shown.",
        "No real-time/streaming evaluation or deployment study is presented.",
        "Quantitative end-to-end comparison of fusion vs baselines (aggregate metrics) is not fully reported in the provided content."
      ],
      "future_work": [],
      "motivation": "Mitigate DoS threats to online service availability by addressing ML interpretability limits and performance gaps; leverage combinatorial fusion to combine diverse models’ strengths to achieve superior detection (especially of low-profile attacks) and improved interpretability.",
      "potential_research_ideas": [
        "Integrate calibrated deep models (e.g., calibrated CNN/RNN/Transformer or gradient boosting like XGBoost/LightGBM) into CFA and study how model class affects CD and fusion gains.",
        "Develop adaptive, per-class and time-varying fusion weights that respond to concept drift in attack traffic.",
        "Learn fusion weights via a meta-learner (stacking) with constraints informed by CD to balance performance and diversity.",
        "Extend CFA to online/streaming IDS with sliding-window RSC/CD estimation for near-real-time fusion.",
        "Incorporate uncertainty estimation and probability calibration (Platt scaling/isotonic) to improve score-based fusion robustness.",
        "Evaluate adversarial robustness of fusion vs individual models under evasion attacks targeting probability outputs.",
        "Cross-dataset validation (e.g., newer IDS corpora) and domain adaptation to assess generalization.",
        "Explainability: map RSC/CD variations back to feature-level attributions to interpret why certain model pairs are complementary."
      ],
      "architectural_improvement_recommendations": [
        "Use probability calibration for all base models before fusion to ensure comparable, well-calibrated scores.",
        "Adopt meta-learning (stacked generalization) with CD-regularization to jointly learn fusion weights while encouraging complementary models.",
        "Implement class-conditional and instance-adaptive weighting that combines recall-based weights with uncertainty and CD cues.",
        "Expand base model pool with modern tree ensembles (XGBoost/LightGBM/CatBoost) and calibrated neural nets to increase cognitive diversity.",
        "Introduce drift detection to trigger dynamic reweighting or model refreshing in streaming settings.",
        "Evaluate rank vs score fusion under different calibration regimes to formally select the optimal pathway per class."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Evolving DoS attack tactics requiring adaptive models and fusion weights.",
        "Interpretability and explainability challenges in ML-based IDS.",
        "Detection of low-profile and minority-class attacks under class imbalance.",
        "Potential lack of diversity among top-performing models limiting fusion gains.",
        "Transferability from offline dataset experiments to production network environments."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Examined key ML models commonly used for DoS attack detection as a basis for integrating combinatorial fusion analysis.",
      "Developed a CFA-based fusion methodology encompassing score and rank combinations, weighted techniques (by diversity strength and by performance/recall), and analysis of diversity strength across scoring systems.",
      "Conducted evaluations demonstrating improvements in detection accuracy, precision, and false positive rates, and specifically addressed low-profile attack classification via model fusion."
    ]
  },
  {
    "arxiv_id": "2309.12968v3",
    "title": "PassViz: A Visualisation System for Analysing Leaked Passwords",
    "authors": "Sam Parker; Haiyue Yuan; Shujun Li",
    "abstract": "Passwords remain the most widely used form of user authentication, despite advancements in other methods. However, their limitations, such as susceptibility to attacks, especially weak passwords defined by human users, are well-documented. The existence of weak human-defined passwords has led to repeated password leaks from websites, many of which are of large scale. While such password leaks are unfortunate security incidents, they provide security researchers and practitioners with good opportunities to learn valuable insights from such leaked passwords, in order to identify ways to improve password policies and other security controls on passwords. Researchers have proposed different data visualisation techniques to help analyse leaked passwords. However, many approaches rely solely on frequency analysis, with limited exploration of distance-based graphs. This paper reports PassViz, a novel method that combines the edit distance with the t-SNE (t-distributed stochastic neighbour embedding) dimensionality reduction algorithm for visualising and analysing leaked passwords in a 2-D space. We implemented PassViz as an easy-to-use command-line tool for visualising large-scale password databases, and also as a graphical user interface (GUI) to support interactive visual analytics of small password databases. Using the \"000webhost\" leaked database as an example, we show how PassViz can be used to visually analyse different aspects of leaked passwords and to facilitate the discovery of previously unknown password patterns. Overall, our approach empowers researchers and practitioners to gain valuable insights and improve password security through effective data visualisation and analysis.",
    "published_date": "2023-09-22",
    "pdf_link": "https://arxiv.org/pdf/2309.12968v3",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Authentication and Access Control",
      "subdomain": "Password Security",
      "specific_problem": "Visual analysis of leaked password datasets to discover patterns and inform password policies",
      "attack_types": [
        "password guessing/cracking"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Dimensionality Reduction",
        "specific": "t-SNE (t-distributed stochastic neighbor embedding)",
        "novel_contribution": "Use of t-SNE on an anchor-based Levenshtein distance matrix to embed passwords into 2-D for visual analytics"
      },
      {
        "type": "primary",
        "category": "Distance Metric",
        "specific": "Levenshtein (edit) distance",
        "novel_contribution": "Anchor-based construction of an M×N distance matrix from passwords to a small set of representative anchors to enable scalability"
      },
      {
        "type": "baseline",
        "category": "Clustering",
        "specific": "k-means",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Clustering",
        "specific": "OPTICS",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Clustering",
        "specific": "DBSCAN",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "000webhost leaked passwords",
        "type": "public",
        "domain": "password_strings",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "SecLists xato-net-10-million-passwords-10000.txt (example subset)",
        "type": "public",
        "domain": "password_strings",
        "link": "https://github.com/danielmiessler/SecLists/blob/master/Passwords/xato-net-10-million-passwords-10000.txt",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "RockYou leaked passwords (mentioned in related work)",
        "type": "public",
        "domain": "password_strings",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "4-digit iPhone PIN dataset (mentioned in related work)",
        "type": "public",
        "domain": "pin_codes",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Yahoo leaked passwords (mentioned in related work)",
        "type": "public",
        "domain": "password_strings",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "12306 leaked passwords (mentioned in related work)",
        "type": "public",
        "domain": "password_strings",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can distance-based visualizations reveal structural patterns in leaked passwords beyond frequency-based methods?",
        "How can we scale distance-based analysis to very large password datasets without computing a full pairwise distance matrix?"
      ],
      "gaps_identified": [
        "“many approaches rely solely on frequency analysis, with limited exploration of distance-based graphs.”",
        "Limited prior work using graph-based methods with edit distances for password visualization; prior uses typically relied on thresholded graphs rather than dimensionality reduction into 2-D.",
        "Computing a full M×M distance matrix is time- and memory-intensive for large leaks."
      ],
      "limitations": [
        "High time and space complexity of computing all-pairs distances; full matrices are infeasible for large datasets.",
        "Interactive web GUI cannot handle very large password datasets; recommended only for small subsets.",
        "Anchor selection and number introduce an approximation that may affect global structure preservation.",
        "t-SNE primarily preserves local structure; global distances may be distorted."
      ],
      "future_work": [
        "“investigate how the time and space complexity of this interactive application can be improved to handle larger password databases, e.g., leveraging parallel processing using multiple cloud servers and GPUs on a single machine.”"
      ],
      "motivation": "Provide an effective distance-based 2-D visualization to analyze leaked passwords and uncover structural patterns to improve password policies and security controls.",
      "potential_research_ideas": [
        "Evaluate alternative distance measures (e.g., Damerau–Levenshtein, weighted edit costs, token/segment-aware distances) and quantify their effect on discovered patterns.",
        "Replace or complement t-SNE with UMAP/LargeVis/TriMap to better preserve global structure and enable incremental updates for streaming leaks.",
        "Develop principled anchor selection (k-center, k-medoids, farthest-point sampling) and adaptive anchor refinement to balance accuracy and scalability.",
        "Integrate semantic segmentation (dates, names, keyboard walks) to form hybrid embedding spaces combining structural and semantic features.",
        "Design privacy-preserving analysis (e.g., secure enclaves or differential privacy summaries) for organizations to analyze internal password data safely.",
        "Automate pattern mining over embeddings (e.g., density-aware motif discovery, subgraph mining) and link findings to actionable policy updates.",
        "Benchmark visualization fidelity with quantitative trustworthiness/continuity metrics and human-in-the-loop studies on analyst performance."
      ],
      "architectural_improvement_recommendations": [
        "Adopt GPU-accelerated or approximate edit distance (e.g., bit-parallel Levenshtein, CUDA kernels) and approximate nearest neighbors to reduce compute.",
        "Use FIt-SNE/fft-SNE or Barnes–Hut t-SNE implementations; consider UMAP for faster, scalable embeddings with good global preservation.",
        "Implement anchor selection via k-medoids or k-center to maximize coverage and minimize distortion; allow iterative anchor augmentation.",
        "Add HDBSCAN with stability-based cluster selection and cluster validity indices (silhouette, Davies–Bouldin) for robust cluster discovery.",
        "Move GUI rendering to WebGL/WebGPU and compute to WebAssembly/worker threads for large subset interactivity; support out-of-core tiling.",
        "Provide parameter search wizards (perplexity, learning rate, n_anchors) and quality diagnostics (trustworthiness/continuity) within the tool."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/samcparker/passviz-cli",
      "frameworks": [
        "Python",
        "Matplotlib",
        "openTSNE",
        "polyleven"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Full pairwise distance matrix is infeasible for large leaks (e.g., 700,000×700,000 distances would require ~490 GB even under optimistic assumptions). Anchor-based M×N distance matrix (e.g., 720,302×2,000) used to scale; GPU/parallelization suggested for future."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Large memory/time cost for full distance matrices on big leaks.",
        "Web GUI cannot process very large datasets; intended for small subsets.",
        "Selecting number and type of anchors to balance accuracy and performance.",
        "Ethical/legal considerations around handling leaked passwords."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces PassViz, a novel visualization method combining Levenshtein distance with t-SNE to embed passwords into 2-D for analysis.",
      "Proposes an anchor-based distance matrix to scale distance computations to large password datasets.",
      "Implements both a Python CLI for large datasets and an interactive GUI for small subsets, with regex filtering and clustering (k-means, OPTICS, DBSCAN).",
      "Demonstrates utility on the 000webhost leak (15,251,074 passwords; 720,302 unique) with examples of cluster patterns and analyses."
    ]
  },
  {
    "arxiv_id": "2310.06257v1",
    "title": "SCAR: Power Side-Channel Analysis at RTL-Level",
    "authors": "Amisha Srivastava; Sanjay Das; Navnil Choudhury; Rafail Psiakis; Pedro Henrique Silva; Debjit Pal; Kanad Basu",
    "abstract": "Power side-channel attacks exploit the dynamic power consumption of cryptographic operations to leak sensitive information of encryption hardware. Therefore, it is necessary to conduct power side-channel analysis for assessing the susceptibility of cryptographic systems and mitigating potential risks. Existing power side-channel analysis primarily focuses on post-silicon implementations, which are inflexible in addressing design flaws, leading to costly and time-consuming post-fabrication design re-spins. Hence, pre-silicon power side-channel analysis is required for early detection of vulnerabilities to improve design robustness. In this paper, we introduce SCAR, a novel pre-silicon power side-channel analysis framework based on Graph Neural Networks (GNN). SCAR converts register-transfer level (RTL) designs of encryption hardware into control-data flow graphs and use that to detect the design modules susceptible to side-channel leakage. Furthermore, we incorporate a deep learning-based explainer in SCAR to generate quantifiable and human-accessible explanation of our detection and localization decisions. We have also developed a fortification component as a part of SCAR that uses large-language models (LLM) to automatically generate and insert additional design code at the localized zone to shore up the side-channel leakage. When evaluated on popular encryption algorithms like AES, RSA, and PRESENT, and postquantum cryptography algorithms like Saber and CRYSTALS-Kyber, SCAR, achieves up to 94.49% localization accuracy, 100% precision, and 90.48% recall. Additionally, through explainability analysis, SCAR reduces features for GNN model training by 57% while maintaining comparable accuracy. We believe that SCAR will transform the security-critical hardware design cycle, resulting in faster design closure at a reduced design cost.",
    "published_date": "2023-10-10",
    "pdf_link": "https://arxiv.org/pdf/2310.06257v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Hardware Security",
      "subdomain": "Side-Channel Analysis",
      "specific_problem": "Pre-silicon (RTL-level) detection, localization, and mitigation of power side-channel leakage in cryptographic hardware",
      "attack_types": [
        "Power side-channel attacks",
        "Differential Power Analysis (DPA)",
        "Simple Power Analysis (SPA)",
        "Correlation Power Analysis (CPA)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Graph Neural Network",
        "specific": "Graph Convolutional Network (GCN)",
        "novel_contribution": "Node-level classification over RTL Control-Data Flow Graphs using custom power/structure features (vulnerable-key paths, node degree, Hamming distance from VCD, operation-type flags) to localize leakage-prone modules pre-silicon."
      },
      {
        "type": "primary",
        "category": "Explainability/XAI",
        "specific": "GNNExplainer",
        "novel_contribution": "Deep learning-based explainer integrated to produce quantifiable, human-accessible explanations and enable 57% feature reduction while maintaining comparable accuracy."
      },
      {
        "type": "primary",
        "category": "Transformer/LLM",
        "specific": null,
        "novel_contribution": "Automated mitigation: LLM-generated Verilog code inserted at localized RTL lines to mask or fortify leakage."
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "AES_Comp (Composite-field AES RTL)",
        "type": "",
        "domain": "hardware_rtl",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      },
      {
        "name": "AES_PPRM1",
        "type": "",
        "domain": "hardware_rtl",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      },
      {
        "name": "AES (other implementations, unseen in training)",
        "type": "",
        "domain": "hardware_rtl",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      },
      {
        "name": "RSA (RTL implementation)",
        "type": "",
        "domain": "hardware_rtl",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      },
      {
        "name": "PRESENT (RTL implementation)",
        "type": "",
        "domain": "hardware_rtl",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      },
      {
        "name": "Saber (PQC, RTL implementation)",
        "type": "",
        "domain": "hardware_rtl",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      },
      {
        "name": "CRYSTALS-Kyber (PQC, RTL implementation)",
        "type": "",
        "domain": "hardware_rtl",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "localization accuracy",
      "precision",
      "recall",
      "feature reduction (%)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can RTL-level CDFGs with GNNs accurately detect and localize power side-channel leakage-prone modules pre-silicon?",
        "Can explainability techniques (GNNExplainer) yield human-accessible rationales and reduce feature sets without sacrificing accuracy?",
        "Can LLMs automatically generate effective mitigation code to fortify localized leakage in RTL?"
      ],
      "gaps_identified": [
        "Existing power side-channel analysis primarily focuses on post-silicon implementations, making fixes inflexible and costly.",
        "Pre-silicon analyses at layout/power-modeling levels are time-consuming and inflexible.",
        "Prior RTL approaches rely on dynamic power simulations and complex estimation models that are computationally intensive and lack fine-grained localization and fortification strategies."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Reduce costly post-fabrication design re-spins by enabling pre-silicon detection and mitigation of power side-channel leakage at RTL with explainable ML and automated fortification.",
      "potential_research_ideas": [
        "Generalize training beyond AES_Comp by curating a large, diverse RTL corpus across many cryptographic and non-crypto designs to improve cross-design transfer.",
        "Self-supervised or contrastive pretraining on RTL/CDFG graphs to reduce label dependence and improve out-of-distribution generalization.",
        "Develop formal verification and security proofs for LLM-generated mitigation code to ensure functional correctness and side-channel resilience.",
        "Integrate timing/physical estimators (e.g., early switching capacitance estimates) to jointly model structural and approximate physical features.",
        "Create a benchmark suite for RTL side-channel localization with standardized labels and evaluation protocols.",
        "Study adversarial robustness of the GNN predictions (e.g., graph perturbations, feature noise) and defenses.",
        "Active learning pipeline that queries expensive simulations selectively to refine labels and improve localization.",
        "Joint regression-classification models predicting both leakage presence and a continuous leakage score at node/line granularity."
      ],
      "architectural_improvement_recommendations": [
        "Adopt heterogeneous GNNs or relational GNNs with typed nodes/edges (control vs data edges, operation types) and learnable edge features.",
        "Use attention-based graph models (e.g., GAT or Graph Transformers) to better capture long-range dependencies in large RTL graphs.",
        "Hierarchical graph modeling from module-level to line-level (HGNN) to scale to large designs and improve fine-grained localization.",
        "Incorporate temporal modeling of switching activity (sequence models over time bins) alongside structural CDFG features.",
        "Employ learned explainers (e.g., PGExplainer) and counterfactual explanations to improve faithfulness and reduce feature reliance.",
        "Calibration and uncertainty estimation for leakage predictions to guide human-in-the-loop review and mitigation prioritization.",
        "Use program analysis to refine variable-dependency graphs and derive richer semantics (taint tracking of key material, bit-level sensitivity)."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "Icarus Verilog",
        "GOLDMINE"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "SCAR: a novel RTL-level power side-channel analysis technique using CDFGs and GNNs to identify leakage-prone modules.",
      "Integrated deep learning-based explainer providing quantifiable, human-accessible explanations and annotated subgraphs with feature importance.",
      "Line-level localization within vulnerable RTL modules via source code analysis.",
      "Automatic mitigation by using an LLM to generate and insert protective masking code at vulnerable lines.",
      "Validation against post-synthesis dynamic power results showing identified vulnerabilities induce power fluctuations.",
      "Generalization to unseen AES implementations and to RSA, PRESENT, Saber, and CRYSTALS-Kyber with up to 94.49% accuracy, 100% precision, and 97.88% recall (and up to 91.84% accuracy, 85.94% precision, and 94.62% recall on PQC).",
      "Explainability-driven feature reduction by 57% while maintaining comparable accuracy."
    ]
  },
  {
    "arxiv_id": "2309.15324v1",
    "title": "DefectHunter: A Novel LLM-Driven Boosted-Conformer-based Code Vulnerability Detection Mechanism",
    "authors": "Jin Wang; Zishan Huang; Hengli Liu; Nianyi Yang; Yinhao Xiao",
    "abstract": "One of the most pressing threats to computing systems is software vulnerabilities, which can compromise both hardware and software components. Existing methods for vulnerability detection remain suboptimal. Traditional techniques are both time-consuming and labor-intensive, while machine-learning-based approaches often underperform when applied to complex datasets, due to their inability to capture high-dimensional relationships. Previous deep-learning strategies also fall short in capturing sufficient feature information. Although self-attention mechanisms can process information over long distances, they fail to capture structural information. In this paper, we introduce DefectHunter, an innovative model for vulnerability identification that employs the Conformer mechanism. This mechanism fuses self-attention with convolutional networks to capture both local, position-wise features and global, content-based interactions. Furthermore, we optimize the self-attention mechanisms to mitigate the issue of excessive attention heads introducing extraneous noise by adjusting the denominator. We evaluated DefectHunter against ten baseline methods using six industrial and two highly complex datasets. On the QEMU dataset, DefectHunter exhibited a 20.62\\% improvement in accuracy over Pongo-70B, and for the CWE-754 dataset, its accuracy was 14.64\\% higher. To investigate how DefectHunter comprehends vulnerabilities, we conducted a case study, which revealed that our model effectively understands the mechanisms underlying vulnerabilities.",
    "published_date": "2023-09-27",
    "pdf_link": "https://arxiv.org/pdf/2309.15324v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Vulnerability Detection",
      "specific_problem": "Source code-level vulnerability identification by fusing structural (AST/CFG/DFG) and semantic (LLM embeddings) features using a modified Conformer",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer variant",
        "specific": "Conformer",
        "novel_contribution": "Modified self-attention denominator (Softmax(QK^T / (1+√dk))) to reduce noise from excessive attention heads; fused sinusoidal positional encodings with inputs before a fully connected layer; applied Conformer to code vulnerability detection with structural+semantic fusion."
      },
      {
        "type": "primary",
        "category": "Pretrained encoder",
        "specific": "UniXcoder",
        "novel_contribution": "Used as a code embedding backbone (CSE) with custom tokenization fixes via NLTK and a specialized word list to preserve domain-specific tokens."
      },
      {
        "type": "primary",
        "category": "CNN",
        "specific": null,
        "novel_contribution": "Convolutional module within Conformer to capture local, position-wise features in source code sequences."
      },
      {
        "type": "primary",
        "category": "MLP",
        "specific": null,
        "novel_contribution": "Final classifier head (multi-layer perceptron) for vulnerability presence prediction."
      },
      {
        "type": "primary",
        "category": "Graph-based features",
        "specific": "AST/CFG/DFG",
        "novel_contribution": "Structural features extracted via tree-sitter; graphs converted to matrices and integrated with semantic embeddings for joint modeling."
      },
      {
        "type": "baseline",
        "category": "LLM",
        "specific": "Pongo-70B",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "CodeBERT",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "CodeT5",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "GNN",
        "specific": "Devign",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "GNN",
        "specific": "FUNDED",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "SVM",
        "specific": "VCCFinder",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Random Forest",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "LLM",
        "specific": "GPT-4",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "QEMU dataset",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "FFmpeg dataset",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CWE-754 dataset",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Curated LLM pretraining dataset (authors)",
        "type": "proprietary",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "Curated DefectHunter training dataset (authors)",
        "type": "proprietary",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Pongo-70B",
        "paper_reference": null,
        "metric": "Accuracy (ACC) on QEMU",
        "their_result": "“On the QEMU dataset, DefectHunter exhibited a 20.62% improvement in accuracy over Pongo-70B.”",
        "baseline_result": null
      },
      {
        "method_name": "Pongo-70B",
        "paper_reference": null,
        "metric": "Accuracy (ACC) on CWE-754",
        "their_result": "“for the CWE-754 dataset, its accuracy was 14.64% higher.”",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Accuracy (ACC)",
      "F1-Score"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can combining self-attention and convolution (Conformer) with structural graphs and LLM-based code embeddings improve vulnerability detection on complex, industrial datasets?",
        "Does modifying the self-attention computation (adjusting the denominator to 1+√dk) reduce noise from excessive attention heads and improve accuracy?",
        "How does DefectHunter comprehend vulnerabilities in practice (via case study)?",
        "Can leveraging a pre-trained large language model (for CSE) improve training efficiency and performance for vulnerability detection?"
      ],
      "gaps_identified": [
        "Traditional static/dynamic analysis is time-consuming and error-prone.",
        "Classical ML models underperform on complex, industrial-scale datasets due to insufficient high-dimensional feature capture.",
        "Prior deep-learning approaches fail to sufficiently capture both structural and semantic information.",
        "Self-attention alone captures long-range dependencies but not structural information; fusion with structure is needed.",
        "Excessive attention heads can introduce unwarranted noise due to softmax forcing annotation even when heads lack relevant information.",
        "Tokenization issues in code (e.g., compound identifiers) degrade embedding quality; standard tokenizers can be imprecise.",
        "Length limits of pre-trained encoders (e.g., UniXcoder supports up to 768 tokens) constrain modeling of long code."
      ],
      "limitations": [
        "Tokenization challenges required ad-hoc fixes (NLTK and custom word list).",
        "Input length limitation of UniXcoder to 768 tokens.",
        "Exact dataset details and availability for curated datasets are not specified in the provided text."
      ],
      "future_work": [],
      "motivation": "Improve industrial applicability of code vulnerability detection by jointly modeling structural (AST/CFG/DFG) and semantic information, and by stabilizing attention to reduce noise from many heads, achieving better performance on complex datasets.",
      "potential_research_ideas": [
        "Integrate a graph neural network (e.g., GAT or GGNN) over AST/CFG/DFG and co-train with the Conformer for explicit structural reasoning.",
        "Use contrastive learning to align structural graph embeddings with semantic code embeddings for better fusion.",
        "Adopt long-sequence attention (e.g., Performer/Longformer) to overcome the 768-token limit and handle large functions or files.",
        "Design a code-aware tokenizer trained on programming corpora to further reduce tokenization errors vs. generic NLP tokenizers.",
        "Incorporate path-based representations (e.g., AST paths, dataflow paths) and relation-aware attention to emphasize vulnerability-relevant relations.",
        "Pre-train the encoder with vulnerability-centric objectives (e.g., masked vulnerability patterns, CWE-specific tasks) before fine-tuning.",
        "Use retrieval-augmented modeling to fetch similar vulnerable code snippets or CWE guidance during inference.",
        "Add an attention-head gating/sparsification mechanism so heads can abstain when uninformative, reducing noise beyond denominator adjustment.",
        "Evaluate and harden against adversarial code transformations (semantics-preserving obfuscations) to ensure robustness.",
        "Explore multi-modal signals (build logs, static analyzer warnings) to complement code features in real-world pipelines."
      ],
      "architectural_improvement_recommendations": [
        "Add a dedicated GNN branch over AST/CFG/DFG with cross-attention to the Conformer sequence for tighter structure–token fusion.",
        "Replace standard MHSA with sparse or kernelized attention (Performer/Nystrom/Longformer) to scale to longer code with lower quadratic cost.",
        "Introduce learnable relative positional encodings or Rotary Position Embeddings (RoPE) tailored for code structure.",
        "Implement head dropout/gating and L0 regularization to promote useful, sparse attention heads.",
        "Use hierarchical encoding (function → file → project) with pooling to capture broader context when needed.",
        "Apply curriculum learning: start with short snippets, progressively increase sequence length and structural complexity.",
        "Leverage parameter-efficient fine-tuning (LoRA/adapters) on UniXcoder for domain adaptation to specific CWEs.",
        "Augment training with semantics-preserving code transformations (variable renaming, dead code insertion) to improve robustness."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/WJ-8/DefectHunter",
      "frameworks": [
        "TensorFlow/Keras",
        "HuggingFace Transformers",
        "NLTK",
        "tree-sitter"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Tokenizer precision for code identifiers required custom fixes.",
        "Input-length limits of the encoder constrain large functions/files.",
        "Fusion of structural and semantic features increases complexity and may impact throughput.",
        "Potential noise from multi-head attention necessitates careful stabilization."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces DefectHunter, a Conformer-based vulnerability detection model that fuses structural (AST/CFG/DFG) and semantic code information; code released on GitHub.",
      "Optimizes self-attention by adjusting the softmax denominator to reduce noise from excessive attention heads.",
      "Leverages a pre-trained LLM (UniXcoder) to extract code semantics and improve training efficiency.",
      "Implements practical tokenization fixes (NLTK + custom word list) to preserve code tokens.",
      "Demonstrates improved accuracy over 10 baselines on eight datasets; specifically, +20.62% ACC over Pongo-70B on QEMU and +14.64% ACC on CWE-754.",
      "Provides an implementation in Keras with custom layers for MHSA and sinusoidal position embeddings, and a case study showing how the model understands vulnerabilities."
    ]
  },
  {
    "arxiv_id": "2310.00144v1",
    "title": "Probabilistic Sampling-Enhanced Temporal-Spatial GCN: A Scalable Framework for Transaction Anomaly Detection in Ethereum Networks",
    "authors": "Stefan Kambiz Behfar; Jon Crowcroft",
    "abstract": "The rapid evolution of the Ethereum network necessitates sophisticated techniques to ensure its robustness against potential threats and to maintain transparency. While Graph Neural Networks (GNNs) have pioneered anomaly detection in such platforms, capturing the intricacies of both spatial and temporal transactional patterns has remained a challenge. This study presents a fusion of Graph Convolutional Networks (GCNs) with Temporal Random Walks (TRW) enhanced by probabilistic sampling to bridge this gap. Our approach, unlike traditional GCNs, leverages the strengths of TRW to discern complex temporal sequences in Ethereum transactions, thereby providing a more nuanced transaction anomaly detection mechanism. Preliminary evaluations demonstrate that our TRW-GCN framework substantially advances the performance metrics over conventional GCNs in detecting anomalies and transaction bursts. This research not only underscores the potential of temporal cues in Ethereum transactional data but also offers a scalable and effective methodology for ensuring the security and transparency of decentralized platforms. By harnessing both spatial relationships and time-based transactional sequences as node features, our model introduces an additional layer of granularity, making the detection process more robust and less prone to false positives. This work lays the foundation for future research aimed at optimizing and enhancing the transparency of blockchain technologies, and serves as a testament to the significance of considering both time and space dimensions in the ever-evolving landscape of the decentralized platforms.",
    "published_date": "2023-09-29",
    "pdf_link": "https://arxiv.org/pdf/2310.00144v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Blockchain Security",
      "subdomain": "Cryptocurrency Transaction Monitoring / Anomaly Detection",
      "specific_problem": "Temporal-spatial graph learning for transaction anomaly and burst detection in the Ethereum network using probabilistic sampling-enhanced GCN",
      "attack_types": [
        "front-running",
        "pump-and-dump",
        "malware-related subgraphs",
        "transaction bursts",
        "suspiciously timed transactions",
        "unusual smart contract execution patterns"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "GNN / GCN",
        "specific": "GCN with Temporal Random Walks (TRW) and probabilistic sampling",
        "novel_contribution": "Integrates a temporal transition matrix into GCN via TRW and uses probabilistic sampling to prioritize temporally relevant nodes; combines spatial relations with time-based sequences as node features for scalable anomaly detection in Ethereum."
      },
      {
        "type": "baseline",
        "category": "GNN / GCN",
        "specific": "Standard GCN (no TRW)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Clustering",
        "specific": "DBSCAN",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "SVM (one-class)",
        "specific": "One-Class SVM (OCSVM)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Ensemble anomaly detection",
        "specific": "Isolation Forest",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Density-based anomaly detection",
        "specific": "Local Outlier Factor (LOF)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "Ethereum Mainnet transaction graph (100 blocks)",
        "type": "public",
        "domain": "blockchain_transactions",
        "link": "https://www.infura.io/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Ethereum Mainnet transaction graph (1000 blocks; 83,252 nodes, 101,403 edges)",
        "type": "public",
        "domain": "blockchain_transactions",
        "link": "https://www.alchemy.com/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Standard GCN embeddings + Isolation Forest (no TRW) vs TRW-GCN embeddings + Isolation Forest",
        "paper_reference": null,
        "metric": "number_of_anomalies_detected (1000 blocks)",
        "their_result": "“over 4000 anomalies detected” (TRW-GCN embeddings)",
        "baseline_result": "“0” (standard GCN embeddings)"
      },
      {
        "method_name": "Standard GCN embeddings + LOF (no TRW) vs TRW-GCN embeddings + LOF",
        "paper_reference": null,
        "metric": "number_of_anomalies_detected (1000 blocks)",
        "their_result": "“over 4000 anomalies detected” (TRW-GCN embeddings)",
        "baseline_result": "“0” (standard GCN embeddings)"
      },
      {
        "method_name": "Standard GCN embeddings + DBSCAN vs TRW-GCN embeddings + DBSCAN",
        "paper_reference": null,
        "metric": "number_of_anomalies_detected (100 and 1000 blocks)",
        "their_result": "High anomaly counts observed; authors note likely higher false positives; similar counts with and without TRW on 100 blocks",
        "baseline_result": null
      },
      {
        "method_name": "Standard GCN embeddings + One-Class SVM vs TRW-GCN embeddings + One-Class SVM",
        "paper_reference": null,
        "metric": "number_of_anomalies_detected (100 and 1000 blocks)",
        "their_result": "High anomaly counts observed; authors note likely higher false positives; similar counts with and without TRW on 100 blocks",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "number_of_anomalies_detected",
      "qualitative feature importance (feature-wise contribution visualization)",
      "training_efficiency (qualitative)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Does incorporating Temporal Random Walks (TRW) into GCN improve anomaly detection in Ethereum transactions?",
        "Can probabilistic sampling via TRW provide efficient training on large Ethereum graphs without sacrificing accuracy?",
        "Are temporal sequences essential for capturing anomalies and transaction bursts missed by standard GCNs?",
        "Can TRW-GCN detect sophisticated timing-dependent attacks (e.g., front-running) more effectively?"
      ],
      "gaps_identified": [
        "“there is a conspicuous absence of comprehensive research that deeply integrates TRW with GCNs, and probabilistic sampling, especially within the blockchain environment.”",
        "Prior temporal graph learning often considers only first-order temporal information, leading to sub-optimal performance.",
        "Challenge of jointly modeling spatial and temporal dynamics in rapidly evolving Ethereum transaction networks.",
        "Lack of labeled ground truth for anomalies in Ethereum hampers supervised evaluation."
      ],
      "limitations": [
        "“There is a limitation that is inherent in unsupervised anomaly detection, especially in domains like Ethereum transactions where a definitive ground truth may not be readily available.”",
        "Preliminary evaluation; figures report anomaly counts rather than precision/recall/ROC with labels.",
        "Results shown only for restricted block ranges (e.g., 100 vs 1000 blocks).",
        "High computational and storage demands for full-network processing acknowledged.",
        "Potentially high false positive rates for some detectors (DBSCAN, SVM) per authors’ discussion."
      ],
      "future_work": [
        "“This work lays the foundation for future research aimed at optimizing and enhancing the transparency of blockchain technologies.”",
        "“We would need to compare the performance of GCN with and without TRW on a temporal dataset to see tangible benefits.”",
        "Expand evaluations with larger temporal windows and more comprehensive metrics (precision/recall) and labeled datasets.",
        "Investigate detection of specific attack types (front-running, pump-and-dump) with ground truth."
      ],
      "motivation": "Improve scalability and effectiveness of anomaly detection in Ethereum by combining spatial relations and temporal sequences, addressing lack of deep integration of TRW, GCN, and probabilistic sampling.",
      "potential_research_ideas": [
        "Construct and release a labeled temporal Ethereum anomaly dataset (front-running, pump-and-dump, phishing) to enable supervised and robust benchmarking.",
        "Develop an end-to-end temporal GNN (e.g., TGAT/TGN/EvolveGCN) tailored to Ethereum with learned temporal attention instead of fixed TRW weighting.",
        "Use contrastive/self-supervised temporal objectives (e.g., time-shift prediction, subgraph contrast) to pretrain robust embeddings before unsupervised anomaly scoring.",
        "Incorporate heterogeneous graph modeling (addresses, contracts, tokens, DEX pools) with edge types and amounts; augment with smart contract bytecode/ABI features.",
        "Streaming/online anomaly detection with incremental updates and change-point detection for real-time monitoring.",
        "Adversarial robustness study against evasive transaction patterns and sampling manipulation; design robust temporal sampling strategies.",
        "Explainability via subgraph/rule extraction to attribute anomalies to concrete transaction motifs and temporal bursts.",
        "Cross-chain anomaly detection by aligning temporal embeddings across multiple EVM chains; detect coordinated attacks."
      ],
      "architectural_improvement_recommendations": [
        "Replace/augment GCN with temporal attention models (TGAT/TGN) to learn time-decay and multi-scale temporal patterns instead of fixed α in T.",
        "Model edge features explicitly (value, gas price, token type) and learn a parametric temporal kernel f(t) with neural time encodings.",
        "Adopt advanced sampling (GraphSAINT, Cluster-GCN) combined with TRW for better scalability and variance reduction.",
        "Train anomaly detector end-to-end (e.g., Deep SVDD or graph autoencoder with temporal decoder) rather than post-hoc classical detectors.",
        "Hyperparameterize and learn α in the temporal transition matrix jointly with GCN parameters; add attention over neighbors.",
        "Introduce hierarchical/bipartite sender-receiver modeling and community-aware pooling to capture meso-scale bursts.",
        "Implement mini-batch dynamic GNN training with PyTorch Geometric Temporal and memory modules for streaming Ethereum data.",
        "Calibrate thresholds using EVT/calibration layers and evaluate with ROC/AUPRC using partial labels or weak supervision."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "PyTorch",
        "PyTorch Geometric"
      ],
      "reproducibility_score": "low",
      "computational_requirements": "Authors note training/testing on Ethereum requires significant computational resources due to >10M blocks; experiments use block-range subgraphs; no specific GPU/time provided."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Blockchain analytics on Ethereum Mainnet via node providers (Infura/Alchemy)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Building and maintaining large, evolving transaction graphs at scale.",
        "Absence of labeled ground truth for reliable evaluation and thresholding.",
        "Potential high false positive rates with some unsupervised detectors (DBSCAN/SVM).",
        "Significant computational/storage requirements for full-network processing.",
        "Need for robust temporal feature engineering and windowing choices.",
        "Dependency on Ethereum node access (Infura/Alchemy) and data throughput limits."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces a probabilistic sampling-enhanced Temporal-Spatial GCN that integrates Temporal Random Walks (TRW) into GCN for Ethereum anomaly detection.",
      "Defines a temporal transition matrix replacing adjacency in GCN to weight temporally proximate interactions; combines spatial and temporal node features.",
      "Proposes a scalable training strategy by sampling subgraphs via TRW to prioritize recent activity.",
      "Provides an algorithm to construct Ethereum transaction graphs over block ranges and design of 7 temporal/spatial node features.",
      "Preliminary empirical analysis showing improved anomaly/burst detection with TRW-GCN embeddings for IsoForest/LOF on 1000-block graphs, e.g., “over 4000 anomalies detected vs 0.”"
    ]
  },
  {
    "arxiv_id": "2309.16422v1",
    "title": "Cyber Sentinel: Exploring Conversational Agents in Streamlining Security Tasks with GPT-4",
    "authors": "Mehrdad Kaheh; Danial Khosh Kholgh; Panos Kostakos",
    "abstract": "In an era where cyberspace is both a battleground and a backbone of modern society, the urgency of safeguarding digital assets against ever-evolving threats is paramount. This paper introduces Cyber Sentinel, an innovative task-oriented cybersecurity dialogue system that is effectively capable of managing two core functions: explaining potential cyber threats within an organization to the user, and taking proactive/reactive security actions when instructed by the user. Cyber Sentinel embodies the fusion of artificial intelligence, cybersecurity domain expertise, and real-time data analysis to combat the multifaceted challenges posed by cyber adversaries. This article delves into the process of creating such a system and how it can interact with other components typically found in cybersecurity organizations. Our work is a novel approach to task-oriented dialogue systems, leveraging the power of chaining GPT-4 models combined with prompt engineering across all sub-tasks. We also highlight its pivotal role in enhancing cybersecurity communication and interaction, concluding that not only does this framework enhance the system's transparency (Explainable AI) but also streamlines the decision-making process and responding to threats (Actionable AI), therefore marking a significant advancement in the realm of cybersecurity communication.",
    "published_date": "2023-09-28",
    "pdf_link": "https://arxiv.org/pdf/2309.16422v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Security Operations",
      "subdomain": "Threat Intelligence and SIEM/SOAR Automation",
      "specific_problem": "Using an LLM-driven conversational agent to (a) explain CTI/logs to analysts and (b) execute instructed security actions (e.g., SIEM rule/blacklist updates, firewall actions)",
      "attack_types": [
        "Indicators of Compromise (IPs, URLs, file hashes)",
        "malware",
        "phishing",
        "network intrusions"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer LLM",
        "specific": "GPT-4",
        "novel_contribution": "Sequential chaining of GPT-4 calls with carefully designed system prompts to extract intents/slots for cybersecurity tasks and drive CTI queries and SIEM actions"
      },
      {
        "type": "primary",
        "category": "Prompt Engineering",
        "specific": null,
        "novel_contribution": "Task-oriented system prompt that classifies user messages into intents (irrelevant, cybersecurity, query, action) and guides structured responses"
      },
      {
        "type": "primary",
        "category": "Dialogue State Tracking / Slot-filling",
        "specific": null,
        "novel_contribution": "Intent/slot schema for cybersecurity queries and actions; agent elicitation of missing slots before executing actions or queries"
      }
    ],
    "learning_paradigm": [
      "Prompt-based zero-shot/few-shot inference",
      "Tool-augmented LLM chaining via APIs"
    ],
    "datasets": [
      {
        "name": "Abuse URL (URLhaus API)",
        "type": "public",
        "domain": "threat_intelligence_feeds",
        "link": "https://urlhaus-api.abuse.ch/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Abuese Malware",
        "type": "public",
        "domain": "threat_intelligence_feeds",
        "link": "https://urlhaus-api.abuse.ch/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Malware Bazaar",
        "type": "public",
        "domain": "threat_intelligence_feeds",
        "link": "https://bazaar.abuse.ch/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "AlienVault OTX",
        "type": "public",
        "domain": "threat_intelligence_feeds",
        "link": "https://otx.alienvault.com/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Anomali (Threat Intelligence)",
        "type": "public",
        "domain": "threat_intelligence_feeds",
        "link": "https://www.anomali.com/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "IoC Signature Database (Elasticsearch index aggregated from OSINT feeds)",
        "type": "proprietary",
        "domain": "threat_intelligence_feeds",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "RQ1: Can LLMs be utilized to understand cybersecurity logs, events, and threat feeds and explain them adequately to a human operator? (Explainable AI)",
        "RQ2: Can LLMs take security actions based on instructions from a human operator? (Actionable AI)"
      ],
      "gaps_identified": [
        "The application of LLMs in cybersecurity remains relatively underexplored compared to other fields.",
        "LLMs face difficulties with ambiguous or highly complex queries and specialized abbreviations, leading to imprecise responses.",
        "Evaluating precise effectiveness of LLM-driven agents in operational environments is challenging."
      ],
      "limitations": [
        "Proof-of-concept with basic functionalities; non-exhaustive intents and slots.",
        "Effectiveness in real operational environments is not rigorously measured.",
        "No quantitative evaluation or baseline comparisons.",
        "Reliance on external GPT-4 API and OSINT feeds; potential issues with latency/cost/privacy are not empirically assessed."
      ],
      "future_work": [
        "Expand the set of intents and slot schemas to cover more SOC workflows.",
        "More rigorous evaluation in operational environments, including quantitative metrics and user studies.",
        "Integrate additional CTI sources and automate more SOAR actions with approval workflows.",
        "Harden the system against prompt injection and adversarial inputs.",
        "Investigate domain adaptation or fine-tuning for cybersecurity terminology and logs."
      ],
      "motivation": "To explore whether LLMs (GPT-4) can streamline SOC workflows by explaining CTI/logs to humans (Explainable AI) and by executing operator-approved security actions (Actionable AI), addressing a relatively underexplored application of LLMs in cybersecurity.",
      "potential_research_ideas": [
        "Develop a standardized evaluation benchmark for LLM-driven SOC assistants with labeled tasks, gold rationales, and safe-action checks.",
        "Design a safety layer for action execution using formal policy verification and multi-step approvals to prevent misuse or harmful actions.",
        "RAG pipeline that indexes SIEM/EDR/KV stores and CTI with structured extraction for grounded answers and reduced hallucinations.",
        "On-prem deployment via fine-tuned or adapter-based open LLMs to mitigate privacy and latency concerns; compare with GPT-4.",
        "Adversarial robustness studies for prompt injection, tool-use jailbreaks, and malicious CTI poisoning, with red teaming protocols.",
        "Confidence calibration and provenance scoring that fuses CTI source reputation with model uncertainty for analyst-facing explanations.",
        "Multi-agent SOC framework where specialized agents handle CTI triage, correlation, action simulation, and human-in-the-loop validation.",
        "Data-centric improvements: synthetic generation of labeled SOC dialogues and action traces to train intent/slot models and validators."
      ],
      "architectural_improvement_recommendations": [
        "Implement function-calling/tool-use with strict JSON schema and constrained decoding for intents/slots and actions.",
        "Add explicit dialog state tracker backed by a schema/ontology; persist state in a transactional store.",
        "Introduce Retrieval-Augmented Generation over Elasticsearch/Kibana indices and CTI knowledge bases for grounding.",
        "Create an approval and RBAC policy layer with simulation (dry-run) before enforcement; log all actions immutably.",
        "Guardrail middleware for prompt injection filtering, input validation, and output sanitization.",
        "Set up evaluation harness with unit tests for intents/slots, end-to-end task success, and safe-action criteria.",
        "Caching, batching, and fallback models (smaller LLMs) to reduce latency/cost; selective routing based on task complexity.",
        "Consider fine-tuning domain-specific models or LoRA adapters on SOC dialogues and incident reports."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "OpenAI API (GPT-4)",
        "Elasticsearch",
        "Wazuh",
        "Kibana"
      ],
      "reproducibility_score": "low",
      "computational_requirements": "Requires access to GPT-4 API; an Elasticsearch cluster for CTI indexing; Wazuh manager/agents for SIEM/IDS; standard server resources for integration."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Prototype integration with Wazuh SIEM/IDS and Elasticsearch CTI index; intended for enterprise SOC environments.",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Data privacy concerns when sending logs/threat data to a cloud LLM API.",
        "Latency and cost of GPT-4 for continuous SOC operations.",
        "Risk of hallucinations or incorrect extractions leading to unsafe actions.",
        "Prompt injection and tool-use jailbreak risks.",
        "Need for RBAC, approvals, and auditability before executing security actions.",
        "Integration complexity with diverse SIEM/SOAR stacks beyond Wazuh."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces Cyber Sentinel, a task-oriented cybersecurity dialogue system capable of explaining CTI/logs and executing operator-instructed security actions.",
      "Proposes a framework that chains GPT-4 calls with prompt engineering across sub-tasks (intent/slot extraction, querying, action execution).",
      "Integrates OSINT CTI feeds into an Elasticsearch-backed IoC database and connects to Wazuh SIEM/IDS for alerts and response actions.",
      "Provides a system design with intents, slots, and a system instruction prompt template; demonstrates example interactions and logic flow.",
      "Positions the system within Explainable AI (transparent summaries to analysts) and Actionable AI (triggering mitigations), highlighting potential benefits and limitations."
    ]
  },
  {
    "arxiv_id": "2310.04197v1",
    "title": "Threat Trekker: An Approach to Cyber Threat Hunting",
    "authors": "Ángel Casanova Bienzobas; Alfonso Sánchez-Macián",
    "abstract": "Threat hunting is a proactive methodology for exploring, detecting and mitigating cyberattacks within complex environments. As opposed to conventional detection systems, threat hunting strategies assume adversaries have infiltrated the system; as a result they proactively search out any unusual patterns or activities which might indicate intrusion attempts.   Historically, this endeavour has been pursued using three investigation methodologies: (1) Hypothesis-Driven Investigations; (2) Indicator of Compromise (IOC); and (3) High-level machine learning analysis-based approaches. Therefore, this paper introduces a novel machine learning paradigm known as Threat Trekker. This proposal utilizes connectors to feed data directly into an event streaming channel for processing by the algorithm and provide feedback back into its host network.   Conclusions drawn from these experiments clearly establish the efficacy of employing machine learning for classifying more subtle attacks.",
    "published_date": "2023-10-06",
    "pdf_link": "https://arxiv.org/pdf/2310.04197v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Threat Hunting",
      "specific_problem": "Machine-learning-based classification of network logs/flows into benign vs. malicious and MITRE ATT&CK tactic classes within a streaming SOC pipeline",
      "attack_types": [
        "Reconnaissance",
        "Discovery",
        "Credential Access",
        "Privilege Escalation",
        "Exfiltration",
        "Lateral Movement",
        "Resource Development",
        "Defense Evasion",
        "Initial Access",
        "Persistence"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Random Forest",
        "specific": "Random Forest classifier",
        "novel_contribution": "Used as the core classifier in the Threat Trekker pipeline for multi-class (MITRE tactic) and binary attack detection on network log datasets"
      },
      {
        "type": "primary",
        "category": "Preprocessing/Encoding",
        "specific": "One-Hot Encoding for categorical fields",
        "novel_contribution": "Applied to fields such as connection state and protocol type to prepare heterogeneous logs for ML"
      },
      {
        "type": "primary",
        "category": "Feature Engineering",
        "specific": "IPv4 to 32-bit integer; timestamps to Unix epoch",
        "novel_contribution": "Deterministic IPv4 encoding algorithm (Algorithm 1) and timestamp normalization to unify datatypes across datasets"
      },
      {
        "type": "primary",
        "category": "Imbalance Handling",
        "specific": "Undersampling, Random Oversampling, SMOTE",
        "novel_contribution": "Three-step balancing pipeline to counter severe class imbalance in UWF-2022 while retaining minority classes"
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "UWF-2022",
        "type": "",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      },
      {
        "name": "CIC-IDS2017",
        "type": "",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      },
      {
        "name": "TON IoT",
        "type": "",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "accuracy"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Little standardization among organizations for collecting and storing tracking/log information",
        "Access to authentic enterprise data is restricted due to PII and legal data-protection constraints",
        "Severe class imbalance in real-world threat datasets complicates training and evaluation"
      ],
      "limitations": [
        "No real-world deployment due to lack of an authentic environment and PII constraints",
        "Classes with only one instance were removed, which may limit coverage of rare tactics",
        "Oversampling (including SMOTE) may cause overfitting and reduce generalization",
        "Binary consolidation of attack classes sacrifices ability to detect the least probable attacks"
      ],
      "future_work": [
        "Disseminate training outcomes back onto the network via the event streaming mechanism",
        "Integrate with SIEM systems and defensive applications (e.g., antivirus) to generate alerts based on predicted attack types",
        "Full implementation of the connector-based streaming deployment once an authentic environment is available"
      ],
      "motivation": "Scale proactive threat hunting beyond manual analysis by leveraging machine learning over large volumes of heterogeneous network logs/flows, and integrate it into a streaming SOC pipeline.",
      "potential_research_ideas": [
        "Federated or privacy-preserving learning to enable training on sensitive enterprise data without centralizing PII",
        "Domain adaptation and transfer learning across heterogeneous datasets (UWF-2022, CIC-IDS2017, TON IoT) for cross-environment generalization",
        "Self-supervised or representation learning on raw logs/flows to reduce reliance on heavy feature engineering",
        "Hierarchical classification aligned to MITRE ATT&CK (tactic→technique) to improve rare-class performance",
        "Online learning with concept drift detection for streaming deployment in dynamic networks",
        "Explainable ML tailored to ATT&CK mapping to aid analyst triage and hypotheses",
        "Adversarial robustness studies against evasion/poisoning on network log classifiers",
        "Multimodal fusion of logs, flows, and host telemetry via event streaming for improved detection of subtle APT behaviors"
      ],
      "architectural_improvement_recommendations": [
        "Adopt class-imbalance-aware learning (cost-sensitive RF, focal loss via tree ensembles, or calibrated thresholding) instead of heavy oversampling",
        "Evaluate stronger tabular models (XGBoost/LightGBM/CatBoost) and compare to RF on the same pipeline",
        "Implement online/streaming training and inference with drift detectors (ADWIN, DDM) integrated into the event stream",
        "Replace numeric IP encoding with categorical hashing or learned embeddings to avoid artificial ordinality",
        "Build a hierarchical output head aligned to MITRE ATT&CK to reduce confusion among rare tactics",
        "Add probability calibration (Platt/Isotonic) for reliable alerting thresholds and SIEM integration",
        "Establish a feature store within the streaming architecture to ensure consistent training-serving features",
        "Comprehensive evaluation across datasets with unified schemas and cross-domain validation"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Planned SIEM-integrated enterprise network via event streaming connectors (future vision)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Access to real-world data restricted due to PII and legal constraints",
        "Heterogeneous and non-standardized logging across organizations",
        "Severe class imbalance in operational data",
        "Integration complexity with SIEM and event streaming connectors"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces Threat Trekker, a machine-learning-based threat hunting paradigm with connectors feeding an event streaming channel and future feedback to the host network",
      "Implements a full preprocessing pipeline for network logs including one-hot encoding, IPv4 integer encoding, and timestamp normalization",
      "Proposes and evaluates imbalance handling using undersampling, random oversampling, and SMOTE on highly skewed classes",
      "Validates applicability across multiple datasets (UWF-2022, CIC-IDS2017, TON IoT) to demonstrate adaptability to different environments",
      "Explores both multi-class (MITRE ATT&CK tactics) and binary (benign vs. malicious) classification formulations for threat hunting"
    ]
  },
  {
    "arxiv_id": "2310.12914v2",
    "title": "Network-Aware AutoML Framework for Software-Defined Sensor Networks",
    "authors": "Emre Horsanali; Yagmur Yigit; Gokhan Secinti; Aytac Karameseoglu; Berk Canberk",
    "abstract": "As the current detection solutions of distributed denial of service attacks (DDoS) need additional infrastructures to handle high aggregate data rates, they are not suitable for sensor networks or the Internet of Things. Besides, the security architecture of software-defined sensor networks needs to pay attention to the vulnerabilities of both software-defined networks and sensor networks. In this paper, we propose a network-aware automated machine learning (AutoML) framework which detects DDoS attacks in software-defined sensor networks. Our framework selects an ideal machine learning algorithm to detect DDoS attacks in network-constrained environments, using metrics such as variable traffic load, heterogeneous traffic rate, and detection time while preventing over-fitting. Our contributions are two-fold: (i) we first investigate the trade-off between the efficiency of ML algorithms and network/traffic state in the scope of DDoS detection. (ii) we design and implement a software architecture containing open-source network tools, with the deployment of multiple ML algorithms. Lastly, we show that under the denial of service attacks, our framework ensures the traffic packets are still delivered within the network with additional delays.",
    "published_date": "2023-10-19",
    "pdf_link": "https://arxiv.org/pdf/2310.12914v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "IoT Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "DDoS detection and mitigation in Software-Defined Sensor Networks (SDSN) via a network-aware AutoML model selection framework",
      "attack_types": [
        "DDoS",
        "DoS"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "AutoML",
        "specific": "Network-aware model selection heuristic (weighted objective on accuracy and detection time)",
        "novel_contribution": "AutoML selects among multiple ML classifiers using network/system parameters (variable traffic payloads, node power levels, heterogeneous traffic speeds) with a per-algorithm weighted objective argmax_i (alpha_i * accuracy_i + beta_i * detection_time_i)."
      },
      {
        "type": "baseline",
        "category": "Decision Tree",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Random Forest",
        "specific": null,
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Baseline SDSN DDoS detection datasets (9 variants by payload size and traffic speed)",
        "type": "synthetic",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Greedy algorithm (non-ML) for DDoS detection",
        "paper_reference": null,
        "metric": "Round Trip Time (RTT) and network reachability under attack",
        "their_result": "AutoML-based system maintains reachability with improved RTT vs. no mitigation and better RTT than greedy, while still incurring additional delays (no absolute numbers provided).",
        "baseline_result": "Greedy algorithm restores reachability but RTT remains high for sensor networks; without mitigation the system is unreachable for substantial periods."
      },
      {
        "method_name": "Random Forest classifier",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "Framework may not select RF if detection time is suboptimal for current network; overall system prioritizes network-aware trade-off.",
        "baseline_result": "Random Forest achieves the best accuracy among the six tested ML algorithms (no absolute value reported)."
      },
      {
        "method_name": "Decision Tree classifier",
        "paper_reference": null,
        "metric": "Detection time (inference/decision latency)",
        "their_result": "AutoML often favors faster models when network-constrained; selection balances accuracy and detection time.",
        "baseline_result": "Decision Tree offers the best performance in terms of detection time among tested models (no absolute value reported)."
      }
    ],
    "performance_metrics_used": [
      "accuracy",
      "detection time",
      "round trip time (RTT)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Existing DDoS detection solutions require additional infrastructures and are unsuitable for sensor networks/IoT due to high aggregate data rates.",
        "SDSN security must address vulnerabilities of both SDN and sensor networks (controller, nodes, protocols).",
        "Security for SDSN is underexplored; most current SDSN work focuses on architectural frameworks rather than robust attack detection.",
        "Resource constraints of sensor nodes limit applicability of compute-heavy detection methods."
      ],
      "limitations": [
        "Evaluation conducted on a simple SDSN topology simulated in Mininet.",
        "Non-real-time AutoML buffer interval is relatively large (60–300 seconds) in current experiments."
      ],
      "future_work": [
        "Expand the framework using a more complex topology.",
        "Reduce the buffer time interval to 1–60 seconds to approximate real-life SDSN scenarios."
      ],
      "motivation": "Provide a DDoS detection approach suitable for resource-constrained Software-Defined Sensor Networks that balances ML accuracy with network conditions by automatically selecting the most appropriate model.",
      "potential_research_ideas": [
        "Develop a multi-objective AutoML scheme using Pareto-optimal selection (accuracy, detection latency, controller load, energy) instead of a fixed weighted scalarization.",
        "Incorporate online/streaming learning with concept drift detection to adapt to evolving DDoS behaviors in SDSN traffic.",
        "Integrate lightweight deep models with knowledge distillation to edge gateways and compare against classic ML under tight CPU/energy budgets.",
        "Leverage transfer learning or meta-learning across different SDSN topologies to reduce cold-start time for model selection.",
        "Augment with adversarially robust training and evasion-aware feature engineering for flow-statistics-based detectors.",
        "Provide interpretable selection and decision explanations (feature importances, SHAP) to support operator trust and tuning."
      ],
      "architectural_improvement_recommendations": [
        "Replace fixed weighted objective with adaptive or Pareto-based multi-objective optimization; learn alpha/beta from network KPIs.",
        "Add stream/incremental learners (e.g., Hoeffding Trees, Online Bagging) for continuous training without pausing the system.",
        "Implement hierarchical/federated detection: pre-filter at sensor gateways, escalate to controller to reduce latency and controller load.",
        "Introduce feature selection and normalization pipelines; cache model artifacts per traffic regime for faster switching.",
        "Extend candidate pool with calibrated linear models, k-NN, Naive Bayes, and lightweight neural nets; include model distillation.",
        "Automate mitigation policy selection (rate limiting vs. flow deletion) via reinforcement learning based on network impact."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "scikit-learn",
        "RYU SDN Controller",
        "Mininet",
        "D-ITG"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "RYU 4.34 (Python 3.7.10), Mininet 2.3.0 on Ubuntu 16.04; no GPU/CPU requirements reported."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Laboratory SDSN testbed using RYU SDN controller and Mininet (simulated OpenFlow switches/hosts).",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Overhead of periodic OpenFlow statistics collection for monitoring.",
        "Non-real-time AutoML selection relies on buffer intervals, introducing selection latency.",
        "Trade-off between accuracy and detection time under constrained resources.",
        "Mitigation via flow deletion may introduce additional delays and packet drops.",
        "Centralized controller reliance may become a bottleneck during large-scale attacks.",
        "Tuning alpha/beta weights per algorithm requires calibration to network conditions."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Investigates the trade-off between ML algorithm efficiency (accuracy and detection time) and network/traffic state for DDoS detection in SDSN.",
      "Designs and implements a network-aware AutoML framework and open-source toolchain-based architecture (RYU, Mininet, D-ITG, scikit-learn) that selects the ML algorithm best suited to current network constraints.",
      "Demonstrates that under DDoS attacks, the framework sustains packet delivery across the network albeit with additional delays, improving reachability and RTT compared to no mitigation and a simple greedy approach."
    ]
  },
  {
    "arxiv_id": "2309.09807v1",
    "title": "Efficient Concept Drift Handling for Batch Android Malware Detection Models",
    "authors": "Molina-Coronado B.; Mori U.; Mendiburu A.; Miguel-Alonso J",
    "abstract": "The rapidly evolving nature of Android apps poses a significant challenge to static batch machine learning algorithms employed in malware detection systems, as they quickly become obsolete. Despite this challenge, the existing literature pays limited attention to addressing this issue, with many advanced Android malware detection approaches, such as Drebin, DroidDet and MaMaDroid, relying on static models. In this work, we show how retraining techniques are able to maintain detector capabilities over time. Particularly, we analyze the effect of two aspects in the efficiency and performance of the detectors: 1) the frequency with which the models are retrained, and 2) the data used for retraining. In the first experiment, we compare periodic retraining with a more advanced concept drift detection method that triggers retraining only when necessary. In the second experiment, we analyze sampling methods to reduce the amount of data used to retrain models. Specifically, we compare fixed sized windows of recent data and state-of-the-art active learning methods that select those apps that help keep the training dataset small but diverse. Our experiments show that concept drift detection and sample selection mechanisms result in very efficient retraining strategies which can be successfully used to maintain the performance of the static Android malware state-of-the-art detectors in changing environments.",
    "published_date": "2023-09-18",
    "pdf_link": "https://arxiv.org/pdf/2309.09807v1",
    "paper_types": [
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Mobile Security",
      "subdomain": "Android Malware Detection",
      "specific_problem": "Maintaining and adapting batch/static Android malware detectors under concept drift",
      "attack_types": [
        "Android malware (general)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Concept Drift Detection",
        "specific": "Page-Hinkley test (PH)",
        "novel_contribution": "Used as a model-agnostic supervisor to trigger retraining only when performance degradation is detected"
      },
      {
        "type": "primary",
        "category": "Active Learning",
        "specific": "Uncertainty Sampling",
        "novel_contribution": "Applied to select a small but informative subset of apps for retraining to reduce labeling/retraining cost"
      },
      {
        "type": "primary",
        "category": "Contrastive Learning",
        "specific": "Contrastive Learning for OOD sample selection",
        "novel_contribution": "Used as a model-agnostic mechanism to identify out-of-distribution/drifting samples for inclusion in retraining"
      },
      {
        "type": "primary",
        "category": "Data Management Policy",
        "specific": "Fixed-size Sliding Window",
        "novel_contribution": "Evaluated as a simple forgetting mechanism to maintain a compact, recent training set"
      },
      {
        "type": "primary",
        "category": "Sample Selection",
        "specific": "Ad-hoc problem-specific sample selection",
        "novel_contribution": "An Android-specific strategy to remove uninformative data and keep the training set small yet diverse"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Active Learning",
      "Batch Learning",
      "Concept Drift/Change Detection"
    ],
    "datasets": [
      {
        "name": "Android apps dataset (time-evolving, labeled goodware/malware)",
        "type": "public",
        "domain": "mobile_android_apps",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Fixed-period retraining",
        "paper_reference": null,
        "metric": "Amean (mean of TPR and TNR), TPR, TNR",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Sliding window retraining (fixed-size recent data)",
        "paper_reference": null,
        "metric": "Amean (mean of TPR and TNR), TPR, TNR",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Static detectors without retraining (e.g., Drebin, DroidDet, MaMaDroid)",
        "paper_reference": null,
        "metric": "Amean (mean of TPR and TNR), TPR, TNR",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "TPR (True Positive Rate)",
      "TNR (True Negative Rate)",
      "Amean = (TPR+TNR)/2"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can simple retraining policies maintain the performance of static batch Android malware detectors over time under concept drift?",
        "What retraining frequency policy is more effective: fixed periodic retraining or drift-triggered retraining via Page-Hinkley?",
        "How can we reduce the amount of data (and labeling cost) needed for retraining while preserving performance?"
      ],
      "gaps_identified": [
        "Limited attention in the Android malware literature to concept drift handling; many detectors rely on static models.",
        "Existing adaptive approaches often propose entirely new detectors rather than adapting state-of-the-art static detectors with retraining.",
        "Labeling cost and storage/computation for frequent retraining grow with incoming data volume.",
        "Pseudo-labeling can cause model contamination over time.",
        "Prior OOD/sample selection methods are not directly model-agnostic or tailored to Android-specific dynamics."
      ],
      "limitations": [
        "Retraining requires labeled data; labeling incurs non-trivial human/operational cost.",
        "Fixed-size sliding windows may forget reoccurring or stable behaviors, leading to potential bias.",
        "PH threshold selection trades off false alarms vs delayed updates; suboptimal tuning affects effectiveness.",
        "Majority groups of near-duplicate apps can bias training sets if not explicitly handled."
      ],
      "future_work": [],
      "motivation": "To provide a practical, model-agnostic retraining framework that efficiently maintains the effectiveness of state-of-the-art batch Android malware detectors in the presence of concept drift.",
      "potential_research_ideas": [
        "Combine drift-triggered retraining with semi-supervised labeling (e.g., selective human-in-the-loop verification) to cut labeling costs while avoiding contamination.",
        "Meta-learning or bandit-based controllers that adaptively choose retraining frequency and sample selection policy based on online performance signals.",
        "Self-supervised representations of Android apps (code and metadata) to improve OOD detection and diversity-aware sampling for retraining.",
        "Rehearsal buffers with deduplication and diversity constraints to mitigate forgetting of reoccurring behaviors.",
        "Cross-modal fusion of static and lightweight dynamic features for drift-resilient detection with retraining.",
        "Calibrated uncertainty (e.g., temperature scaling, deep ensembles) to improve active learning sample selection reliability.",
        "Evaluate privacy-preserving retraining pipelines (e.g., federated labeling signals) for enterprise deployments."
      ],
      "architectural_improvement_recommendations": [
        "Warm-start retraining using previous model weights/parameters to reduce training cost and stabilize performance between updates.",
        "Diversity-constrained sampling (e.g., clustering + uncertainty) to avoid over-representation of near-duplicate apps.",
        "Adaptive PH thresholds based on recent volatility of performance metrics to balance sensitivity and false alarms.",
        "Ensemble models with drift-aware weighting/gating to increase robustness to transient shifts.",
        "Maintain a small rehearsal set of historically important behaviors via reservoir sampling and deduplication.",
        "Integrate calibrated uncertainty estimation (e.g., Monte Carlo dropout, deep ensembles) to improve active learning choices."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Ongoing labeling cost and data storage/processing overhead for retraining.",
        "Choosing retraining frequency/thresholds under variable drift rates.",
        "Managing dataset imbalance and near-duplicate majority groups to prevent bias.",
        "Risk of contamination when using pseudo-labels if adopted."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Empirical study of retraining strategies to handle concept drift for batch Android malware detectors.",
      "Comparison of fixed periodic retraining versus drift-triggered retraining using the Page-Hinkley test.",
      "Evaluation of data reduction strategies for retraining: fixed-size sliding windows, uncertainty sampling, contrastive learning OOD selection, and an Android-specific sample selection method.",
      "Demonstration that concept drift detection and sample selection mechanisms enable efficient retraining that maintains performance of state-of-the-art static detectors in changing environments."
    ]
  },
  {
    "arxiv_id": "2310.11763v1",
    "title": "PhishReplicant: A Language Model-based Approach to Detect Generated Squatting Domain Names",
    "authors": "Takashi Koide; Naoki Fukushi; Hiroki Nakano; Daiki Chiba",
    "abstract": "Domain squatting is a technique used by attackers to create domain names for phishing sites. In recent phishing attempts, we have observed many domain names that use multiple techniques to evade existing methods for domain squatting. These domain names, which we call generated squatting domains (GSDs), are quite different in appearance from legitimate domain names and do not contain brand names, making them difficult to associate with phishing. In this paper, we propose a system called PhishReplicant that detects GSDs by focusing on the linguistic similarity of domain names. We analyzed newly registered and observed domain names extracted from certificate transparency logs, passive DNS, and DNS zone files. We detected 3,498 domain names acquired by attackers in a four-week experiment, of which 2,821 were used for phishing sites within a month of detection. We also confirmed that our proposed system outperformed existing systems in both detection accuracy and number of domain names detected. As an in-depth analysis, we examined 205k GSDs collected over 150 days and found that phishing using GSDs was distributed globally. However, attackers intensively targeted brands in specific regions and industries. By analyzing GSDs in real time, we can block phishing sites before or immediately after they appear.",
    "published_date": "2023-10-18",
    "pdf_link": "https://arxiv.org/pdf/2310.11763v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Web Security",
      "subdomain": "Phishing Detection",
      "specific_problem": "Detection of generated squatting domains (GSDs) used for phishing by leveraging linguistic similarity of domain names",
      "attack_types": [
        "phishing",
        "domain squatting",
        "typosquatting",
        "combosquatting",
        "deceptive subdomains"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "Sentence-BERT (SBERT) fine-tuned from all-mpnet-base-v2 with Triplet Loss",
        "novel_contribution": "Adapts SBERT to domain-name strings by adding TLD and brand tokens to the tokenizer; uses triplet-loss fine-tuning to learn semantic similarity among GSDs"
      },
      {
        "type": "primary",
        "category": "Siamese/Triplet Network",
        "specific": "Triplet loss metric learning",
        "novel_contribution": "Learns an embedding space where GSDs similar to known phishing domains are close; enables similarity search rather than direct classification"
      },
      {
        "type": "primary",
        "category": "Clustering",
        "specific": "DBSCAN (cosine distance eps=0.04, MinPts=3)",
        "novel_contribution": "Clusters phishing-domain embeddings to extract sets of similar GSD patterns used to generate automated matching rules"
      },
      {
        "type": "primary",
        "category": "Approximate Nearest Neighbor Search",
        "specific": "Faiss",
        "novel_contribution": "Scalable k-NN search over millions of new domain embeddings for real-time detection"
      },
      {
        "type": "baseline",
        "category": "String distance",
        "specific": "Damerau-Levenshtein-based rule (edit distance 1–3) on subdomains",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Other",
        "specific": "CrowdCanary (supervised classifier) to source phishing-related Twitter posts",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Metric Learning",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "Certificate Transparency logs",
        "type": "public",
        "domain": "dns_certificates",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Passive DNS traffic (66 DNS cache servers in 18 countries)",
        "type": "proprietary",
        "domain": "passive_dns",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "DNS zone files / lists of registered domain names",
        "type": "public",
        "domain": "dns_zone_files",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "PhishTank feed",
        "type": "public",
        "domain": "phishing_urls",
        "link": "https://www.phishtank.com/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "OpenPhish feed",
        "type": "public",
        "domain": "phishing_urls",
        "link": "https://openphish.com/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Twitter phishing-reports via CrowdCanary",
        "type": "private",
        "domain": "social_media_reports",
        "link": null,
        "is_new_contribution": false,
        "availability": "available_on_request"
      },
      {
        "name": "PhishPedia brand list (277 brands)",
        "type": "public",
        "domain": "brand_list",
        "link": "https://github.com/rshaojimmy/PhishPedia",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "205k GSDs collected over 150 days",
        "type": "proprietary",
        "domain": "domain_names",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "GSD triplet training set (derived from phishing TI)",
        "type": "proprietary",
        "domain": "domain_names",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Damerau-Levenshtein edit-distance rule on subdomains",
        "paper_reference": null,
        "metric": "False positives on one-day CT logs",
        "their_result": null,
        "baseline_result": "over 4,500 false positives"
      }
    ],
    "performance_metrics_used": [
      "detection accuracy",
      "number of domains detected",
      "false positives",
      "proportion of detected domains later used for phishing"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can language-model embeddings capture linguistic similarity among domain names to detect GSDs that evade traditional squatting detectors?",
        "How does PhishReplicant compare to existing rule-based and ML-based systems in detection accuracy and coverage of domains not containing exact brand names?",
        "Can real-time analysis of CT logs, passive DNS, and registered domain lists detect attacker-acquired GSDs before or immediately after they are used for phishing?"
      ],
      "gaps_identified": [
        "Attackers generate GSDs by combining multiple squatting techniques, making rule-based detection difficult and labor-intensive to maintain.",
        "Simple string-distance metrics cannot express the similarity among GSDs that involve word-level insertions, deletions, and substitutions.",
        "Classification-based ML approaches yield many false positives and require frequent fine-tuning as new GSD patterns emerge."
      ],
      "limitations": [
        "This study excluded domains whose subdomains contain UUIDs or IPv4 addresses, and domains shorter than seven characters or all-numeric after removing the TLD.",
        "The approach relies on similarity to known phishing domains (from phishing TI) to extract clusters/patterns."
      ],
      "future_work": [],
      "motivation": "Detect generated squatting domains by focusing on linguistic similarity using a language model and automated matching rules, enabling real-time blocking of phishing sites before or immediately after they appear.",
      "potential_research_ideas": [
        "Integrate additional modalities (WHOIS, hosting ASN, TLS cert features, web content snapshots) for multi-signal fusion to reduce false positives while maintaining early detection.",
        "Extend to robust IDN/punycode handling and cross-lingual tokenization to capture homograph and non-Latin brand-targeting GSDs.",
        "Develop adversarial training or data augmentation that simulates multi-technique GSD generation to harden the embedding model.",
        "Online/streaming clustering with concept drift detection to adapt clusters and thresholds as attacker tactics evolve.",
        "Graph-based modeling of domains, subdomains, and lexical tokens (GNNs) to capture higher-order relations among GSD families.",
        "Generative models to synthesize plausible GSD candidates for proactive detection and sinkholing.",
        "Human-in-the-loop active learning for high-uncertainty candidates to iteratively refine the embedding space."
      ],
      "architectural_improvement_recommendations": [
        "Hard-negative mining in triplet training (e.g., domains from benign but lexically similar distributions) to sharpen the decision boundary.",
        "Per-cluster adaptive thresholds and learned rule induction (e.g., differentiable rule learners) instead of fixed eps/MinPts and simple rule templates.",
        "Use character-level or byte-level Transformer encoders specialized for short strings and domain syntax.",
        "Employ HNSW or IVF-PQ indexing in Faiss with quantization-aware re-ranking to scale further with bounded recall loss.",
        "Incorporate punycode normalization and confusable-character mapping before embedding to mitigate homograph attacks.",
        "Joint contrastive training with URL path features (when available) to better separate benign lookalikes."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/tkoide398/PhishReplicant",
      "frameworks": [
        "SentenceTransformers",
        "PyTorch",
        "Faiss",
        "scikit-learn"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Real-time pipeline using CT logs, passive DNS from 66 DNS cache servers in 18 countries, and lists of registered domain names",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes PhishReplicant, which utilizes language models and automated matching rules to detect GSDs by analyzing similarity between phishing domain names.",
      "Real-time 4-week experiment: \"We detected 3,498 domain names acquired by attackers in a four-week experiment, of which 2,821 were used for phishing sites within a month of detection.\"",
      "Comparative evaluation: \"We also confirmed that our proposed system outperformed existing systems in both detection accuracy and number of domain names detected.\"",
      "In-depth analysis of 205k GSDs over 150 days showing global distribution, regional/industry targeting bias, and multi-day persistence of GSD clusters (median 41 days)."
    ]
  },
  {
    "arxiv_id": "2310.08275v4",
    "title": "Harnessing the Power of LLM to Support Binary Taint Analysis",
    "authors": "Puzhuo Liu; Chengnian Sun; Yaowen Zheng; Xuan Feng; Chuan Qin; Yuncheng Wang; Zhenyang Xu; Zhi Li; Peng Di; Yu Jiang; Limin Sun",
    "abstract": "This paper proposes LATTE, the first static binary taint analysis that is powered by a large language model (LLM). LATTE is superior to the state of the art (e.g., Emtaint, Arbiter, Karonte) in three aspects. First, LATTE is fully automated while prior static binary taint analyzers need rely on human expertise to manually customize taint propagation rules and vulnerability inspection rules. Second, LATTE is significantly effective in vulnerability detection, demonstrated by our comprehensive evaluations. For example, LATTE has found 37 new bugs in real-world firmware which the baselines failed to find, and 7 of them have been assigned CVE numbers. Lastly, LATTE incurs remarkably low engineering cost, making it a cost-efficient and scalable solution for security researchers and practitioners. We strongly believe that LATTE opens up a new direction to harness the recent advance in LLMs to improve vulnerability analysis for binary programs.",
    "published_date": "2023-10-12",
    "pdf_link": "https://arxiv.org/pdf/2310.08275v4",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Binary Analysis / Vulnerability Detection",
      "specific_problem": "LLM-powered static binary taint analysis for taint-style vulnerability detection in compiled binaries and embedded firmware",
      "attack_types": [
        "CWE-78 OS Command Injection",
        "CWE-120 Buffer Overflow",
        "CWE-134 Format String",
        "CWE-190 Integer Overflow or Wraparound",
        "CWE-606 Unchecked Loop Condition"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Large Language Model (Transformer)",
        "specific": null,
        "novel_contribution": "Uses prompt-engineered multi-step LLM reasoning driven by static code slicing to automate taint source/sink identification, taint propagation reasoning, and sink inspection in binaries"
      }
    ],
    "learning_paradigm": [
      "Prompt-based",
      "Zero-shot / In-context learning"
    ],
    "datasets": [
      {
        "name": "Juliet Test Suite (taint-style vulnerability subjects; compiled and stripped)",
        "type": "public",
        "domain": "binary_executables (compiled C/C++ test cases for vulnerabilities)",
        "link": "https://samate.nist.gov/SARD/testsuite.php",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Karonte embedded device firmware dataset",
        "type": "public",
        "domain": "embedded_firmware_binaries",
        "link": "https://github.com/ucsb-seclab/karonte",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Emtaint",
        "paper_reference": "[11]",
        "metric": "Accuracy and F1 on Juliet taint-style vulnerability types",
        "their_result": "“The accuracy and F1 of LATTE vulnerability checking exceeds Emtaint [11] and Arbiter [86] on every vulnerability type.”",
        "baseline_result": null
      },
      {
        "method_name": "Arbiter",
        "paper_reference": "[86]",
        "metric": "Accuracy and F1 on Juliet taint-style vulnerability types",
        "their_result": "“The accuracy and F1 of LATTE vulnerability checking exceeds Emtaint [11] and Arbiter [86] on every vulnerability type.”",
        "baseline_result": null
      },
      {
        "method_name": "Karonte",
        "paper_reference": "[72]",
        "metric": "Number of unique bugs found in Karonte firmware dataset",
        "their_result": "“LATTE detected a total of 119 unique bugs, including 37 previously unknown bugs (10 CVE numbers have been given due to high threat), which outperforms the state of the art [11, 72, 86].”",
        "baseline_result": "“at least 21 more than the baselines.”"
      },
      {
        "method_name": "SATC",
        "paper_reference": "[9]",
        "metric": "Qualitative comparison of supported CWE types and rule engineering needs",
        "their_result": "Automation without manual rule crafting; wider coverage via LLM reasoning",
        "baseline_result": null
      },
      {
        "method_name": "FBI",
        "paper_reference": "[43]",
        "metric": "Qualitative comparison of supported CWE types and rule engineering needs",
        "their_result": "Automation without manual rule crafting; wider coverage via LLM reasoning",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "F1 Score",
      "Coverage of sink identification (correct coverage)",
      "Coverage of source identification (correct coverage)",
      "Dangerous flow extraction coverage (recall of correct flows)",
      "Number of unique bugs found in firmware",
      "Number of new bugs found",
      "Number of assigned CVEs"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can LLMs automate and improve static binary taint analysis to reduce human-crafted rules while maintaining or improving accuracy?",
        "How to design prompt sequences and encode code context so LLMs can effectively reason about taint propagation and vulnerability patterns within context limits?"
      ],
      "gaps_identified": [
        "Prior static binary taint analyzers rely on extensive manual rule engineering for sources, sanitizers, propagation, and sink inspection, leading to false positives/negatives and poor scalability.",
        "Binaries lack rich semantic information (symbols, types), making manual rule coverage incomplete across architectures and compilers.",
        "Existing LLM applications to code struggle with large codebases due to context length limitations and naive prompting."
      ],
      "limitations": [
        "LLM context length constraints require careful encoding and slicing of code context to fit prompts.",
        "Prompt engineering quality significantly influences results; poorly constructed prompts can lead to unsatisfactory analysis.",
        "Reliance on LLMs may introduce non-determinism and dependency on proprietary models (implied)."
      ],
      "future_work": [
        "“We strongly believe that LATTE opens up a new direction to harness the recent advance in LLMs to improve vulnerability analysis for binary programs.”"
      ],
      "motivation": "Automate and scale static binary taint analysis by leveraging LLMs' code understanding, eliminating manual rule crafting and improving vulnerability detection effectiveness in real-world binaries and firmware.",
      "potential_research_ideas": [
        "Fine-tune or distill open-source code LLMs on decompiled binary corpora and CWE-aligned examples to reduce dependence on proprietary models and improve robustness to decompiler artifacts.",
        "Integrate symbolic execution or SMT-based validators to automatically verify LLM-proposed taint flows and sink exploits, reducing false positives.",
        "Use retrieval-augmented prompting with library/API semantics (e.g., libc, OpenSSL, firmware-specific APIs) to enhance source/sanitizer recognition.",
        "Adopt multi-agent LLM workflows (planner, slicer, verifier) to cross-check taint reasoning and resolve conflicts.",
        "Hybrid graph learning: encode interprocedural data/control-flow graphs with GNNs to preselect candidate flows, then hand off to LLM for semantic inspection.",
        "Long-context or hierarchical prompting that summarizes callees and composes summaries up the call chain to mitigate context-length limits.",
        "Counterfactual prompting to assess sanitization sufficiency by asking LLMs to construct failing inputs and then checking feasibility with lightweight constraint solving."
      ],
      "architectural_improvement_recommendations": [
        "Introduce a verification layer that replays LLM-identified flows using lightweight symbolic execution or range analysis before reporting.",
        "Adopt retrieval modules that attach API specs and sanitizer patterns to prompts, improving consistency across libraries.",
        "Cache and reuse function-level semantic summaries to reduce token usage and improve scalability across large firmware images.",
        "Leverage open-source long-context LLMs or chunking with hierarchical summaries to expand analyzable scope per prompt.",
        "Implement uncertainty estimation (self-consistency or majority voting over diverse prompts) to flag low-confidence findings for deeper analysis."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Offline analysis of embedded device firmware and compiled binaries",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "LLM context length constraints necessitate careful code slicing and summarization",
        "Prompt engineering sensitivity affects analysis quality",
        "Potential dependence on proprietary LLM APIs and associated data privacy concerns when analyzing proprietary firmware",
        "Binary variability across architectures/compilers complicates consistent decompilation and semantic recovery"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces LATTE, the first LLM-powered static binary taint analysis that automates source/sink identification, taint propagation reasoning, and sink inspection.",
      "Presents a code-slicing-driven, multi-step prompt engineering pipeline combining static analyses (flow, alias, taint) with LLM reasoning under context constraints.",
      "Demonstrates superior effectiveness: exceeds Emtaint and Arbiter in Accuracy and F1 on Juliet; 100% correct coverage for sink/source identification; >95% correct dangerous flow extraction; detects 119 unique firmware bugs including 37 new (10 CVEs), at least 21 more than baselines.",
      "Claims lower engineering cost and improved scalability compared to prior manual rule-based taint analyzers."
    ]
  },
  {
    "arxiv_id": "2310.03667v1",
    "title": "Enhancing Exfiltration Path Analysis Using Reinforcement Learning",
    "authors": "Riddam Rishu; Akshay Kakkar; Cheng Wang; Abdul Rahman; Christopher Redino; Dhruv Nandakumar; Tyler Cody; Ryan Clark; Daniel Radke; Edward Bowen",
    "abstract": "Building on previous work using reinforcement learning (RL) focused on identification of exfiltration paths, this work expands the methodology to include protocol and payload considerations. The former approach to exfiltration path discovery, where reward and state are associated specifically with the determination of optimal paths, are presented with these additional realistic characteristics to account for nuances in adversarial behavior. The paths generated are enhanced by including communication payload and protocol into the Markov decision process (MDP) in order to more realistically emulate attributes of network based exfiltration events. The proposed method will help emulate complex adversarial considerations such as the size of a payload being exported over time or the protocol on which it occurs, as is the case where threat actors steal data over long periods of time using system native ports or protocols to avoid detection. As such, practitioners will be able to improve identification of expected adversary behavior under various payload and protocol assumptions more comprehensively.",
    "published_date": "2023-10-05",
    "pdf_link": "https://arxiv.org/pdf/2310.03667v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Data Exfiltration and Post-Exploitation Path Planning",
      "specific_problem": "Reinforcement learning-based discovery of exfiltration paths that incorporate protocol preference and payload size/rate constraints under firewall detection risk",
      "attack_types": [
        "data exfiltration",
        "protocol tunneling/masquerading (e.g., HTTPS, DHCP service usage)",
        "slow/low-and-slow exfiltration to evade detection",
        "lateral movement via vulnerability exploitation"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Reinforcement Learning (Policy Gradient, Actor-Critic)",
        "specific": "Proximal Policy Optimization (PPO)",
        "novel_contribution": "MDP augments state, reward, and path selection to account for protocol coverage and payload-size/time dynamics; integrates protocol-based path selection heuristic into RL with firewall-triggered penalties."
      },
      {
        "type": "primary",
        "category": "Multi-Layer Perceptron (Feed-forward Neural Network)",
        "specific": "Two-layer MLP (64, 32 neurons) for actor and critic",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Reinforcement Learning"
    ],
    "datasets": [
      {
        "name": "Synthetic Network 1 (10 subnets, 56 hosts)",
        "type": "synthetic",
        "domain": "network_topology_simulation",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "Synthetic Network 2 (101 subnets, 1444 hosts)",
        "type": "synthetic",
        "domain": "network_topology_simulation",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "episode reward (sum of rewards per episode)",
      "episode length",
      "number of steps to task completion",
      "summary statistics (mean, std, min, max) for steps and rewards",
      "convergence episodes"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How can exfiltration path discovery incorporate protocol preferences and payload size/rate to better emulate realistic adversary behavior?",
        "Can PPO-based RL learn efficient exfiltration strategies that maximize protocol coverage while avoiding firewall detection thresholds?"
      ],
      "gaps_identified": [
        "Prior RL approaches for exfiltration path discovery did not consider payload size or protocol preference; they are only realistic for nominal payload sizes.",
        "Ontological agent models used in prior work are misaligned with network/cyber terrain structure and are not operationally interpretable for SOC analysts.",
        "RL for post-exploitation activities such as exfiltration remains under-studied."
      ],
      "limitations": [
        "The environment is a simulated network with assumptions; real-world generalization is unverified.",
        "Firewall modeling is simplified using threshold-based detection (max upload volume/time) and periodic/emergency updates.",
        "Evaluation lacks comparisons to alternative RL algorithms or heuristic baselines (e.g., shortest-path or DQN).",
        "Only single-protocol preference per campaign is modeled; multi-protocol switching and adaptive defenses are not considered.",
        "No analysis of partial observability or noisy/incomplete scan data in policy learning."
      ],
      "future_work": [],
      "motivation": "Improve realism and operational utility of RL-based exfiltration path analysis by incorporating protocol usage and payload dynamics to emulate adversarial behavior that evades detection.",
      "potential_research_ideas": [
        "Benchmark the method against heuristic planners (e.g., shortest path maximizing protocol coverage) and alternative RL algorithms (DQN, A2C, SAC) on standardized simulated enterprise topologies.",
        "Extend to multi-protocol strategies with dynamic switching when protocol coverage is intermittent or defenses adapt.",
        "Model defenders as learning agents (adversarial/self-play) to co-evolve firewall/IDS strategies and evaluate attacker policy robustness.",
        "Introduce partial observability with noisy scans and apply POMDP RL (e.g., recurrent policies) and belief-space planning.",
        "Graph neural network (GNN)-based state encoders for large-scale networks to improve scalability to tens of thousands of hosts.",
        "Constrained RL or safe RL to explicitly enforce stealth constraints (bandwidth/time caps, timing jitter) during learning.",
        "Offline RL using historical red-team/blue-team logs to reduce exploration cost and improve sample efficiency.",
        "Explainable RL outputs that attribute reward contributions and protocol-coverage rationales per chosen path for SOC interpretability.",
        "Robustness evaluation under topology changes and variable QoS/bandwidth; curriculum learning across diverse network families.",
        "Integrate realistic IDS/traffic models (statistical or ML-based) to generate detection signals beyond simple thresholds."
      ],
      "architectural_improvement_recommendations": [
        "Adopt hierarchical RL: high-level planner selects exfil path/protocol over a graph abstraction; low-level controller executes scans/exploits/uploads with stealth constraints.",
        "Use recurrent policies (LSTM/GRU) for partial observability and to model time-dependent stealth tactics (sleep scheduling, trickle rates).",
        "Encode network topology with GNNs (GraphSAGE/GAT) to produce node/path embeddings, replacing hand-engineered state features.",
        "Implement constrained PPO (e.g., Lagrangian methods) with explicit constraints for upload volume/time to reduce reliance on reward penalties.",
        "Add multi-agent roles (scout vs exfiltrator) with shared/centralized critic to specialize exploration vs exfiltration.",
        "Incorporate distributional value functions or uncertainty estimation to reduce risky actions near firewall thresholds.",
        "Augment reward shaping with path-smoothness/variance penalties to avoid unnecessary exploits and reduce detection risk."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Avoiding firewall detection thresholds for upload volume and active time",
        "Ensuring protocol availability along the entire chosen path",
        "Handling periodic/emergency firewall updates that patch vulnerabilities and block outbound traffic",
        "Balancing upload rate with stealth via sleep/no-op timing"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "An approach for modeling data exfiltration on networks that accounts for choices in protocol with varying size of payload.",
      "The implementation of RL-based algorithms for discovering exfiltration paths in network models."
    ]
  },
  {
    "arxiv_id": "2310.00710v2",
    "title": "How well does LLM generate security tests?",
    "authors": "Ying Zhang; Wenjia Song; Zhengjie Ji;  Danfeng;  Yao; Na Meng",
    "abstract": "Developers often build software on top of third-party libraries (Libs) to improve programmer productivity and software quality. The libraries may contain vulnerabilities exploitable by hackers to attack the applications (Apps) built on top of them. People refer to such attacks as supply chain attacks, the documented number of which has increased 742% in 2022. People created tools to mitigate such attacks, by scanning the library dependencies of Apps, identifying the usage of vulnerable library versions, and suggesting secure alternatives to vulnerable dependencies. However, recent studies show that many developers do not trust the reports by these tools; they ask for code or evidence to demonstrate how library vulnerabilities lead to security exploits, in order to assess vulnerability severity and modification necessity. Unfortunately, manually crafting demos of application-specific attacks is challenging and time-consuming, and there is insufficient tool support to automate that procedure.   In this study, we used ChatGPT-4.0 to generate security tests, and to demonstrate how vulnerable library dependencies facilitate the supply chain attacks to given Apps. We explored various prompt styles/templates, and found that ChatGPT-4.0 generated tests for all 55 Apps, demonstrating 24 attacks successfully. It outperformed two state-of-the-art security test generators -- TRANSFER and SIEGE -- by generating a lot more tests and achieving more exploits. ChatGPT-4.0 worked better when prompts described more on the vulnerabilities, possible exploits, and code context. Our research will shed light on new research in security test generation. The generated tests will help developers create secure by design and secure by default software.",
    "published_date": "2023-10-01",
    "pdf_link": "https://arxiv.org/pdf/2310.00710v2",
    "paper_types": [
      "empirical_analysis",
      "benchmark"
    ],
    "security_domain": {
      "primary": "Software Supply Chain Security",
      "subdomain": "Vulnerable Dependency Exploitation and Security Test Generation",
      "specific_problem": "Generating application-specific security tests that demonstrate how vulnerabilities in third-party libraries propagate to client applications and enable exploits",
      "attack_types": [
        "Denial of Service (DoS)",
        "Directory Traversal",
        "Remote Code Execution (RCE)",
        "XML Injection",
        "Zip Slip (Arbitrary File Write via Archive Extraction)",
        "Server-Side Request Forgery (SSRF, blind)",
        "Cross-Site Scripting (XSS)",
        "Covert Channel",
        "Authentication bypass / improper password verification"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Large Language Model (Transformer)",
        "specific": "ChatGPT-4.0 (GPT-4)",
        "novel_contribution": "Systematic evaluation and prompt design/ablation for using an LLM to generate application-specific security tests that reproduce supply-chain vulnerabilities in client apps"
      },
      {
        "type": "baseline",
        "category": "Search-based software testing (EvoSuite-based)",
        "specific": "SIEGE",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Search-based software testing + program analysis (EvoSuite + static/dynamic analysis)",
        "specific": "TRANSFER",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Prompt-based in-context learning",
      "Few-shot prompting (with library-level security test as exemplar)"
    ],
    "datasets": [
      {
        "name": "Curated dataset of 30 vulnerability entries across 26 Java libraries with 55 client applications",
        "type": "public",
        "domain": "software_repositories_and_vulnerability_metadata",
        "link": null,
        "is_new_contribution": true,
        "availability": "available_on_request"
      },
      {
        "name": "Initial vulnerability entry pool from prior work (628 entries) used to seed dataset construction",
        "type": "public",
        "domain": "vulnerability_metadata",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "TRANSFER",
        "paper_reference": "Kang et al. [67]",
        "metric": "Number of apps where a successful exploit was demonstrated (out of 55 apps)",
        "their_result": "24/55 exploited",
        "baseline_result": "4/55 exploited"
      },
      {
        "method_name": "SIEGE",
        "paper_reference": "Iannone et al. [64]",
        "metric": "Number of apps where a successful exploit was demonstrated (out of 55 apps)",
        "their_result": "24/55 exploited",
        "baseline_result": "0/55 exploited"
      }
    ],
    "performance_metrics_used": [
      "Number of tests generated",
      "Number of successful exploits (apps where vulnerability was demonstrated)",
      "Ablation outcome by prompt information elements (presence/absence of vulnerability/test/context descriptions)"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "RQ1: How effectively does ChatGPT generate security tests?",
        "RQ2: How does ChatGPT’s security performance differ given various types of prompts?",
        "RQ3: How does ChatGPT compare with existing tools of security test generation?"
      ],
      "gaps_identified": [
        "Developers often do not trust vulnerability scanners because they lack concrete, application-specific exploit evidence.",
        "Insufficient tool support to automatically craft app-specific attack demonstrations.",
        "Existing security test generators (e.g., SIEGE, TRANSFER) frequently fail to generate effective security tests and often produce irrelevant tests."
      ],
      "limitations": [
        "Effectiveness heavily depends on providing a library-level security test exemplar; without it, \"none of the generated tests by ChatGPT could successfully exploit any vulnerability.\"",
        "Dataset restricted to Java libraries and Java client applications.",
        "For some clients, vulnerable dependencies were injected via manual downgrades to ensure coverage (e.g., \"we downgraded the library dependency to 3.1\").",
        "Reliance on a proprietary black-box LLM (ChatGPT-4.0) may affect reproducibility and cost."
      ],
      "future_work": [],
      "motivation": "Provide developers with concrete, app-specific security tests that demonstrate how vulnerabilities in third-party libraries can be exploited in their applications, thereby improving trust in vulnerability reports and aiding remediation decisions.",
      "potential_research_ideas": [
        "Develop an automated RAG pipeline that retrieves CVE/JIRA details, vulnerable code regions, and library tests to condition the LLM for security test generation.",
        "Fine-tune or instruction-tune a code-oriented LLM specifically on security test generation corpora spanning diverse vulnerability classes and libraries.",
        "Integrate static analysis (call graphs, dataflow to vulnerable APIs) to automatically extract app-side entry points and parameter constraints for the LLM.",
        "Closed-loop generation with execution feedback: automatically compile/run generated tests, capture failures, and iteratively repair via the LLM.",
        "Generalize beyond Java to multi-language ecosystems (JavaScript, Python, .NET) with ecosystem-aware build and test orchestration.",
        "Automated oracle synthesis that differentiates vulnerable vs patched behaviors using differential testing across versions.",
        "Create a public benchmark with standardized metrics, seeds, and harnesses for LLM-based security test generation.",
        "Explore minimal-context prompting that eliminates the need for a library-level test by synthesizing exploit schemas from vulnerability descriptions alone.",
        "Combine concolic execution or fuzzing with LLM guidance to discover exploit inputs for deep paths.",
        "Safety-aware generation that avoids producing harmful POCs unless within controlled research settings."
      ],
      "architectural_improvement_recommendations": [
        "Add retrieval augmentation from vulnerability databases (CVE/NVD/JIRA) and code indexes to ground prompts with precise API signatures, versions, and patch diffs.",
        "Use program analysis to auto-extract app methods that reach vulnerable APIs and generate targeted harnesses and mock inputs.",
        "Implement an automated build-and-run harness with iterative LLM repair (self-reflection) to fix compilation/runtime issues.",
        "Leverage differential execution across vulnerable vs patched versions to auto-derive assertions/oracles.",
        "Adopt tool-use agents that can invoke static analyzers, build tools (Maven/Gradle), and test runners, feeding results back into the LLM.",
        "Employ structured prompting/templates with explicit fields for vulnerability class, API contracts, and expected faulty behavior.",
        "Incorporate unit-to-integration test scaffolding generation to better reflect real app contexts."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "ChatGPT-4.0",
        "Java",
        "JUnit",
        "EvoSuite (for baselines)"
      ],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Evaluation on real open-source Java applications with vulnerable dependencies",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Effectiveness depends on availability of precise vulnerability/test context from the library.",
        "Black-box LLM access, cost, and determinism may hinder consistent large-scale generation."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Constructed a dataset covering 30 vulnerability entries across 26 Java libraries and 55 client applications that call vulnerable APIs.",
      "Designed prompt templates and conducted ablations on information elements (vulnerability description, possible exploits, code context, library security test).",
      "Empirical evaluation showing ChatGPT-4.0 generated tests for all 55 apps and demonstrated 24 successful exploits, outperforming SIEGE (0) and TRANSFER (4).",
      "Insight that providing a library-level security test exemplar is critical; without it, no successful exploits were generated.",
      "Demonstrated potential of LLMs for security test generation to aid developers in assessing vulnerability severity and necessity of fixes."
    ]
  },
  {
    "arxiv_id": "2311.00691v1",
    "title": "Software Repositories and Machine Learning Research in Cyber Security",
    "authors": "Mounika Vanamala; Keith Bryant; Alex Caravella",
    "abstract": "In today's rapidly evolving technological landscape and advanced software development, the rise in cyber security attacks has become a pressing concern. The integration of robust cyber security defenses has become essential across all phases of software development. It holds particular significance in identifying critical cyber security vulnerabilities at the initial stages of the software development life cycle, notably during the requirement phase. Through the utilization of cyber security repositories like The Common Attack Pattern Enumeration and Classification (CAPEC) from MITRE and the Common Vulnerabilities and Exposures (CVE) databases, attempts have been made to leverage topic modeling and machine learning for the detection of these early-stage vulnerabilities in the software requirements process. Past research themes have returned successful outcomes in attempting to automate vulnerability identification for software developers, employing a mixture of unsupervised machine learning methodologies such as LDA and topic modeling. Looking ahead, in our pursuit to improve automation and establish connections between software requirements and vulnerabilities, our strategy entails adopting a variety of supervised machine learning techniques. This array encompasses Support Vector Machines (SVM), Naïve Bayes, random forest, neural networking and eventually transitioning into deep learning for our investigation. In the face of the escalating complexity of cyber security, the question of whether machine learning can enhance the identification of vulnerabilities in diverse software development scenarios is a paramount consideration, offering crucial assistance to software developers in developing secure software.",
    "published_date": "2023-11-01",
    "pdf_link": "https://arxiv.org/pdf/2311.00691v1",
    "paper_types": [
      "position",
      "empirical_analysis",
      "survey"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Secure Software Development Lifecycle (SDLC) / Requirements Security Analysis",
      "specific_problem": "Automated early-stage identification of software vulnerabilities from Software Requirements Specifications (SRS) by linking to security knowledge bases (CAPEC/CVE/CWE/OWASP).",
      "attack_types": [
        "CAPEC attack patterns",
        "OWASP Top 10 web application risks",
        "General software vulnerability categories (CVE/CWE)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Topic Modeling",
        "specific": "Latent Dirichlet Allocation (LDA) via Gensim",
        "novel_contribution": "Used in prior work by the authors to map SRS/CVE topics to CAPEC/OWASP and recommend attack patterns; reused here as core reviewed approach."
      },
      {
        "type": "baseline",
        "category": "Similarity / Distance Metric",
        "specific": "Cosine Similarity",
        "novel_contribution": "Used to measure similarity between topic distributions of SRS and CAPEC in prior work."
      },
      {
        "type": "baseline",
        "category": "Classical ML (planned)",
        "specific": "Support Vector Machines (SVM)",
        "novel_contribution": "Proposed future supervised approach to improve automation."
      },
      {
        "type": "baseline",
        "category": "Classical ML (planned)",
        "specific": "Naïve Bayes",
        "novel_contribution": "Proposed future supervised approach."
      },
      {
        "type": "baseline",
        "category": "Ensemble Trees (planned)",
        "specific": "Random Forest",
        "novel_contribution": "Proposed future supervised approach."
      },
      {
        "type": "baseline",
        "category": "Neural Network (planned)",
        "specific": "Feedforward/Deep Learning",
        "novel_contribution": "Proposed transition towards deep learning."
      },
      {
        "type": "baseline",
        "category": "Sentence Embeddings (referenced)",
        "specific": "Sentence-BERT (SBERT)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Vector Space Model (referenced)",
        "specific": "TF-IDF",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Sentence Embeddings (referenced)",
        "specific": "Universal Sentence Encoder (USE)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Topic Modeling + Word Embeddings (referenced)",
        "specific": "Joint Topic Word-embedding (JWT)",
        "novel_contribution": "Addressed polysemy; reported superior results to fine-tuned ELMo/BERT for topic modeling."
      }
    ],
    "learning_paradigm": [
      "Unsupervised",
      "Supervised"
    ],
    "datasets": [
      {
        "name": "CAPEC (Common Attack Pattern Enumeration and Classification)",
        "type": "public",
        "domain": "attack_patterns",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CVE (Common Vulnerabilities and Exposures)",
        "type": "public",
        "domain": "vulnerability_records",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CWE (Common Weakness Enumeration)",
        "type": "public",
        "domain": "weakness_types",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "MITRE ATT&CK",
        "type": "public",
        "domain": "tactics_techniques_procedures",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "National Vulnerability Database (NVD) CVE records (1999–2019, 121,716 entries)",
        "type": "public",
        "domain": "vulnerability_records",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "OWASP Top 10",
        "type": "public",
        "domain": "security_risk_categories",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "OWASP Benchmark (1,200 test cases)",
        "type": "public",
        "domain": "code_vulnerability_testcases",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "150 open-source Java applications from GitHub (for SAM evaluation)",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "TF-IDF (compared to SBERT and USE)",
        "paper_reference": "Kanakogi et al., 2022",
        "metric": "Evaluated under document, per-section, and section-average settings",
        "their_result": "TF-IDF was the most suitable algorithm across all testing scenarios.",
        "baseline_result": null
      },
      {
        "method_name": "Joint Topic Word-embedding (JWT) vs fine-tuned ELMo/BERT",
        "paper_reference": "Zhu et al., 2019",
        "metric": "Topic modeling accuracy under polysemy",
        "their_result": "JWT yields considerably superior results compared to the direct fine-tuning of pre-trained ELMo or BERT.",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Cosine similarity (topic distribution similarity between SRS and CAPEC)",
      "Standard deviation (for comparing mappings)",
      "Coefficient of variation (for comparing mappings)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can machine learning enhance the identification of vulnerabilities in diverse software development scenarios, particularly at early stages (requirements phase)?",
        "How effectively can automated methods link software requirements to relevant CAPEC attack patterns and other repositories?"
      ],
      "gaps_identified": [
        "Need to automatically connect heterogeneous cybersecurity repositories (e.g., CVE-CAPEC linkage) to enrich context.",
        "Handling linguistic challenges such as polysemy in topic modeling for security texts.",
        "Limited automation in early-stage (requirements) vulnerability identification for developers.",
        "Opportunity to explore ensemble learning and incorporate a wider array of professional cybersecurity resources."
      ],
      "limitations": [
        "This paper primarily reviews prior work and outlines prospective methods; no new experimental evaluation or quantitative results are presented.",
        "Existing topic modeling approaches rely on unsupervised methods that may struggle with polysemy and fine-grained category mapping."
      ],
      "future_work": [
        "Adopt supervised ML techniques (SVM, Naïve Bayes, Random Forest, neural networks) and transition into deep learning.",
        "Explore ensemble learning to combine multiple algorithms and data sources.",
        "Expand to include a wider array of professional cybersecurity resources in the automated linking pipeline."
      ],
      "motivation": "Reduce time, cost, and human error by automating the identification of critical cybersecurity vulnerabilities during the requirements phase, enabling developers to build secure software early in the SDLC.",
      "potential_research_ideas": [
        "Construct a unified knowledge graph linking CAPEC–CWE–CVE–ATT&CK–OWASP and learn embeddings over the graph for requirement-to-attack mapping.",
        "Use contrastive learning or Siamese networks (e.g., Sentence-BERT fine-tuned) to align SRS sentences with CAPEC descriptions using distant supervision from CVE↔CWE↔CAPEC cross-references.",
        "Develop a hierarchical multi-label classifier that respects the CAPEC taxonomy to recommend parent/child attack patterns with calibrated uncertainty.",
        "Human-in-the-loop active learning with developer feedback on recommendations to iteratively improve models with minimal labeling effort.",
        "Apply domain-adaptive pretraining of transformers on large security corpora (CVE/NVD/CAPEC/ATT&CK reports) for improved requirement understanding.",
        "Leverage weak supervision (e.g., Snorkel-style labeling functions) using CWE/OWASP rules to generate training labels for supervised models.",
        "Evaluate robustness across domains (web, mobile, cloud) and across requirement quality (noisy vs. formal SRS)."
      ],
      "architectural_improvement_recommendations": [
        "Implement a Siamese Sentence-BERT model fine-tuned on requirement–CAPEC pairing with hard negative mining and metric learning loss (e.g., triplet loss).",
        "Add a retrieval-augmented layer: BM25/TF-IDF candidate generation followed by neural re-ranking (cross-encoder).",
        "Build a knowledge-graph encoder (R-GCN or GraphSAGE) over CAPEC–CWE–CVE–ATT&CK to provide structured context features to the classifier.",
        "Use hierarchical multi-label loss with taxonomy-aware regularization to enforce consistency across CAPEC tree levels.",
        "Introduce data augmentation via paraphrasing requirements (back-translation) and template-based synthesis using CWE/CAPEC vocab for class balancing.",
        "Adopt ensemble methods blending LDA/TF-IDF retrieval with neural encoders and calibrate outputs for developer-facing confidence scores."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "Gensim",
        "pyLDAvis"
      ],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Heterogeneous data formats and evolving schemas across CAPEC/CVE/CWE/ATT&CK/NVD.",
        "Quality and ambiguity of natural-language SRS documents (polysemy, domain-specific jargon).",
        "Need for labeled pairs (SRS↔CAPEC) or reliable distant supervision for supervised training.",
        "Model token-length limits for long requirement documents.",
        "Maintaining model currency as repositories are updated frequently."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Review of key cybersecurity repositories (CAPEC, CVE, CWE, ATT&CK, NIST) and their roles in ML-driven vulnerability identification.",
      "Synthesis of prior topic modeling efforts: mapping CVE topics to OWASP Top 10 and recommending CAPEC attack patterns from SRS using LDA and cosine similarity.",
      "Discussion of recent methods linking CAPEC to CVE, noting that \"TF-IDF was the most suitable algorithm across all testing scenarios.\"",
      "Positioning and roadmap to move from unsupervised topic modeling to supervised and deep learning methods for early-stage vulnerability detection."
    ]
  },
  {
    "arxiv_id": "2309.15687v3",
    "title": "Breaking On-Chip Communication Anonymity using Flow Correlation Attacks",
    "authors": "Hansika Weerasena; Prabhat Mishra",
    "abstract": "Network-on-Chip (NoC) is widely used to facilitate communication between components in sophisticated System-on-Chip (SoC) designs. Security of the on-chip communication is crucial because exploiting any vulnerability in shared NoC would be a goldmine for an attacker that puts the entire computing infrastructure at risk. We investigate the security strength of existing anonymous routing protocols in NoC architectures, making two pivotal contributions. Firstly, we develop and perform a machine learning (ML)-based flow correlation attack on existing anonymous routing techniques in Network-on-Chip (NoC) systems, revealing that they provide only packet-level anonymity. Secondly, we propose a novel, lightweight anonymous routing protocol featuring outbound traffic tunneling and traffic obfuscation. This protocol is designed to provide robust defense against ML-based flow correlation attacks, ensuring both packet-level and flow-level anonymity. Experimental evaluation using both real and synthetic traffic demonstrates that our proposed attack successfully deanonymizes state-of-the-art anonymous routing in NoC architectures with high accuracy (up to 99%) for diverse traffic patterns. It also reveals that our lightweight anonymous routing protocol can defend against ML-based attacks with minor hardware and performance overhead.",
    "published_date": "2023-09-27",
    "pdf_link": "https://arxiv.org/pdf/2309.15687v3",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Hardware Security",
      "subdomain": "On-Chip Network (NoC) Security",
      "specific_problem": "Deanonymization of anonymous NoC routing via ML-based flow correlation; and design of a lightweight anonymous routing protocol providing flow-level anonymity",
      "attack_types": [
        "flow correlation",
        "deanonymization",
        "traffic analysis",
        "metadata analysis",
        "hardware-Trojan-assisted eavesdropping",
        "timing side-channel on inter-flit delays"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "CNN (Convolutional Neural Network)",
        "specific": "Custom 1D/2D CNN for multivariate time-series correlation (two conv layers with max-pooling + three fully connected layers, ReLU, sigmoid output)",
        "novel_contribution": "Applies a tailored CNN to correlate NoC inter-flit delay flow pairs ({IFD^o_S, IFD^i_D}) across hops to break flow-level anonymity in on-chip anonymous routing; architecture and hyperparameters tuned for NoC traffic characteristics."
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Synthetic NoC traffic (inter-flit delay traces)",
        "type": "synthetic",
        "domain": "network_traffic (on-chip NoC inter-flit delays)",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Real NoC traffic traces (benchmarks mapped to NoC)",
        "type": "private",
        "domain": "network_traffic (on-chip NoC inter-flit delays)",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "ARNoC (Anonymous Routing for NoC)",
        "paper_reference": "[10] (as cited in the paper)",
        "metric": "Deanonymization accuracy of ML flow correlation",
        "their_result": "\"high accuracy (up to 99%)\" deanonymization for diverse traffic patterns",
        "baseline_result": null
      },
      {
        "method_name": "SAR (Stochastic Anonymous Routing)",
        "paper_reference": "[31, 35] (as cited in the paper)",
        "metric": "Deanonymization accuracy of ML flow correlation",
        "their_result": "\"high accuracy (up to 99%)\" deanonymization for diverse traffic patterns",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "accuracy"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can existing anonymous routing protocols for NoC (ARNoC, SAR) be deanonymized at the flow level using ML-based flow correlation attacks?",
        "Do current NoC anonymity solutions provide only packet-level anonymity rather than flow-level anonymity?",
        "Can a lightweight anonymous routing protocol with outbound traffic tunneling and traffic obfuscation provide robust defense (packet- and flow-level anonymity) against ML-based flow correlation attacks?"
      ],
      "gaps_identified": [
        "Existing NoC anonymous routing (e.g., ARNoC, SAR) provides packet-level anonymity but lacks flow-level anonymity.",
        "Prior flow-correlation attacks from traditional networks (e.g., Tor) are not directly applicable to NoC due to fixed flit sizes, traffic characteristics, and all nodes acting as onion routers.",
        "Lack of prior ML-based deanonymization attacks and corresponding countermeasures specifically targeting NoC anonymity."
      ],
      "limitations": [
        "Attack assumes presence of malicious boundary links with Hardware Trojans and a colluding application (collector) to exfiltrate inter-flit delays.",
        "Model training relies on emulation/simulation of target systems and periodic retraining to stay effective over time.",
        "Effectiveness may depend on traffic patterns, process mapping, and NoC configurations; robustness discussed but full generalization bounds not provided."
      ],
      "future_work": [],
      "motivation": "Evaluate and demonstrate the security strength (or weakness) of existing NoC anonymous routing, show vulnerability to ML-based flow correlation, and propose a lightweight protocol that ensures flow-level anonymity with minor overhead.",
      "potential_research_ideas": [
        "Develop Siamese/contrastive learning frameworks for flow correlation to reduce labeling needs and improve generalization across NoC sizes and workloads.",
        "Investigate transformer or attention-based temporal models for correlating long-range inter-flit delay dependencies across multiple hops.",
        "Design adaptive, learning-based traffic obfuscation that dynamically tunes tunneling/obfuscation intensity to meet performance and anonymity SLAs.",
        "Formalize flow-level anonymity metrics for NoC and provide provable privacy guarantees under realistic traffic and queuing models.",
        "Explore defenses against stronger adversaries (e.g., multiple colluding HT-infected links) and evaluate resilience under partial/compromised endpoints.",
        "Apply domain adaptation/transfer learning to maintain attack effectiveness across different NoC topologies, process nodes, and traffic regimes without full retraining.",
        "Integrate detection mechanisms for HT-assisted telemetry leakage at boundary links using side-channel monitors or runtime attestation."
      ],
      "architectural_improvement_recommendations": [
        "For the attack: replace plain CNN with Siamese CNN/contrastive loss or temporal CNN + attention to directly learn similarity between flows; incorporate dilated convolutions for longer contexts; evaluate transformers for multivariate time series.",
        "For training: employ curriculum learning over increasing congestion/noise levels; add domain-adversarial training for topology-generalization; augment with realistic queuing models and stochastic delays.",
        "For the defense: add adaptive randomized timing/padding and burst shaping conditioned on congestion; multi-path mixing across disjoint tunnels; periodic route/key rotation to limit correlation windows.",
        "Harden against HT exfiltration with link-level authenticated enclaves or encrypted/obfuscated telemetry at NIs; integrate runtime HT detection with anomaly scoring on boundary link behavior."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "On-chip NoC within multi-core/MPSoC; simulated/emulated for training and evaluation",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Attack requires insertion of Hardware Trojans on boundary links and a colluding application (collector) to exfiltrate inter-flit delays.",
        "Defense requires modifying NoC routing to support outbound tunneling and traffic obfuscation, with hardware/performance overhead (claimed minor).",
        "Maintaining robustness over time may require periodic model retraining (for attackers) or re-tuning obfuscation parameters (for defenders)."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": true,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes an ML-based flow correlation attack on NoC anonymous routing that \"successfully deanonymizes state-of-the-art anonymous routing ... with high accuracy (up to 99%)\".",
      "Demonstrates that existing solutions provide only packet-level anonymity and motivates the need for flow-level anonymity in NoC.",
      "Assesses robustness of the attack across diverse configurations and traffic patterns.",
      "Introduces a novel, lightweight anonymous routing protocol with outbound traffic tunneling and traffic obfuscation that ensures both packet-level and flow-level anonymity.",
      "Shows experimentally that the proposed protocol can defend against ML-based attacks with minor hardware and performance overhead."
    ]
  },
  {
    "arxiv_id": "2309.14541v1",
    "title": "Cluster-based Method for Eavesdropping Identification and Localization in Optical Links",
    "authors": "Haokun Song; Rui Lin; Andrea Sgambelluri; Filippo Cugini; Yajie Li; Jie Zhang; Paolo Monti",
    "abstract": "We propose a cluster-based method to detect and locate eavesdropping events in optical line systems characterized by small power losses. Our findings indicate that detecting such subtle losses from eavesdropping can be accomplished solely through optical performance monitoring (OPM) data collected at the receiver. On the other hand, the localization of such events can be effectively achieved by leveraging in-line OPM data.",
    "published_date": "2023-09-25",
    "pdf_link": "https://arxiv.org/pdf/2309.14541v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Optical Network Security",
      "specific_problem": "Detection and localization of low-loss eavesdropping events in optical fiber links using optical performance monitoring (OPM) data",
      "attack_types": [
        "Eavesdropping",
        "Fiber-bending eavesdropping",
        "Clip-on coupler-based eavesdropping"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Clustering",
        "specific": "Bisecting k-means",
        "novel_contribution": "Applies bisecting k-means to OPM data to detect (0.5–3 dB) and localize (e.g., 0.8 dB fiber-bending) eavesdropping; shows receiver-only OPM is sufficient for detection while in-line OPM enables precise localization."
      }
    ],
    "learning_paradigm": [
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "Simulation-based two-channel DP-QPSK WDM OPM dataset (1400 samples)",
        "type": "synthetic",
        "domain": "optical_opm",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Label Matching Rate",
      "Sum of Squared Errors (SSE)",
      "SSE per Dimension"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "What is the threshold of detectable power loss for eavesdropping using only receiver OPM data?",
        "Can subtle eavesdropping losses be localized using in-line OPM data across the link?",
        "Which OPM parameters (OSNR, BER, Prx, Ptx, span powers) are most effective for clustering-based detection/localization?"
      ],
      "gaps_identified": [
        "There is currently no universally established eavesdropping procedure, making it challenging to determine what should be monitored to detect eavesdropping activities.",
        "Existing physical-layer monitoring methods (e.g., OTDR/Rayleigh-based) can be costly and complex for broad deployment.",
        "Supervised approaches require labeled data that are hard to obtain; subtle-loss events (e.g., low-loss eavesdropping) remain challenging for both detection and localization.",
        "Most prior anomaly detection studies rely on large separations between normal and abnormal parameter distributions; subtle impairments violate this assumption."
      ],
      "limitations": [
        "Validation is performed in simulation; no experimental/field validation yet.",
        "Results rely on a specific two-channel DP-QPSK WDM setup with four spans; generalization to larger systems is untested.",
        "BER in the simulation is less reliable for clustering due to how it is computed (“calculated from the number of statistical BER bits”).",
        "Localization requires access to in-line OPM data (transmitter, receiver, spans), which may not always be available.",
        "Evaluation uses known labels to assess unsupervised clustering; robustness to real-world noise/drift not evaluated."
      ],
      "future_work": [
        "Validation of this method in an experimental system.",
        "Attempting more challenging fuzzy clustering."
      ],
      "motivation": "Enhance physical-layer security of optical links by detecting and localizing subtle (low-loss) eavesdropping using readily available OPM data without requiring labeled datasets or complex reflection/scattering-based equipment.",
      "potential_research_ideas": [
        "Field-trial validation on operational optical links with real OPM telemetry and environmental variations.",
        "Robust/online clustering for drifting OPM distributions (e.g., HDBSCAN, streaming k-means, change-point detection).",
        "Probabilistic clustering (GMM, DP-GMM) with model selection (BIC/AIC) for automatic K determination and uncertainty estimates.",
        "Semi-supervised or self-supervised approaches leveraging abundant unlabeled OPM and sparse labels from incidents.",
        "Multivariate time-series modeling (HMMs, state-space models, RNN/Transformer forecasting) to exploit temporal dynamics for detection/localization.",
        "Topology-aware localization using graph-based methods that encode span relationships and amplifier placement.",
        "Augment feature space with spectral shape, polarization metrics, and amplifier telemetry to improve separability under low-loss scenarios.",
        "Domain adaptation to transfer models across networks/vendors with different OPM calibration baselines.",
        "Joint detection of eavesdropping vs benign soft-faults (e.g., connector degradation) to reduce false positives.",
        "Active diagnosis: adaptive probing (power sweeps) to disambiguate adjacent-span hypotheses with minimal traffic impact."
      ],
      "architectural_improvement_recommendations": [
        "Replace bisecting k-means with density-based clustering (HDBSCAN) or GMMs for non-spherical clusters and automatic cluster selection.",
        "Use ensemble clustering and stability selection to improve robustness under measurement noise.",
        "Perform feature selection and scaling pipelines (e.g., standardization, PCA/KPCA) to reduce redundancy and improve cluster separation.",
        "Incorporate uncertainty scoring and abstention thresholds for operational alarms.",
        "Adopt graph-based localization where nodes are spans/amplifiers and features are OPM vectors; apply message passing to infer most likely span.",
        "Leverage Bayesian change-point detection over time to trigger re-clustering only during distribution shifts.",
        "Design a hierarchical system: coarse detection with receiver OPM, then fine localization with in-line OPM where available."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Optical WDM transport link (transmitter, in-line spans/amplifiers, receiver) with OPM telemetry via control/management plane",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Access to in-line OPM data (transmitter and per-span powers) may be limited; data must be communicated via control/management plane.",
        "Distinguishing eavesdropping from benign soft-faults that also cause small power losses.",
        "Calibration inconsistencies and noise in OPM measurements across devices/vendors.",
        "BER reliability varies; reliance on OSNR/Prx may be preferable in practice.",
        "Generalization from simulation to field conditions with environmental variations and traffic dynamics."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes a cluster-based (bisecting k-means) method for eavesdropping detection and localization using only OPM data.",
      "Shows that detecting subtle losses from eavesdropping can be accomplished solely through OPM data collected at the receiver.",
      "Demonstrates that localization can be effectively achieved by leveraging in-line OPM data (transmitter, receiver, span powers).",
      "Reports “100% label matching rate” for detection across 0.5–3 dB power loss using receiver OPM; SSE at 3 dB classification down to 4.16.",
      "For 0.8 dB fiber-bending eavesdropping, achieves “100% label matching rate” localization with transmitter, receiver, and spans data.",
      "Analyzes parameter utility: OSNR + Prx yields best rough clustering; BER is dispersed and less beneficial in this simulation.",
      "Suggests that at least N−1 span power parameters are needed to distinguish N-span segments for localization granularity."
    ]
  },
  {
    "arxiv_id": "2312.04096v1",
    "title": "MediHunt: A Network Forensics Framework for Medical IoT Devices",
    "authors": "Ayushi Mishra; Tej Kiran Boppana; Priyanka Bagade",
    "abstract": "The Medical Internet of Things (MIoT) has enabled small, ubiquitous medical devices to communicate with each other to facilitate interconnected healthcare delivery. These devices interact using communication protocols like MQTT, Bluetooth, and Wi-Fi. However, as MIoT devices proliferate, these networked devices are vulnerable to cyber-attacks. This paper focuses on the vulnerabilities present in the Message Queuing Telemetry and Transport (MQTT) protocol. The MQTT protocol is prone to cyber-attacks that can harm the system's functionality. The memory-constrained MIoT devices enforce a limitation on storing all data logs that are required for comprehensive network forensics. This paper solves the data log availability challenge by detecting the attack in real-time and storing the corresponding logs for further analysis with the proposed network forensics framework: MediHunt. Machine learning (ML) techniques are the most real safeguard against cyber-attacks. However, these models require a specific dataset that covers diverse attacks on the MQTT-based IoT system for training. The currently available datasets do not encompass a variety of applications and TCP layer attacks. To address this issue, we leveraged the usage of a flow-based dataset containing flow data for TCP/IP layer and application layer attacks. Six different ML models are trained with the generated dataset to evaluate the effectiveness of the MediHunt framework in detecting real-time attacks. F1 scores and detection accuracy exceeded 0.99 for the proposed MediHunt framework with our custom dataset.",
    "published_date": "2023-12-07",
    "pdf_link": "https://arxiv.org/pdf/2312.04096v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "IoT Security",
      "subdomain": "Network Forensics and Intrusion Detection",
      "specific_problem": "Real-time detection and evidence preservation of MQTT-based attacks in Medical IoT (MIoT) networks using flow-based features",
      "attack_types": [
        "invalid subscribe",
        "invalid publish",
        "TCP SYN flood",
        "brute force",
        "malformed packet",
        "port scanning",
        "WILL payload",
        "TCP/IP layer attacks",
        "application-layer (MQTT) attacks"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Decision Tree",
        "specific": "Decision Tree (DT)",
        "novel_contribution": "Used within the MediHunt framework for attack detection; no novel architecture"
      },
      {
        "type": "primary",
        "category": "Random Forest",
        "specific": "Random Forest (RF)",
        "novel_contribution": "Best-performing classifier in experiments (F1 up to 1.0 in binary over-sampling); integrated into the framework"
      },
      {
        "type": "primary",
        "category": "SVM",
        "specific": "Support Vector Machine (SVM)",
        "novel_contribution": "Evaluated as part of the framework; no novel architecture"
      },
      {
        "type": "primary",
        "category": "Naive Bayes",
        "specific": "Gaussian Naive Bayes (NB)",
        "novel_contribution": "Evaluated as part of the framework; observed to have the least training time"
      },
      {
        "type": "primary",
        "category": "Feedforward Neural Network",
        "specific": "Multilayer Perceptron (MLP)",
        "novel_contribution": "Evaluated as part of the framework; no novel architecture"
      },
      {
        "type": "primary",
        "category": "Gradient Boosting",
        "specific": "XGBoost (XGB)",
        "novel_contribution": "Evaluated as part of the framework; no novel architecture"
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Flow-based MQTT dataset (from authors' prior work [3])",
        "type": "private",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "accuracy",
      "precision",
      "recall",
      "f1_score",
      "training_time",
      "inference_time"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can a real-time ML-based network forensics framework detect MQTT attacks in MIoT networks and preserve relevant evidence despite device log storage constraints?",
        "Do flow-based features enable accurate and low-latency intrusion detection for MQTT networks without relying on packet payload inspection?",
        "How do common classifiers (DT, RF, SVM, NB, MLP, XGBoost) perform for binary and multi-class detection of MQTT and TCP/IP attacks in a flow-based setting?"
      ],
      "gaps_identified": [
        "Unavailability of publicly available flow-based MQTT-specific datasets to train attack detection systems",
        "Packet-based approaches may require payload inspection, which is infeasible with encryption and raises privacy concerns",
        "Rule-based techniques require in-depth domain knowledge and may not focus on detecting active attacks",
        "Recurrent models are generally too slow for real-time traffic analysis on constrained devices",
        "MIoT devices are memory-constrained and cannot store comprehensive logs for forensics"
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "MIoT systems using MQTT are vulnerable to attacks while devices are too resource-constrained to store all logs. Existing datasets and packet-based IDS approaches are inadequate for diverse attacks and for low-latency, privacy-conscious detection.",
      "potential_research_ideas": [
        "Release a public, standardized flow-based MQTT dataset with broader attack coverage and realistic benign traffic to facilitate benchmarking",
        "Online/streaming learning with concept drift handling for evolving MIoT/MQTT traffic patterns",
        "Federated or split learning across multiple MIoT sites to preserve privacy while improving model generalization",
        "Adversarial robustness evaluation and defenses for flow-based MQTT IDS (e.g., adversarial training on flow features)",
        "Multi-broker and distributed detection across edge, fog, and cloud layers with coordinated forensics evidence collection",
        "Cross-protocol generalization to other IoT protocols (CoAP, Zigbee, Bluetooth) using multi-task learning",
        "Explainability methods tailored to flow features for operator trust and forensic triage",
        "Lightweight sequence-aware models (e.g., TCNs) for temporal flow patterns while maintaining low latency",
        "Integration with SIEM/SOAR pipelines and automated evidence chain-of-custody management",
        "Energy-aware inference scheduling and model compression for battery-powered medical devices"
      ],
      "architectural_improvement_recommendations": [
        "Use calibrated probability outputs and cost-sensitive thresholds to balance false positives/negatives in medical settings",
        "Feature selection and drift-aware feature monitoring to sustain performance over time",
        "Model compression and quantization (e.g., for RF/XGB via tree pruning, or MLP via quantization) for smaller footprint on MIoT",
        "Ensemble stacking of lightweight models (e.g., RF + XGB) with gating by traffic context",
        "Adopt streaming flow generation (e.g., eBPF-based) to reduce capture-to-decision latency",
        "Add an unsupervised anomaly detection component (e.g., AE/OCSVM) to catch novel attacks prior to supervised retraining",
        "Hyperparameter optimization under compute constraints (e.g., multi-fidelity HPO) to maintain low training/inference cost"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Evaluated on GCP VMs and Raspberry Pi 3B devices; inference time reported as ~2 ms on cloud and ~0.17 ms on Raspberry Pi; Naive Bayes had the least training time among compared models."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Raspberry Pi 3B devices; Google Cloud Platform VMs; Mosquitto broker; PAHO MQTT clients",
      "scalability_discussed": true,
      "inference_time": "Approximately 2 ms on cloud and 0.17 ms on Raspberry Pi (as reported)",
      "deployment_challenges": [
        "Limited memory/log storage on MIoT devices necessitating selective evidence preservation",
        "Packet-based IDS approaches have higher latency; flow-based approach chosen for efficiency"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "MediHunt: a real-time MQTT network forensics framework for MIoT that detects intrusions and stores corresponding logs for forensic analysis",
      "Use of flow-based features (via tcpdump + Tranalyzer) to avoid payload inspection, reduce latency, and support forensics",
      "Comprehensive evaluation of six ML classifiers (DT, RF, SVM, NB, MLP, XGBoost) on a flow-based MQTT dataset covering TCP/IP and application-layer attacks",
      "High performance reported: e.g., binary classification with over-sampling achieved up to 1.0 accuracy and F1 for DT, RF, and XGB; multi-class F1 up to 0.9969",
      "Feasibility study on Raspberry Pi 3B devices demonstrating low-latency inference and suitability for resource-constrained MIoT environments"
    ]
  },
  {
    "arxiv_id": "2310.00594v1",
    "title": "Performance evaluation of Machine learning algorithms for Intrusion Detection System",
    "authors": "Sudhanshu Sekhar Tripathy; Bichitrananda Behera",
    "abstract": "The escalation of hazards to safety and hijacking of digital networks are among the strongest perilous difficulties that must be addressed in the present day. Numerous safety procedures were set up to track and recognize any illicit activity on the network's infrastructure. IDS are the best way to resist and recognize intrusions on internet connections and digital technologies. To classify network traffic as normal or anomalous, Machine Learning (ML) classifiers are increasingly utilized. An IDS with machine learning increases the accuracy with which security attacks are detected. This paper focuses on intrusion detection systems (IDSs) analysis using ML techniques. IDSs utilizing ML techniques are efficient and precise at identifying network assaults. In data with large dimensional spaces, however, the efficacy of these systems degrades. correspondingly, the case is essential to execute a feasible feature removal technique capable of getting rid of characteristics that have little effect on the classification process. In this paper, we analyze the KDD CUP-'99' intrusion detection dataset used for training and validating ML models. Then, we implement ML classifiers such as Logistic Regression, Decision Tree, K-Nearest Neighbour, Naive Bayes, Bernoulli Naive Bayes, Multinomial Naive Bayes, XG-Boost Classifier, Ada-Boost, Random Forest, SVM, Rocchio classifier, Ridge, Passive-Aggressive classifier, ANN besides Perceptron (PPN), the optimal classifiers are determined by comparing the results of Stochastic Gradient Descent and back-propagation neural networks for IDS, Conventional categorization indicators, such as \"accuracy, precision, recall, and the f1-measure, have been used to evaluate the performance of the ML classification algorithms.",
    "published_date": "2023-10-01",
    "pdf_link": "https://arxiv.org/pdf/2310.00594v1",
    "paper_types": [
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Network intrusion detection on KDD Cup 99; evaluating multiple ML classifiers to classify normal vs. attack traffic and reduce false positives",
      "attack_types": [
        "DoS",
        "Probe",
        "R2L",
        "U2R"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Linear Model",
        "specific": "Logistic Regression",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Tree",
        "specific": "Decision Tree",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Instance-based",
        "specific": "K-Nearest Neighbour (KNN)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Naive Bayes",
        "specific": "Gaussian Naive Bayes",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Naive Bayes",
        "specific": "Bernoulli Naive Bayes",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Naive Bayes",
        "specific": "Multinomial Naive Bayes",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Ensemble/Boosting",
        "specific": "XGBoost (Extreme Gradient Boosting)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Ensemble/Boosting",
        "specific": "AdaBoost",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Ensemble/Bagging",
        "specific": "Random Forest",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "SVM",
        "specific": "Support Vector Machine",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Distance-based",
        "specific": "Rocchio (Nearest Centroid) classifier",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Linear Model",
        "specific": "Ridge classifier",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Online Linear",
        "specific": "Passive-Aggressive classifier",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Neural Network",
        "specific": "Artificial Neural Network (ANN) with backpropagation",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Linear Model / Online SGD",
        "specific": "Stochastic Gradient Descent (SGD) classifier",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Linear Model",
        "specific": "Perceptron",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "KDD Cup 1999",
        "type": "public",
        "domain": "network_traffic",
        "link": "http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "NSL-KDD",
        "type": "public",
        "domain": "network_traffic",
        "link": "https://www.unb.ca/cic/datasets/nsl.html",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "UNSW-NB15 (2015)",
        "type": "public",
        "domain": "network_traffic",
        "link": "https://research.unsw.edu.au/projects/unsw-nb15-dataset",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CICIDS2017",
        "type": "public",
        "domain": "network_traffic",
        "link": "https://www.unb.ca/cic/datasets/ids-2017.html",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Kyoto 2006+",
        "type": "public",
        "domain": "network_traffic",
        "link": "http://www.takakura.com/Kyoto_data/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CIDDS-001 (Coburg Intrusion Detection Data Sets)",
        "type": "public",
        "domain": "network_traffic",
        "link": "https://www.hs-coburg.de/cidds",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Kaggle IDS dataset (unspecified)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "accuracy",
      "precision",
      "recall",
      "f1-score"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Which conventional ML classifiers perform best for IDS when trained and evaluated on KDD Cup ’99 using accuracy, precision, recall, and F1?",
        "Can feature removal/selection improve IDS effectiveness in high-dimensional data and help reduce false alarms?"
      ],
      "gaps_identified": [
        "High false positive (false alarm) rates reduce IDS effectiveness.",
        "Effectiveness of ML-based IDS degrades on high-dimensional feature spaces, motivating feature removal/selection."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Evaluate a broad set of ML classifiers for IDS, highlight the impact of high-dimensional data on performance, and provide a comparative reference for reducing false alarms using KDD Cup ’99.",
      "potential_research_ideas": [
        "Replicate and extend the comparison on modern datasets (UNSW-NB15, CICIDS2017, Kyoto 2006+, CIDDS-001) and cross-dataset generalization to assess robustness.",
        "Investigate embedded and wrapper feature selection (e.g., L1-regularization, tree-based importance, Boruta, recursive feature elimination) to reduce dimensionality and false alarms.",
        "Apply cost-sensitive learning and calibration (e.g., class-weighting, focal loss, threshold tuning) to explicitly minimize false positive rate.",
        "Evaluate online/streaming IDS with Passive-Aggressive/SGD and drift detection for changing traffic distributions.",
        "Incorporate explainability (e.g., SHAP, LIME) to interpret alerts and reduce analyst workload.",
        "Assess adversarial robustness of evaluated classifiers under evasion/poisoning attacks for IDS.",
        "Develop hybrid systems combining signature-based filters with ML anomaly detectors and post-alert clustering to reduce alarm floods."
      ],
      "architectural_improvement_recommendations": [
        "Adopt a two-stage pipeline: fast anomaly prefilter (e.g., Isolation Forest/autoencoder) followed by a calibrated discriminative classifier (e.g., XGBoost/Calibrated SVM).",
        "Use feature engineering with flow-based aggregation and protocol-aware features; add embedded feature selection (L1, tree-based).",
        "Implement threshold calibration and cost-sensitive training to penalize false positives; deploy per-attack-type thresholds.",
        "Leverage model ensembling (stacking/bagging) and cross-validation with temporal splits to reduce variance and optimize generalization.",
        "Integrate alert deduplication/clustering (e.g., density-based clustering) post-detection to mitigate alarm storms."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Excessive false positives (false alarms) diminish IDS utility.",
        "High-dimensional feature spaces degrade classifier effectiveness, motivating feature selection."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Empirical comparison of a broad set of ML classifiers (LR, DT, KNN, Naive Bayes variants, XGBoost, AdaBoost, RF, SVM, Rocchio, Ridge, Passive-Aggressive, ANN, Perceptron, SGD) for IDS on KDD Cup ’99 using accuracy, precision, recall, and F1.",
      "Discussion of the need for feature removal/selection to handle high-dimensional IDS data and reduce false alarms.",
      "Presentation of a generic five-step IDS workflow (data preprocessing, feature extraction/selection, training, detection, evaluation).",
      "Conceptual hybrid IDS (HIDS) architecture and schematic block design illustrations.",
      "Literature review summarizing recent IDS research and datasets."
    ]
  },
  {
    "arxiv_id": "2309.16021v1",
    "title": "HuntGPT: Integrating Machine Learning-Based Anomaly Detection and Explainable AI with Large Language Models (LLMs)",
    "authors": "Tarek Ali; Panos Kostakos",
    "abstract": "Machine learning (ML) is crucial in network anomaly detection for proactive threat hunting, reducing detection and response times significantly. However, challenges in model training, maintenance, and frequent false positives impact its acceptance and reliability. Explainable AI (XAI) attempts to mitigate these issues, allowing cybersecurity teams to assess AI-generated alerts with confidence, but has seen limited acceptance from incident responders. Large Language Models (LLMs) present a solution through discerning patterns in extensive information and adapting to different functional requirements. We present HuntGPT, a specialized intrusion detection dashboard applying a Random Forest classifier using the KDD99 dataset, integrating XAI frameworks like SHAP and Lime for user-friendly and intuitive model interaction, and combined with a GPT-3.5 Turbo, it delivers threats in an understandable format. The paper delves into the system's architecture, components, and technical accuracy, assessed through Certified Information Security Manager (CISM) Practice Exams, evaluating response quality across six metrics. The results demonstrate that conversational agents, supported by LLM and integrated with XAI, provide robust, explainable, and actionable AI solutions in intrusion detection, enhancing user understanding and interactive experience.",
    "published_date": "2023-09-27",
    "pdf_link": "https://arxiv.org/pdf/2309.16021v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Network anomaly detection with explainable outputs and conversational summarization for analyst support",
      "attack_types": [
        "Denial of Service (DoS)",
        "spoofing",
        "network intrusion (general)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Random Forest",
        "specific": null,
        "novel_contribution": "Used as the anomaly detection classifier within the HuntGPT IDS dashboard"
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "GPT-3.5 Turbo",
        "novel_contribution": "Conversational agent integrated with IDS + XAI to generate explainable, actionable summaries for detected threats"
      },
      {
        "type": "primary",
        "category": "XAI",
        "specific": "SHAP",
        "novel_contribution": "Generates local feature-attribution explanations for RF predictions shown in the dashboard and stored in Elasticsearch"
      },
      {
        "type": "primary",
        "category": "XAI",
        "specific": "LIME",
        "novel_contribution": "Generates local surrogate explanations for RF predictions; outputs surfaced in dashboard and indices"
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "KDD Cup 1999 (KDD99)",
        "type": "public",
        "domain": "network_traffic",
        "link": "http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Certified Information Security Manager (CISM) Practice Exams",
        "type": "proprietary",
        "domain": "security_exam_questions",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Technical accuracy on CISM practice exam questions",
      "Readability quality across six metrics (unspecified in text)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can integrating an ML-based anomaly detector with XAI (SHAP, LIME) and an LLM assistant produce explainable and actionable outputs for intrusion detection?",
        "What is the technical accuracy of the conversational agent when assessed using CISM practice exams?",
        "How readable are the AI-generated explanations as measured by six readability metrics?"
      ],
      "gaps_identified": [
        "High false positives from ML-based anomaly detection hinder acceptance in operations",
        "Challenges in training and maintaining ML models in SOC environments reduce trust and adoption",
        "XAI tools have seen limited acceptance from incident responders and do not fully meet analysts’ decision-making needs",
        "Information overload for incident responders and ML maintainers slows adoption",
        "Strong performance of ML in controlled settings may not translate to real-world operational efficacy"
      ],
      "limitations": [
        "Prototype system; no real-world deployment reported",
        "Evaluation of the conversational component relies on CISM practice exams and readability metrics rather than operational SOC workflows",
        "Core anomaly detection trained on the outdated KDD99 dataset; no evaluation on modern network datasets reported",
        "No quantitative comparison against alternative IDS/LLM/XAI baselines provided"
      ],
      "future_work": [],
      "motivation": "Improve trust, usability, and actionability of ML-based intrusion detection by integrating explainability and a conversational LLM interface to support analysts.",
      "potential_research_ideas": [
        "Evaluate HuntGPT on modern IDS datasets (e.g., UNSW-NB15, CIC-IDS2017, ToN-IoT) and real enterprise traffic to assess external validity",
        "Augment LLM with retrieval-augmented generation (RAG) from SIEM/SOAR/threat intel to ground explanations in current context and TTPs",
        "Incorporate uncertainty estimation and calibration (e.g., conformal prediction) into RF and expose it in explanations and prompts",
        "Online/continual learning to address concept drift in network traffic with human-in-the-loop feedback from the chatbot interactions",
        "Test with SOC analysts in user studies to measure decision quality, time-to-triage, trust, and explanation usefulness",
        "Investigate LLM prompt hardening and safety filters to mitigate prompt injection/data leakage from telemetry",
        "Replace or complement RF with modern tabular DL (e.g., TabNet, FT-Transformer) and compare explainability fidelity vs. SHAP/LIME",
        "Use programmatic weak supervision or self-training to reduce labeling cost on real network logs",
        "Develop an evaluation benchmark for explanation quality and actionability specific to IDS use cases",
        "Explore multi-agent setups where one agent explains, another validates, and a third suggests mitigations with playbook integration"
      ],
      "architectural_improvement_recommendations": [
        "Introduce a data-to-text layer that structures SHAP/LIME outputs into a concise schema for the LLM (RAG over ES indices)",
        "Add uncertainty estimates and anomaly scoring thresholds to both dashboard and LLM prompts for calibrated recommendations",
        "Implement feedback capture (thumbs up/down plus rationale) to refine prompts and optionally fine-tune an open LLM offline",
        "Support streaming/online inference pipeline (e.g., Kafka -> featureization -> model -> ES) with backpressure and autoscaling",
        "Evaluate modern explainers (e.g., TreeSHAP fastpath, Anchors, Integrated Gradients for tabular DL) and compare fidelity/stability",
        "Harden LLM integration via content filters, PII redaction, prompt templates, and toolformer-style function calling for playbooks",
        "Modularize model backend to swap RF with pluggable models and A/B test detection + explanation impact",
        "Add SOC tool integrations (SOAR ticketing, EDR, firewall APIs) for one-click action suggestions with explicit human approval"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "scikit-learn",
        "SHAP",
        "LIME",
        "Gradio",
        "Elasticsearch",
        "AWS S3",
        "OpenAI API"
      ],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Trust and acceptance by analysts due to false positives and model opacity",
        "XAI integration into analyst workflows and avoiding information overload",
        "Data privacy and security considerations when interfacing telemetry with external LLM APIs",
        "Operationalizing and maintaining ML models (drift, updates, monitoring)"
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces HuntGPT, a specialized IDS dashboard that integrates a Random Forest anomaly detector with SHAP and LIME explanations and a GPT-3.5 Turbo conversational agent",
      "Details a modular three-layer architecture (analytics engine, data storage with Elasticsearch/S3, and Gradio-based UI) and component design",
      "Defines Elasticsearch schemas for detected and original packets, storing explanation artifacts and visualizations",
      "Evaluation: “assessed through Certified Information Security Manager (CISM) Practice Exams, evaluating response quality across six metrics.”",
      "Claim: “conversational agents, supported by LLM and integrated with XAI, provide robust, explainable, and actionable AI solutions in intrusion detection, enhancing user understanding and interactive experience.”"
    ]
  },
  {
    "arxiv_id": "2309.05889v1",
    "title": "Systemization of Knowledge (SoK)- Cross Impact of Transfer Learning in Cybersecurity: Offensive, Defensive and Threat Intelligence Perspectives",
    "authors": "Sofiya Makar; Ali Dehghantanha; Fattane Zarrinkalam; Gautam Srivastava; Abbas Yazdinejad",
    "abstract": "Recent literature highlights a significant cross-impact between transfer learning and cybersecurity. Many studies have been conducted on using transfer learning to enhance security, leading to various applications in different cybersecurity tasks. However, previous research is focused on specific areas of cybersecurity. This paper presents a comprehensive survey of transfer learning applications in cybersecurity by covering a wide range of domains, identifying current trends, and shedding light on under-explored areas. The survey highlights the significance of transfer learning in addressing critical issues in cybersecurity, such as improving detection accuracy, reducing training time, handling data imbalance, and enhancing privacy preservation. Additional insights are provided on the common problems solved using transfer learning, such as the lack of labeled data, different data distributions, and privacy concerns. The paper identifies future research directions and challenges that require community attention, including the need for privacy-preserving models, automatic tools for knowledge transfer, metrics for measuring domain relatedness, and enhanced privacy preservation mechanisms. The insights and roadmap presented in this paper will guide researchers in further advancing transfer learning in cybersecurity, fostering the development of robust and efficient cybersecurity systems to counter emerging threats and protect sensitive information. To the best of our knowledge, this paper is the first of its kind to present a comprehensive taxonomy of all areas of cybersecurity that benefited from transfer learning and propose a detailed future roadmap to shape the possible research direction in this area.",
    "published_date": "2023-09-12",
    "pdf_link": "https://arxiv.org/pdf/2309.05889v1",
    "paper_types": [
      "empirical_analysis",
      "survey"
    ],
    "security_domain": {
      "primary": "Multiple",
      "subdomain": "Transfer learning across cybersecurity domains",
      "specific_problem": "Systemization and taxonomy of transfer learning applications in cybersecurity, identifying benefits, challenges, and future directions",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transfer Learning",
        "specific": "Federated Transfer Learning (FTL), Heterogeneous FTL (HFTL), Feature-based FTL",
        "novel_contribution": "Comprehensive taxonomy and systemization of TL uses in cybersecurity; identification of benefits, challenges, and roadmap"
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "MobileNetV1/V2, VGG16, ResNet50, DenseNet121, custom lightweight triple-pool CNN",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Siamese Network",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Reinforcement Learning",
        "specific": "DDPG, TD3 (S-TD3)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Federated Learning",
        "specific": "SPDZ-based secure MPC, Secret Sharing, Homomorphic Encryption",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Adversarial Learning",
        "specific": "Adversarial game framework for privacy-aware representation learning (PrivNet)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Privacy-preserving ML",
        "specific": "Fully Homomorphic Encryption integrated NN training; secure multi-party computation",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Classical ML",
        "specific": "Random Forest, Support Vector Machines, Gaussian Naïve Bayes",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Reinforcement Learning",
      "Transfer Learning",
      "Federated Learning"
    ],
    "datasets": [
      {
        "name": "MIMIC-III",
        "type": "public",
        "domain": "electronic_health_records",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CEDAR Signature Dataset",
        "type": "public",
        "domain": "biometric_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Kaggle signature dataset (unspecified)",
        "type": "public",
        "domain": "biometric_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "ICDAR 2011 SigComp (Chinese signatures)",
        "type": "public",
        "domain": "biometric_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "ORL face database",
        "type": "public",
        "domain": "biometric_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "IMM face database",
        "type": "public",
        "domain": "biometric_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "AR face database",
        "type": "public",
        "domain": "biometric_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Simulated channel data and real experiment data (for TL-PHA)",
        "type": "synthetic",
        "domain": "wireless_channel_characteristics",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "SPDZ-based Secret Sharing FTL vs prior FTL",
        "paper_reference": "Sharma et al. (2019) as cited",
        "metric": "Execution time; communication cost",
        "their_result": "“reducing the execution time from 35 seconds to 0.8 seconds (semi-honest case) and 1.4 seconds (malicious case) for 500 samples.”",
        "baseline_result": "35 seconds execution time for 500 samples (prior work)"
      },
      {
        "method_name": "VGG16 (Siamese NN, Euclidean + Gaussian NB) on signature forgery",
        "paper_reference": "Manikantha et al. as cited",
        "metric": "Accuracy",
        "their_result": "“achieves a maximum accuracy of 100% on the CEDAR and Kaggle datasets (the smallest datasets).”",
        "baseline_result": null
      },
      {
        "method_name": "ResNet50 on ICDAR 2011 SigComp",
        "paper_reference": "Manikantha et al. as cited",
        "metric": "Accuracy",
        "their_result": "“ResNet50 achieves the highest accuracy of 98.29% for detecting forgery in Chinese signatures from the ICDAR 2011 SigComp dataset.”",
        "baseline_result": null
      },
      {
        "method_name": "DeepZeroID",
        "paper_reference": "Salem et al. as cited",
        "metric": "F1 score; False positive rate",
        "their_result": "“verification F1 score of 95.47% … with zero false positives.”",
        "baseline_result": null
      },
      {
        "method_name": "FbFTL vs parameter-update FL",
        "paper_reference": "Wang et al. as cited",
        "metric": "Uplink payload reduction",
        "their_result": "“reduces the uplink payload by over five orders of magnitude compared to existing approaches by uploading extracted features and outputs instead of parameter updates.”",
        "baseline_result": "Parameter-update based FL with >5 orders of magnitude higher uplink payload"
      }
    ],
    "performance_metrics_used": [
      "accuracy",
      "detection rate",
      "F1 score",
      "execution time",
      "communication cost",
      "throughput",
      "latency",
      "training complexity",
      "uplink payload"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "RQ1: What are the main application and state-of-the-art of TL in cybersecurity?",
        "RQ2: What are the unexplored areas and possible future research directions for TL in cybersecurity?",
        "RQ3: What are the specific benefits of TL in the field of cybersecurity, and how do they contribute to enhancing the performance, efficiency, and resilience of cybersecurity systems?"
      ],
      "gaps_identified": [
        "Need for privacy-preserving models in transfer learning",
        "Lack of automatic tools for knowledge transfer",
        "Absence of metrics for measuring domain relatedness",
        "Need for enhanced privacy preservation mechanisms",
        "Common problems: lack of labeled data, domain/data distribution shift, privacy concerns",
        "Prior surveys focus narrowly on specific areas (e.g., fault diagnosis), lacking holistic taxonomy"
      ],
      "limitations": [
        "Survey time window and inclusion criteria: papers after 2018, English only, non-preprints",
        "Limited number of available publications; some exclusion criteria relaxed due to scarcity",
        "Search performed across selected libraries; may omit works outside these sources",
        "Cutoff of the primary search period (Oct 2021–Jan 2022) with few 2022/2023 additions"
      ],
      "future_work": [
        "Develop privacy-preserving transfer learning models",
        "Create automatic tools for knowledge transfer",
        "Design metrics for measuring domain relatedness",
        "Enhance privacy preservation mechanisms",
        "Improve model robustness and applicability in real-world privacy-sensitive settings"
      ],
      "motivation": "Provide the first comprehensive taxonomy and systemization of transfer learning applications across all cybersecurity areas, identify benefits/challenges, and chart a future roadmap.",
      "potential_research_ideas": [
        "Design differential privacy-enhanced transfer learning that balances utility and privacy for cross-organization security analytics",
        "Create automated domain relatedness estimators to predict transferability and avoid negative transfer in cybersecurity datasets",
        "Develop AutoTL tools that select source models and layers to transfer for new security tasks with limited labels",
        "Investigate adversarially robust transfer learning for security tasks (e.g., malware, NIDS) under distribution shifts and adaptive attackers",
        "Cross-modal transfer for cybersecurity (e.g., transfer from text-based threat intel to code/malware representations)",
        "Cross-lingual TL for threat intelligence NLP to leverage multilingual OSINT with minimal labeled data",
        "Apply TL to under-explored domains such as ICS/OT, automotive and satellite security with realistic constraints",
        "Causal transfer learning to reduce spurious correlations when transferring between network environments"
      ],
      "architectural_improvement_recommendations": [
        "Combine federated transfer learning with homomorphic encryption and secure aggregation while using feature-based compression to cut communication overhead",
        "Use domain-adversarial training and invariant risk minimization to learn domain-invariant representations that reduce negative transfer",
        "Apply knowledge distillation to produce compact student models that preserve transferred knowledge with lower privacy leakage",
        "Incorporate meta-learning to rapidly adapt transferred models to new organizations with few labels",
        "Add calibrated uncertainty estimation to decide when not to transfer or when to request labels",
        "Leverage parameter-efficient transfer (e.g., adapters, LoRA) to minimize computation and data sharing in constrained environments"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Data privacy and legal constraints preventing data sharing across organizations",
        "Computational overhead of homomorphic encryption and secure MPC",
        "Communication overhead and wireless link constraints in federated settings",
        "Data imbalance and distribution shift between source and target domains",
        "Risk of negative transfer and lack of domain relatedness metrics"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": true,
      "fairness_bias": false
    },
    "contributions": [
      "First-of-its-kind literature review covering implementations of transfer learning across computer security domains",
      "Comprehensive taxonomy categorizing TL applications by cybersecurity domains and subdomains",
      "Insights into benefits and challenges of TL in cybersecurity (e.g., detection accuracy, reduced training time, handling data imbalance, privacy preservation)",
      "Future research roadmap highlighting unexplored areas and community challenges"
    ]
  },
  {
    "arxiv_id": "2310.11409v6",
    "title": "LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks",
    "authors": "Andreas Happe; Aaron Kaplan; Juergen Cito",
    "abstract": "Penetration-testing is crucial for identifying system vulnerabilities, with privilege-escalation being a critical subtask to gain elevated access to protected resources. Language Models (LLMs) presents new avenues for automating these security practices by emulating human behavior. However, a comprehensive understanding of LLMs' efficacy and limitations in performing autonomous Linux privilege-escalation attacks remains under-explored. To address this gap, we introduce hackingBuddyGPT, a fully automated LLM-driven prototype designed for autonomous Linux privilege-escalation. We curated a novel, publicly available Linux privilege-escalation benchmark, enabling controlled and reproducible evaluation.   Our empirical analysis assesses the quantitative success rates and qualitative operational behaviors of various LLMs -- GPT-3.5-Turbo, GPT-4-Turbo, and Llama3 -- against baselines of human professional pen-testers and traditional automated tools. We investigate the impact of context management strategies, different context sizes, and various high-level guidance mechanisms on LLM performance.   Results show that GPT-4-Turbo demonstrates high efficacy, successfully exploiting 33-83% of vulnerabilities, a performance comparable to human pen-testers (75%). In contrast, local models like Llama3 exhibited limited success (0-33%), and GPT-3.5-Turbo achieved moderate rates (16-50%). We show that both high-level guidance and state-management through LLM-driven reflection significantly boost LLM success rates.   Qualitative analysis reveals both LLMs' strengths and weaknesses in generating valid commands and highlights challenges in common-sense reasoning, error handling, and multi-step exploitation, particularly with temporal dependencies. Cost analysis indicates that GPT-4-Turbo can achieve human-comparable performance at competitive costs, especially with optimized context management.",
    "published_date": "2023-10-17",
    "pdf_link": "https://arxiv.org/pdf/2310.11409v6",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "benchmark",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Offensive Security",
      "subdomain": "Penetration Testing",
      "specific_problem": "Autonomous Linux local privilege-escalation exploitation using LLM agents",
      "attack_types": [
        "Privilege Escalation (Local)",
        "Linux PrivEsc"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "LLM Agent with Reflection/Guidance",
        "specific": "GPT-4-Turbo (controller/agent)",
        "novel_contribution": "hackingBuddyGPT: fully automated LLM-driven agent for Linux privilege escalation with explicit state management via LLM-driven reflection and high-level guidance strategies; evaluation of context management and guidance effects"
      },
      {
        "type": "primary",
        "category": "Prompting / In-Context Learning",
        "specific": null,
        "novel_contribution": "Systematic study of context size and high-level guidance mechanisms on exploit success and cost"
      },
      {
        "type": "baseline",
        "category": "Transformer LLM",
        "specific": "GPT-3.5-Turbo",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer LLM",
        "specific": "Llama3 (local)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "In-context learning (prompting)",
      "Agent-based LLM control",
      "No training (zero-/few-shot)"
    ],
    "datasets": [
      {
        "name": "Linux Privilege-Escalation Benchmark (benchmark-privesc-linux)",
        "type": "public",
        "domain": "ctf_virtual_machines",
        "link": "https://github.com/ipa-lab/benchmark-privesc-linux",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "hackingBuddyGPT Trajectory Data (agent runs, commands, outcomes)",
        "type": "public",
        "domain": "agent_trajectories",
        "link": "https://github.com/ipa-lab/hackingbuddy-results",
        "is_new_contribution": true,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Human professional penetration testers",
        "paper_reference": null,
        "metric": "Success rate (percentage of vulnerabilities exploited)",
        "their_result": "GPT-4-Turbo: “successfully exploiting 33–83% of vulnerabilities …” and with guidance up to “83%”",
        "baseline_result": "“human penetration testers (75%)”"
      },
      {
        "method_name": "GPT-3.5-Turbo",
        "paper_reference": null,
        "metric": "Success rate (percentage of vulnerabilities exploited)",
        "their_result": "GPT-4-Turbo: 33–83% (depending on guidance/reflection)",
        "baseline_result": "“16–50%”"
      },
      {
        "method_name": "Llama3 (local model)",
        "paper_reference": null,
        "metric": "Success rate (percentage of vulnerabilities exploited)",
        "their_result": "GPT-4-Turbo: 33–83% (depending on settings)",
        "baseline_result": "“0–33%”"
      },
      {
        "method_name": "Traditional automated tools (e.g., enumeration scripts such as linPEAS, linux-smart-enumeration, linenum.sh)",
        "paper_reference": null,
        "metric": "Success rate (exploitation ability)",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Success rate (vulnerabilities exploited / total)",
      "Cost per exploited vulnerability (API cost)",
      "Effect of context size and state management on performance",
      "Qualitative error/behavior analysis"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "RQ1: What is the efficacy of LLMs in performing autonomous Linux privilege-escalation attacks? (including: How do quantitative success rates and qualitative operational behaviors compare against human penetration-testers and traditional automated tools? What are the primary challenging areas and qualitative limitations in LLM-generated commands?)",
        "RQ2: How do various context management strategies and context sizes impact the efficacy and efficiency of LLM-driven privilege-escalation agents?",
        "RQ3: To what extent do different high-level guidance mechanisms influence the success rates of attack vectors by LLM-based privilege-escalation agents?"
      ],
      "gaps_identified": [
        "Underexplored efficacy and limitations of LLMs in autonomous Linux privilege-escalation attacks",
        "Lack of Linux privilege-escalation benchmarks and standards/methodologies",
        "Automation in Linux priv-esc traditionally limited to enumeration; lack of automated exploitation",
        "Limited understanding of how context management and high-level guidance affect LLM performance in this domain"
      ],
      "limitations": [
        "LLMs show challenges in common-sense reasoning, error handling, and multi-step exploitation with temporal dependencies",
        "Evaluation conducted on curated, single-vulnerability CTF-style Linux VMs (scope may not cover multi-vulnerability real-world systems)",
        "Focus on Linux only; Windows/macOS/container contexts not evaluated",
        "Local small models (e.g., Llama3) exhibited limited success in this setup",
        "Safety constraints necessitate isolated testbeds, which may differ from production environments"
      ],
      "future_work": [
        "Guide future research toward more effective and reliable LLM-guided penetration-testing, including improved guidance and context/state management"
      ],
      "motivation": "Investigate how off-the-shelf LLMs perform in Linux privilege-escalation to provide a baseline for LLM-guided pen-testing; determine whether advanced techniques (e.g., CoT/task trees) are necessary if unaided models already succeed; ensure solutions are cost-effective and time-efficient for practitioners.",
      "potential_research_ideas": [
        "Integrate retrieval-augmented guidance from curated priv-esc knowledge bases (e.g., HackTricks, GTFOBins) to improve exploit selection and command synthesis",
        "Introduce an explicit planner (e.g., Pentest Task Tree or formal planning) with memory to handle multi-step exploits and temporal dependencies",
        "Leverage multi-agent collaboration (planner, verifier, executor) and self-consistency/debate to reduce hallucinations and improve error handling",
        "Fine-tune or instruct-train small local models on released trajectories to approach GPT-4-Turbo performance offline",
        "Develop command verifiers/simulators and sandboxed validation to catch dangerous or incorrect commands before execution",
        "Extend benchmark to multi-vulnerability VMs, containers/K8s priv-esc, and kernel-level exploits for broader coverage",
        "Optimize cost-performance via adaptive context windows, selective tool outputs, and learned state summarization",
        "Use program-of-thought/action graphs to model dependencies and track progress across steps"
      ],
      "architectural_improvement_recommendations": [
        "Add an explicit Task Planner module with persistent memory and a working set (plan-and-execute architecture) distinct from the Executor",
        "Employ structured tool-use with function calling and a command schema plus static checks (allow/deny lists) prior to execution",
        "Incorporate RAG over vetted priv-esc corpora with citation grounding to reduce hallucinations",
        "Adopt reflection with outcome-aware state summarization and temporal precondition tracking for multi-step exploits",
        "Integrate an execution monitor that validates post-conditions and auto-recovers from common errors",
        "Implement adaptive context management (summarization + retrieval) to keep prompts concise while retaining salient state"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/ipa-lab/hackingBuddyGPT",
      "frameworks": [],
      "reproducibility_score": "high",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Isolated local virtual machines on a private virtual network",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Ensuring safety when executing autonomous exploitation commands",
        "LLM limitations in common-sense reasoning and error handling",
        "Managing temporal dependencies across multi-step exploits",
        "Context-size limits and state management trade-offs",
        "Monetary cost and token usage optimization"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "A publicly available Linux privilege-escalation benchmark of distinct, single-vulnerability VMs that can be run locally",
      "A fully automated LLM-driven Linux privilege-escalation prototype (hackingBuddyGPT)",
      "Empirical evaluation of GPT-3.5-Turbo, GPT-4-Turbo, and Llama3 against human professional pen-testers and traditional automated tools",
      "Systematic study of context management strategies, context sizes, and high-level guidance; showing reflection-based state management boosts success",
      "Quantitative results: “GPT-4-Turbo demonstrates high efficacy, successfully exploiting 33–83% of vulnerabilities … comparable to human penetration testers (75%)”; Llama3 “0–33%”; GPT-3.5-Turbo “16–50%”",
      "Demonstration that high-level guidance increases GPT-4-Turbo success from “33% to 66% … or from 66% to 83%”; reflection doubled unaided GPT-4-Turbo from “33% to 66%”",
      "Qualitative analysis of command validity, reasoning errors, error handling, and multi-step temporal dependencies",
      "Cost analysis showing GPT-4-Turbo can achieve human-comparable performance at competitive costs with optimized context management",
      "Public release of source code, benchmark testbed, and captured trajectory data"
    ]
  },
  {
    "arxiv_id": "2310.04381v2",
    "title": "Hermes: Unlocking Security Analysis of Cellular Network Protocols by Synthesizing Finite State Machines from Natural Language Specifications",
    "authors": "Abdullah Al Ishtiaq; Sarkar Snigdha Sarathi Das; Syed Md Mukit Rashid; Ali Ranjbar; Kai Tu; Tianwei Wu; Zhezheng Song; Weixuan Wang; Mujtahid Akon; Rui Zhang; Syed Rafiul Hussain",
    "abstract": "In this paper, we present Hermes, an end-to-end framework to automatically generate formal representations from natural language cellular specifications. We first develop a neural constituency parser, NEUTREX, to process transition-relevant texts and extract transition components (i.e., states, conditions, and actions). We also design a domain-specific language to translate these transition components to logical formulas by leveraging dependency parse trees. Finally, we compile these logical formulas to generate transitions and create the formal model as finite state machines. To demonstrate the effectiveness of Hermes, we evaluate it on 4G NAS, 5G NAS, and 5G RRC specifications and obtain an overall accuracy of 81-87%, which is a substantial improvement over the state-of-the-art. Our security analysis of the extracted models uncovers 3 new vulnerabilities and identifies 19 previous attacks in 4G and 5G specifications, and 7 deviations in commercial 4G basebands.",
    "published_date": "2023-10-06",
    "pdf_link": "https://arxiv.org/pdf/2310.04381v2",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "benchmark",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Cellular/Telecom Protocol Security",
      "specific_problem": "Automatic synthesis of formal finite state machine (FSM) models from natural language 3GPP cellular specifications to enable security analysis (model checking and implementation deviation detection)",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Neural Constituency Parser",
        "specific": "NEUTREX (adapted span-based neural constituency parser)",
        "novel_contribution": "Domain-knowledge-informed nested grammar for cellular transition components; training curriculum that prioritizes token labeling then span boundaries; handles long paragraphs and nested control blocks"
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "CellulaRoBERTa (RoBERTa pretrained on 22k cellular documents)",
        "novel_contribution": "Domain-specific pretraining on 22,000 cellular technical documents (specs, change requests, reports) to close domain shift for cellular terminology"
      },
      {
        "type": "primary",
        "category": "Neural Dependency Parser",
        "specific": null,
        "novel_contribution": "Used to map transition component spans to a designed DSL via dependency trees to synthesize logical formulas (IR) for states, conditions, and actions"
      },
      {
        "type": "primary",
        "category": "Domain Specific Language (DSL)",
        "specific": null,
        "novel_contribution": "Custom DSL and AST mapping to translate extracted components into intermediate logical representations compilable to FSM transitions and SMV"
      },
      {
        "type": "baseline",
        "category": "NLP Pipeline",
        "specific": "RFCNLP",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised (domain pretraining)"
    ],
    "datasets": [
      {
        "name": "Curated dataset of natural language transition components in cellular specifications",
        "type": "public",
        "domain": "protocol_spec_text",
        "link": "https://github.com/SyNSec-den/hermes-spec-to-fsm",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "3GPP 4G NAS (TS 24.301, Release 16) specification text",
        "type": "public",
        "domain": "protocol_spec_text",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "3GPP 5G NAS (Release 17) specification text",
        "type": "public",
        "domain": "protocol_spec_text",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "3GPP 5G RRC (Release 17) specification text",
        "type": "public",
        "domain": "protocol_spec_text",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "RFC documents corpus (for RFCNLP comparison)",
        "type": "public",
        "domain": "protocol_spec_text",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Cellular pretraining corpus (22,000 documents of specs, change requests, technical reports)",
        "type": "proprietary",
        "domain": "protocol_spec_text",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "Commercial 4G baseband implementation behaviors (9 basebands) for deviation analysis",
        "type": "proprietary",
        "domain": "device_firmware_behavior",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "RFCNLP (cellular specifications task)",
        "paper_reference": "[67]",
        "metric": "Labeled F1-score (transition component extraction)",
        "their_result": "68.69%",
        "baseline_result": "38.52%"
      },
      {
        "method_name": "RFCNLP (RFC documents task)",
        "paper_reference": "[67]",
        "metric": "Labeled F1-score (transition component extraction)",
        "their_result": "57.06%",
        "baseline_result": "47.76%"
      }
    ],
    "performance_metrics_used": [
      "accuracy",
      "F1-score"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can we automatically synthesize formal FSM models from natural language cellular specifications at scale?",
        "How can neural parsing (constituency and dependency) and a DSL be combined to reliably extract states, conditions, and actions from 3GPP texts?",
        "Do automatically extracted models enable effective security analyses (model checking and implementation deviation detection) that find known and new issues?"
      ],
      "gaps_identified": [
        "Absence of formal FSM models from 3GPP; reliance on hand-crafted models that are tedious, error-prone, and not updated with frequent spec changes",
        "Off-the-shelf NLP models perform poorly on cellular specs due to domain shift, nested structures, and long contexts",
        "Lack of annotated datasets/benchmarks for translating cellular NL to logical forms suitable for formal analysis"
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Manually building and maintaining formal models for large, frequently updated 3GPP specifications is infeasible; automated extraction enables up-to-date security analyses to uncover design flaws and implementation deviations.",
      "potential_research_ideas": [
        "Extend Hermes to additional 3GPP layers (e.g., RLC, PDCP) and non-3GPP telecom protocols; evaluate cross-standard generalization",
        "Incorporate active learning/human-in-the-loop annotation to iteratively improve NEUTREX on ambiguous constructs",
        "Explore sequence-to-DSL large language models with constrained decoding against the DSL grammar to reduce dependency on dependency parsing",
        "Integrate symbolic reasoning/type checking earlier in the pipeline to detect and correct inconsistent or incomplete extractions",
        "Automate continuous model updates triggered by new 3GPP change requests to keep FSMs synchronized with releases",
        "Combine text-derived models with implementation traces (e.g., over-the-air logs) to reconcile spec and implementation FSMs",
        "Assess and improve robustness of the extraction pipeline to minor textual perturbations and spec formatting changes"
      ],
      "architectural_improvement_recommendations": [
        "Add constrained decoding over the DSL grammar using a neural parser to directly emit ASTs, reducing error propagation from separate dependency parsing",
        "Adopt long-context transformers (e.g., efficient attention) to better handle long paragraphs without splitting, preserving cross-sentence dependencies",
        "Introduce uncertainty estimation and calibration at each stage (span extraction, DSL mapping) to flag low-confidence transitions for optional human review",
        "Use multi-task learning to jointly train constituency and dependency parsing on cellular texts to share representations",
        "Leverage retrieval-augmented parsing that indexes spec glossaries/tables to resolve variable names and message fields consistently"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/SyNSec-den/hermes-spec-to-fsm",
      "frameworks": [],
      "reproducibility_score": "high",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Commercial 4G baseband devices (UE) compared against Hermes-generated FSMs; model checking via SMV on extracted FSMs",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Handling ambiguities and nested constructs in natural language specifications",
        "Keeping models up to date with frequent 3GPP releases and change requests",
        "Domain shift and terminology coverage for embedding models",
        "Potential error propagation across pipeline stages (span extraction -> DSL mapping -> FSM synthesis)"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Design and implementation of Hermes, an end-to-end framework to automatically extract formal FSMs from natural language cellular specifications",
      "Development of NEUTREX, a neural constituency parser with domain-informed grammar for transition component extraction",
      "Creation of a DSL and dependency parsing-based IRSynthesizer to translate extracted components into logical formulas and FSM transitions",
      "Evaluation on 4G NAS, 5G NAS, and 5G RRC achieving overall transition accuracy of 81–87% and NEUTREX labeled F1 of 68.69% on cellular specs (vs 38.52% for RFCNLP); on RFCs 57.06% vs 47.76%",
      "Security analyses: model checking identifies 19 previous and 3 new vulnerabilities in 4G/5G specifications; implementation comparison uncovers 7 deviations across 9 commercial 4G basebands (78% accuracy)",
      "Release of curated annotated dataset of cellular transition components, source code, and properties as a benchmark at https://github.com/SyNSec-den/hermes-spec-to-fsm"
    ]
  },
  {
    "arxiv_id": "2311.16169v3",
    "title": "Understanding the Effectiveness of Large Language Models in Detecting Security Vulnerabilities",
    "authors": "Avishree Khare; Saikat Dutta; Ziyang Li; Alaia Solko-Breslin; Rajeev Alur; Mayur Naik",
    "abstract": "While automated vulnerability detection techniques have made promising progress in detecting security vulnerabilities, their scalability and applicability remain challenging. The remarkable performance of Large Language Models (LLMs), such as GPT-4 and CodeLlama, on code-related tasks has prompted recent works to explore if LLMs can be used to detect vulnerabilities. In this paper, we perform a more comprehensive study by concurrently examining a higher number of datasets, languages and LLMs, and qualitatively evaluating performance across prompts and vulnerability classes while addressing the shortcomings of existing tools. Concretely, we evaluate the effectiveness of 16 pre-trained LLMs on 5,000 code samples from five diverse security datasets. These balanced datasets encompass both synthetic and real-world projects in Java and C/C++ and cover 25 distinct vulnerability classes.   Overall, LLMs across all scales and families show modest effectiveness in detecting vulnerabilities, obtaining an average accuracy of 62.8% and F1 score of 0.71 across datasets. They are significantly better at detecting vulnerabilities only requiring intra-procedural analysis, such as OS Command Injection and NULL Pointer Dereference. Moreover, they report higher accuracies on these vulnerabilities than popular static analysis tools, such as CodeQL.   We find that advanced prompting strategies that involve step-by-step analysis significantly improve performance of LLMs on real-world datasets in terms of F1 score (by upto 0.18 on average). Interestingly, we observe that LLMs show promising abilities at performing parts of the analysis correctly, such as identifying vulnerability-related specifications and leveraging natural language information to understand code behavior (e.g., to check if code is sanitized). We expect our insights to guide future work on LLM-augmented vulnerability detection systems.",
    "published_date": "2023-11-16",
    "pdf_link": "https://arxiv.org/pdf/2311.16169v3",
    "paper_types": [
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Vulnerability Detection",
      "specific_problem": "Detecting security vulnerabilities in source code snippets (Java and C/C++) and identifying CWE class using pre-trained LLMs with different prompting strategies",
      "attack_types": [
        "OS Command Injection (CWE-78)",
        "NULL Pointer Dereference (CWE-476)",
        "Out-of-bounds Read (CWE-125)",
        "Out-of-bounds Write (CWE-787)",
        "Path Traversal (CWE-22)",
        "Integer Overflow (CWE-190)",
        "Buffer Overflow"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer LLM",
        "specific": "GPT-4, GPT-3.5",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Transformer LLM",
        "specific": "Gemini-1.5-Flash",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Transformer LLM (code-focused)",
        "specific": "CodeLlama (7B/13B/34B), Llama-3.1 (8B/70B), Mistral Codestral-22B, DeepSeekCoder (7B/15B/33B), Qwen2.5 (14B/32B), Qwen2.5-Coder (1.5B/7B)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Prompt Engineering / Reasoning",
        "specific": "Dataflow analysis-based prompt (CWE-DF): source–sink–sanitizer step-by-step analysis",
        "novel_contribution": "Introduces a dataflow analysis-inspired prompting strategy that improves F1 on real-world datasets by up to 0.18 on average"
      },
      {
        "type": "baseline",
        "category": "Deep Learning (unspecified)",
        "specific": "DeepDFA",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Deep Learning (unspecified)",
        "specific": "LineVul",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Zero-shot",
      "In-context prompting"
    ],
    "datasets": [
      {
        "name": "OWASP Benchmark (Java)",
        "type": "public",
        "domain": "source_code",
        "link": "https://owasp.org/www-project-benchmark/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Juliet Test Suite (SARD) - C/C++",
        "type": "public",
        "domain": "source_code",
        "link": "https://samate.nist.gov/SARD/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Juliet Test Suite (SARD) - Java",
        "type": "public",
        "domain": "source_code",
        "link": "https://samate.nist.gov/SARD/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CVEFixes - C/C++",
        "type": "public",
        "domain": "source_code",
        "link": "https://github.com/soarsmu/CVEfixes",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CVEFixes - Java",
        "type": "public",
        "domain": "source_code",
        "link": "https://github.com/soarsmu/CVEfixes",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "CodeQL (static analysis)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "DeepDFA",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "LineVul",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "Precision",
      "Recall",
      "F1 score"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "RQ1: How do different pre-trained LLMs perform in detecting security vulnerabilities across different languages and datasets?",
        "RQ2: How do different prompting strategies affect the performance of LLMs?",
        "RQ3: How does the performance of LLMs vary across different vulnerability classes?",
        "RQ4: How do LLMs compare to state-of-the-art static analysis tools?",
        "RQ5: How do LLMs compare to state-of-the-art deep-learning-based tools?"
      ],
      "gaps_identified": [
        "Traditional static and dynamic techniques face scalability and applicability challenges (manual API specs, build/compile requirements, fuzz driver creation).",
        "Existing LLM studies are limited in dataset diversity, language coverage, and balance; many real-world datasets are not two-sided or lack CVE/CWE metadata.",
        "LLMs show modest end-to-end vulnerability reasoning and struggle with vulnerabilities requiring inter-procedural/global context or complex data structures.",
        "Performance on real-world datasets is lower than on synthetic datasets (average accuracy gap reported as 10.5%).",
        "Effectiveness varies significantly by prompting strategy and vulnerability class; basic prompts underperform compared to structured, step-by-step prompts.",
        "Context length and single-file constraints limit detection for multi-file/codebase-level issues."
      ],
      "limitations": [
        "Evaluation limited to MITRE Top-25 CWE categories from each dataset.",
        "Samples exceeding 2048 tokens and benchmarks spanning multiple files were excluded due to prompt/context limitations.",
        "Evaluation restricted to Java and C/C++.",
        "Balanced sample selection of 500 vulnerable and 500 non-vulnerable per dataset (total 5,000) from much larger corpora.",
        "Zero-shot top-1 predictions with deterministic decoding (temperature=0); no fine-tuning.",
        "Dataflow analysis-based prompt is more costly (longer responses/tokens).",
        "Few-shot and chain-of-thought prompting were not pursued beyond preliminary trials on small subsets."
      ],
      "future_work": [
        "Develop LLM-augmented vulnerability detection systems that combine partial reasoning strengths of LLMs (e.g., source/sink/sanitizer identification) with static analysis.",
        "Expand evaluations to multi-file, cross-function, and larger-context scenarios and additional languages.",
        "Explore improved prompting and tool-use strategies to address inter-procedural reasoning and complex data structures.",
        "Investigate training or fine-tuning strategies specialized for vulnerability classes while leveraging LLMs’ API knowledge."
      ],
      "motivation": "Assess whether state-of-the-art pre-trained LLMs can effectively detect security vulnerabilities in code, given their strong performance on code-related tasks and the limitations of traditional vulnerability detection approaches.",
      "potential_research_ideas": [
        "Hybrid LLM + static analysis pipeline where LLMs extract candidate sources/sinks/sanitizers and hypotheses that guide targeted CodeQL/taint analyses.",
        "Retrieval-augmented prompting that injects CWE-specific rules, secure coding guidelines, and project API specs to improve class-wise detection.",
        "Program-of-thought prompting with tool-use (e.g., symbolic execution, dataflow analyzers) to support inter-procedural reasoning.",
        "Self-consistency or multi-agent deliberation for reducing false positives/negatives in real-world datasets.",
        "Lightweight supervised fine-tuning or preference optimization on curated, balanced, two-sided vulnerability corpora for specific CWE families.",
        "Scaling to multi-file reasoning using call-graph summaries and hierarchical chunking to preserve global context.",
        "Confidence calibration and abstention mechanisms for practical deployment (triage suggestions rather than binary labels)."
      ],
      "architectural_improvement_recommendations": [
        "Enforce structured intermediate outputs (identified sources, sinks, sanitizers, and paths) before final vulnerability verdict.",
        "Incorporate retrieval of CWE definitions/APIs and project-specific sanitizer specifications into prompts.",
        "Use iterative multi-turn analysis: initial hypothesis generation, evidence gathering, and refinement with external tools (CodeQL/taint engines).",
        "Apply self-consistency (multiple sampled chains) and voting to improve robustness on hard classes.",
        "Introduce a routing ensemble: route samples to specialized LLMs or static analyzers based on predicted CWE or complexity.",
        "Augment with semantic code summaries and call-graph/context windows via hierarchical encoding."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "OpenAI API (ChatCompletions)",
        "Google Gemini API",
        "Hugging Face API"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Open-source LLM runs on a cluster with NVIDIA A100, A6000, and RTX 2080 GPUs; temperature=0, max_tokens=1024; inputs filtered to <=2048 tokens; 16 LLMs evaluated over 5,000 samples."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Limited ability to analyze multi-file and inter-procedural vulnerabilities due to context constraints.",
        "Prompt/context length and token cost for step-by-step dataflow prompts.",
        "Lower performance on real-world datasets than synthetic benchmarks.",
        "Potential false positives/negatives and need for human verification.",
        "Privacy and compliance concerns when sending proprietary code to external APIs."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Largest comprehensive empirical study of LLMs for vulnerability detection across 16 models, 5 datasets (Java and C/C++), and 25 vulnerability classes with 5,000 balanced samples.",
      "Quantitative and qualitative analysis of three prompting strategies, including a novel dataflow analysis-based prompt.",
      "LLMs show modest overall effectiveness (average accuracy 62.8%, average F1 0.71) and perform better on intra-procedural vulnerabilities like OS Command Injection and NULL Pointer Dereference.",
      "Dataflow analysis-based prompting improves F1 on real-world datasets by up to 0.18 on average.",
      "Comparison with CodeQL and DL-based tools (DeepDFA, LineVul): LLMs outperform on certain classes (e.g., Path Traversal, OS Command Injection) but underperform overall on synthetic datasets; CodeQL stronger on Integer Overflow.",
      "Insights that some smaller models outperform larger ones on real-world datasets; qualitative evidence that LLMs can correctly infer partial specifications (sources/sinks/sanitizers) even when end-to-end reasoning fails."
    ]
  },
  {
    "arxiv_id": "2309.14677v1",
    "title": "XGV-BERT: Leveraging Contextualized Language Model and Graph Neural Network for Efficient Software Vulnerability Detection",
    "authors": "Vu Le Anh Quan; Chau Thuan Phat; Kiet Van Nguyen; Phan The Duy; Van-Hau Pham",
    "abstract": "With the advancement of deep learning (DL) in various fields, there are many attempts to reveal software vulnerabilities by data-driven approach. Nonetheless, such existing works lack the effective representation that can retain the non-sequential semantic characteristics and contextual relationship of source code attributes. Hence, in this work, we propose XGV-BERT, a framework that combines the pre-trained CodeBERT model and Graph Neural Network (GCN) to detect software vulnerabilities. By jointly training the CodeBERT and GCN modules within XGV-BERT, the proposed model leverages the advantages of large-scale pre-training, harnessing vast raw data, and transfer learning by learning representations for training data through graph convolution. The research results demonstrate that the XGV-BERT method significantly improves vulnerability detection accuracy compared to two existing methods such as VulDeePecker and SySeVR. For the VulDeePecker dataset, XGV-BERT achieves an impressive F1-score of 97.5%, significantly outperforming VulDeePecker, which achieved an F1-score of 78.3%. Again, with the SySeVR dataset, XGV-BERT achieves an F1-score of 95.5%, surpassing the results of SySeVR with an F1-score of 83.5%.",
    "published_date": "2023-09-26",
    "pdf_link": "https://arxiv.org/pdf/2309.14677v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Vulnerability Detection",
      "specific_problem": "Supervised detection of vulnerabilities in C/C++ source code using contextual code embeddings and graph representations",
      "attack_types": [
        "buffer overflow",
        "library/API misuse",
        "array misuse/out-of-bounds"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "CodeBERT",
        "novel_contribution": "Jointly trained with a GCN over code graphs to capture contextual and non-sequential semantic relationships in source code"
      },
      {
        "type": "primary",
        "category": "GNN",
        "specific": "GCN",
        "novel_contribution": "Graph convolution over graphs constructed from code slices to enhance connections between words and slices; integrated end-to-end with CodeBERT"
      },
      {
        "type": "baseline",
        "category": "RNN",
        "specific": "BiLSTM (VulDeePecker)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Transfer Learning",
      "Fine-tuning",
      "Joint training"
    ],
    "datasets": [
      {
        "name": "VulDeePecker dataset",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "SySeVR dataset",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "SARD (Software Assurance Reference Dataset)",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "NVD (National Vulnerability Database)",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "VulDeePecker",
        "paper_reference": "[17]",
        "metric": "F1-score",
        "their_result": "97.5%",
        "baseline_result": "78.3%"
      },
      {
        "method_name": "SySeVR",
        "paper_reference": "[20]",
        "metric": "F1-score",
        "their_result": "95.5%",
        "baseline_result": "83.5%"
      }
    ],
    "performance_metrics_used": [
      "F1-score"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Existing works lack effective representations that retain non-sequential semantic characteristics and contextual relationships of source code attributes.",
        "Prior methods (e.g., SySeVR, VulDeBERT) use isolated code statements lacking contextual linkage, reducing precision.",
        "Need to capture relationships between words and between words and slices via graph structures to better model code semantics."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Improve automated software vulnerability detection by combining large-scale pretrained code representations with graph-based modeling to better capture contextual and non-sequential semantics.",
      "potential_research_ideas": [
        "Extend to heterogeneous program graphs that combine AST, CFG, and PDG/CPG for richer structural context in vulnerability detection.",
        "Incorporate attention-based GNNs (e.g., GAT) or powerful graph encoders (e.g., GIN) to better weigh critical code relations.",
        "Adopt contrastive or self-supervised pretraining on code graphs before supervised fine-tuning to reduce labeled data needs.",
        "Multi-task learning to jointly predict vulnerability presence and type, or to localize vulnerable lines/spans.",
        "Integrate taint/data-flow analysis to guide graph construction and message passing toward vulnerability-relevant flows.",
        "Cross-language generalization (e.g., C#, Java) by leveraging multilingual code LMs (e.g., CodeBERT variants) with language-agnostic graphs.",
        "Leverage larger code LMs (e.g., CodeT5, GraphCodeBERT) and parameter-efficient fine-tuning (adapters/LoRA) combined with GNNs.",
        "Add post-hoc explainability modules (e.g., graph saliency or attention visualization) to highlight vulnerable tokens and edges.",
        "Evaluate adversarial robustness to code obfuscations/semantics-preserving transformations and harden via augmentation.",
        "Scale to project-level or repository-level vulnerability risk prediction with hierarchical graph modeling."
      ],
      "architectural_improvement_recommendations": [
        "Replace or complement GCN with GAT/GNN variants (GAT, GIN, GraphSAGE) to capture importance of specific code relations.",
        "Use heterogeneous GNN over a joint AST+CFG+PDG/CPG graph; learn relation-specific transformations.",
        "Introduce a contrastive alignment loss between CodeBERT token embeddings and graph node embeddings to improve fusion.",
        "Employ hierarchical pooling/readout (e.g., DiffPool, SAGPool) for multi-granularity representations (statement, function, file).",
        "Adopt GraphCodeBERT or CodeT5 as the textual backbone and compare with CodeBERT within the same framework.",
        "Parameter-efficient fine-tuning (adapters/LoRA) for the LM to ease training and improve stability in joint training.",
        "Incorporate vulnerability-type auxiliary heads for multi-task supervision and richer gradients.",
        "Pretrain the graph encoder with masked node/edge prediction on large unlabeled code graphs."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Use CodeBERT embeddings to represent vulnerable source code instead of Word2Vec used in prior studies.",
      "Propose XGV-BERT: a joint framework combining CodeBERT and a Graph Convolutional Network (GCN) for C/C++ vulnerability detection.",
      "Demonstrate improved performance over state-of-the-art methods (VulDeePecker, SySeVR) on datasets derived from SARD and NVD, achieving F1-scores of 97.5% (VulDeePecker dataset) and 95.5% (SySeVR dataset)."
    ]
  },
  {
    "arxiv_id": "2310.02530v2",
    "title": "CompVPD: Iteratively Identifying Vulnerability Patches Based on Human Validation Results with a Precise Context",
    "authors": "Tianyu Chen; Lin Li; Taotao Qian; Jingyi Liu; Wei Yang; Ding Li; Guangtai Liang; Qianxiang Wang; Tao Xie",
    "abstract": "Applying security patches in open source software timely is critical for ensuring the security of downstream applications. However, it is challenging to apply these patches promptly because notifications of patches are often incomplete and delayed. To address this issue, existing approaches employ deep-learning (DL) models to identify additional vulnerability patches by determining whether a code commit addresses a vulnerability. Nonetheless, these approaches suffer from low accuracy due to the imprecise context provided for the patches. To provide precise context for patches, we propose a multi-granularity slicing algorithm and an adaptive-expanding algorithm to accurately identify code related to the patches. Additionally, the precise context enables to design an iterative identification framework, CompVPD, which utilizes the human validation results, and substantially improve the effectiveness. We empirically compare CompVPD with four state-of-the-art/practice (SOTA) approaches in identifying vulnerability patches. The results demonstrate that CompVPD improves the F1 score by 20% compared to the best scores of the SOTA approaches. Additionally, CompVPD contributes to security practice by helping identify 20 vulnerability patches and 18 fixes for high-risk bugs from 2,500 recent code commits in five highly popular open-source projects.",
    "published_date": "2023-10-04",
    "pdf_link": "https://arxiv.org/pdf/2310.02530v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Vulnerability Management and Patch Management",
      "specific_problem": "Automatic identification of vulnerability-fixing commits (vulnerability patches) in open-source repositories",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "CodeBERT",
        "novel_contribution": "Used within CompVPD with precise context generation and iterative fine-tuning leveraging human validation to identify vulnerability patches"
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "StarCoder",
        "novel_contribution": "Alternative backbone within CompVPD; benefits from precise context and iterative identification framework"
      },
      {
        "type": "primary",
        "category": "Human-in-the-loop learning",
        "specific": null,
        "novel_contribution": "Iterative identification framework that fine-tunes on human validation results to transform an unseen repository into a seen one and improve accuracy"
      },
      {
        "type": "primary",
        "category": "Program analysis aided preprocessing",
        "specific": "Multi-granularity slicing + Adaptive context-expanding",
        "novel_contribution": "Two novel algorithms to generate precise (reduced yet comprehensive) context by slicing irrelevant files/methods/statements and expanding nearest statement blocks"
      },
      {
        "type": "baseline",
        "category": "Graph Neural Network",
        "specific": "RGCN",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Sequence Model",
        "specific": "DeepJIT",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer/Sequence + heuristic features",
        "specific": "VulFixMiner",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer/Sequence model",
        "specific": "MiDas",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Sequence model",
        "specific": "Sun et al. (2019/2020-style commit classification)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Human-in-the-loop",
      "Iterative fine-tuning"
    ],
    "datasets": [
      {
        "name": "VulFix (dataset from VulFixMiner)",
        "type": "public",
        "domain": "source_code_commits",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Recent commits from five OSS projects (jenkins, hutool, dubbo, hiro, light-4j)",
        "type": "proprietary",
        "domain": "source_code_commits",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "DeepJIT",
        "paper_reference": "[16]",
        "metric": "F1",
        "their_result": "“CompVPD improves the F1 score by 20% compared to the best scores of the SOTA approaches.”",
        "baseline_result": null
      },
      {
        "method_name": "VulFixMiner",
        "paper_reference": "[52]",
        "metric": "F1",
        "their_result": "“CompVPD improves the F1 score by 20% compared to the best scores of the SOTA approaches.”",
        "baseline_result": null
      },
      {
        "method_name": "Sun et al.",
        "paper_reference": "[35]",
        "metric": "F1",
        "their_result": "“CompVPD improves the F1 score by 20% compared to the best scores of the SOTA approaches.”",
        "baseline_result": null
      },
      {
        "method_name": "MiDas",
        "paper_reference": "[30]",
        "metric": "F1",
        "their_result": "“CompVPD improves the F1 score by 20% compared to the best scores of the SOTA approaches.”",
        "baseline_result": "“MiDas achieves an F1 score of only 0.200.”"
      },
      {
        "method_name": "RGCN",
        "paper_reference": "[8]",
        "metric": "F1",
        "their_result": "“CompVPD improves the F1 score by 20% compared to the best scores of the SOTA approaches.”",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "F1",
      "AUC",
      "inference_time_per_commit",
      "number_of_real_world_patches_found"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Existing DL approaches use imprecise context: either only changed lines (insufficient context) or entire methods/files (too coarse).",
        "Large context size inflates tokens (avg ~7992) leading to quadratic memory/time cost, forcing small models and low accuracy.",
        "Cross-project generalization is limited; models struggle on unseen repositories.",
        "Patch notifications (e.g., NVD) are delayed and incomplete, motivating proactive identification."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Timely application of security patches in OSS is hard due to delayed/incomplete notifications; existing DL-based identification suffers low accuracy from imprecise context and efficiency constraints.",
      "potential_research_ideas": [
        "Extend CompVPD to multi-language and multi-ecosystem repositories (e.g., C/C++, Python, JavaScript) with language-agnostic slicing.",
        "Incorporate uncertainty estimation and active learning to select commits for human validation more effectively.",
        "Integrate static analysis alerts (e.g., taint flows, CWE-specific patterns) and dynamic tests to augment model inputs and labels.",
        "Develop a contrastive pretraining task on commit diffs to better encode vulnerability-fixing semantics.",
        "Add retrieval-augmented modeling that pulls related historical commits/CVEs during inference.",
        "Perform domain adaptation for cross-project settings via meta-learning or adversarial feature alignment.",
        "Jointly predict vulnerability type (CWE) alongside patch identification to improve representation via multitask learning.",
        "Study continual learning methods to avoid forgetting across iterative fine-tuning cycles."
      ],
      "architectural_improvement_recommendations": [
        "Use a hierarchical transformer that encodes file/method/block/diff-hunk levels before fusion, reducing token pressure.",
        "Fuse graph representations (CFG/DFG/call graph) with transformers via GNN-Transformer co-encoders.",
        "Adopt a diff-aware encoder (insertion/deletion tags and edit-tree structure) rather than plain concatenation.",
        "Apply curriculum or self-training during iterative cycles, seeding with high-confidence pseudo-labels under human oversight.",
        "Leverage parameter-efficient fine-tuning (LoRA/IA3) for rapid iteration on new repositories.",
        "Implement uncertainty-based sampling (e.g., MC Dropout) to prioritize human validation.",
        "Cache and reuse repository-level context (e.g., stable call graph summaries) across commits to cut inference costs."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Reported to process one code commit in less than 0.5 seconds; token count reduction via precise context improves fine-tuning and inference efficiency."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Open-source project scanning workflow with human security engineer validation",
      "scalability_discussed": true,
      "inference_time": "< 0.5 seconds per commit",
      "deployment_challenges": [
        "Requires human validation of identified commits; labeling budget management.",
        "Cross-project generalization prior to iterative adaptation.",
        "Complexity of robust program slicing and context extraction across varied codebases."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposed multi-granularity slicing and adaptive context-expanding algorithms to generate precise context for code commits.",
      "Introduced CompVPD, an iterative identification framework that leverages human validation of identified patches for further fine-tuning.",
      "Empirically demonstrated effectiveness and efficiency: “CompVPD improves the F1 score by 20% compared to the best scores of the SOTA approaches,” identifies 35% (CodeBERT) and 13% (StarCoder) more patches under the same human effort, processes a commit in <0.5s, and found 20 vulnerability patches and 18 high-risk bug fixes from 2,500 recent commits across five OSS projects."
    ]
  },
  {
    "arxiv_id": "2310.03498v1",
    "title": "The Anatomy of Deception: Technical and Human Perspectives on a Large-scale Phishing Campaign",
    "authors": "Anargyros Chrysanthou; Yorgos Pantis; Constantinos Patsakis",
    "abstract": "In an era dominated by digital interactions, phishing campaigns have evolved to exploit not just technological vulnerabilities but also human traits. This study takes an unprecedented deep dive into large-scale phishing campaigns aimed at Meta's users, offering a dual perspective on the technical mechanics and human elements involved. Analysing data from over 25,000 victims worldwide, we highlight the nuances of these campaigns, from the intricate techniques deployed by the attackers to the sentiments and behaviours of those who were targeted. Unlike prior research conducted in controlled environments, this investigation capitalises on the vast, diverse, and genuine data extracted directly from active phishing campaigns, allowing for a more holistic understanding of the drivers, facilitators, and human factors. Through the application of advanced computational techniques, including natural language processing and machine learning, this work unveils critical insights into the psyche of victims and the evolving tactics of modern phishers. Our analysis illustrates very poor password selection choices from the victims but also persistence in the revictimisation of a significant part of the users. Finally, we reveal many correlations regarding demographics, timing, sentiment, emotion, and tone of the victims' responses.",
    "published_date": "2023-10-05",
    "pdf_link": "https://arxiv.org/pdf/2310.03498v1",
    "paper_types": [
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Social Engineering and Phishing",
      "subdomain": "Phishing Campaign Analysis and Digital Forensics",
      "specific_problem": "Large-scale real-world analysis of Meta (Facebook/Instagram) impersonation phishing campaigns, including victims' password hygiene, re-victimization behavior, temporal and demographic correlations, and NLP-based sentiment/emotion/tone analysis of appeal-text responses.",
      "attack_types": [
        "email phishing",
        "credential harvesting",
        "brand impersonation",
        "typosquatting",
        "URL obfuscation",
        "hosting abuse (Google Firebase web.app)",
        "data exfiltration via Telegram bot API",
        "data storage via Firebase Storage"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": null,
        "novel_contribution": "Applied transformer-based NLP methods to sentiment, emotion, and tone analysis of free-text provided by victims in real phishing campaign appeal forms at large scale (>25k victims)."
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "Real-world Meta impersonation phishing campaign victim submissions (appeal forms, credentials, timestamps, limited demographics)",
        "type": "proprietary",
        "domain": "phishing_victim_forms",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "OSINT URL intelligence for meta*.web.app (URLScan.io/SecurityTrails searches)",
        "type": "public",
        "domain": "phishing_urls_osint",
        "link": "https://urlscan.io/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "frequency/percentage statistics (e.g., leaked password prevalence, re-victimization rates)",
      "temporal analysis (timing of responses)",
      "correlation analysis (demographics, timing, sentiment, emotion, tone)",
      "distributional analysis of sentiment/emotion/tone"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "What are the technical mechanics and infrastructure characteristics of large-scale phishing campaigns targeting Meta users?",
        "What sentiments, emotions, and tone do victims express in their appeal-text responses, and how do these relate to demographics and timing?",
        "How do victims select passwords, and what is the prevalence of weak/reused or previously leaked passwords among victims?",
        "How persistent is re-victimization among users across campaign interactions?"
      ],
      "gaps_identified": [
        "Existing research often relies on controlled environments or awareness campaigns that introduce biases and lack scale and diversity.",
        "Lack of large-scale, real, and diverse datasets directly extracted from active phishing campaigns capturing genuine victim behavior."
      ],
      "limitations": [
        "“while we cannot collect fine-grained demographic information about the victims”",
        "Dependence on operational/technical deficiencies of phishers for data collection implies potential sampling bias and incomplete coverage (implied)."
      ],
      "future_work": [],
      "motivation": "Provide a holistic, dual technical-and-human perspective on modern phishing operations using genuine, large-scale data from active campaigns to understand drivers, facilitators, and human factors beyond controlled study biases.",
      "potential_research_ideas": [
        "Design and evaluate intervention strategies that leverage detected sentiment/emotion/tone to deliver just-in-time phishing warnings or tailored education.",
        "Model and predict re-victimization risk using behavioral and temporal features to enable proactive protection.",
        "Cross-lingual and cross-cultural analysis of phishing appeal texts to study language-specific persuasion tactics and susceptibility.",
        "Integrate infrastructure telemetry (hosting, domain patterns, exfiltration channels) with NLP-derived victim signals for early campaign attribution and takedown prioritization.",
        "Develop privacy-preserving pipelines (e.g., secure enclaves/differential privacy) for analyzing sensitive phishing-victim data at scale.",
        "Construct benchmark tasks/datasets (with rigorous anonymization) for phishing-focused sentiment/emotion/tone analysis to standardize evaluation."
      ],
      "architectural_improvement_recommendations": [
        "Fine-tune domain-adapted multilingual transformer models on annotated subsets of appeal texts for phishing-specific sentiment/emotion/tone categories.",
        "Employ weak supervision and data programming to expand labeled training data while minimizing manual annotation on sensitive content.",
        "Use hierarchical/multitask architectures combining sentiment, emotion, toxicity, urgency, and intent classification to better capture victim states.",
        "Augment NLP features with temporal and metadata features (e.g., response delay, time-of-day, region proxy) in a joint predictive model for re-victimization risk.",
        "Apply graph-based infrastructure analysis (domains, certificates, hosting, exfil channels) for campaign clustering and actor attribution.",
        "Implement robust anonymization and PII redaction with human-in-the-loop validation to facilitate safe data sharing for research."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Accessing sensitive data from active phishing infrastructures while maintaining ethical and legal constraints",
        "Rapidly evolving attacker TTPs and infrastructure (e.g., mass use of Google Firebase web.app)",
        "Multilingual and noisy user-generated text complicating NLP analysis",
        "Potential biases due to reliance on operational deficiencies for data collection",
        "Coordinating with hosting providers and takedown processes"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "First large-scale real-world analysis of phishing campaigns targeting Meta with data from genuine victims: “Analysing data from over 25,000 victims worldwide”.",
      "Dual technical-and-human perspective: infrastructure/TTPs plus NLP-based sentiment, emotion, and tone analysis of victims’ appeal texts.",
      "Password hygiene findings: “our analysis illustrates very poor password selection choices from the victims” and “almost six out of ten passwords have been published in password leaks at least two years ago.”",
      "Behavioral insight: “persistence in the re-victimisation of a significant number of users”.",
      "Correlation findings across demographics, timing, sentiment, emotion, and tone: “we reveal many correlations regarding demographics, timing, sentiment, emotion, and tone of the victims’ responses.”",
      "Campaign infrastructure characterization including widespread abuse of Google Firebase web.app, domain patterns (e.g., meta-business-appeal*.web.app), and exfiltration/storage via Telegram bot API and Firebase Storage.",
      "Attribution-relevant insights via OSINT and operational commonalities, identifying >6,500 campaign-related sites and a common modus operandi."
    ]
  },
  {
    "arxiv_id": "2310.20067v1",
    "title": "Vignat: Vulnerability identification by learning code semantics via graph attention networks",
    "authors": "Shuo Liu; Gail Kaiser",
    "abstract": "Vulnerability identification is crucial to protect software systems from attacks for cyber-security. However, huge projects have more than millions of lines of code, and the complex dependencies make it hard to carry out traditional static and dynamic methods. Furthermore, the semantic structure of various types of vulnerabilities differs greatly and may occur simultaneously, making general rule-based methods difficult to extend. In this paper, we propose \\textit{Vignat}, a novel attention-based framework for identifying vulnerabilities by learning graph-level semantic representations of code. We represent codes with code property graphs (CPGs) in fine grain and use graph attention networks (GATs) for vulnerability detection. The results show that Vignat is able to achieve $57.38\\%$ accuracy on reliable datasets derived from popular C libraries. Furthermore, the interpretability of our GATs provides valuable insights into vulnerability patterns.",
    "published_date": "2023-10-30",
    "pdf_link": "https://arxiv.org/pdf/2310.20067v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Vulnerability Detection",
      "specific_problem": "Learning-based source code vulnerability identification (function-level classification) using code property graphs",
      "attack_types": [
        "integer overflow",
        "divide-by-zero"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "GNN",
        "specific": "Graph Attention Network (GAT)",
        "novel_contribution": "Applies GAT to code property graphs (CPGs) for graph-level vulnerability classification; uses attention coefficients for interpretability of vulnerability patterns."
      },
      {
        "type": "primary",
        "category": "Graph Representation",
        "specific": "Code Property Graph (AST+CFG simplified)",
        "novel_contribution": "Constructs fine-grained CPGs from AST and CFG using Joern; averages token embeddings per node; simplifies heterogeneity and duplicate edges for tractable GNN input."
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "BERT",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "DistilBERT",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "RoBERTa",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "CodeBERT",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "GNN",
        "specific": "Graph Convolutional Network (GCN)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Word/Token Embedding",
        "specific": "RoBERTa/BERT/DistilBERT/CodeBERT/Word2Vec token embeddings averaged per CPG node",
        "novel_contribution": "Systematically evaluates multiple pretrained embeddings within the GAT-over-CPG framework."
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Devign",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Devign - FFmpeg subset (medium-length functions <1200 tokens)",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Devign - QEMU subset (medium-length functions <1200 tokens)",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "BERT (Transformer on natural code sequence)",
        "paper_reference": "Devlin et al., 2018",
        "metric": "Accuracy (FFmpeg)",
        "their_result": "63.93%",
        "baseline_result": "53.33%"
      },
      {
        "method_name": "RoBERTa (Transformer on natural code sequence)",
        "paper_reference": "Conneau et al., 2019",
        "metric": "Accuracy (FFmpeg)",
        "their_result": "63.93%",
        "baseline_result": "53.33%"
      },
      {
        "method_name": "CodeBERT (Transformer on natural code sequence)",
        "paper_reference": "Feng et al., 2020",
        "metric": "Accuracy (FFmpeg)",
        "their_result": "63.93%",
        "baseline_result": "51.67%"
      },
      {
        "method_name": "GCN on CPG (Word2Vec)",
        "paper_reference": "Kipf & Welling, 2016",
        "metric": "Accuracy (FFmpeg)",
        "their_result": "63.93%",
        "baseline_result": "55.74%"
      },
      {
        "method_name": "GCN on CPG (BERT embeddings)",
        "paper_reference": "Kipf & Welling, 2016",
        "metric": "Accuracy (FFmpeg)",
        "their_result": "60.66% (GAT on CPG, BERT embeddings)",
        "baseline_result": "57.38%"
      }
    ],
    "performance_metrics_used": [
      "accuracy",
      "precision",
      "recall",
      "F1"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How does Vignat compare to the transformers that are based on the NCS?",
        "How does Vignat perform on different kinds of code representations?",
        "How do different kinds of word embedding methods affect the performance of Vignat?",
        "How does attention-mechanism powered Vignat compare to GCNs?",
        "How to infer patterns of code vulnerabilities through attention coefficients?"
      ],
      "gaps_identified": [
        "Traditional dynamic methods suffer from limited code coverage, execution dependencies, and high overhead.",
        "Rule-based static analyzers rely on predefined rules and struggle with complex projects and extensibility.",
        "Transformer-based sequence models lack comprehensive understanding of program semantics due to structured nature of code.",
        "Real-world vulnerabilities are sparse, causing bias and generalization challenges."
      ],
      "limitations": [
        "Only AST and CFG were used; DFG was excluded due to complexity: \"As the DFG edges are labeled with the variables involved, it tremendously complicates embedded graphs. Therefore, we only extracted ASTs and CFGs.\"",
        "CPGs were simplified by disregarding heterogeneity of various relationships and eliminating duplicate edges; only connectivity passed to GAT.",
        "Graphs were truncated/padded to a maximum of 225 nodes per function.",
        "Evaluation focused on two projects (FFmpeg and QEMU) and medium-length functions (<1200 tokens).",
        "Reported accuracy remains modest (e.g., 57.38%–63.93% on FFmpeg depending on setting).",
        "No cross-project generalization or external validation beyond Devign subsets was reported.",
        "No runtime/inference latency on full pipelines was provided (only CPG generation time).",
        "No evaluation of robustness to code obfuscation or adversarial perturbations."
      ],
      "future_work": [],
      "motivation": "Improve static vulnerability identification by capturing rich code semantics and relationships via graph representations and leveraging attention for both performance and interpretability.",
      "potential_research_ideas": [
        "Integrate full heterogeneous CPGs including DFG and typed/relational edges using relational/heterogeneous GNNs to capture fine-grained data dependencies for vulnerability detection.",
        "Develop a graph-transformer with structural and positional encodings tailored to AST/CFG/DFG to better model long-range dependencies in code.",
        "Multi-task learning to jointly predict vulnerability presence and type/category (e.g., overflow, divide-by-zero), improving supervision and interpretability.",
        "Cross-project/domain generalization study with domain adaptation or invariant representation learning to mitigate project-specific bias and vulnerability sparsity.",
        "Self-supervised pretraining on large code graphs (e.g., contrastive or masked subgraph objectives) followed by fine-tuning on vulnerability labels.",
        "Active learning or curriculum learning to handle sparsity and class imbalance by prioritizing informative functions/subgraphs.",
        "Combine static analyzers (warnings) as weak labels with graph learning for semi-supervised training and noise-robust loss functions.",
        "Robustness analysis against code transformations/obfuscations and defenses (e.g., adversarial training on code graphs).",
        "Inter-procedural and cross-file graph construction to capture broader context beyond single functions."
      ],
      "architectural_improvement_recommendations": [
        "Use heterogeneous/relational GAT (e.g., R-GAT/HAN) to model edge types and attributes (AST, CFG, DFG) rather than collapsing them.",
        "Incorporate edge features and attention over edge types; learnable relation-specific parameters.",
        "Adopt hierarchical pooling/readout (e.g., DiffPool, SAGPool, hierarchical transformers) for graph-level classification.",
        "Augment node embeddings with structural encodings (depth in AST, dominance/post-dominance in CFG) and relative positional features.",
        "Employ graph transformers with global attention and sparsity patterns aligned to control/data dependencies.",
        "Leverage multi-view fusion of AST/CFG/DFG with gated cross-attention between views.",
        "Calibrate attention and add explanation regularizers to improve faithfulness of highlighted edges.",
        "Optimize preprocessing by incremental CPG construction and caching; support larger graphs with neighbor sampling/mini-batch graph training."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "PyTorch"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Training on a single 16GB Tesla V100-SXM2 GPU; PyTorch 2.0.0, CUDA 11.8; 100 epochs, batch size 8, learning rate 1e-4; node feature size 768; max 225 nodes per graph (truncate/pad). CPG generation ~0.0358 s/sample; training time ~116–127 minutes per setting."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": "CPG generation ~0.0358 s per function; inference latency not reported.",
      "deployment_challenges": [
        "Complex and large codebases produce large graphs that may exceed node limits and require truncation.",
        "Heterogeneous relationships in CPGs were simplified, potentially losing important semantics.",
        "Including DFG increases complexity considerably, complicating embedding and training.",
        "Real-world vulnerability sparsity may hinder generalization and require careful handling."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Obtain comprehensive representation of code functions by embedding them into code property graphs (CPGs) capturing syntactic structure, control flow, and data dependencies.",
      "Propose Vignat, a GAT-based framework to model complex relationships among code graph nodes, with attention weights used to reveal vulnerability patterns.",
      "Evaluate Vignat on manually labeled datasets from real-world C projects (Devign), showing up to ~10% accuracy and ~5% F1 improvements over baseline methods; demonstrate interpretability by highlighting salient edges indicative of vulnerabilities."
    ]
  },
  {
    "arxiv_id": "2310.12924v2",
    "title": "Digital Twin-Enabled Intelligent DDoS Detection Mechanism for Autonomous Core Networks",
    "authors": "Yagmur Yigit; Bahadir Bal; Aytac Karameseoglu; Trung Q. Duong; Berk Canberk",
    "abstract": "Existing distributed denial of service attack (DDoS) solutions cannot handle highly aggregated data rates; thus, they are unsuitable for Internet service provider (ISP) core networks. This article proposes a digital twin-enabled intelligent DDoS detection mechanism using an online learning method for autonomous systems. Our contributions are three-fold: we first design a DDoS detection architecture based on the digital twin for ISP core networks. We implemented a Yet Another Next Generation (YANG) model and an automated feature selection (AutoFS) module to handle core network data. We used an online learning approach to update the model instantly and efficiently, improve the learning model quickly, and ensure accurate predictions. Finally, we reveal that our proposed solution successfully detects DDoS attacks and updates the feature selection method and learning model with a true classification rate of ninety-seven percent. Our proposed solution can estimate the attack within approximately fifteen minutes after the DDoS attack starts.",
    "published_date": "2023-10-19",
    "pdf_link": "https://arxiv.org/pdf/2310.12924v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "DDoS detection in ISP core networks (router-level) using Digital Twin, YANG-driven feature ingestion, online learning, and AutoFS",
      "attack_types": [
        "DDoS"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "MLP (Feedforward Neural Network)",
        "specific": "5-layer MLP with ReLU (hidden) and Softmax (output), dropout; weights updated online",
        "novel_contribution": "Used as the online-updated classifier within a Digital Twin-driven ISP core network detection system"
      },
      {
        "type": "primary",
        "category": "Ensemble/Meta-learning",
        "specific": "Ensemble labeling combining K-Means and EM with a baseline dataset",
        "novel_contribution": "Proposed labeling algorithm to pseudo-label unlabeled streaming data using K-Means for EM initialization and ensemble of two EM runs with a designed baseline dataset"
      },
      {
        "type": "primary",
        "category": "Clustering",
        "specific": "K-Means (K=2)",
        "novel_contribution": "Used to initialize EM ranges to improve convergence and stability for labeling"
      },
      {
        "type": "primary",
        "category": "Probabilistic Model (EM/GMM)",
        "specific": "Expectation-Maximization",
        "novel_contribution": "Applied twice (with and without baseline dataset) and ensembled for probabilistic labeling of unlabeled data"
      },
      {
        "type": "primary",
        "category": "Feature Selection",
        "specific": "ANOVA F-value Selection",
        "novel_contribution": "Part of AutoFS to select top-10 features per router from YANG-ingested features"
      },
      {
        "type": "primary",
        "category": "Feature Selection",
        "specific": "Chi-square",
        "novel_contribution": "Part of AutoFS; suitable for best-effort traffic per authors"
      },
      {
        "type": "primary",
        "category": "Feature Selection",
        "specific": "Backward Feature Elimination (BFE)",
        "novel_contribution": "Part of AutoFS; suited for non-real-time traffic per authors"
      },
      {
        "type": "primary",
        "category": "Feature Selection",
        "specific": "Fisher Score",
        "novel_contribution": "Part of AutoFS; suited for non-real-time traffic per authors"
      },
      {
        "type": "primary",
        "category": "Feature Selection",
        "specific": "Recursive Feature Elimination (RFE)",
        "novel_contribution": "Part of AutoFS; suited for real-time traffic per authors"
      }
    ],
    "learning_paradigm": [
      "Online learning",
      "Semi-supervised (pseudo-labeling with EM/ensemble)",
      "Supervised (MLP classification after labeling)",
      "Unsupervised (K-Means/EM for labeling)"
    ],
    "datasets": [
      {
        "name": "CICDDoS2019",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "ToN_IoT",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Baseline Dataset for Labeling Algorithm",
        "type": "synthetic",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Industry DDoS Solutions 1–4 (anonymized)",
        "paper_reference": null,
        "metric": "Detection latency (time-to-detect after attack start)",
        "their_result": "≈15 minutes after DDoS attack starts",
        "baseline_result": "≈100 minutes after DDoS attack starts"
      }
    ],
    "performance_metrics_used": [
      "true classification rate",
      "precision",
      "recall (sensitivity)",
      "F-measure (F1-score)",
      "detection latency/time-to-detect"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How can we detect DDoS attacks in ISP core networks that handle highly aggregated data rates?",
        "Can a Digital Twin with YANG-based feature ingestion and online learning provide timely and accurate router-level DDoS detection?",
        "How to automatically select and adapt the best feature subset and model online for non-stationary core network traffic?"
      ],
      "gaps_identified": [
        "Existing DDoS solutions target data centers/edges and are unsuitable for ISP core networks with multi-400 Gbps links.",
        "High detection latency (~100 minutes) and insufficient detection rates in deployed industry solutions for core networks.",
        "Most ML-based DDoS detection uses offline learning; may not achieve high accuracy on real, evolving core network traffic.",
        "Use of Digital Twins for network anomaly detection is underexplored.",
        "No hardware-based solutions for core network-wide DDoS detection; routers avoid heavy per-packet operations."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "To enable autonomous core networks with timely and accurate DDoS detection where existing edge/data-center solutions and offline ML are inadequate for high-rate ISP core environments.",
      "potential_research_ideas": [
        "Integrate concept-drift detection (e.g., ADWIN, DDM) to trigger model/feature updates more robustly than static thresholds.",
        "Extend DT framework to closed-loop mitigation (automated NETCONF/YANG actions like rate-limiting, blackholing, or traffic engineering).",
        "Evaluate temporal architectures (Temporal CNNs, LSTMs, Transformers) for improved early detection versus feedforward MLP.",
        "Apply federated or split learning across routers/POPs to preserve privacy and scale training without centralizing raw telemetry.",
        "Enhance pseudo-labeling with confidence calibration and teacher–student self-training to reduce label noise.",
        "Incorporate cost-sensitive learning or focal loss to counter severe class imbalance observed in D1/D2.",
        "Add explainability (e.g., SHAP on selected features) for operator trust and faster incident response.",
        "Robustness evaluation under adversarial manipulation of telemetry and poisoning of online updates."
      ],
      "architectural_improvement_recommendations": [
        "Replace or complement MLP with streaming learners (Hoeffding Trees/Adaptive Random Forest) and drift detectors for true online adaptation.",
        "Use a multi-objective controller to jointly optimize recall and detection latency with SLA-aware thresholds.",
        "Adopt a hybrid FS: start with statistical FS (ANOVA/Chi2) then refine with embedded FS (L1/Lasso, tree-based importance) online.",
        "Leverage sequence features (flow inter-arrival, burstiness) and sketch-based telemetry (Count-Min Sketch) to reduce overhead.",
        "Implement per-router lightweight inference with gRPC/REST and central coordination via DT knowledge graph for scalability.",
        "Introduce online calibration (Platt/temperature scaling) to maintain probability quality as data drifts."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "Microsoft Azure Digital Twins (ADT)",
        "Digital Twins Definition Language (DTDL)",
        "NETCONF/YANG"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Per authors, worst-case classification runtime has an upper bound depending on packet arrival rate and number of core routers; detection achievable ≈15 minutes after attack start. No GPU/CPU specs reported."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "ISP core network routers (simulated and orchestrated via Azure Digital Twins); microservice-based brain; router-by-router detection",
      "scalability_discussed": true,
      "inference_time": "≈15 minutes time-to-detect after attack begins",
      "deployment_challenges": [
        "High bandwidth core links (multiple 400 Gbps interfaces) make real-time processing challenging",
        "Routers designed to forward packets, not perform heavy analytics",
        "Massive, multidimensional telemetry necessitates selective feature ingestion (YANG Paths/KPIs)",
        "Severe class imbalance in realistic datasets",
        "Need for online updates and stable performance under evolving traffic"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposed a Digital Twin-enabled intelligent DDoS detection architecture for ISP core networks aligned with IETF DTN concepts",
      "Implemented YANG models with two KPIs (92 sensors total) to ingest only required router features",
      "Designed an AutoFS module to automatically select the best feature selection method and top-10 features per router",
      "Adopted online learning to update the MLP model when performance drops, maintaining stable detection",
      "Proposed a labeling algorithm combining K-Means and EM with a tailored baseline dataset to pseudo-label unlabeled data",
      "Demonstrated detection with reported true classification rate ≈97% and detection time ≈15 minutes after attack start on CICDDoS2019 and ToN_IoT datasets",
      "Provided a microservice-based implementation integrated with Microsoft Azure Digital Twins for synchronized DT–network–brain operation"
    ]
  },
  {
    "arxiv_id": "2310.02655v1",
    "title": "AGIR: Automating Cyber Threat Intelligence Reporting with Natural Language Generation",
    "authors": "Filippo Perrina; Francesco Marchiori; Mauro Conti; Nino Vincenzo Verde",
    "abstract": "Cyber Threat Intelligence (CTI) reporting is pivotal in contemporary risk management strategies. As the volume of CTI reports continues to surge, the demand for automated tools to streamline report generation becomes increasingly apparent. While Natural Language Processing techniques have shown potential in handling text data, they often struggle to address the complexity of diverse data sources and their intricate interrelationships. Moreover, established paradigms like STIX have emerged as de facto standards within the CTI community, emphasizing the formal categorization of entities and relations to facilitate consistent data sharing. In this paper, we introduce AGIR (Automatic Generation of Intelligence Reports), a transformative Natural Language Generation tool specifically designed to address the pressing challenges in the realm of CTI reporting. AGIR's primary objective is to empower security analysts by automating the labor-intensive task of generating comprehensive intelligence reports from formal representations of entity graphs. AGIR utilizes a two-stage pipeline by combining the advantages of template-based approaches and the capabilities of Large Language Models such as ChatGPT. We evaluate AGIR's report generation capabilities both quantitatively and qualitatively. The generated reports accurately convey information expressed through formal language, achieving a high recall value (0.99) without introducing hallucination. Furthermore, we compare the fluency and utility of the reports with state-of-the-art approaches, showing how AGIR achieves higher scores in terms of Syntactic Log-Odds Ratio (SLOR) and through questionnaires. By using our tool, we estimate that the report writing time is reduced by more than 40%, therefore streamlining the CTI production of any organization and contributing to the automation of several CTI tasks.",
    "published_date": "2023-10-04",
    "pdf_link": "https://arxiv.org/pdf/2310.02655v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Threat Intelligence",
      "subdomain": "CTI Reporting Automation",
      "specific_problem": "Data-to-text generation of Cyber Threat Intelligence (CTI) reports from STIX graphs",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Template-based NLG",
        "specific": null,
        "novel_contribution": "First-stage template-based content selection and realization from STIX graphs tailored to four CTI report types (Overview, Subject, Timeline, Vulnerability)"
      },
      {
        "type": "primary",
        "category": "Transformer (LLM)",
        "specific": "ChatGPT (OpenAI)",
        "novel_contribution": "Second-stage LLM rewriting to enhance fluency and perceived utility while preserving the factual content extracted from STIX"
      },
      {
        "type": "baseline",
        "category": "Rule-based NLG",
        "specific": "Narrator",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Template-based",
      "Prompt-based (zero-shot) generation"
    ],
    "datasets": [
      {
        "name": "Internal Knowledge Base of STIX graphs",
        "type": "private",
        "domain": "threat_intel_graphs",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "MITRE ATT&CK CTI (STIX/TAXII)",
        "type": "public",
        "domain": "threat_intel_graphs",
        "link": "https://github.com/mitre/cti",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "AGIR Sample Generated Reports",
        "type": "public",
        "domain": "cti_reports",
        "link": "https://github.com/Mhackiori/AGIR",
        "is_new_contribution": true,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Narrator",
        "paper_reference": "S. Polzunov and J. Abraham, Narrator: generating CTI reports from STIX JSON",
        "metric": "SLOR; human questionnaires (fluency/utility)",
        "their_result": "“AGIR achieves higher scores in terms of Syntactic Log-Odds Ratio (SLOR) and through questionnaires.”",
        "baseline_result": null
      },
      {
        "method_name": "State-of-the-art approaches (unspecified)",
        "paper_reference": null,
        "metric": "SLOR; human questionnaires",
        "their_result": "“AGIR achieves higher scores in terms of Syntactic Log-Odds Ratio (SLOR) and through questionnaires.”",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Recall (information coverage of STIX facts in generated reports)",
      "Hallucination presence (qualitative; none reported)",
      "SLOR (Syntactic Log-Odds Ratio) for fluency",
      "Human questionnaires (fluency and utility)",
      "Time reduction for analysts (%)"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "NLG for CTI report generation is underexplored compared to NLP information extraction in CTI.",
        "Neural NLG approaches lack control and can introduce inaccuracies/hallucinations.",
        "CTI data involves diverse sources and complex interrelationships that standard NLP struggles to handle.",
        "Need to adhere to CTI standards like STIX for consistent, shareable representations."
      ],
      "limitations": [
        "Dependence on ChatGPT API introduces cost and vendor lock-in.",
        "Lack of control typical of neural-based NLG models.",
        "Currently supports only four report types/templates."
      ],
      "future_work": [
        "Extend the pipeline with additional report formats/templates.",
        "Integrate with upstream CTI IE systems (e.g., STIXnet) to approach fully automated CTI pipelines.",
        "Leverage a Knowledge Base more extensively for dynamic, user-driven report scoping."
      ],
      "motivation": "Reduce the time and effort required by analysts to author CTI reports by generating high-quality natural language from structured STIX graphs while preserving factual accuracy and adhering to CTI standards.",
      "potential_research_ideas": [
        "Create a public benchmark of STIX graphs paired with reference CTI reports to standardize evaluation of CTI NLG (fluency, faithfulness, utility).",
        "Study constrained or structured decoding methods to guarantee faithfulness (e.g., knowledge-grounded generation that enforces STIX edge/entity constraints).",
        "Develop retrieval-augmented generation over a CTI Knowledge Base to improve coverage and reduce hallucinations.",
        "Fine-tune open-source LLMs on CTI style guides and reports for on-prem, privacy-preserving deployment.",
        "Add an edit-feedback loop to learn from analyst post-edits (online learning/active learning) to improve templates and prompts.",
        "Investigate multilingual CTI report generation to support international sharing.",
        "Design automatic factuality checkers that align generated sentences back to STIX triples for verifiable reporting.",
        "Robustness studies against poisoned or low-quality STIX inputs; defenses for adversarial manipulation of structured inputs."
      ],
      "architectural_improvement_recommendations": [
        "Introduce a content planning module that first linearizes STIX graphs into plans before surface realization.",
        "Use constrained decoding or semantic validators to block unsupported facts and enforce one-to-one mapping to STIX entities/relations.",
        "Replace black-box API with an on-prem fine-tuned LLM and/or smaller controllable NLG models for sensitive environments.",
        "Adopt retrieval-augmented generation over the CTI KB with citation insertion to each generated paragraph.",
        "Implement a tracing layer that annotates each sentence with provenance (entity/edge IDs) to improve explainability and verification.",
        "Parameterize templates for easier maintenance, and auto-generate templates from schema definitions (STIX 2.1)."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": "https://github.com/Mhackiori/AGIR",
      "frameworks": [
        "OpenAI ChatGPT API"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "No training required; generation via API. Report generation cost reported as ~$0.0024 per report; minimal compute if using hosted LLM."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "API cost considerations per report",
        "Lack of control/faithfulness risks in neural generation",
        "Data privacy concerns when sending CTI to third-party LLMs",
        "Template maintenance and extensibility",
        "Integration with Knowledge Base and STIX APIs"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces AGIR, an NLG tool to automate CTI report creation from structured STIX data.",
      "Proposes a two-stage pipeline combining template-based generation with LLM rewriting for fluency.",
      "Design emphasizes scalability and extensibility to include multiple report types (Overview, Subject, Timeline, Vulnerability).",
      "Quantitative and qualitative evaluation showing high recall (0.99) with no hallucinations reported, superior SLOR, and higher utility scores than state-of-the-art approaches.",
      "Estimates 42.6% reduction in report writing time based on user questionnaires.",
      "Releases several sample generated reports (GitHub: https://github.com/Mhackiori/AGIR)."
    ]
  },
  {
    "arxiv_id": "2309.05646v1",
    "title": "A Novel Supervised Deep Learning Solution to Detect Distributed Denial of Service (DDoS) attacks on Edge Systems using Convolutional Neural Networks (CNN)",
    "authors": "Vedanth Ramanathan; Krish Mahadevan; Sejal Dua",
    "abstract": "Cybersecurity attacks are becoming increasingly sophisticated and pose a growing threat to individuals, and private and public sectors. Distributed Denial of Service attacks are one of the most harmful of these threats in today's internet, disrupting the availability of essential services. This project presents a novel deep learning-based approach for detecting DDoS attacks in network traffic using the industry-recognized DDoS evaluation dataset from the University of New Brunswick, which contains packet captures from real-time DDoS attacks, creating a broader and more applicable model for the real world. The algorithm employed in this study exploits the properties of Convolutional Neural Networks (CNN) and common deep learning algorithms to build a novel mitigation technique that classifies benign and malicious traffic. The proposed model preprocesses the data by extracting packet flows and normalizing them to a fixed length which is fed into a custom architecture containing layers regulating node dropout, normalization, and a sigmoid activation function to out a binary classification. This allows for the model to process the flows effectively and look for the nodes that contribute to DDoS attacks while dropping the \"noise\" or the distractors. The results of this study demonstrate the effectiveness of the proposed algorithm in detecting DDOS attacks, achieving an accuracy of .9883 on 2000 unseen flows in network traffic, while being scalable for any network environment.",
    "published_date": "2023-09-11",
    "pdf_link": "https://arxiv.org/pdf/2309.05646v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Detection of Distributed Denial of Service (DDoS) attacks from network traffic on edge systems using CNN-based flow classification",
      "attack_types": [
        "DDoS",
        "SYN flood",
        "UDP flood",
        "HTTP flood"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "CNN",
        "specific": "Custom 2D CNN with 64 3x3 filters, dropout (0.5) with ReLU, GlobalMaxPooling2D, fully connected sigmoid output",
        "novel_contribution": "Lightweight CNN tailored for fixed-length flow representations generated by a dataset-agnostic PCAP-to-flow preprocessing algorithm for binary DDoS detection on edge devices"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Binary classification"
    ],
    "datasets": [
      {
        "name": "CIC-DDoS2019 (UNB Canadian Institute for Cybersecurity DDoS Evaluation Dataset)",
        "type": "public",
        "domain": "network_traffic",
        "link": "https://www.unb.ca/cic/datasets/ddos-2019.html",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "accuracy"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can a supervised CNN operating on fixed-length flow representations detect DDoS attacks in network traffic with high accuracy while remaining suitable for edge systems?",
        "Can a dataset-agnostic PCAP-to-flow preprocessing pipeline enable scalable and reliable DDoS detection across different network environments?"
      ],
      "gaps_identified": [
        "Legacy DDoS detection has limited adaptability to evolving attack patterns, high resource consumption, reliance on signatures, and poor scalability.",
        "Prior DL-based approaches reported weak performance on comprehensive datasets like CIC-DDoS2019 or considered only a narrow set of attack types; some reshaping/padding choices may distort learning.",
        "Challenge of achieving efficient detection, effective response, acceptable false alarm rate, and real-time processing simultaneously in commonplace networks."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Address limitations of traditional DDoS detection and underutilization of CNNs in cybersecurity by developing a scalable, flexible, reliable, and predictable edge-suitable detector using real-world PCAP data.",
      "potential_research_ideas": [
        "Extend from binary to multi-class detection to recognize specific DDoS attack types (e.g., SYN, UDP, HTTP, amplification variants).",
        "Incorporate temporal sequence modeling (1D CNNs, LSTMs, Temporal Convolutional Networks, or Transformers) over packet/flow time series to capture low-rate and stealthy attacks.",
        "Cross-dataset generalization study across CIC-IDS2017, BoT-IoT, MAWI/CAIDA traces to assess robustness and reduce dataset bias.",
        "Online/streaming deployment pipeline with concept-drift detection and adaptive re-training on edge.",
        "Adversarial robustness evaluation against traffic perturbations and mimicry; explore adversarial training or randomized smoothing for network data.",
        "Explainability for operators (e.g., saliency over packets/features, integrated gradients) to support incident response and policy tuning.",
        "Privacy-preserving training (federated learning across edge sites) while maintaining detection accuracy.",
        "Automated hyperparameter optimization of time window t and max packets m for different environments; policy-driven dynamic windowing."
      ],
      "architectural_improvement_recommendations": [
        "Switch to 1D convolutions over packet sequences (features as channels) to better match data topology and reduce compute.",
        "Add residual or depthwise-separable convolutions and batch normalization to improve accuracy-latency trade-offs on edge.",
        "Introduce attention mechanisms (self-attention or channel/spatial attention) after convolutional blocks to focus on discriminative packets/features.",
        "Use focal loss or class-weighting with calibrated decision thresholds (ROC/PR-based) to handle class imbalance and optimize false positives.",
        "Perform model compression (pruning, quantization-aware training) and measure end-to-end latency on representative edge hardware.",
        "Joint learning on multi-window inputs (short- and long-duration flows) to capture both volumetric and low-rate attacks.",
        "Systematically tune normalization schemes and padding strategies; explore learned positional encodings for packet order."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Not specified; authors emphasize suitability for resource-constrained edge devices."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Edge systems across private and public networks (claimed suitability); no deployment evaluation reported.",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Dataset-agnostic PCAP-to-flow preprocessing algorithm (Algorithm 1) that groups packets by time windows and flow identifiers, normalizes features to [0,1], and zero-pads to a fixed length for CNN input with class balancing.",
      "Custom lightweight 2D CNN architecture (64 3x3 conv, dropout 0.5 with ReLU, GlobalMaxPooling2D, dense sigmoid) for binary DDoS vs. benign traffic classification suitable for edge devices.",
      "Use of the real-world CIC-DDoS2019 PCAP dataset with multiple attack types (SYN, UDP, HTTP floods).",
      "Reported result: \"achieving an accuracy of .9883 on 2000 unseen flows in network traffic\".",
      "Emphasis on scalability, flexibility, reliability, and predictability for DDoS detection in edge environments."
    ]
  },
  {
    "arxiv_id": "2310.14429v1",
    "title": "Text generation for dataset augmentation in security classification tasks",
    "authors": "Alexander P. Welsh; Matthew Edwards",
    "abstract": "Security classifiers, designed to detect malicious content in computer systems and communications, can underperform when provided with insufficient training data. In the security domain, it is often easy to find samples of the negative (benign) class, and challenging to find enough samples of the positive (malicious) class to train an effective classifier. This study evaluates the application of natural language text generators to fill this data gap in multiple security-related text classification tasks. We describe a variety of previously-unexamined language-model fine-tuning approaches for this purpose and consider in particular the impact of disproportionate class-imbalances in the training set. Across our evaluation using three state-of-the-art classifiers designed for offensive language detection, review fraud detection, and SMS spam detection, we find that models trained with GPT-3 data augmentation strategies outperform both models trained without augmentation and models trained using basic data augmentation strategies already in common usage. In particular, we find substantial benefits for GPT-3 data augmentation strategies in situations with severe limitations on known positive-class samples.",
    "published_date": "2023-10-22",
    "pdf_link": "https://arxiv.org/pdf/2310.14429v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Cybercrime and Online Abuse Detection",
      "subdomain": "Content and Communication Abuse Detection",
      "specific_problem": "Improving text-based security classifiers via LLM-generated data augmentation under severe positive-class scarcity and class imbalance (offensive language, deceptive reviews, SMS spam)",
      "attack_types": [
        "offensive/abusive language",
        "opinion spam (fake reviews)",
        "SMS spam"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer (LLM)",
        "specific": "GPT-3 (Curie) fine-tuned for text generation",
        "novel_contribution": "Previously-unexamined fine-tuning strategies for augmentation under disproportionate positive-class scarcity (disproportionate, proportionate, and positive-only fine-tuning); evaluation of impact across security tasks"
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "BERT-based classifier (Dai et al.)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN+RNN (BiLSTM with attention)",
        "specific": "Combined CNN-LSTM with doc2vec and TF-IDF (Salunkhe)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "RNN (LSTM)",
        "specific": "LSTM with GloVe embeddings (Chandra & Khatri)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Lexical augmentation",
        "specific": "Synonym replacement (WordNet)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Lexical augmentation",
        "specific": "Random word insertion",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Model-guided augmentation",
        "specific": "BERT-guided word insertion",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Word Embeddings",
        "specific": "GloVe (glove.6B) embeddings",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Self-supervised (language modeling)",
      "Transfer Learning"
    ],
    "datasets": [
      {
        "name": "Offensive Language Identification Dataset (OLID)",
        "type": "public",
        "domain": "social_media_posts",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Deceptive Opinion Spam (Hotel Reviews) by Ott et al. (2011, 2013)",
        "type": "public",
        "domain": "reviews",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "SMS Spam Collection (Almeida et al.)",
        "type": "public",
        "domain": "sms_messages",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "GloVe (glove.6B) word vectors",
        "type": "public",
        "domain": "embeddings",
        "link": "https://nlp.stanford.edu/projects/glove/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "WordNet (for synonym replacement)",
        "type": "public",
        "domain": "lexical_database",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Unaugmented training (proportionate cut, prop)",
        "paper_reference": null,
        "metric": "Average percentage gap to best F1 (OLID Task 1)",
        "their_result": "“gen1 0.74%; gen2 1.88%; gen3 1.48%” (average gap to best)",
        "baseline_result": "“prop 2.53%” (average gap to best)"
      },
      {
        "method_name": "Unaugmented training (disproportionate cut, disp)",
        "paper_reference": null,
        "metric": "Average percentage gap to best F1 (OLID Task 1)",
        "their_result": "“gen1 0.74%; gen2 1.88%; gen3 1.48%” (average gap to best)",
        "baseline_result": "“disp 3.9%” (average gap to best)"
      },
      {
        "method_name": "Basic data augmentation: synonym replacement (WordNet)",
        "paper_reference": null,
        "metric": "Average percentage gap to best F1 (OLID Task 1)",
        "their_result": "“gen1 0.74%; gen2 1.88%; gen3 1.48%” (average gap to best)",
        "baseline_result": "“bda1 13.76%” (average gap to best)"
      },
      {
        "method_name": "Basic data augmentation: random word insertion",
        "paper_reference": null,
        "metric": "Average percentage gap to best F1 (OLID Task 1)",
        "their_result": "“gen1 0.74%; gen2 1.88%; gen3 1.48%” (average gap to best)",
        "baseline_result": "“bda2 14.16%” (average gap to best)"
      },
      {
        "method_name": "Basic data augmentation: BERT-guided word insertion",
        "paper_reference": null,
        "metric": "Average percentage gap to best F1 (OLID Task 1)",
        "their_result": "“gen1 0.74%; gen2 1.88%; gen3 1.48%” (average gap to best)",
        "baseline_result": "“bda3 12.47%” (average gap to best)"
      }
    ],
    "performance_metrics_used": [
      "F1 score",
      "Average percentage gap to best F1"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can modern text generation with a fine-tuned LLM improve security classifiers by augmenting scarce positive-class training data?",
        "How does severe class imbalance (disproportionate scarcity of positives) affect classifier performance, and can LLM-generated data mitigate it?",
        "Which fine-tuning strategies for the generator (disproportionate, proportionate, positive-only) are most effective under different data regimes?"
      ],
      "gaps_identified": [
        "“prior research has not considered elements key to the context of security-focused tasks. Data-limited scenarios have been examined, but not imbalanced and adversarial datasets, and the fine-tuning process required has not been rigorously explored.”",
        "Limitations of basic text augmentation methods that either change samples too little or introduce distortions"
      ],
      "limitations": [
        "“with a minor exception, models trained with such data augmentation outperform…” (indicates at least one case where augmentation did not outperform)",
        "“We find mixed results pointing to possible tradeoffs between strategies depending on classifier selection and the quantity of data available for fine-tuning.”"
      ],
      "future_work": [],
      "motivation": "Address positive-class scarcity in security classification tasks by using modern LLM text generation for data augmentation, especially under severe class imbalance.",
      "potential_research_ideas": [
        "Evaluate open-source and larger LLMs (e.g., GPT-3.5/4-class, Llama, Mistral) for augmentation, comparing cost/quality tradeoffs and robustness across security tasks.",
        "Integrate automatic filtering of generated samples (e.g., classifier confidence, dual-model agreement, perplexity thresholds) and compare to unfiltered augmentation.",
        "Adversarially-informed generation: condition the generator to produce hard positives and near-miss negatives to improve decision boundaries.",
        "Active learning loop: iteratively fine-tune the generator and classifier using misclassified/uncertain real samples to guide generation.",
        "Domain-specific prompting templates and control codes to better capture subtypes (e.g., targeted vs. untargeted abuse; phishing vs. promotional spam).",
        "Calibration study: assess when positive-only fine-tuning vs. proportionate/disproportionate fine-tuning is preferable under varying label scarcity.",
        "Evaluate augmentation under distribution shift (new platforms/languages) and multilingual extension for global security scenarios."
      ],
      "architectural_improvement_recommendations": [
        "Use a discriminator-in-the-loop to filter generator outputs (GAN-style or rejection sampling with a strong classifier).",
        "Adopt reward modeling or reinforcement learning to optimize generation toward classifier improvement (e.g., RLHF-like objectives tied to validation F1).",
        "Leverage conditional generation with structured prompts including class subtype tags and style exemplars to increase label fidelity.",
        "Combine embedding-space augmentation (mixup/manifold mixup on text embeddings) with LLM-generated samples for diversity.",
        "Contrastive training by pairing generated positives with synthesized hard negatives for more robust boundaries."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Scarcity of labeled positive (malicious) data relative to negatives in real deployments",
        "Severe class imbalance common in security text classification tasks"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "“We evaluate modern text generation as a method of dataset augmentation for malicious content classification tasks. We find that, with a minor exception, models trained with such data augmentation outperform both models lacking augmentation and models using more widely-used basic data augmentation strategies.”",
      "“We provide what we believe is the first investigation into fine-tuning text generators in the context of disproportionately data-limited datasets… We find that modern text generators can be especially effective for data augmentation in cases where positive-class samples are least available.”",
      "“We assess different methods of fine-tuning language models for text generation as a method of dataset augmentation. We find mixed results pointing to possible tradeoffs between strategies depending on classifier selection and the quantity of data available for fine-tuning.”"
    ]
  },
  {
    "arxiv_id": "2309.09649v1",
    "title": "VULNERLIZER: Cross-analysis Between Vulnerabilities and Software Libraries",
    "authors": "Irdin Pekaric; Michael Felderer; Philipp Steinmüller",
    "abstract": "The identification of vulnerabilities is a continuous challenge in software projects. This is due to the evolution of methods that attackers employ as well as the constant updates to the software, which reveal additional issues. As a result, new and innovative approaches for the identification of vulnerable software are needed. In this paper, we present VULNERLIZER, which is a novel framework for cross-analysis between vulnerabilities and software libraries. It uses CVE and software library data together with clustering algorithms to generate links between vulnerabilities and libraries. In addition, the training of the model is conducted in order to reevaluate the generated associations. This is achieved by updating the assigned weights. Finally, the approach is then evaluated by making the predictions using the CVE data from the test set. The results show that the VULNERLIZER has a great potential in being able to predict future vulnerable libraries based on an initial input CVE entry or a software library. The trained model reaches a prediction accuracy of 75% or higher.",
    "published_date": "2023-09-18",
    "pdf_link": "https://arxiv.org/pdf/2309.09649v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Vulnerability Management and Prediction",
      "specific_problem": "Predicting vulnerable software libraries and linking CVE entries to likely affected libraries via cross-analysis of vulnerability databases and software library co-occurrence in Docker images",
      "attack_types": [
        "SQL injection",
        "Denial of Service",
        "Cross-Site Scripting",
        "Remote code execution via URL parameters",
        "Sensitive information exposure",
        "Directory traversal",
        "Buffer overflow",
        "Vendor/product-specific classes (e.g., Adobe software, Microsoft servers)",
        "Other (mixed/unspecified CVE categories)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Graph-based link prediction",
        "specific": null,
        "novel_contribution": "Library Cohesion Graph with per-edge learnable weights over engineered scores (CVE co-occurrence, co-vulnerability on same machine, cluster overlap, cluster match, temporal score), combined via a sigmoid function and trained with gradient descent/backpropagation"
      },
      {
        "type": "primary",
        "category": "Clustering",
        "specific": "K-means",
        "novel_contribution": "Used to derive vulnerability clusters that inform feature engineering (cluster overlap between libraries) for the cohesion graph"
      },
      {
        "type": "primary",
        "category": "Clustering",
        "specific": "DBSCAN",
        "novel_contribution": "Complementary density-based clustering to capture varied cluster shapes/sizes for vulnerability grouping"
      },
      {
        "type": "primary",
        "category": "Topic modeling",
        "specific": "Non-negative Matrix Factorization (NMF)",
        "novel_contribution": "Extracts CVE description topics (10-topic model) used as additional features (cluster assignments and cluster match) for the graph model"
      },
      {
        "type": "primary",
        "category": "Dimensionality reduction",
        "specific": "PCA",
        "novel_contribution": "Reduces feature dimensionality before clustering to mitigate overfitting"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "National Vulnerability Database (NVD) CVE data",
        "type": "public",
        "domain": "vulnerability_database",
        "link": "https://nvd.nist.gov",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Vulners vulnerability data",
        "type": "public",
        "domain": "vulnerability_database",
        "link": "https://vulners.com",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "DockerHub images (tags: 'vulnerable' and 'debian') for Debian-based package inventories",
        "type": "public",
        "domain": "docker_images",
        "link": "https://hub.docker.com",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "accuracy"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Existing SVP approaches often rely on historical source code data and many vendors do not disclose source code, complicating dataset construction.",
        "Prior matching/analysis approaches did not consider Docker containers/images as a data source for software library information.",
        "Evolving attacker methods and frequent software updates make vulnerability identification difficult using traditional techniques."
      ],
      "limitations": [
        "Packet manager used (dpkg) is only applicable to Debian-based systems; other systems were not considered.",
        "It was not possible to add more Docker Images due to large image processing time.",
        "Focus on public Docker images may bias towards certain software/library ecosystems."
      ],
      "future_work": [],
      "motivation": "Rising numbers of vulnerabilities and challenges in obtaining source-code-based datasets motivate leveraging publicly available Docker images and combining them with CVE data to predict vulnerable libraries.",
      "potential_research_ideas": [
        "Extend beyond Debian to multiple package ecosystems (RPM, APK, npm, PyPI, Maven) and unify package–CVE linking across OS and language ecosystems.",
        "Time-aware modeling with temporal point processes or survival analysis to better capture vulnerability occurrence dynamics per library.",
        "Leverage SBOMs (SPDX/CycloneDX) and OSV advisories to improve precision of library–CVE matching and version constraints.",
        "Use a graph neural network (GNN) over the library cohesion graph to learn higher-order co-vulnerability patterns and generalize to sparse libraries.",
        "Incorporate semantic embeddings from pretrained language models over CVE descriptions and vendor advisories to augment topic features.",
        "Establish stronger evaluation: time-based splits to avoid leakage, ROC-AUC/PR-AUC, calibration metrics, and ablations over each score component.",
        "Active learning to selectively process additional container images that maximize coverage of uncertain libraries.",
        "Online/continual learning to update edge weights as new CVEs and images arrive, with drift detection."
      ],
      "architectural_improvement_recommendations": [
        "Replace per-edge logistic weighting with a learnable GNN architecture (e.g., GraphSAGE/GCN) that ingests engineered features and learns message passing across the library graph.",
        "Introduce negative sampling and a pairwise ranking or cross-entropy loss for link prediction; report calibrated probabilities with temperature scaling.",
        "Adopt a strict temporal validation (train on CVEs up to time T, test on T+Δ) to avoid future leakage and better reflect deployment.",
        "Integrate OSV and vendor feeds for normalized package naming/version ranges; add a resolver to map distro package names to upstream libraries.",
        "Parallelize container analysis using container-native tooling (e.g., trivy/grype, oras, BuildKit) and caching to scale image processing."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "AWS EC2 used to analyze 1250 Docker images; MariaDB for storage; clustering/topic modeling over ~122k CVEs; no GPU requirements mentioned; image processing time was a bottleneck."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Large image processing time limited dataset size (“it was not possible to add more DIs to the set due to the large image processing time”).",
        "Debian-only package manager (dpkg) restricts applicability; other OS/package ecosystems not covered.",
        "Maintaining up-to-date CVE feeds and consistent merging between NVD and Vulners.",
        "Potential noise in matching package names/versions from images to CVE entries."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces VULNERLIZER, a framework for cross-analysis between vulnerabilities and software libraries using CVE data and Docker image–extracted library data.",
      "Builds a Library Cohesion Graph with engineered edge/node features (CVE co-occurrence, co-vulnerability on machines, topic/cluster overlap, temporal behavior) and trains per-edge weights via backpropagation with a sigmoid activation to predict vulnerable libraries.",
      "Applies unsupervised techniques (K-means, DBSCAN, NMF) to derive vulnerability clusters/topics that enhance predictive features.",
      "Implements an end-to-end data pipeline: CVE Downloader (NVD and Vulners via API), Docker Image Analyzer (dpkg-based library extraction on AWS EC2), and Cohesion Graph Generator.",
      "Empirical results: “The trained model reaches a prediction accuracy of 75% or higher.”",
      "Constructs and describes datasets comprising 122,364 CVEs related to 48,833 software libraries with an 80/20 train/test split, and analyzes 1,250 Docker images from DockerHub (tags 'vulnerable' and 'debian')."
    ]
  },
  {
    "arxiv_id": "2310.10670v1",
    "title": "Smart OMVI: Obfuscated Malware Variant Identification using a novel dataset",
    "authors": "Suleman Qamar",
    "abstract": "Cybersecurity has become a significant issue in the digital era as a result of the growth in everyday computer use. Cybercriminals now engage in more than virus distribution and computer hacking. Cyberwarfare has developed as a result because it has become a threat to a nation's survival. Malware analysis serves as the first line of defence against an attack and is a significant component of cybercrime. Every day, malware attacks target a large number of computer users, businesses, and governmental agencies, causing billions of dollars in losses. Malware may evade multiple AV software with a very minor, cunning tweak made by its designers, despite the fact that security experts have a variety of tools at their disposal to identify it. To address this challenge, a new dataset called the Obfuscated Malware Dataset (OMD) has been developed. This dataset comprises 40 distinct malware families having 21924 samples, and it incorporates obfuscation techniques that mimic the strategies employed by malware creators to make their malware variations different from the original samples. The purpose of this dataset is to provide a more realistic and representative environment for evaluating the effectiveness of malware analysis techniques. Different conventional machine learning algorithms including but not limited to Support Vector Machine (SVM), Random Forrest (RF), Extreme Gradient Boosting (XGBOOST) etc are applied and contrasted. The results demonstrated that XGBoost outperformed the other algorithms, achieving an accuracy of f 82%, precision of 88%, recall of 80%, and an F1-Score of 83%.",
    "published_date": "2023-09-24",
    "pdf_link": "https://arxiv.org/pdf/2310.10670v1",
    "paper_types": [
      "new_dataset",
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Malware Analysis",
      "subdomain": "Malware Family Classification",
      "specific_problem": "Obfuscated/polymorphic malware variant identification and family classification from grayscale-image representations",
      "attack_types": [
        "Trojan",
        "Worm",
        "Backdoor",
        "Adware",
        "Dialer",
        "TrojanDownloader",
        "Rogue",
        "PWS (Password Stealer)",
        "Obfuscated/Polymorphic Malware"
      ]
    },
    "ml_techniques": [
      {
        "type": "baseline",
        "category": "Gradient Boosting",
        "specific": "XGBoost",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Random Forest",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "SVM",
        "specific": "Support Vector Machine",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Obfuscated Malware Dataset (OMD)",
        "type": "synthetic",
        "domain": "malware_images",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "Malimg",
        "type": "public",
        "domain": "malware_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Kaggle Microsoft Malware Classification Challenge (BIG 2015)",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Tiny Obfuscated Malware Dataset (TinyOMD)",
        "type": "synthetic",
        "domain": "malware_images",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "MOTIF",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "VX Heaven",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "MalDozer",
        "type": "public",
        "domain": "android_apps",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "EMBER",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "MalGenome",
        "type": "public",
        "domain": "android_apps",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Variant (2015)",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Malheur",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Drebin",
        "type": "public",
        "domain": "android_apps",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Malicia",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Malpedia",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Malsign",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "MaLabel",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "MtNet",
        "type": "proprietary",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "XGBoost",
        "paper_reference": null,
        "metric": "Accuracy, Precision, Recall, F1",
        "their_result": "Accuracy 82%, Precision 88%, Recall 80%, F1 83%",
        "baseline_result": null
      },
      {
        "method_name": "Support Vector Machine (SVM)",
        "paper_reference": null,
        "metric": "Accuracy, Precision, Recall, F1",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Random Forest",
        "paper_reference": null,
        "metric": "Accuracy, Precision, Recall, F1",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "Precision",
      "Recall",
      "F1-score"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can a new dataset incorporating systematic obfuscation better mimic polymorphic malware for realistic evaluation?",
        "How do conventional supervised ML algorithms perform on obfuscated malware family classification?"
      ],
      "gaps_identified": [
        "“Although there are some datasets that has a few obfuscated malware samples, no dataset is purely focused on obfuscated malware classification.”",
        "Existing public datasets may yield biased or incorrect evaluations for newer polymorphic malware.",
        "Limited availability of datasets covering non-Windows operating systems (Linux, macOS, iOS) for malware classification."
      ],
      "limitations": [
        "Scope limited to Windows-oriented datasets and grayscale image representations; authors note other OS (Linux, macOS, iOS) are outside the article’s purview: “there aren’t many prominent datasets that contain malware that targets other operating systems (including Linux, macOS, and iOS), but this research is outside the purview of our article.”",
        "The paper does not provide code, detailed hyperparameters, or train/test splits, limiting reproducibility.",
        "Availability of the new OMD/TinyOMD datasets is not specified."
      ],
      "future_work": [],
      "motivation": "Signature-based AV can be evaded through minor obfuscations; need a realistic dataset that incorporates obfuscation to evaluate and improve malware analysis techniques.",
      "potential_research_ideas": [
        "Develop obfuscation-invariant representation learning using contrastive/self-supervised pretraining on original vs. obfuscated pairs.",
        "Evaluate deep vision models (CNNs/ResNets, EfficientNet, Vision Transformers) on OMD versus classic ML.",
        "Multi-modal fusion of static modalities (bytes, ASM, images) and dynamic behavioral features to increase robustness.",
        "Automatic detection and classification of obfuscation type as an auxiliary task to improve primary family classification.",
        "Adversarial training and robustness evaluation against gradient-based perturbations and realistic obfuscations.",
        "Domain generalization and test-time adaptation methods to handle unseen obfuscation patterns and families.",
        "Active learning or weak supervision using VirusTotal labels to scale annotation with minimal manual effort.",
        "Expand dataset to include Linux, macOS, and iOS malware, enabling cross-OS transfer learning studies.",
        "Benchmark standardization: release predefined splits and leaderboards for reproducible comparisons."
      ],
      "architectural_improvement_recommendations": [
        "Adopt a CNN/Transformer hybrid with shallow CNN stem and ViT encoder trained with strong augmentations mirroring obfuscation block-II (blur, warp, scramble).",
        "Use Siamese/contrastive architecture to pull together features of original and obfuscated variants (InfoNCE/Triplet loss) alongside cross-entropy.",
        "Incorporate obfuscation-adversarial training with a gradient reversal layer (DANN) to reduce obfuscation-specific bias.",
        "Multi-branch network: one branch on images, one on opcode/ASM token sequences (e.g., 1D Transformer) with late fusion.",
        "Calibrate with temperature scaling and evaluate OOD detection for unseen families/obfuscations.",
        "Leverage class-balanced loss or focal loss to address class imbalance across the 40 families."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Evasion via obfuscation/polymorphism can degrade static signature-based and shallow ML detectors."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "“A large malware dataset named Obfucated Malware Dataset (OMD) is generated … applying various obfuscation techniques resulting in 40 different families of malware. The final dataset contains 40 classes and 21924 samples.”",
      "“In contrast to other datasets, all samples in OMD are obfuscated using different techniques resulting in a dataset that can mimic new or polymorphic malwares.”",
      "Two-stage obfuscation pipeline: Block I (assembly-level: dead-code insertion, subroutine reordering, register reassignment, XOR-operation, instruction substitution, code transposition) and Block II (image-level: masking, in-painting, blurring, warping, scrambling, tokenization), plus random encryption of a subset.",
      "Empirical evaluation of conventional ML algorithms (SVM, Random Forest, XGBoost) on OMD; “XGBoost outperformed the other algorithms, achieving an accuracy of 82%, precision of 88%, recall of 80%, and an F1-Score of 83%.”"
    ]
  },
  {
    "arxiv_id": "2309.08188v1",
    "title": "Privacy-Aware Joint Source-Channel Coding for image transmission based on Disentangled Information Bottleneck",
    "authors": "Lunan Sun; Caili Guo; Mingzhe Chen; Yang Yang",
    "abstract": "Current privacy-aware joint source-channel coding (JSCC) works aim at avoiding private information transmission by adversarially training the JSCC encoder and decoder under specific signal-to-noise ratios (SNRs) of eavesdroppers. However, these approaches incur additional computational and storage requirements as multiple neural networks must be trained for various eavesdroppers' SNRs to determine the transmitted information. To overcome this challenge, we propose a novel privacy-aware JSCC for image transmission based on disentangled information bottleneck (DIB-PAJSCC). In particular, we derive a novel disentangled information bottleneck objective to disentangle private and public information. Given the separate information, the transmitter can transmit only public information to the receiver while minimizing reconstruction distortion. Since DIB-PAJSCC transmits only public information regardless of the eavesdroppers' SNRs, it can eliminate additional training adapted to eavesdroppers' SNRs. Experimental results show that DIB-PAJSCC can reduce the eavesdropping accuracy on private information by up to 20\\% compared to existing methods.",
    "published_date": "2023-09-15",
    "pdf_link": "https://arxiv.org/pdf/2309.08188v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Communications and Network Security",
      "subdomain": "Physical-layer Security",
      "specific_problem": "Privacy-aware joint source-channel coding (JSCC) for image transmission over a wiretap channel independent of eavesdropper SNR",
      "attack_types": [
        "Eavesdropping"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Information Bottleneck",
        "specific": "Disentangled Information Bottleneck (DIB)",
        "novel_contribution": "Derives a new DIB objective for JSCC that disentangles private (ys) and public (yt) subcodewords while minimizing reconstruction distortion; transmits only public information."
      },
      {
        "type": "primary",
        "category": "Autoencoder",
        "specific": "Split-encoder JSCC (public encoder f_phi_t and private encoder f_phi_s) with decoder D_theta_B",
        "novel_contribution": "Encoder is decomposed into separate public and private branches; only yt is transmitted to Bob; two-step training to avoid degenerate solution."
      },
      {
        "type": "primary",
        "category": "Mutual Information Estimation via Density-Ratio Discriminator",
        "specific": "Discriminator Dis_ε to estimate I(yt; ys)",
        "novel_contribution": "Uses density-ratio trick to estimate and minimize I(yt; ys) for enforcing independence between yt and ys in JSCC."
      },
      {
        "type": "primary",
        "category": "Supervised Classification",
        "specific": "Classifier C_γ to approximate q(s|ys) and maximize lower bound of I(ys; s)",
        "novel_contribution": "Optimizes a variational lower bound on I(ys; s) via a classifier to concentrate private information in ys."
      },
      {
        "type": "baseline",
        "category": "GAN-based secure JSCC",
        "specific": "Adversarial privacy-aware JSCC (adversarially trained encoder/decoder against eavesdropper at fixed SNR)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Adversarial"
    ],
    "datasets": [
      {
        "name": "Colored MNIST",
        "type": "public",
        "domain": "images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "UTK Face (UTKFace)",
        "type": "public",
        "domain": "images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Adversarial privacy-aware JSCC",
        "paper_reference": "[8] Marchioro et al., ICASSP 2020; [9] Erdemir et al., ICASSP 2022",
        "metric": "Eavesdropping accuracy on private information; Reconstruction MSE",
        "their_result": "“DIB-PAJSCC can reduce the eavesdropping accuracy on private information by up to 20% compared to existing methods.” Also, on Colored MNIST, eavesdropping accuracy is “close to random guess (about 0.1 for 10 categories)” and shows minimal variation with SNR_AE.",
        "baseline_result": "Adversarial methods show higher eavesdropping accuracy that “increases obviously as SNR_AE increases,” and require training multiple networks for different assumed eavesdropper SNRs."
      }
    ],
    "performance_metrics_used": [
      "Eavesdropping accuracy on private information",
      "Mean Squared Error (MSE) of reconstruction"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can we disentangle private and public information in JSCC codewords so that only public information is transmitted while maintaining low distortion?",
        "Can privacy-aware JSCC avoid dependence on eavesdropper SNR and eliminate additional training for different SNRs?"
      ],
      "gaps_identified": [
        "Prior privacy-aware JSCC methods require multiple neural networks trained for different eavesdropper SNRs, incurring extra computation and storage.",
        "They assume eavesdropper SNR during optimization matches inference, leading to degradation under SNR mismatch.",
        "Due to the inexplicability of neural networks, prior methods cannot separate private and public information in transmitted codewords."
      ],
      "limitations": [
        "“Even though DIB-PAJSCC sacrifices a little reconstruction quality to defend against eavesdropping, the key facial information … is not damaged.”",
        "Potential degenerate solution where all information collapses into ys; mitigated via a two-step training strategy.",
        "Requires labeled private attribute s to train the classifier for the variational lower bound on I(ys; s).",
        "Evaluated on image datasets (Colored MNIST, UTKFace) and AWGN channels; generalization to other modalities/channels not shown."
      ],
      "future_work": [],
      "motivation": "Avoid private information leakage in image JSCC without costly retraining for varying eavesdropper SNRs by explicitly disentangling private and public information.",
      "potential_research_ideas": [
        "Extend DIB-PAJSCC to other modalities (speech, video) and to realistic wireless channels (Rayleigh fading, MIMO) with channel state uncertainty.",
        "Unsupervised or weakly supervised discovery of private attributes to remove, reducing reliance on explicit private labels s.",
        "Formal information-theoretic bounds on privacy leakage (e.g., upper bounds on I(yt; s)) and distortion–privacy trade-off analysis.",
        "Adaptive rate allocation between Mt and Ms under bandwidth constraints, possibly with learnable capacity schedulers.",
        "Integrate differential privacy or DP-SGD into training to provide formal privacy guarantees in addition to disentanglement.",
        "Robustness against model inversion or side-channel attacks on Bob/Eve beyond AWGN eavesdropping.",
        "Joint design with channel coding/precoding (e.g., MIMO beamforming) to spatially suppress leakage while using DIB for feature-level privacy.",
        "Contrastive or mutual information neural estimators (MINE/InfoNCE) to tighten MI estimates and improve disentanglement."
      ],
      "architectural_improvement_recommendations": [
        "Use stochastic encoders (e.g., VAE-style latent variables) to better control mutual information and avoid deterministic leakage.",
        "Replace or complement the density-ratio discriminator with MINE or InfoNCE-based objectives for stabler MI estimation.",
        "Channel-aware conditioning (e.g., FiLM or attention) in the public encoder to adapt yt to Bob’s SNR without encoding private info.",
        "Learnable capacity splitter that dynamically allocates Mt and Ms given privacy-distortion targets.",
        "Multi-task decoder that jointly reconstructs and verifies absence of private attributes (adversarial head ensuring yt is s-invariant).",
        "Curriculum or alternating schedules with regularization to avoid degenerate collapse into ys without two-step freezing.",
        "Incorporate perceptual/SSIM losses to reduce the slight reconstruction quality sacrifice while maintaining privacy."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": true,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes DIB-PAJSCC: a privacy-aware JSCC scheme that disentangles private and public information and transmits only public information.",
      "Derives a tractable disentangled information bottleneck objective with a lower bound for I(ys; s) via a classifier and an estimation of I(yt; ys) via a density-ratio discriminator.",
      "Introduces a two-step training strategy to prevent degenerate solutions and ensure yt captures public information while ys captures private information.",
      "Eliminates the need for multiple models tailored to eavesdropper SNRs; training depends only on Bob’s SNR, making the method effective across all eavesdropper SNRs.",
      "Demonstrates empirically that eavesdropping accuracy on private information is reduced by up to 20% compared to adversarial privacy-aware JSCC while maintaining similar reconstruction MSE."
    ]
  },
  {
    "arxiv_id": "2310.04479v1",
    "title": "Leveraging Data Geometry to Mitigate CSM in Steganalysis",
    "authors": "Rony Abecidan; Vincent Itier; Jérémie Boulanger; Patrick Bas; Tomáš Pevný",
    "abstract": "In operational scenarios, steganographers use sets of covers from various sensors and processing pipelines that differ significantly from those used by researchers to train steganalysis models. This leads to an inevitable performance gap when dealing with out-of-distribution covers, commonly referred to as Cover Source Mismatch (CSM). In this study, we consider the scenario where test images are processed using the same pipeline. However, knowledge regarding both the labels and the balance between cover and stego is missing. Our objective is to identify a training dataset that allows for maximum generalization to our target. By exploring a grid of processing pipelines fostering CSM, we discovered a geometrical metric based on the chordal distance between subspaces spanned by DCTr features, that exhibits high correlation with operational regret while being not affected by the cover-stego balance. Our contribution lies in the development of a strategy that enables the selection or derivation of customized training datasets, enhancing the overall generalization performance for a given target. Experimental validation highlights that our geometry-based optimization strategy outperforms traditional atomistic methods given reasonable assumptions. Additional resources are available at github.com/RonyAbecidan/LeveragingGeometrytoMitigateCSM.",
    "published_date": "2023-10-06",
    "pdf_link": "https://arxiv.org/pdf/2310.04479v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Multimedia Security",
      "subdomain": "Steganalysis",
      "specific_problem": "Mitigating cover-source mismatch (CSM) by selecting/deriving a training dataset that best generalizes to an unlabeled target set (unknown cover-stego balance) in JPEG steganalysis",
      "attack_types": [
        "JPEG steganography (UERD, 0.5 bpnzac)",
        "Distribution shift / Cover Source Mismatch (CSM)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Dimensionality Reduction + Geometric Metric",
        "specific": "PCA + normalized squared chordal distance (NSCD) between subspaces spanned by DCTR features",
        "novel_contribution": "Use of chordal distance between PCA-estimated subspaces (from DCTR features) as a label-free, cover-stego balance-invariant proxy highly correlated with operational regret to select/derive training datasets; geometry-based optimization (e.g., simulated annealing) to minimize NSCD to a target"
      },
      {
        "type": "primary",
        "category": "Heuristic Optimization",
        "specific": "Simulated annealing for parameter search over image development pipelines",
        "novel_contribution": "Optimizes development parameters (denoising, downsampling, sharpening) to minimize NSCD to the target and construct a target-specific source"
      },
      {
        "type": "baseline",
        "category": "Kernel two-sample test / Distribution distance",
        "specific": "Maximum Mean Discrepancy (MMD) with Energy Distance kernel (via geomloss)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Distance metric",
        "specific": "L2 distance between centers of gravity (means) of source and target features",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Linear Model",
        "specific": "Linear classifier trained on DCTR features for steganalysis",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Multi-class classification",
        "specific": "Multi-classifier to select the most representative source per image (from a small set of representative sources)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Ensemble decision rule",
        "specific": "Majority vote among representative sources",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "ALASKA RAW images (derived JPEGs at QF=85 using development pipelines)",
        "type": "synthetic",
        "domain": "images",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "ALASKA / ALASKABASE",
        "type": "public",
        "domain": "images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "ALASKA2 Challenge (smart cropping technique referenced for data preparation)",
        "type": "public",
        "domain": "images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "BOSSBASE",
        "type": "public",
        "domain": "images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Multi-classifier (source selector over representative sources)",
        "paper_reference": "[6] (authors' prior methodology for extracting representative sources)",
        "metric": "Operational regret distribution (All three balancing cases show identical stats)",
        "their_result": "Proposed NSCD-based selection (balanced): Q1=0.1, median=0.7, Q3=1.4, Max=16, %R>5=2.0",
        "baseline_result": "Multi-classifier: Min=0.0, Q1=0.4, median=1.4, Q3=2.8, Max=25, %R>5=8.7"
      },
      {
        "method_name": "Majority vote (among representative sources)",
        "paper_reference": null,
        "metric": "Operational regret distribution (balanced)",
        "their_result": "Proposed NSCD-based selection (balanced): Q1=0.1, median=0.7, Q3=1.4, Max=16, %R>5=2.0",
        "baseline_result": "Majority vote: Min=0.0, Q1=6.6, median=8.0, Q3=9.4, Max=26.1, %R>5=89"
      },
      {
        "method_name": "Min L2 distance between centers of gravity (means)",
        "paper_reference": "[3]",
        "metric": "Operational regret distribution (balanced)",
        "their_result": "Proposed NSCD-based selection (balanced): Q1=0.1, median=0.7, Q3=1.4, Max=16, %R>5=2.0",
        "baseline_result": "min L2_CG (balanced): Min=0.0, Q1=0.8, median=1.7, Q3=3.5, Max=26, %R>5=20"
      },
      {
        "method_name": "Min MMD (Energy Distance kernel via geomloss)",
        "paper_reference": "[11], [12]",
        "metric": "Operational regret distribution (balanced)",
        "their_result": "Proposed NSCD-based selection (balanced): Q1=0.1, median=0.7, Q3=1.4, Max=16, %R>5=2.0",
        "baseline_result": "min MMD (balanced): Min=0.0, Q1=0.4, median=1.5, Q3=2.9, Max=19, %R>5=8.7"
      }
    ],
    "performance_metrics_used": [
      "Operational regret (difference in error when trained on source vs target intrinsic difficulty)",
      "Intrinsic Difficulty (PE: probability of error on same-source train/test)",
      "NSCD (normalized squared chordal distance between PCA subspaces)",
      "MMD (with Energy Distance kernel)",
      "L2 distance between centers of gravity",
      "Quantile statistics (Q1, median, Q3, 90th percentile)",
      "Probability of error (PE)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How to identify, without labels and with unknown cover-stego balance, a training dataset that maximizes generalization to a given target in the presence of CSM?",
        "Which metric computed on unlabeled target data best correlates with operational regret and is invariant to cover-stego balance?",
        "Can we automatically construct or adapt a source (training set) to a target by optimizing a geometry-based criterion?"
      ],
      "gaps_identified": [
        "Prior work does not comprehensively explore the relevance of specific sources to particular targets in CSM scenarios.",
        "Existing similarity metrics (e.g., L2 between means) can depend on cover-stego balance and may not reliably imply low regret.",
        "Need for an interpretable, label-free metric strongly correlated with operational regret for source selection/derivation."
      ],
      "limitations": [
        "Authors note theoretical cases where NSCD can be low while regret is high (e.g., significant shift in the source manifold), though such cases were not present in their experiments.",
        "Manifold estimation via PCA depends on sample size and the choice to compress to JPEG QF=85 to facilitate estimation.",
        "Experiments rely on linear classifiers with DCTR features and one embedding scheme (UERD, 0.5 bpnzac); generalization to other detectors/embeddings is not empirically shown."
      ],
      "future_work": [],
      "motivation": "Operational steganalysis faces cover-source mismatch due to diverse acquisition and processing pipelines; investigators often lack labels and even class balance for targets. The goal is to find or derive a training source that generalizes best to the target using a label-free, balance-invariant metric correlated with regret.",
      "potential_research_ideas": [
        "Extend geometry-based selection to deep feature spaces (e.g., subspaces of CNN features or modern deep steganalyzers) and compare NSCD correlations with regret.",
        "Incorporate manifold alignment or geodesic flow kernels to handle cases with significant manifold shifts where NSCD may mislead.",
        "Active data acquisition: iteratively request/collect minimal additional unlabeled target samples to improve subspace estimation and reduce variance.",
        "Multi-target optimization: jointly derive a source that minimizes aggregate NSCD to a distribution over multiple targets (e.g., federated or multi-tenant settings).",
        "Payload/embedding-agnostic selection: evaluate robustness of NSCD correlation under varying payloads and embedding algorithms (e.g., J-UNIWARD, UERD variants).",
        "Hybrid metric learning: learn a weighted combination of NSCD, MMD, and other geometry/statistical distances optimized to predict regret.",
        "Uncertainty-aware NSCD: incorporate confidence intervals for subspace estimation to guide conservative training source selection."
      ],
      "architectural_improvement_recommendations": [
        "Replace linear DCTR-based detector with a modern CNN/JPEG-domain network and compute subspaces in its penultimate-layer feature space.",
        "Use robust PCA or subspace estimation with shrinkage to reduce sensitivity to limited samples and noise.",
        "Adopt stochastic or Bayesian optimization (e.g., BO with Gaussian Processes) instead of simulated annealing for pipeline parameter search.",
        "Combine NSCD with MMD in a multi-objective optimization to hedge against cases where manifold shifts degrade NSCD reliability.",
        "Calibrate subspace dimensionality via cross-validated explained variance or information criteria to balance bias-variance trade-offs."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/RonyAbecidan/LeveragingGeometrytoMitigateCSM",
      "frameworks": [
        "geomloss",
        "ImageMagick"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Unknown cover-stego balance and absence of labels in operational targets",
        "Diversity of acquisition/processing pipelines causing severe distribution shifts (CSM)",
        "Reliable subspace estimation under limited unlabeled target samples",
        "Selecting/deriving training sources among many candidate pipelines efficiently"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Identify a geometrical metric (normalized squared chordal distance between PCA subspaces of DCTR features) that exhibits high correlation with operational regret and is invariant to cover-stego balance.",
      "Propose a strategy to select or derive customized training datasets for a given unlabeled target by minimizing NSCD.",
      "Demonstrate, on a universe of 103 development pipelines (varying denoising, downsampling, sharpening; JPEG QF=85), that NSCD-based selection outperforms traditional atomistic methods (L2 means, MMD, majority vote) under multiple balancing scenarios.",
      "Provide an optimization scheme (e.g., simulated annealing) to construct target-specific relevant sources using the geometry-based metric.",
      "Empirically show that manifold geometry clarifies why certain sources generalize better and can identify irrelevant sources."
    ]
  },
  {
    "arxiv_id": "2309.13612v1",
    "title": "Digital Twins and the Future of their Use Enabling Shift Left and Shift Right Cybersecurity Operations",
    "authors": "Ahmad Mohsin; Helge Janicke; Surya Nepal; David Holmes",
    "abstract": "Digital Twins (DTs), optimize operations and monitor performance in Smart Critical Systems (SCS) domains like smart grids and manufacturing. DT-based cybersecurity solutions are in their infancy, lacking a unified strategy to overcome challenges spanning next three to five decades. These challenges include reliable data accessibility from Cyber-Physical Systems (CPS), operating in unpredictable environments. Reliable data sources are pivotal for intelligent cybersecurity operations aided with underlying modeling capabilities across the SCS lifecycle, necessitating a DT. To address these challenges, we propose Security Digital Twins (SDTs) collecting realtime data from CPS, requiring the Shift Left and Shift Right (SLSR) design paradigm for SDT to implement both design time and runtime cybersecurity operations. Incorporating virtual CPS components (VC) in Cloud/Edge, data fusion to SDT models is enabled with high reliability, providing threat insights and enhancing cyber resilience. VC-enabled SDT ensures accurate data feeds for security monitoring for both design and runtime. This design paradigm shift propagates innovative SDT modeling and analytics for securing future critical systems. This vision paper outlines intelligent SDT design through innovative techniques, exploring hybrid intelligence with data-driven and rule-based semantic SDT models. Various operational use cases are discussed for securing smart critical systems through underlying modeling and analytics capabilities.",
    "published_date": "2023-09-24",
    "pdf_link": "https://arxiv.org/pdf/2309.13612v1",
    "paper_types": [
      "position"
    ],
    "security_domain": {
      "primary": "Industrial Control Systems (ICS) Security",
      "subdomain": "Security Operations and Monitoring",
      "specific_problem": "Designing Security Digital Twins (SDTs) that enable Shift-Left and Shift-Right cybersecurity operations for smart critical systems via Virtual Components (VCs), reliable data fusion, and hybrid intelligence",
      "attack_types": [
        "Advanced Persistent Threats (APTs)",
        "Ransomware",
        "Software supply chain attacks",
        "Safety-system manipulation (e.g., TRITON-like)",
        "False data injection",
        "Man-in-the-middle (MitM)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Hybrid (Semantic + ML)",
        "specific": null,
        "novel_contribution": "Vision of hybrid intelligence for SDTs combining state-based modeling, semantic technologies, Knowledge Graphs, and ML/generative AI for cybersecurity operations"
      },
      {
        "type": "primary",
        "category": "Knowledge Graphs / Semantic Reasoning",
        "specific": "Knowledge Graphs; semantic annotation; rule-based reasoning",
        "novel_contribution": "Use of semantic models and KGs within SDTs to model threats, vulnerabilities, and relationships for protection/detection/response"
      },
      {
        "type": "primary",
        "category": "Transformer / Large Language Models",
        "specific": "LLMs (general)",
        "novel_contribution": "Proposed use of LLMs integrated with SDT models for cybersecurity analytics and operations (conceptual)"
      },
      {
        "type": "primary",
        "category": "State-space / Time-series modeling",
        "specific": null,
        "novel_contribution": "Use of state-space-based methods in SDTs to predict behavior changes and preempt unsafe states for intrusion prevention (conceptual)"
      }
    ],
    "learning_paradigm": [],
    "datasets": [
      {
        "name": "CVE (Common Vulnerabilities and Exposures)",
        "type": "public",
        "domain": "vulnerability_database",
        "link": "https://cve.mitre.org/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CWE (Common Weakness Enumeration)",
        "type": "public",
        "domain": "vulnerability_database",
        "link": "https://cwe.mitre.org/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "DT-based cybersecurity solutions are in their infancy and lack a unified long-term strategy for the next 3–5 decades.",
        "Reliable, secure, and continuous data accessibility from CPS to DTs is challenging due to unpredictable environments and heterogeneous OT/IT components.",
        "Current SIEM/IDS tools struggle to capture core assets, correlate threats/vulnerabilities, and support timely incident response for SCS.",
        "Existing DT integrations face difficulties with uninterrupted connectivity and data synchronization between physical and digital counterparts.",
        "Lack of integrated design-time (shift-left) and runtime (shift-right) cybersecurity operations tied to DTs for SCS.",
        "Interoperability and data exchange issues between diverse CPS components and DTs hinder effective security analytics."
      ],
      "limitations": [
        "Establishing and maintaining uninterrupted connection and synchronization of data between CPS and SDTs is a significant challenge (acknowledged).",
        "The paper is a vision/position work without empirical validation or benchmarks (implied by paper type)."
      ],
      "future_work": [
        "Develop SDT modeling capabilities that incorporate state-based modeling, semantic technologies, Knowledge Graphs, and emerging ML/generative AI approaches.",
        "Research challenges for implementing DT-driven cybersecurity of future systems (outlined at a high level)."
      ],
      "motivation": "Improve cybersecurity resilience of smart critical systems by introducing Security Digital Twins (SDTs) and a Shift Left/Shift Right design paradigm using Virtual Components to ensure reliable data and enable intelligent security operations across the lifecycle.",
      "potential_research_ideas": [
        "Prototype an SDT with VCs for a specific ICS domain (e.g., substation automation) and evaluate protection/detection/response efficacy against staged APT and false-data-injection attacks.",
        "Design standardized VC interfaces (Purdue model-aligned) for PLC/RTU/IIoT that ensure secure, low-latency, and verifiable control loops with SDT co-execution at edge/cloud.",
        "Develop a threat-vulnerability KG for SCS that fuses CVE/CWE, asset inventories, network topology, process states, and attack graphs to support automated threat modeling and what-if simulations in SDTs.",
        "Investigate hybrid pipelines that combine rule-based safety envelopes with ML/LLM-driven anomaly reasoning for explainable interventions in ICS.",
        "Create a public dataset/benchmark of synchronized CPS-VC-SDT telemetry and labeled security events to spur SDT-based cybersecurity research.",
        "Formal verification of SDT-driven interventions to guarantee safety constraints during automated response (e.g., temporal logic over state-space predictions).",
        "Adversarial robustness analysis for SDT pipelines (sensor spoofing, command injection on VCs, model poisoning of ML components).",
        "Integrate LLMs for operator co-pilots that translate SDT analytics into action plans while grounding in KGs to reduce hallucinations.",
        "Edge-first SDT deployments: study latency, reliability, and failover strategies for VC control loops under degraded networks.",
        "Privacy-preserving data fusion for SDTs (federated learning across plants) while keeping safety and auditability."
      ],
      "architectural_improvement_recommendations": [
        "Introduce a modular SDT reference architecture with decoupled layers: data ingestion (VCs), semantic layer (KG/ontologies), analytics (ML/LLM), and orchestration (policy/response).",
        "Implement a safety-verified control layer enforcing invariant checks over state-space predictions before any automated intervention.",
        "Adopt standardized secure protocols (OPC UA Security, MQTT over TLS) with mutual attestation between VCs and SDTs; add PKI-based identity for assets.",
        "Use a digital thread for provenance: cryptographically sign configuration changes and interventions for auditability and rollback.",
        "Leverage edge-cloud co-design: run time-critical detection and safety checks at edge VCs; offload heavy analytics/LLMs to cloud with caching and drift monitoring.",
        "Integrate attack graphs with process-algebra/state machines to tie cyber paths to physical process hazard analysis within SDTs."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Edge/Cloud with Virtual Components interfacing sensors/actuators and SDTs for SCS (per Purdue model levels)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Reliable and secure data access from CPS; uninterrupted connectivity and synchronization between CPS and SDT.",
        "Interoperability across heterogeneous OT/IT components and protocols (OPC UA, Modbus, MQTT/HTTPS).",
        "Risk management for automated interventions to avoid safety violations; need for fail-safe mechanisms.",
        "Edge/cloud latency and reliability for control loops when VCs directly interact with physical processes.",
        "Complexity of integrating asset/vulnerability context for accurate threat modeling and response.",
        "Operationalizing hybrid semantic-ML analytics with auditability and compliance in critical environments."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes Security Digital Twins (SDTs) to enable integrated Shift-Left (design-time) and Shift-Right (runtime) cybersecurity operations for smart critical systems.",
      "Introduces Virtual Components (VCs) to replace/augment CPS elements, improving reliable data feeds and enabling SDT-driven monitoring, simulation, and interventions.",
      "Articulates a vision for hybrid intelligence in SDTs combining state-based modeling, semantic technologies, Knowledge Graphs, and ML/LLMs for security analytics.",
      "Maps SDT-enabled operational use cases across protection, detection, prevention, and response aligned with NIST CSF for critical systems."
    ]
  },
  {
    "arxiv_id": "2310.10661v1",
    "title": "TII-SSRC-23 Dataset: Typological Exploration of Diverse Traffic Patterns for Intrusion Detection",
    "authors": "Dania Herzalla; Willian T. Lunardi; Martin Andreoni Lopez",
    "abstract": "The effectiveness of network intrusion detection systems, predominantly based on machine learning, are highly influenced by the dataset they are trained on. Ensuring an accurate reflection of the multifaceted nature of benign and malicious traffic in these datasets is essential for creating models capable of recognizing and responding to a wide array of intrusion patterns. However, existing datasets often fall short, lacking the necessary diversity and alignment with the contemporary network environment, thereby limiting the effectiveness of intrusion detection. This paper introduces TII-SSRC-23, a novel and comprehensive dataset designed to overcome these challenges. Comprising a diverse range of traffic types and subtypes, our dataset is a robust and versatile tool for the research community. Additionally, we conduct a feature importance analysis, providing vital insights into critical features for intrusion detection tasks. Through extensive experimentation, we also establish firm baselines for supervised and unsupervised intrusion detection methodologies using our dataset, further contributing to the advancement and adaptability of intrusion detection models in the rapidly changing landscape of network security. Our dataset is available at https://kaggle.com/datasets/daniaherzalla/tii-ssrc-23.",
    "published_date": "2023-09-14",
    "pdf_link": "https://arxiv.org/pdf/2310.10661v1",
    "paper_types": [
      "new_dataset",
      "survey",
      "benchmark",
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Creation of a diverse network traffic dataset for IDS, with feature importance analysis and supervised/unsupervised baseline evaluation",
      "attack_types": [
        "DoS (TCP flag floods: ACK, CWR, ECN, FIN, PSH, RST, SYN, URG)",
        "DoS UDP",
        "DoS HTTP",
        "DoS ICMP",
        "DoS MAC",
        "Brute-force (DNS, FTP, HTTP, SSH, Telnet)",
        "Information gathering (scanning/probing)",
        "Mirai botnet DDoS (ACK, DNS, GRE over Ethernet, GRE over IP, HTTP, SYN, UDP)",
        "Scan and Bruteforce (Mirai variant)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Feature importance analysis",
        "specific": null,
        "novel_contribution": "Comprehensive feature importance analysis of network flow features on the new dataset"
      },
      {
        "type": "primary",
        "category": "Supervised classifiers (unspecified)",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Unsupervised anomaly detection (unspecified)",
        "specific": null,
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "TII-SSRC-23",
        "type": "public",
        "domain": "network_traffic",
        "link": "https://kaggle.com/datasets/daniaherzalla/tii-ssrc-23",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "DARPA98",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "KDD99",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "NSL-KDD",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Kyoto 2006+",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "UNIBS",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CTU-13",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "TUIDS",
        "type": "private",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "ISCX 2012",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "UNSW-NB15",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "DDoS 2016",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CICIDS2017",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CIC DoS 2017",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "N-BaIoT",
        "type": "public",
        "domain": "iot_network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "BoT-IoT",
        "type": "public",
        "domain": "iot_network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "TON-IoT",
        "type": "public",
        "domain": "iot_network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CIC IoT 2022",
        "type": "public",
        "domain": "iot_network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "LATAM-DDoS-IoT",
        "type": "public",
        "domain": "iot_network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Edge-IIoTset",
        "type": "public",
        "domain": "iiot_network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Existing IDS datasets often lack diversity, especially within malicious classes, limiting model generalization to unseen intrusions.",
        "Many widely used datasets contain outdated patterns, inherent biases, and obsolete features not aligned with modern traffic.",
        "IoT datasets often fail to capture the full spectrum of device interactions and potential intrusions in heterogeneous environments.",
        "Models performing well on in-dataset evaluations degrade significantly in real-world contexts due to limited traffic diversity."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Provide a comprehensive, diverse, and contemporary network traffic dataset (with PCAP and flow features) to improve the robustness and generalization of ML-based IDS, along with feature importance insights and supervised/unsupervised baselines.",
      "potential_research_ideas": [
        "Cross-dataset generalization studies using TII-SSRC-23 as training and evaluating on other modern datasets to quantify transferability.",
        "Domain adaptation and continual learning for IDS to handle concept drift and evolving traffic patterns using this dataset’s diverse subtypes.",
        "Self-supervised pretraining on raw PCAP or flow time-series to learn protocol-agnostic representations, followed by fine-tuning for IDS.",
        "Graph-based intrusion detection that models communication graphs between nodes and flow interactions captured in the dataset.",
        "Multimodal IDS combining flow statistics with packet payload metadata or RF/interference context used during data collection.",
        "Per-subtype anomaly detection benchmarks and metric suites to encourage robustness across diverse DoS flag variants and Mirai DDoS modes.",
        "Adversarial robustness evaluation protocols on TII-SSRC-23 (e.g., evasion and poisoning) to stress-test ML IDS.",
        "Data-centric methods: targeted augmentation and rebalancing across rare subtypes (e.g., ICMP DoS) and calibration to reduce false positives."
      ],
      "architectural_improvement_recommendations": [
        "Adopt temporal models (e.g., sequence models over flows) to capture bursty behaviors in DoS/DDoS and brute-force campaigns.",
        "Use contrastive/self-supervised learning on unlabeled flows from the PCAP to improve unsupervised anomaly detection baselines.",
        "Incorporate feature selection pipelines guided by the provided feature-importance to reduce dimensionality and improve efficiency.",
        "Leverage ensemble methods combining supervised classifiers with unsupervised detectors for robust performance across diverse subtypes.",
        "Introduce calibration and cost-sensitive learning to mitigate high false positive/negative trade-offs in deployment.",
        "Explore packet-level deep models (e.g., 1D CNN/Transformer over packet sequences) alongside flow-level features for richer context."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Generalization gap between emulated/testbed-trained models and real-world traffic.",
        "Dataset bias and outdated patterns in prior datasets hinder deployment performance.",
        "Heterogeneity of IoT environments and diverse device behaviors complicate IDS tuning.",
        "Managing high false positives/negatives when facing unseen intrusion variants."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces TII-SSRC-23, a heterogeneous network traffic dataset (27.5 GB) with raw PCAP and CSV features, covering 8 types and 32 subtypes (6 benign, 26 malicious).",
      "Provides an exhaustive survey of 18 existing network traffic datasets, aiding dataset selection for IDS research.",
      "Performs comprehensive feature importance analysis to identify critical features for intrusion detection tasks.",
      "Establishes firm supervised and unsupervised baseline performances on the new dataset to foster robust IDS development."
    ]
  },
  {
    "arxiv_id": "2310.07005v1",
    "title": "Sound-skwatter (Did You Mean: Sound-squatter?) AI-powered Generator for Phishing Prevention",
    "authors": "Rodolfo Valentim; Idilio Drago; Marco Mellia; Federico Cerutti",
    "abstract": "Sound-squatting is a phishing attack that tricks users into malicious resources by exploiting similarities in the pronunciation of words. Proactive defense against sound-squatting candidates is complex, and existing solutions rely on manually curated lists of homophones. We here introduce Sound-skwatter, a multi-language AI-based system that generates sound-squatting candidates for proactive defense. Sound-skwatter relies on an innovative multi-modal combination of Transformers Networks and acoustic models to learn sound similarities. We show that Sound-skwatter can automatically list known homophones and thousands of high-quality candidates. In addition, it covers cross-language sound-squatting, i.e., when the reader and the listener speak different languages, supporting any combination of languages. We apply Sound-skwatter to network-centric phishing via squatted domain names. We find ~ 10% of the generated domains exist in the wild, the vast majority unknown to protection solutions. Next, we show attacks on the PyPI package manager, where ~ 17% of the popular packages have at least one existing candidate. We believe Sound-skwatter is a crucial asset to mitigate the sound-squatting phenomenon proactively on the Internet. To increase its impact, we publish an online demo and release our models and code as open source.",
    "published_date": "2023-10-10",
    "pdf_link": "https://arxiv.org/pdf/2310.07005v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Web Security",
      "subdomain": "Phishing Detection and Prevention",
      "specific_problem": "Proactive generation of sound-squatting (homophone/quasi-homophone) candidates for domains and package names, including cross-language cases",
      "attack_types": [
        "sound-squatting (homophone-based phishing)",
        "domain squatting",
        "package squatting (e.g., PyPI)",
        "cross-language sound-squatting"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "Encoder-Decoder (sequence-to-sequence)",
        "novel_contribution": "Multi-modal training signal: jointly decodes graphemes and reconstructs Mel spectrogram from IPA encoding with a duration predictor to learn pronunciation similarity"
      },
      {
        "type": "primary",
        "category": "Sequence Modeling",
        "specific": "Beam Search Decoding",
        "novel_contribution": "Beam search in Post-Processor to generate multiple high-probability quasi-homophones with configurable breadth/depth"
      },
      {
        "type": "primary",
        "category": "Acoustic Modeling",
        "specific": "Mel Spectrogram reconstruction",
        "novel_contribution": "Decoder-to-Mel trained jointly to guide grapheme generation towards pronunciation-preserving outputs"
      },
      {
        "type": "primary",
        "category": "Temporal Modeling",
        "specific": "Duration Predictor (CTC-trained)",
        "novel_contribution": "Pre-trained Conv + LSTM duration predictor aligns phoneme sequence with spectrogram timing to coordinate multi-modal supervision"
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "IPA-to-Grapheme Transformer without audio modality",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Multi-task Learning"
    ],
    "datasets": [
      {
        "name": "Known English homophone lists",
        "type": "public",
        "domain": "lexicon_words",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Top popular websites list (for domain squatting analysis)",
        "type": "public",
        "domain": "domain_names",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "PyPI popular packages (for package squatting analysis)",
        "type": "public",
        "domain": "software_packages",
        "link": "https://pypi.org",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Synthetic TTS audio–Mel spectrogram pairs generated from words (via eSpeak NG + to-Mel)",
        "type": "synthetic",
        "domain": "speech_audio",
        "link": "https://github.com/espeak-ng/espeak-ng",
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "IPA-to-Grapheme Transformer (no acoustic/Mel supervision)",
        "paper_reference": null,
        "metric": "Homophone coverage / quality of quasi-homophones",
        "their_result": null,
        "baseline_result": "Authors state the acoustic-feedback model performs better than the version without audio"
      },
      {
        "method_name": "Manually curated homophone lists",
        "paper_reference": null,
        "metric": "Coverage of known homophones by generator",
        "their_result": "“Sound-skwatter can automatically generated 84% of known English homophones”",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Coverage of known homophones (%)",
      "Proportion of generated domains existing in the wild (%)",
      "Proportion of popular PyPI packages with at least one existing candidate (%)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can an AI system automatically generate high-quality sound-squatting (quasi-homophone) candidates beyond manually curated homophone lists?",
        "Does adding acoustic (Mel spectrogram) supervision improve IPA-to-grapheme generation quality compared to text-only models?",
        "How prevalent are generated sound-squatted domains on the Internet and in software package ecosystems (PyPI)?",
        "Can the approach support cross-language sound-squatting generation?"
      ],
      "gaps_identified": [
        "Existing defenses rely on manually curated homophone lists with limited coverage",
        "Prior solutions operate at word level and ignore sub-word character/syllable substitutions with minor pronunciation impact",
        "Existing approaches are largely same-language; cross-language sound-squatting is underexplored"
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Proactively mitigate sound-squatting by automatically generating pronunciation-similar candidates across languages, overcoming the limitations of hand-crafted homophone lists.",
      "potential_research_ideas": [
        "Train with real human speech corpora to complement TTS-generated spectrograms and study benefit vs. synthetic-only supervision",
        "Extend to registrar risk scoring: combine phonetic likelihood with domain registration/WHOIS/DNS features for prioritization",
        "Cross-language phoneme mapping and accent modeling to target specific L1-L2 confusion matrices",
        "Active learning loop with takedown/abuse feedback to refine candidate ranking",
        "Evaluate/defend against adversarial manipulations of pronunciation and TTS artifacts",
        "Generalize to other ecosystems (npm, Maven, PyPI mirrors) and voice assistants for end-to-end risk assessment"
      ],
      "architectural_improvement_recommendations": [
        "Replace or augment Mel reconstruction branch with self-supervised speech encoders (e.g., wav2vec 2.0/HuBERT/XLS-R) for richer acoustic representations",
        "Introduce contrastive learning between IPA encodings, grapheme outputs, and audio embeddings to enforce pronunciation invariance",
        "Incorporate language-ID and bilingual phoneme alignment layers for robust cross-language generation",
        "Add pronunciation-aware decoding constraints and lexicon priors to reduce implausible outputs",
        "Multi-task with grapheme-to-phoneme (G2P) consistency loss to stabilize IPA–grapheme mappings"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces Sound-skwatter, an AI-powered generator of sound-squatting candidates using a multi-modal Transformer with acoustic supervision",
      "Enables cross-language sound-squatting generation and mitigation",
      "Empirical analyses on domains and PyPI: “∼10% of the generated domains exist in the wild”; “∼17% of the popular [PyPI] packages have at least one existing candidate”",
      "Shows the acoustic-feedback model outperforms a version without the audio component",
      "Provides an online demo and states models and code will be released as open source"
    ]
  },
  {
    "arxiv_id": "2309.09498v2",
    "title": "Combating Advanced Persistent Threats: Challenges and Solutions",
    "authors": "Yuntao Wang; Han Liu; Zhendong Li; Zhou Su; Jiliang Li",
    "abstract": "The rise of advanced persistent threats (APTs) has marked a significant cybersecurity challenge, characterized by sophisticated orchestration, stealthy execution, extended persistence, and targeting valuable assets across diverse sectors. Provenance graph-based kernel-level auditing has emerged as a promising approach to enhance visibility and traceability within intricate network environments. However, it still faces challenges including reconstructing complex lateral attack chains, detecting dynamic evasion behaviors, and defending smart adversarial subgraphs. To bridge the research gap, this paper proposes an efficient and robust APT defense scheme leveraging provenance graphs, including a network-level distributed audit model for cost-effective lateral attack reconstruction, a trust-oriented APT evasion behavior detection strategy, and a hidden Markov model based adversarial subgraph defense approach. Through prototype implementation and extensive experiments, we validate the effectiveness of our system. Lastly, crucial open research directions are outlined in this emerging field.",
    "published_date": "2023-09-18",
    "pdf_link": "https://arxiv.org/pdf/2309.09498v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Advanced Persistent Threat (APT) detection with network-level provenance graphs: lateral movement reconstruction, evasion behavior detection, and adversarial subgraph defense",
      "attack_types": [
        "Advanced Persistent Threat (APT)",
        "Lateral Movement",
        "Evasion (provenance graph evasion behaviors)",
        "Adversarial attacks on provenance graphs (adversarial subgraphs)",
        "Domain Controller Hijacking",
        "Zero-day exploitation (as part of lateral movement chains)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Dimensionality Reduction / Discriminant Analysis",
        "specific": "Linear Discriminant Analysis (LDA)",
        "novel_contribution": "Used to compute projection vectors to weight edges in provenance graphs by maximizing separation of alarm-related vs non-alarm-related edges"
      },
      {
        "type": "primary",
        "category": "Clustering",
        "specific": "K-means++",
        "novel_contribution": "Multi-round clustering of provenance graph edges prior to LDA-based weighting"
      },
      {
        "type": "primary",
        "category": "Probabilistic Graphical Model",
        "specific": "Hidden Markov Model (HMM)",
        "novel_contribution": "Adversarial subgraph detection strategy to improve robustness against adversarial manipulations of provenance subgraphs"
      },
      {
        "type": "primary",
        "category": "Heuristic / Algorithmic",
        "specific": "Causality Preserved Aggregation (CPA)",
        "novel_contribution": "Graph compression to mitigate dependency explosion while preserving causal semantics for efficient network-level auditing"
      },
      {
        "type": "primary",
        "category": "Trust/Scoring Mechanism",
        "specific": null,
        "novel_contribution": "Dynamic trust-oriented evasion behavior detection using a forgetting factor, penalty coefficients, and sequence partitioning of trustworthy/untrustworthy/uncertain operations"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "DARPA Transparent Computing (TC) dataset",
        "type": "public",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "StreamSpot",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "UNICORN",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "ProvDetector",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "SLEUTH",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "NODOZE",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Poirot",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "HOLMES",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "ATLAS",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "DEPIMPACT",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "PROVNINJA",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How can we efficiently reconstruct complex, network-level lateral attack chains from massive, multi-source provenance logs?",
        "How can we dynamically detect APT evasion behaviors that interleave benign IPC and extend attack timelines to evade provenance-based audits?",
        "How can we design robust, self-adaptive defenses against adversarial subgraphs that aim to evade subgraph matching without disrupting attack primitives?"
      ],
      "gaps_identified": [
        "Existing provenance auditing is largely host-level and lacks collaborative, network-level reconstruction of lateral movements",
        "Dependency explosion in host-level provenance graphs impedes practical auditing at scale",
        "Limited consideration of dynamic APT evasion behaviors in prior work",
        "Provenance graph methods are vulnerable to adversarial subgraphs aimed at evading pattern matching",
        "Efficiency and latency shortcomings hinder practical deployment of APT provenance services"
      ],
      "limitations": [
        "Only prototype, lack of large-scale actual deployment",
        "Partly requires prior knowledge"
      ],
      "future_work": [],
      "motivation": "APT campaigns are stealthy, persistent, and coordinated across hosts; existing provenance graph audits struggle with network-level lateral movement reconstruction, dynamic evasion behaviors, and adversarial subgraphs, motivating a more efficient and robust defense scheme.",
      "potential_research_ideas": [
        "Graph neural networks over distributed provenance graphs for end-to-end detection and lateral movement inference with temporal attention",
        "Adversarial training and detection ensembles combining HMMs with discriminative models to harden against adversarial subgraphs",
        "Federated or privacy-preserving cross-host correlation to enable network-level auditing without centralizing raw logs",
        "Active/online learning for adaptive trust thresholds and forgetting factors that respond to evolving evasion tactics",
        "Causal inference methods to distinguish spurious from causal dependencies in provenance graphs to further mitigate dependency explosion",
        "Benchmarking and large-scale real-world deployments to validate robustness and scalability across diverse enterprise environments"
      ],
      "architectural_improvement_recommendations": [
        "Replace or augment LDA with supervised metric learning or graph-based contrastive learning to derive more discriminative edge/vertex weights",
        "Integrate temporal probabilistic models (e.g., HSMMs, CRFs) or neural sequence models for richer temporal correlations in evasion detection",
        "Adopt streaming, distributed processing (e.g., Kafka/Flink) for scalable cross-host provenance ingestion and real-time correlation",
        "Incorporate GNNs with attention over compressed graphs (post-CPA) to jointly learn node/edge importance and lateral paths",
        "Introduce robust aggregation and adversarial defenses (e.g., randomized smoothing, certified defenses) against subgraph perturbations",
        "Develop explainability modules (path attribution/saliency) to accompany alerts and reconstructed chains for analyst triage"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Dependency explosion in large provenance graphs",
        "Coordinating multi-host, network-level auditing and correlation",
        "Detecting and mitigating APT evasion behaviors",
        "Robustness to adversarial subgraph attacks",
        "Lack of large-scale real-world deployment evidence"
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": true,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "A network-level distributed provenance graph audit architecture for lateral attack chain reconstruction",
      "CPA-based graph compression to alleviate dependency explosion while preserving causality",
      "LDA-based weighted graph aggregation using multi-round K-means++ clustering to prioritize attack-relevant edges",
      "Trust-oriented dynamic APT evasion behavior detection with forgetting factor and penalty-based sequence correlation",
      "HMM-based adversarial subgraph detection to improve robustness of provenance auditing",
      "Prototype implementation with experimental validation"
    ]
  },
  {
    "arxiv_id": "2309.10173v2",
    "title": "GCNIDS: Graph Convolutional Network-Based Intrusion Detection System for CAN Bus",
    "authors": "Maloy Kumar Devnath",
    "abstract": "The Controller Area Network (CAN) bus serves as a standard protocol for facilitating communication among various electronic control units (ECUs) within contemporary vehicles. However, it has been demonstrated that the CAN bus is susceptible to remote attacks, which pose risks to the vehicle's safety and functionality. To tackle this concern, researchers have introduced intrusion detection systems (IDSs) to identify and thwart such attacks. In this paper, we present an innovative approach to intruder detection within the CAN bus, leveraging Graph Convolutional Network (GCN) techniques as introduced by Zhang, Tong, Xu, and Maciejewski in 2019. By harnessing the capabilities of deep learning, we aim to enhance attack detection accuracy while minimizing the requirement for manual feature engineering. Our experimental findings substantiate that the proposed GCN-based method surpasses existing IDSs in terms of accuracy, precision, and recall. Additionally, our approach demonstrates efficacy in detecting mixed attacks, which are more challenging to identify than single attacks. Furthermore, it reduces the necessity for extensive feature engineering and is particularly well-suited for real-time detection systems. To the best of our knowledge, this represents the pioneering application of GCN to CAN data for intrusion detection. Our proposed approach holds significant potential in fortifying the security and safety of modern vehicles, safeguarding against attacks and preventing them from undermining vehicle functionality.",
    "published_date": "2023-09-18",
    "pdf_link": "https://arxiv.org/pdf/2309.10173v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Automotive Security",
      "subdomain": "In-vehicle Network Security (CAN)",
      "specific_problem": "Intrusion detection on CAN bus, including detection of mixed attacks (DoS, Fuzzy/Fuzzing, Spoofing, Replay)",
      "attack_types": [
        "DoS",
        "Fuzzy/Fuzzing",
        "Spoofing",
        "Replay",
        "Mixed attacks"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "GNN (Graph Neural Network)",
        "specific": "GCN (Graph Convolutional Network)",
        "novel_contribution": "Applies GCN to CAN data using only two graph-based features (maximum indegree and maximum outdegree) derived from graphs built over windows of CAN messages; targets mixed attacks including Replay with reduced feature engineering."
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "OTIDS (Operational Technology Intrusion Detection System) CAN bus dataset",
        "type": "public",
        "domain": "can_bus_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Refat et al. (2022) graph-property ML IDS",
        "paper_reference": "Refat, Elkhail, Hafeez, and Malik (2022)",
        "metric": "accuracy, precision, recall",
        "their_result": "\"the proposed GCN-based approach outperforms state-of-the-art IDSs in terms of accuracy, precision, and recall\"",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "accuracy",
      "precision",
      "recall"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Minimize the amount of feature engineering compared to existing work (Refat et al., 2022) to ease real-time deployment.",
        "Improve accuracy for 'other' and mixed attacks (combination of DoS, Fuzzy, Spoof, and Replay) without modifying the CAN protocol; prior work did not consider Replay within mixed attacks.",
        "Apply GCN using only two graph-based features (maximum indegree and maximum outdegree) on graph-based CAN data and evaluate if it yields superior results to Refat et al. (2022)."
      ],
      "gaps_identified": [
        "Existing CAN IDSs often detect only specific attack types and struggle with mixed attacks.",
        "Heavy feature engineering in prior works complicates and slows real-time detection.",
        "Prior mixed-attack studies (e.g., Refat et al., 2022) did not include Replay within mixed scenarios.",
        "Need for a generalizable IDS capable of real-time detection across a wide range of attacks on CAN."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "CAN bus is vulnerable to remote attacks; there is a need for robust, generalizable, real-time IDSs that reduce feature engineering and can detect mixed attacks (including Replay).",
      "potential_research_ideas": [
        "Extend to temporal/dynamic GNNs to better capture sequence dynamics of CAN traffic.",
        "Domain adaptation/transfer learning across different vehicles/ECU configurations to improve generalization.",
        "Self-supervised pretraining on large unlabeled CAN logs to reduce labeled data needs.",
        "Evaluate and harden against adversarial examples and poisoning attacks targeting CAN IDSs.",
        "Incorporate additional graph features (e.g., temporal degree changes, edge weights from inter-arrival times) and assess gains vs. complexity.",
        "Online/streaming inference with concept drift detection for long-term deployment.",
        "Cross-dataset benchmarking (OTIDS + other CAN datasets) to validate generality.",
        "Hardware-in-the-loop or on-ECU deployment studies to measure latency and resource usage."
      ],
      "architectural_improvement_recommendations": [
        "Replace/augment GCN with attention-based GNNs (GAT) or GraphSAGE for inductive generalization.",
        "Model temporal relations explicitly via TGAT/TGN or ST-GCN using time-encoded edges/windows.",
        "Use multi-scale windowing and hierarchical graphs to capture both short- and long-range dependencies.",
        "Incorporate edge features (e.g., time deltas, DLC, payload entropy) and message frequency into GNN inputs.",
        "Address class imbalance with focal loss or cost-sensitive training; calibrate decision thresholds for mixed attacks.",
        "Add uncertainty estimation (MC Dropout/Deep Ensembles) for risk-aware alerts in real-time systems."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Automotive in-vehicle CAN bus (data captured from a moving vehicle in OT environment)",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces a GCN-based IDS for CAN bus using only two graph features (max indegree, max outdegree) constructed from windows of CAN messages.",
      "Claims to be the first application of GCN to CAN data for intrusion detection.",
      "Demonstrates improved accuracy, precision, and recall over prior IDSs and effectiveness on mixed attacks (including Replay).",
      "Reduces feature engineering effort and argues suitability for real-time detection."
    ]
  },
  {
    "arxiv_id": "2309.14134v1",
    "title": "One-Class Classification for Intrusion Detection on Vehicular Networks",
    "authors": "Jake Guidry; Fahad Sohrab; Raju Gottumukkala; Satya Katragadda; Moncef Gabbouj",
    "abstract": "Controller Area Network bus systems within vehicular networks are not equipped with the tools necessary to ward off and protect themselves from modern cyber-security threats. Work has been done on using machine learning methods to detect and report these attacks, but common methods are not robust towards unknown attacks. These methods usually rely on there being a sufficient representation of attack data, which may not be available due to there either not being enough data present to adequately represent its distribution or the distribution itself is too diverse in nature for there to be a sufficient representation of it. With the use of one-class classification methods, this issue can be mitigated as only normal data is required to train a model for the detection of anomalous instances. Research has been done on the efficacy of these methods, most notably One-Class Support Vector Machine and Support Vector Data Description, but many new extensions of these works have been proposed and have yet to be tested for injection attacks in vehicular networks. In this paper, we investigate the performance of various state-of-the-art one-class classification methods for detecting injection attacks on Controller Area Network bus traffic. We investigate the effectiveness of these techniques on attacks launched on Controller Area Network buses from two different vehicles during normal operation and while being attacked. We observe that the Subspace Support Vector Data Description method outperformed all other tested methods with a Gmean of about 85%.",
    "published_date": "2023-09-25",
    "pdf_link": "https://arxiv.org/pdf/2309.14134v1",
    "paper_types": [
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Automotive Security",
      "subdomain": "In-vehicle Network Intrusion Detection (CAN bus IDS)",
      "specific_problem": "Detection of CAN bus injection attacks using one-class classification trained only on normal traffic",
      "attack_types": [
        "Random ID Attack (DoS)",
        "Zero ID Attack (DoS)",
        "Replay Attack",
        "Injection attacks on CAN bus"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Kernel method / One-Class",
        "specific": "SVDD (Support Vector Data Description)",
        "novel_contribution": "Applied and evaluated on vehicular CAN injection-attack detection with timing-based features"
      },
      {
        "type": "primary",
        "category": "Kernel method / One-Class",
        "specific": "S-SVDD (Subspace Support Vector Data Description)",
        "novel_contribution": "Evaluated on vehicular CAN injection-attack detection; found best with \"Gmean of about 85%\""
      },
      {
        "type": "primary",
        "category": "Kernel method / One-Class",
        "specific": "E-SVDD (Ellipsoidal SVDD)",
        "novel_contribution": "Applied and evaluated on vehicular CAN injection-attack detection"
      },
      {
        "type": "primary",
        "category": "Kernel method / One-Class + Graph embedding",
        "specific": "GE-SVDD (Graph-Embedded SVDD)",
        "novel_contribution": "Applied and evaluated on vehicular CAN injection-attack detection"
      },
      {
        "type": "primary",
        "category": "SVM / One-Class",
        "specific": "OC-SVM (One-Class SVM)",
        "novel_contribution": "Applied and evaluated on vehicular CAN injection-attack detection"
      },
      {
        "type": "primary",
        "category": "SVM / One-Class + Graph embedding",
        "specific": "OC-GE-SVM (One-Class Graph-Embedded SVM)",
        "novel_contribution": "Applied and evaluated on vehicular CAN injection-attack detection"
      }
    ],
    "learning_paradigm": [
      "Unsupervised",
      "One-class / Anomaly Detection"
    ],
    "datasets": [
      {
        "name": "Nissan Leaf CAN bus traffic (normal + injected attacks)",
        "type": "proprietary",
        "domain": "automotive_can_bus",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "Chevy Volt CAN bus traffic (normal + injected attacks)",
        "type": "proprietary",
        "domain": "automotive_can_bus",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Gmean",
      "True Positive Rate (TPR)",
      "True Negative Rate (TNR)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can one-class classification methods trained only on normal CAN traffic effectively detect injection attacks on vehicular networks?",
        "Among several state-of-the-art one-class methods (SVDD, S-SVDD, E-SVDD, GE-SVDD, OC-SVM, OC-GE-SVM), which performs best for CAN injection attack detection?",
        "How do these methods perform across data from two different vehicles and across different attack types (Random ID, Zero ID, Replay)?"
      ],
      "gaps_identified": [
        "Common ML-based IDS require sufficient attack data and are not robust to unknown attacks.",
        "Many newer one-class extensions (e.g., S-SVDD, E-SVDD, graph-embedded variants) had not been tested for CAN injection attacks."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Improve robustness of vehicular CAN IDS to unknown attacks by using one-class methods trained only on normal traffic; evaluate newer one-class variants on real CAN injection scenarios.",
      "potential_research_ideas": [
        "Develop multi-feature one-class IDS combining timing features with payload entropy/bit-level statistics while preserving OEM-agnostic deployment.",
        "Domain adaptation or transfer learning for cross-vehicle generalization of one-class CAN models.",
        "Self-supervised pretraining on large normal CAN logs (contrastive or masked-time modeling) before one-class decision fitting.",
        "Online/streaming one-class learning with concept drift detection for evolving CAN traffic patterns.",
        "Synthetic attack generation with generative models to stress-test one-class boundaries and calibrate thresholds per CAN ID.",
        "Ensemble of heterogeneous one-class models (e.g., SVDD + deep autoencoder) with score-level fusion and uncertainty calibration.",
        "Explainability for one-class IDS to attribute anomalies to specific IDs/time-patterns to aid remediation."
      ],
      "architectural_improvement_recommendations": [
        "Augment the feature space with per-ID burstiness, inter-ID transition probabilities, and short-window temporal embeddings; then fit S-SVDD in the learned subspace.",
        "Learn an explicit CAN-ID relationship graph (e.g., from co-occurrence/causality) to better utilize GE-SVDD/OC-GE-SVM.",
        "Kernel selection and multiple-kernel learning within SVDD/S-SVDD to capture heterogeneous per-ID behaviors.",
        "Calibrate decision thresholds per CAN ID or per subspace cluster rather than a single global threshold.",
        "Incorporate incremental training for S-SVDD to support on-vehicle updates with new normal data without full retraining."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "python-can",
        "Linux socketCAN"
      ],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "In-vehicle CAN bus via OBD-II; envisioned security module reporting to a central management system",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Resource constraints on in-vehicle modules motivate low-complexity models versus deep learning approaches.",
        "Variability of CAN IDs and traffic patterns across vehicle models/manufacturers.",
        "Detecting unknown/novel attack types without labeled anomalies."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Empirical evaluation of multiple state-of-the-art one-class classifiers (SVDD, S-SVDD, E-SVDD, GE-SVDD, OC-SVM, OC-GE-SVM) for CAN bus intrusion detection.",
      "Introduces a CAN IDS workflow and a vehicle-agnostic timing-feature extraction (frequency, inter-arrival time, transmission-time std) per CAN ID.",
      "Collected and evaluated on real CAN traffic from two production EVs (2011 Nissan Leaf and Chevy Volt) under normal operation and during injection attacks (Random ID, Zero ID, Replay).",
      "Key finding: \"the Subspace Support Vector Data Description method outperformed all other tested methods with a Gmean of about 85%.\""
    ]
  },
  {
    "arxiv_id": "2309.15995v1",
    "title": "Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems",
    "authors": "Qinghua Xu; Shaukat Ali; Tao Yue",
    "abstract": "Anomaly detection is critical to ensure the security of cyber-physical systems (CPS). However, due to the increasing complexity of attacks and CPS themselves, anomaly detection in CPS is becoming more and more challenging. In our previous work, we proposed a digital twin-based anomaly detection method, called ATTAIN, which takes advantage of both historical and real-time data of CPS. However, such data vary significantly in terms of difficulty. Therefore, similar to human learning processes, deep learning models (e.g., ATTAIN) can benefit from an easy-to-difficult curriculum. To this end, in this paper, we present a novel approach, named digitaL twin-based Anomaly deTecTion wIth Curriculum lEarning (LATTICE), which extends ATTAIN by introducing curriculum learning to optimize its learning paradigm. LATTICE attributes each sample with a difficulty score, before being fed into a training scheduler. The training scheduler samples batches of training data based on these difficulty scores such that learning from easy to difficult data can be performed. To evaluate LATTICE, we use five publicly available datasets collected from five real-world CPS testbeds. We compare LATTICE with ATTAIN and two other state-of-the-art anomaly detectors. Evaluation results show that LATTICE outperforms the three baselines and ATTAIN by 0.906%-2.367% in terms of the F1 score. LATTICE also, on average, reduces the training time of ATTAIN by 4.2% on the five datasets and is on par with the baselines in terms of detection delay time.",
    "published_date": "2023-09-27",
    "pdf_link": "https://arxiv.org/pdf/2309.15995v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Cyber-Physical Systems Security",
      "subdomain": "Anomaly Detection / Intrusion Detection",
      "specific_problem": "Digital twin-based anomaly detection for CPS time-series using curriculum learning to schedule training from easy to difficult samples",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Curriculum Learning",
        "specific": null,
        "novel_contribution": "Introduces predefined and automatic difficulty measurers plus a training scheduler tailored for CPS time-series within a digital twin anomaly detector (LATTICE)"
      },
      {
        "type": "primary",
        "category": "GAN",
        "specific": null,
        "novel_contribution": "Uses GAN as the backbone of the digital twin capability (DTC) for anomaly detection"
      },
      {
        "type": "primary",
        "category": "GNN",
        "specific": "Graph Convolutional Network (GCN)",
        "novel_contribution": "Employs GCN (instead of CNN) in the generator to better capture spatial correlations among CPS sensors/actuators"
      },
      {
        "type": "primary",
        "category": "RNN",
        "specific": "LSTM",
        "novel_contribution": "Uses LSTM to capture temporal characteristics of CPS time-series"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Semi-supervised",
      "Online Learning"
    ],
    "datasets": [
      {
        "name": "Secure Water Treatment (SWaT)",
        "type": "public",
        "domain": "industrial_control_sensors_actuators_time_series",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Water Distribution (WADI)",
        "type": "public",
        "domain": "industrial_control_sensors_actuators_time_series",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "BATADAL (Battle of the Attack Detection Algorithms)",
        "type": "public",
        "domain": "water_distribution_ics_time_series",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "PHM Challenge 2015 dataset",
        "type": "public",
        "domain": "cps_time_series",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Gas Pipeline Dataset",
        "type": "public",
        "domain": "pipeline_ics_time_series",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "ATTAIN",
        "paper_reference": null,
        "metric": "F1 score",
        "their_result": "“LATTICE outperforms the three baselines and ATTAIN by 0.906%-2.367% in terms of the F1 score.”",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "F1 score",
      "Unit Training Time (UTT)",
      "Detection Delay Time (DDT)"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Most CPS anomaly detectors are trained on static data and cannot keep learning during operation (runtime).",
        "Neural network-based detectors require large amounts of labeled data, which are expensive in CPS.",
        "Few works explore curriculum learning for CPS time-series data that are inherently chronological.",
        "Predefined CL can be inflexible and not adapt to novel scenarios; automatic CL for CPS is underexplored."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Improve CPS anomaly detection by leveraging both historical and live data in a digital twin with curriculum learning to handle varying sample difficulty and reduce labeling needs while enabling continuous/online learning.",
      "potential_research_ideas": [
        "Design new automatic difficulty measurers that leverage causal relations among sensors/actuators to better reflect anomaly complexity.",
        "Develop reinforcement learning-based schedulers that adapt curriculum pacing online to plant dynamics and concept drift.",
        "Extend LATTICE to multi-plant transfer scenarios with meta-learning to rapidly adapt curricula to new CPS with minimal labeled data.",
        "Incorporate uncertainty estimation and active learning to selectively request labels from operators/Digital Twin when uncertainty is high.",
        "Investigate adversarial robustness of curriculum strategies against malicious manipulation of difficulty scores or live data streams.",
        "Augment with explainability modules that attribute detected anomalies to specific sensors/actuators and transitions in the timed automaton."
      ],
      "architectural_improvement_recommendations": [
        "Replace or complement the GAN backbone with diffusion or autoregressive density models for improved time-series fidelity and stability.",
        "Adopt a unified spatio-temporal architecture (e.g., ST-GNN or Temporal Transformer over sensor graphs) to jointly model space-time dependencies.",
        "Use a teacher–student framework where the DTM acts as a calibrated teacher providing soft labels/confidence for curriculum weighting.",
        "Implement curriculum pacing functions that consider both loss and temporal novelty (e.g., recency-weighted difficulty) for online adaptation.",
        "Introduce uncertainty-aware scoring (e.g., MC dropout, deep ensembles) to inform automatic difficulty estimation and scheduling.",
        "Apply continual learning regularizers (e.g., EWC, replay buffers) to mitigate forgetting when training with live data under shifting distributions."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/xuqinghua-China/tosem/tree/master",
      "frameworks": [],
      "reproducibility_score": "high",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "CPS/ICS testbeds (water treatment, water distribution, gas pipeline)",
      "scalability_discussed": true,
      "inference_time": "Measured via Detection Delay Time (DDT); reported on-par with baselines and 0.2% average improvement in experiments",
      "deployment_challenges": [
        "Requires constructing/maintaining a digital twin model (timed automaton) aligned with the operating CPS",
        "Quality and availability of live data streams and correct labeling from the DTM",
        "Handling distribution shift and concept drift in online settings within tight ICS latency constraints"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Design of a generic framework that combines curriculum learning with a digital twin-based anomaly detector (extends ATTAIN).",
      "Novel difficulty measurers for CPS time-series: predefined and automatic (context-based), plus a training scheduler for easy-to-difficult sampling.",
      "Extensive empirical study on five real-world CPS datasets, adding two case studies beyond prior work to demonstrate generalizability.",
      "Comprehensive analyses with statistical tests, using coarse- and fine-grained effectiveness metrics and efficiency metrics (UTT, DDT).",
      "Reported improvements: “LATTICE outperforms the three baselines and ATTAIN by 0.906%-2.367% in terms of the F1 score” and “improves UTT by 4.2% and DDT by 0.2% on average.”"
    ]
  },
  {
    "arxiv_id": "2310.09571v1",
    "title": "On the Feasibility of Cross-Language Detection of Malicious Packages in npm and PyPI",
    "authors": "Piergiorgio Ladisa; Serena Elisa Ponta; Nicola Ronzoni; Matias Martinez; Olivier Barais",
    "abstract": "Current software supply chains heavily rely on open-source packages hosted in public repositories. Given the popularity of ecosystems like npm and PyPI, malicious users started to spread malware by publishing open-source packages containing malicious code. Recent works apply machine learning techniques to detect malicious packages in the npm ecosystem. However, the scarcity of samples poses a challenge to the application of machine learning techniques in other ecosystems. Despite the differences between JavaScript and Python, the open-source software supply chain attacks targeting such languages show noticeable similarities (e.g., use of installation scripts, obfuscated strings, URLs).   In this paper, we present a novel approach that involves a set of language-independent features and the training of models capable of detecting malicious packages in npm and PyPI by capturing their commonalities. This methodology allows us to train models on a diverse dataset encompassing multiple languages, thereby overcoming the challenge of limited sample availability. We evaluate the models both in a controlled experiment (where labels of data are known) and in the wild by scanning newly uploaded packages for both npm and PyPI for 10 days.   We find that our approach successfully detects malicious packages for both npm and PyPI. Over an analysis of 31,292 packages, we reported 58 previously unknown malicious packages (38 for npm and 20 for PyPI), which were consequently removed from the respective repositories.",
    "published_date": "2023-10-14",
    "pdf_link": "https://arxiv.org/pdf/2310.09571v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software Supply Chain Security",
      "subdomain": "Malicious Package Detection",
      "specific_problem": "Cross-language machine-learning detection of malicious open-source packages in npm (JavaScript) and PyPI (Python) using language-independent features",
      "attack_types": [
        "Open-source software supply chain attacks",
        "Malicious package publication from scratch",
        "Name confusion attacks (e.g., typosquatting/name confusion)",
        "Subversion of legitimate packages",
        "Installation-time execution via install hooks",
        "Runtime/test-time malicious code execution",
        "Reverse shell",
        "Dropper",
        "Data exfiltration",
        "Denial of Service (DoS)",
        "Financially-motivated malware"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Gradient Boosted Trees",
        "specific": "XGBoost",
        "novel_contribution": "Cross-language classifier trained on 141 language-independent lexical/structural features extracted from npm and PyPI packages"
      },
      {
        "type": "baseline",
        "category": "Decision Tree",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Random Forest",
        "specific": null,
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Backstabber’s Knife Collection (BKC) (commit c2d9691, Oct 2022)",
        "type": "public",
        "domain": "malicious_packages",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Benign popular npm packages selected via libraries.io SourceRank",
        "type": "proprietary",
        "domain": "package_repositories",
        "link": "https://docs.libraries.io/overview.html#sourcerank",
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "Benign popular PyPI packages selected via libraries.io SourceRank",
        "type": "proprietary",
        "domain": "package_repositories",
        "link": "https://docs.libraries.io/overview.html#sourcerank",
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "Mono-language dataset (JavaScript): 102 malicious (from BKC) + 918 benign",
        "type": "proprietary",
        "domain": "source_code_and_install_scripts",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "Mono-language dataset (Python): 92 malicious (from BKC) + 828 benign",
        "type": "proprietary",
        "domain": "source_code_and_install_scripts",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "Cross-language dataset (JavaScript + Python): 194 malicious + 1,640 benign",
        "type": "proprietary",
        "domain": "source_code_and_install_scripts",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "In-the-wild 10-day stream of newly uploaded packages to npm and PyPI (31,292 packages)",
        "type": "proprietary",
        "domain": "package_repositories",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Decision Tree",
        "paper_reference": null,
        "metric": "precision, recall (5-fold cross-validation optimizing precision)",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Random Forest",
        "paper_reference": null,
        "metric": "precision, recall (5-fold cross-validation optimizing precision)",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Mono-language XGBoost (JavaScript)",
        "paper_reference": null,
        "metric": "precision, recall; in-the-wild detection effectiveness",
        "their_result": "Cross-language XGBoost outperformed mono-language models in the 10-day real-world assessment",
        "baseline_result": null
      },
      {
        "method_name": "Mono-language XGBoost (Python)",
        "paper_reference": null,
        "metric": "precision, recall; in-the-wild detection effectiveness",
        "their_result": "Cross-language XGBoost outperformed mono-language models in the 10-day real-world assessment",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "precision",
      "recall"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "RQ1: Which cross-language and mono-language models demonstrate the best trade-off between precision and recall when detecting malicious packages in the case of JavaScript and Python languages?",
        "RQ2: How do the models identified in RQ1 perform in the detection of potentially malicious packages in the wild?"
      ],
      "gaps_identified": [
        "Scarcity of labeled malicious samples, especially outside the npm ecosystem, hampers ML application across ecosystems",
        "Attackers employ a wide variety of evasion techniques, challenging detection approaches",
        "Existing datasets like BKC contain duplicates and campaign-related redundancies that can bias training",
        "Need to enrich malicious package datasets to capture new and emerging behaviors"
      ],
      "limitations": [
        "Approach limited to malicious behaviors represented in BKC (reverse shell, dropper, data exfiltration, DoS, financial gain); other behaviors (e.g., phishing campaigns) are out of scope",
        "Language-independent features exclude language-specific API usage, potentially missing signals tied to particular ecosystems"
      ],
      "future_work": [],
      "motivation": "Overcome limited availability of malicious samples per ecosystem by learning cross-language commonalities in malware targeting npm and PyPI, enabling practical detection to secure OSS supply chains.",
      "potential_research_ideas": [
        "Extend cross-language detection to additional ecosystems (e.g., RubyGems, Maven, NuGet) to test generalization and build multi-ecosystem models",
        "Combine language-independent features with lightweight language-specific semantic features (e.g., sensitive API usage) via multi-view learning",
        "Incorporate dynamic/sandbox analysis of install hooks to detect behaviors that evade static lexical/structural features",
        "Leverage semi-supervised or positive-unlabeled learning to exploit large unlabeled corpora from registries",
        "Use representation learning on code (e.g., code transformers or graph-based code property graphs) to augment handcrafted features",
        "Develop online/continual learning with drift detection to adapt to evolving attacker tactics in registries",
        "Integrate external threat intelligence (URL/domain/IP reputation, VT reports) to enrich features for exfiltration/C2 indicators",
        "Explore weak supervision (labeling functions) to bootstrap labels across ecosystems",
        "Assess and harden against adversarial manipulations (e.g., obfuscation strategies targeting entropy thresholds)"
      ],
      "architectural_improvement_recommendations": [
        "Build an ensemble that stacks cross-language XGBoost with mono-language specialists and cost-sensitive calibration",
        "Apply threshold calibration (e.g., Platt scaling/Isotonic) per-ecosystem to control precision-recall trade-offs",
        "Adopt gradient-boosting variants with class-balanced loss or focal loss to further address imbalance",
        "Introduce feature selection/regularization to reduce redundancy among 141 features and improve robustness",
        "Add graph/code-structure features (AST/CFG/PDG) for install scripts while keeping cross-language generality via normalized schemas",
        "Implement active learning loop with analyst feedback to prioritize uncertain samples from daily streams"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "scikit-learn",
        "XGBoost",
        "Pygments"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Online registry scanning of newly uploaded npm and PyPI packages (10-day period)",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Minimizing false positives to reduce analyst triage burden (precision optimized)",
        "Adapting to evolving attacker evasion techniques",
        "Limited ground-truth for benign packages and class imbalance"
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Propose 141 language-independent lexical/structural features for detecting malicious packages across JavaScript and Python",
      "Train and evaluate Decision Tree, Random Forest, and XGBoost models on mono-language and cross-language datasets; XGBoost performed best in controlled experiments",
      "Conduct a 10-day real-world scan of newly uploaded npm and PyPI packages; cross-language model outperformed mono-language models",
      "Detected and responsibly disclosed 58 previously unknown malicious packages (38 npm, 20 PyPI) across 31,292 analyzed packages; repositories removed them",
      "Enhanced the BKC dataset by uploading the newly identified malicious packages"
    ]
  },
  {
    "arxiv_id": "2309.08208v1",
    "title": "HM-Conformer: A Conformer-based audio deepfake detection system with hierarchical pooling and multi-level classification token aggregation methods",
    "authors": "Hyun-seo Shin; Jungwoo Heo; Ju-ho Kim; Chan-yeong Lim; Wonbin Kim; Ha-Jin Yu",
    "abstract": "Audio deepfake detection (ADD) is the task of detecting spoofing attacks generated by text-to-speech or voice conversion systems. Spoofing evidence, which helps to distinguish between spoofed and bona-fide utterances, might exist either locally or globally in the input features. To capture these, the Conformer, which consists of Transformers and CNN, possesses a suitable structure. However, since the Conformer was designed for sequence-to-sequence tasks, its direct application to ADD tasks may be sub-optimal. To tackle this limitation, we propose HM-Conformer by adopting two components: (1) Hierarchical pooling method progressively reducing the sequence length to eliminate duplicated information (2) Multi-level classification token aggregation method utilizing classification tokens to gather information from different blocks. Owing to these components, HM-Conformer can efficiently detect spoofing evidence by processing various sequence lengths and aggregating them. In experimental results on the ASVspoof 2021 Deepfake dataset, HM-Conformer achieved a 15.71% EER, showing competitive performance compared to recent systems.",
    "published_date": "2023-09-15",
    "pdf_link": "https://arxiv.org/pdf/2309.08208v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Biometric Security",
      "subdomain": "Voice Anti-Spoofing / Deepfake Detection",
      "specific_problem": "Detecting spoofed speech (TTS/VC) in audio deepfake detection for ASV anti-spoofing",
      "attack_types": [
        "Text-to-Speech (TTS)",
        "Voice Conversion (VC)",
        "Deepfake speech"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer + CNN (Conformer)",
        "specific": "Conformer encoder with MHSA and convolution modules",
        "novel_contribution": "Adapted for many-to-one ADD with hierarchical pooling between blocks and multi-level CLS token aggregation trained via auxiliary losses"
      },
      {
        "type": "primary",
        "category": "Pooling",
        "specific": "Hierarchical pooling (downsampling between Conformer blocks)",
        "novel_contribution": "Progressively reduces token sequence length (γ=2) after specific blocks to remove duplicated information and lower computation"
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "Multi-level CLS token aggregation (MCA)",
        "novel_contribution": "Inserts multiple CLS tokens and uses stage-wise classifiers with OC-Softmax auxiliary losses to aggregate discriminative information across blocks"
      },
      {
        "type": "primary",
        "category": "Loss Function",
        "specific": "OC-Softmax",
        "novel_contribution": "Used as the supervision for each stage/classifier with tuned weights (best 4:3:2:1:1) to emphasize lower-layer guidance"
      },
      {
        "type": "primary",
        "category": "Pooling",
        "specific": "SeqPooling",
        "novel_contribution": "Global pooling to produce final embedding for classification"
      },
      {
        "type": "baseline",
        "category": "Transformer + CNN (Conformer)",
        "specific": "Vanilla Conformer adapted with global pooling",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "LFCC-LCNN",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "SE-Rawformer",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "GNN",
        "specific": "AASIST (spectro-temporal graph attention networks)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "SFR-CNN",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "ASVspoof 2019 Logical Access (LA) - train/dev",
        "type": "public",
        "domain": "audio_speech",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "ASVspoof 2021 Deepfake (DF) - evaluation",
        "type": "public",
        "domain": "audio_speech",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "LFCC-LCNN",
        "paper_reference": "Wang et al., 2021",
        "metric": "EER (%)",
        "their_result": "15.71% EER (HM-Conformer)",
        "baseline_result": "23.48% EER"
      },
      {
        "method_name": "SE-Rawformer",
        "paper_reference": "Liu et al., 2023",
        "metric": "EER (%)",
        "their_result": "15.71% EER (HM-Conformer)",
        "baseline_result": "21.65% EER (authors' implementation)"
      },
      {
        "method_name": "AASIST",
        "paper_reference": "Jung et al., 2022",
        "metric": "EER (%)",
        "their_result": "15.71% EER (HM-Conformer)",
        "baseline_result": "20.04% EER"
      },
      {
        "method_name": "SFR-CNN",
        "paper_reference": "Yamagishi et al., 2021 (ASVspoof 2021)",
        "metric": "EER (%)",
        "their_result": "15.71% EER (HM-Conformer)",
        "baseline_result": "19.22% EER"
      },
      {
        "method_name": "Conformer (baseline)",
        "paper_reference": null,
        "metric": "EER (%)",
        "their_result": "15.71% EER (HM-Conformer)",
        "baseline_result": "18.91% EER"
      }
    ],
    "performance_metrics_used": [
      "Equal Error Rate (EER)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Vanilla Conformer, designed for sequence-to-sequence tasks, is sub-optimal for many-to-one audio deepfake detection.",
        "Transformer-based token representations become redundant across layers; classification benefits from compact features via downsampling.",
        "Spoofing evidence exists at both local and global levels; need architectures that jointly capture and aggregate multi-scale information."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Adapt Conformer to audio deepfake detection by reducing sequence redundancy and aggregating multi-level discriminative features to better capture local/global spoofing cues and improve efficiency.",
      "potential_research_ideas": [
        "Learnable hierarchical pooling (e.g., gated/dynamic pooling or token merging) instead of fixed γ=2 downsampling.",
        "Cross-stage attention to fuse CLS tokens across stages rather than simple concatenation.",
        "Self-supervised pretraining on large-scale speech to initialize Conformer and CLS tokens for improved generalization to unseen spoofing methods.",
        "Multimodal deepfake detection by incorporating lip video or text transcripts aligned with audio for cross-modal consistency checks.",
        "Domain generalization and test-time adaptation methods targeting unseen codecs, languages, and speakers.",
        "Adversarial training and evaluation against gradient-based and black-box attacks for robustness.",
        "Calibration and score fusion with speaker verification back-ends for integrated ASV+CM operation.",
        "Phase-based or raw-waveform front-ends and multi-resolution features to complement LFCCs.",
        "Contrastive or metric-learning objectives (e.g., supervised contrastive, AAM-Softmax) alongside OC-Softmax.",
        "Lightweight/distilled HM-Conformer variants for on-device deployment."
      ],
      "architectural_improvement_recommendations": [
        "Replace fixed-rate pooling with learnable token merging or adaptive pooling driven by attention entropy.",
        "Introduce cross-stage transformers that attend among stage-wise CLS tokens before the final classifier.",
        "Use multi-branch multi-scale encoders with different receptive fields and fuse via attention.",
        "Incorporate channel attention (e.g., SE blocks) within Conformer convolution modules.",
        "Jointly optimize a calibration head (e.g., temperature scaling) for better thresholding under distribution shift.",
        "Regularize with stochastic depth and token dropout to improve generalization to unseen attacks."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/talkingnow/HM-Conformer",
      "frameworks": [],
      "reproducibility_score": "high",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Propose HM-Conformer: a Conformer-based ADD system with hierarchical pooling and multi-level CLS token aggregation (MCA).",
      "Achieves 15.71% EER on ASVspoof 2021 DF evaluation, an ~16% relative improvement over a Conformer baseline (18.91% EER).",
      "Validate hierarchical pooling with multiple pooling layers; max and top-k pooling perform best among tested options.",
      "Demonstrate MCA effectiveness via auxiliary OC-Softmax losses; best performance with loss weights 4:3:2:1:1.",
      "Release implementation details and code for reproducibility."
    ]
  },
  {
    "arxiv_id": "2310.10667v1",
    "title": "Enhancing Network Resilience through Machine Learning-powered Graph Combinatorial Optimization: Applications in Cyber Defense and Information Diffusion",
    "authors": "Diksha Goel",
    "abstract": "With the burgeoning advancements of computing and network communication technologies, network infrastructures and their application environments have become increasingly complex. Due to the increased complexity, networks are more prone to hardware faults and highly susceptible to cyber-attacks. Therefore, for rapidly growing network-centric applications, network resilience is essential to minimize the impact of attacks and to ensure that the network provides an acceptable level of services during attacks, faults or disruptions. In this regard, this thesis focuses on developing effective approaches for enhancing network resilience. Existing approaches for enhancing network resilience emphasize on determining bottleneck nodes and edges in the network and designing proactive responses to safeguard the network against attacks. However, existing solutions generally consider broader application domains and possess limited applicability when applied to specific application areas such as cyber defense and information diffusion, which are highly popular application domains among cyber attackers.   This thesis aims to design effective, efficient and scalable techniques for discovering bottleneck nodes and edges in the network to enhance network resilience in cyber defense and information diffusion application domains. We first investigate a cyber defense graph optimization problem, i.e., hardening active directory systems by discovering bottleneck edges in the network. We then study the problem of identifying bottleneck structural hole spanner nodes, which are crucial for information diffusion in the network. We transform both problems into graph-combinatorial optimization problems and design machine learning based approaches for discovering bottleneck points vital for enhancing network resilience.",
    "published_date": "2023-09-22",
    "pdf_link": "https://arxiv.org/pdf/2310.10667v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Enterprise Security",
      "subdomain": "Active Directory Security / Attack Graph Hardening",
      "specific_problem": "Discovering and blocking bottleneck edges in AD attack graphs to minimize attacker success; identifying bottleneck structural hole spanner nodes for resilient information diffusion",
      "attack_types": [
        "attack graph traversal",
        "identity attacks (BloodHound snowball attack)",
        "domain admin compromise"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Neural Network",
        "specific": null,
        "novel_contribution": "Neural network-based dynamic program to accurately learn the attacker policy used within a defender-planning loop for edge blocking in AD graphs."
      },
      {
        "type": "primary",
        "category": "Evolutionary Computation",
        "specific": "Evolutionary Diversity Optimization (EDO)",
        "novel_contribution": "EDO used to generate diverse and effective edge-blocking defensive plans; later extended with a critic network to guide diversity optimization."
      },
      {
        "type": "primary",
        "category": "Reinforcement Learning",
        "specific": "PPO (Proximal Policy Optimization)",
        "novel_contribution": "Attacker policy trained via PPO across multiple defensive-plan environments to improve defender hardening in large-scale AD graphs."
      },
      {
        "type": "primary",
        "category": "Graph Neural Network",
        "specific": "GraphSHS",
        "novel_contribution": "A GNN model that transforms SHS identification into a supervised learning problem using network features and tailored aggregation to detect bottleneck structural hole spanner nodes."
      },
      {
        "type": "primary",
        "category": "Meta-learning",
        "specific": "Meta-GraphSHS",
        "novel_contribution": "Meta-learned GNN that learns generalizable knowledge across diverse graphs and can be fine-tuned to new unseen graphs for SHS discovery."
      },
      {
        "type": "primary",
        "category": "Graph Neural Network",
        "specific": "GNN-SHS",
        "novel_contribution": "A GNN treating dynamic networks as snapshots to learn SHS nodes efficiently; paired with an incremental algorithm for updates."
      },
      {
        "type": "baseline",
        "category": "Centrality Measures",
        "specific": "Betweenness Centrality (BC)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Centrality Measures",
        "specific": "Closeness Centrality (CC)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Reinforcement Learning",
      "Meta-learning"
    ],
    "datasets": [
      {
        "name": "Synthetic AD attack graphs (R500, R1000, R2000)",
        "type": "synthetic",
        "domain": "attack_graphs_active_directory",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Synthetic SHS datasets (ER, Scale-Free)",
        "type": "synthetic",
        "domain": "social_networks / information diffusion graphs",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Real-world SHS datasets (names not provided in excerpt)",
        "type": "public",
        "domain": "social_networks / information diffusion graphs",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Dynamic network datasets for SHS (real-world and synthetic; names not provided in excerpt)",
        "type": "unknown",
        "domain": "dynamic_social_networks / information diffusion graphs",
        "link": null,
        "is_new_contribution": false,
        "availability": "unknown"
      }
    ],
    "baselines": [
      {
        "method_name": "Betweenness Centrality (BC)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Closeness Centrality (CC)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "attacker success rate",
      "attacker’s achievable reward",
      "classification accuracy",
      "runtime",
      "speedup",
      "generalization accuracy",
      "MSE (loss)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How to discover and block bottleneck edges in Active Directory attack graphs to minimize attacker success while scaling to large graphs?",
        "How to effectively identify bottleneck structural hole spanner (SHS) nodes in large-scale, diverse, and dynamic networks to enhance network resilience and information diffusion control?"
      ],
      "gaps_identified": [
        "Existing resilience approaches focus on broad domains and have limited applicability to specific domains like cyber defense and information diffusion.",
        "Need for effective, efficient, and scalable techniques tailored to AD hardening and SHS identification.",
        "Static recomputation for dynamic networks is inefficient; incremental, update-aware methods are required."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Network infrastructures are increasingly complex and prone to attacks and faults; enhancing network resilience requires discovering bottleneck nodes/edges and proactive defense tailored to sensitive domains such as cyber defense (AD) and information diffusion.",
      "potential_research_ideas": [
        "Evaluate the AD hardening methods on real enterprise AD graphs collected via tools like BloodHound to validate robustness and practicality.",
        "Incorporate uncertainty and time-varying detection/failure probabilities into the attacker/defender models to reflect operational environments.",
        "Integrate privilege and credential semantics (e.g., ACL types, session edges) into the kernelization and learning policies for more realistic AD defenses.",
        "Unify the decremental Tracking-SHS algorithm with GNN-SHS via neural algorithmic reasoning to enable learned, incremental updates.",
        "Extend Meta-GraphSHS with domain adaptation techniques for cross-domain transfer (e.g., from social to communication networks).",
        "Add causal/contrastive explanations for SHS and AD edge selections to improve operator trust and actionability.",
        "Investigate robust training of attacker policies against defender-adaptive perturbations (bi-level or Stackelberg training)."
      ],
      "architectural_improvement_recommendations": [
        "Use graph transformers or attention-based message passing within GraphSHS/Meta-GraphSHS to capture higher-order, long-range dependencies.",
        "Employ curriculum and population-based training for PPO attacker with adversarial environment schedules to improve convergence and robustness.",
        "Augment EDO with surrogate modeling (e.g., critic/value networks) and Bayesian optimization for sample-efficient defense search.",
        "Introduce heterogeneous-edge and node-type encodings in AD graphs (users, groups, computers, ACL types) with relational GNNs (R-GCN) for the attacker/defender policies.",
        "Leverage meta-optimization (MAML/Reptile variants) with task-specific adapters for Meta-GraphSHS to reduce fine-tuning cost.",
        "Combine Tracking-SHS with dynamic sparse updates using union-find and incremental betweenness approximations to further reduce recomputation."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Enterprise Active Directory (conceptual target); offline evaluation on synthetic AD graphs and real-world social/information networks for SHS",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Lack of publicly available, labeled real-world AD attack graphs for training and validation.",
        "Translating learned/best-edge blocks into enforceable AD policy changes (e.g., ACL modifications) with minimal business disruption.",
        "Modeling realistic attacker behavior and detection probabilities in production AD environments.",
        "Handling dynamic changes and scale in enterprise networks without frequent full recomputation."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proves deriving an optimal defensive policy for AD edge blocking is #P-hard and introduces fixed-parameter tractable kernelization to condense AD graphs.",
      "Proposes a combined neural network-based dynamic program (attacker policy) with evolutionary diversity optimization (defender) to block bottleneck edges; “The experimental evaluations on synthetic AD attack graphs demonstrate that our defensive policy generates effective defense.”",
      "Introduces an RL-based attacker policy (PPO) trained across multiple defensive-plan environments with critic-assisted EDO for scalable AD hardening; shows improved effectiveness and scalability over the previous NN-DP+EDO approach.",
      "Develops GraphSHS and Meta-GraphSHS for discovering bottleneck structural hole spanner nodes in large and diverse networks; meta-learning enables transfer to unseen graphs; “Our experimental results show that the proposed models are highly effective and efficient.”",
      "For dynamic networks, proposes Tracking-SHS (decremental algorithm) and GNN-SHS for efficient SHS discovery across snapshots; “the proposed approaches achieve significant speedup over re-computations for dynamic graphs.”"
    ]
  },
  {
    "arxiv_id": "2309.07730v1",
    "title": "AIDPS:Adaptive Intrusion Detection and Prevention System for Underwater Acoustic Sensor Networks",
    "authors": "Soumadeep Das; Aryan Mohammadi Pasikhani; Prosanta Gope; John A. Clark; Chintan Patel; Biplab Sikdar",
    "abstract": "Underwater Acoustic Sensor Networks (UW-ASNs) are predominantly used for underwater environments and find applications in many areas. However, a lack of security considerations, the unstable and challenging nature of the underwater environment, and the resource-constrained nature of the sensor nodes used for UW-ASNs (which makes them incapable of adopting security primitives) make the UW-ASN prone to vulnerabilities. This paper proposes an Adaptive decentralised Intrusion Detection and Prevention System called AIDPS for UW-ASNs. The proposed AIDPS can improve the security of the UW-ASNs so that they can efficiently detect underwater-related attacks (e.g., blackhole, grayhole and flooding attacks). To determine the most effective configuration of the proposed construction, we conduct a number of experiments using several state-of-the-art machine learning algorithms (e.g., Adaptive Random Forest (ARF), light gradient-boosting machine, and K-nearest neighbours) and concept drift detection algorithms (e.g., ADWIN, kdqTree, and Page-Hinkley). Our experimental results show that incremental ARF using ADWIN provides optimal performance when implemented with One-class support vector machine (SVM) anomaly-based detectors. Furthermore, our extensive evaluation results also show that the proposed scheme outperforms state-of-the-art bench-marking methods while providing a wider range of desirable features such as scalability and complexity.",
    "published_date": "2023-09-14",
    "pdf_link": "https://arxiv.org/pdf/2309.07730v1",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection and Prevention",
      "specific_problem": "Adaptive decentralized IDS/IPS for Underwater Acoustic Sensor Networks (UW-ASNs) with concept-drift-aware detection of routing attacks",
      "attack_types": [
        "Blackhole attack",
        "Grayhole attack",
        "Flooding (DoS/DDoS) attack"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Ensemble Trees",
        "specific": "Adaptive Random Forest (ARF)",
        "novel_contribution": "Incremental ARF combined with ADWIN drift detector and One-Class SVM anomaly detectors for on-the-fly concept drift adaptation in UW-ASNs"
      },
      {
        "type": "primary",
        "category": "Concept Drift Detection",
        "specific": "ADWIN",
        "novel_contribution": "Used online with ARF and OCSVM; found to provide optimal performance for streaming UW-ASN data"
      },
      {
        "type": "primary",
        "category": "Anomaly Detection",
        "specific": "One-Class SVM (OCSVM)",
        "novel_contribution": "Deployed as anomaly-based detectors within the hybrid incremental IDS pipeline for zero-day/OOD detection"
      },
      {
        "type": "baseline",
        "category": "Gradient Boosted Trees",
        "specific": "LightGBM (Light Gradient-Boosting Machine)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "k-Nearest Neighbors",
        "specific": "KNN",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Concept Drift Detection",
        "specific": "kdqTree",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Concept Drift Detection",
        "specific": "Page-Hinkley",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised",
      "Online/Incremental"
    ],
    "datasets": [
      {
        "name": "UW-ASN dataset (16–64 nodes)",
        "type": "synthetic",
        "domain": "network_traffic",
        "link": "https://drive.google.com/drive/folders/11d6tZAOkqvdrj57A0OFAlgf7YlQgzTzz",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "UW-ASN OOD dataset (64 nodes subset for OOD testing)",
        "type": "synthetic",
        "domain": "network_traffic",
        "link": "https://drive.google.com/drive/folders/11d6tZAOkqvdrj57A0OFAlgf7YlQgzTzz",
        "is_new_contribution": true,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "LightGBM",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "K-Nearest Neighbors (KNN)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "ARF + Page-Hinkley",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "ARF + kdqTree",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can an adaptive decentralized IDS/IPS for UW-ASNs achieve zero-day detection, on-the-fly concept drift handling, scalability, and lightweight operation under resource constraints?",
        "Which combination of incremental learner and drift detector yields optimal performance for evolving UW-ASN traffic?"
      ],
      "gaps_identified": [
        "Existing IDS for UW-ASNs do not jointly satisfy DP1–DP7 (zero-day detection, adaptivity, OOD detection, scalability, on-the-fly detection, lightweight, and integrated IPS).",
        "UW-ASN streaming data are imbalanced and evolve over time, yet prior ML/DL-based IDS lack adaptivity to concept drift.",
        "Prior work is focused on terrestrial/IoT networks and does not address underwater acoustic channel constraints.",
        "Lack of public UW-ASN-specific datasets reflecting routing attacks and topology scaling."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "UW-ASNs are vulnerable due to harsh, evolving underwater environments and resource-constrained nodes; current IDS solutions are not adaptive to concept drift and do not meet the required desirable properties (DP1–DP7).",
      "potential_research_ideas": [
        "Extend detection/prevention to additional UW-ASN routing attacks (e.g., wormhole, Sybil, sinkhole, low-rate DDoS) and cross-layer attacks.",
        "Incorporate federated/edge online learning among nodes to preserve bandwidth and decentralize model updates under acoustic constraints.",
        "Study transfer learning/domain adaptation across different topologies, depths, and environmental conditions to reduce retraining costs.",
        "Develop active learning/online labeling strategies to handle severe class imbalance and rare-event nature of intrusions.",
        "Integrate explainability tailored to streaming tree ensembles for operator trust and root-cause analysis in UW-ASNs.",
        "Assess and harden against data poisoning and evasion attacks on incremental learners in resource-constrained sensor networks.",
        "Combine physical-layer acoustic features with network-layer telemetry for multi-modal detection under challenging SNR conditions."
      ],
      "architectural_improvement_recommendations": [
        "Use a drift-aware dynamic ensemble that weights or prunes base learners based on local concept stability to further reduce false positives.",
        "Evaluate incremental anomaly detectors beyond OCSVM (e.g., Isolation Forest variants, Deep SVDD with streaming updates) for OOD robustness.",
        "Introduce energy-aware feature selection and model compression/quantization to fit stringent node resources.",
        "Adopt lightweight drift explanation modules (e.g., population stability index, streaming SHAP approximations) to contextualize detected drifts.",
        "Leverage hierarchical training: surface/buoy nodes host heavier models and distill to ultra-light on-node detectors.",
        "Explore event-triggered communication protocols to synchronize model updates only upon significant drift to save acoustic bandwidth."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://drive.google.com/drive/folders/11d6tZAOkqvdrj57A0OFAlgf7YlQgzTzz",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Underwater Acoustic Sensor Networks with decentralized architecture (sensor nodes, underwater sink, surface station/buoy); simulated via NS2 for evaluation",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Resource-constrained nodes (energy, compute, storage)",
        "High and variable path loss, Doppler spread, and propagation delay",
        "Limited bandwidth and low data rate of acoustic channels",
        "Evolving data distributions (concept drift) and class imbalance"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "A robust hybrid incremental IDS for UW-ASN routing attacks that detects and adapts to concept drift on-the-fly.",
      "The first incremental cryptography-based IPS, lightweight and isolated against an external adversary, to avoid negative impacts of false positives.",
      "First solution to identify and mitigate grayhole, flooding, and blackhole routing attacks in UW-ASN environments.",
      "A generated UW-ASN dataset (16–64 nodes) for benchmarking, publicly released.",
      "Benchmarking against multiple state-of-the-art ML classifiers and drift detectors; finding: \"incremental ARF using ADWIN provides optimal performance when implemented with One-class SVM anomaly-based detectors.\""
    ]
  },
  {
    "arxiv_id": "2309.04911v1",
    "title": "A Review of Machine Learning-based Security in Cloud Computing",
    "authors": "Aptin Babaei; Parham M. Kebria; Mohsen Moradi Dalvand; Saeid Nahavandi",
    "abstract": "Cloud Computing (CC) is revolutionizing the way IT resources are delivered to users, allowing them to access and manage their systems with increased cost-effectiveness and simplified infrastructure. However, with the growth of CC comes a host of security risks, including threats to availability, integrity, and confidentiality. To address these challenges, Machine Learning (ML) is increasingly being used by Cloud Service Providers (CSPs) to reduce the need for human intervention in identifying and resolving security issues. With the ability to analyze vast amounts of data, and make high-accuracy predictions, ML can transform the way CSPs approach security. In this paper, we will explore some of the most recent research in the field of ML-based security in Cloud Computing. We will examine the features and effectiveness of a range of ML algorithms, highlighting their unique strengths and potential limitations. Our goal is to provide a comprehensive overview of the current state of ML in cloud security and to shed light on the exciting possibilities that this emerging field has to offer.",
    "published_date": "2023-09-10",
    "pdf_link": "https://arxiv.org/pdf/2309.04911v1",
    "paper_types": [
      "survey"
    ],
    "security_domain": {
      "primary": "Cloud Security",
      "subdomain": "Threat Detection and Mitigation in Cloud Computing",
      "specific_problem": "Survey of machine learning techniques for enhancing confidentiality, integrity, and availability in cloud environments",
      "attack_types": [
        "Denial of Service (DoS)",
        "Distributed Denial of Service (DDoS)",
        "Zombie attack",
        "Man-In-The-Middle",
        "Network-based attacks (Port Scanning, Spoofing, Spamming)",
        "VM-based attacks (Hyperjacking, VM Escape, Rollback, Hopping)",
        "Storage-based attacks (Data De-duplication abuse, Data recovery, Data scavenging, Data backup attacks)",
        "Application-based attacks (Malware infusion, Web Services exploitation, Shared design vulnerabilities)"
      ]
    },
    "ml_techniques": [
      {
        "type": "baseline",
        "category": "Linear Model",
        "specific": "Logistic Regression",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Decision Tree",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "SVM",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Naive Bayes",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Linear Model",
        "specific": "Linear Regression",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Linear Model",
        "specific": "Lasso Regression",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Clustering",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Anomaly Detection",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Association Rule Learning",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Semi-supervised",
        "specific": "Self-Training",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Semi-supervised",
        "specific": "Co-Training",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Reinforcement Learning",
        "specific": "Q-Learning",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Reinforcement Learning",
        "specific": "SARSA",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Reinforcement Learning",
        "specific": "Model-based RL",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Reinforcement Learning",
        "specific": "Model-free RL",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised",
      "Semi-supervised",
      "Reinforcement"
    ],
    "datasets": [
      {
        "name": "KDD",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "DRAPA",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "UNSW",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "ISOT",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Accuracy"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "What machine learning algorithms are used to detect, prevent, and resolve cloud security vulnerabilities?",
        "What are the features and effectiveness of different ML algorithms for cloud security?",
        "How can ML reduce human intervention and improve the confidentiality, integrity, and availability of cloud services?"
      ],
      "gaps_identified": [
        "Lack of a comprehensive examination of available machine learning algorithms in the context of cloud security."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Cloud computing’s rapid growth raises confidentiality, integrity, and availability concerns; ML can automate detection and mitigation, motivating a comprehensive review of ML techniques for cloud security.",
      "potential_research_ideas": [
        "Create a standardized benchmark suite for cloud security ML with unified preprocessing and evaluation across datasets covering network, VM, storage, and application layers.",
        "Develop datasets and benchmarks specifically for VM-level and storage-level attacks (e.g., hyperjacking, data scavenging) where public data is scarce.",
        "Investigate semi-supervised and self-training pipelines leveraging abundant unlabeled cloud telemetry to reduce labeled data costs.",
        "Apply reinforcement learning for adaptive DDoS mitigation, autoscaling, and dynamic policy tuning under attack while ensuring SLA adherence.",
        "Design explainable ML for cloud IDS to provide operator-understandable rationales aligned with CIA concerns and compliance.",
        "Explore privacy-preserving and federated learning across tenants/providers to detect cross-tenant threats without sharing raw data.",
        "Model multi-cloud and hybrid-cloud threat detection using graph-based or topology-aware methods capturing inter-cloud dependencies."
      ],
      "architectural_improvement_recommendations": [
        "Adopt a multi-modal fusion architecture combining network flow features, VM telemetry, and application logs with late fusion and attention for improved detection.",
        "Integrate online learning with concept drift detection to maintain performance under evolving cloud workloads and attack patterns.",
        "Leverage graph neural networks to encode cloud topology (tenants, VMs, services) for context-aware anomaly detection.",
        "Use semi-supervised contrastive pretraining on unlabeled cloud telemetry followed by light supervised fine-tuning.",
        "Deploy hierarchical detection: lightweight edge/agent models on VMs for pre-filtering and centralized models for correlation.",
        "Implement RL-based controllers trained in realistic simulators/digital twins for DDoS mitigation and resource allocation."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Multi-tenant isolation to prevent cross-tenant unauthorized access.",
        "Data location and jurisdictional legal constraints for storage and deletion.",
        "VM replication, rollback, escape, and hopping vulnerabilities.",
        "Large-scale DoS/DDoS attacks impacting availability.",
        "API and browser vulnerabilities in cloud access paths.",
        "Limited availability and cost of labeled data for supervised ML."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Comprehensive review of cloud computing service and deployment models with associated security challenges and attacks.",
      "Survey of supervised, unsupervised, semi-supervised, and reinforcement learning techniques applicable to cloud security.",
      "Discussion of benefits and potential limitations of ML for enhancing cloud confidentiality, integrity, and availability, highlighting research gaps."
    ]
  },
  {
    "arxiv_id": "2309.10147v1",
    "title": "Realistic Website Fingerprinting By Augmenting Network Trace",
    "authors": "Alireza Bahramali; Ardavan Bozorgi; Amir Houmansadr",
    "abstract": "Website Fingerprinting (WF) is considered a major threat to the anonymity of Tor users (and other anonymity systems). While state-of-the-art WF techniques have claimed high attack accuracies, e.g., by leveraging Deep Neural Networks (DNN), several recent works have questioned the practicality of such WF attacks in the real world due to the assumptions made in the design and evaluation of these attacks. In this work, we argue that such impracticality issues are mainly due to the attacker's inability in collecting training data in comprehensive network conditions, e.g., a WF classifier may be trained only on samples collected on specific high-bandwidth network links but deployed on connections with different network conditions. We show that augmenting network traces can enhance the performance of WF classifiers in unobserved network conditions. Specifically, we introduce NetAugment, an augmentation technique tailored to the specifications of Tor traces. We instantiate NetAugment through semi-supervised and self-supervised learning techniques. Our extensive open-world and close-world experiments demonstrate that under practical evaluation settings, our WF attacks provide superior performances compared to the state-of-the-art; this is due to their use of augmented network traces for training, which allows them to learn the features of target traffic in unobserved settings. For instance, with a 5-shot learning in a closed-world scenario, our self-supervised WF attack (named NetCLR) reaches up to 80% accuracy when the traces for evaluation are collected in a setting unobserved by the WF adversary. This is compared to an accuracy of 64.4% achieved by the state-of-the-art Triplet Fingerprinting [35]. We believe that the promising results of our work can encourage the use of network trace augmentation in other types of network traffic analysis.",
    "published_date": "2023-09-18",
    "pdf_link": "https://arxiv.org/pdf/2309.10147v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Privacy/Anonymity",
      "subdomain": "Traffic Analysis",
      "specific_problem": "Website Fingerprinting against Tor using augmented network traces",
      "attack_types": [
        "traffic analysis",
        "website fingerprinting"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Data Augmentation",
        "specific": "NetAugment (Tor-tailored burst/cell manipulations)",
        "novel_contribution": "Introduces network-trace augmentation tailored to Tor traces to simulate unobserved conditions (e.g., bandwidth, circuit changes)."
      },
      {
        "type": "primary",
        "category": "Contrastive Learning (Self-Supervised)",
        "specific": "SimCLR-style method named NetCLR",
        "novel_contribution": "Self-supervised representation learning on augmented Tor traces; pre-train with NetAugment views then fine-tune with few labels for WF."
      },
      {
        "type": "primary",
        "category": "Semi-Supervised Learning",
        "specific": "FixMatch-style consistency regularization (weak/strong augmentations) instantiated with NetAugment",
        "novel_contribution": "Applies semi-supervised learning with Tor-specific augmentations to reduce labeled data needs."
      },
      {
        "type": "baseline",
        "category": "Metric Learning / Triplet Networks",
        "specific": "Triplet Fingerprinting (TF)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "Deep Fingerprinting (DF)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "Automated Website Fingerprinting (AWF)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "GAN",
        "specific": "GANDaLF",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Classical ML",
        "specific": "KNN, SVM, Random Forest (historical WF)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Self-supervised",
      "Semi-supervised",
      "Supervised (few-shot fine-tuning)"
    ],
    "datasets": [
      {
        "name": "Authors’ Tor website-trace datasets (superior vs. inferior network conditions)",
        "type": "synthetic",
        "domain": "network_traffic",
        "link": "https://github.com/SPIN-UMass/Realistic-Website-Fingerprinting-By-Augmenting-Network-Traces",
        "is_new_contribution": true,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Triplet Fingerprinting (TF)",
        "paper_reference": "[35] Sirinam et al.",
        "metric": "Closed-world accuracy, 5-shot, evaluation traces from unobserved setting",
        "their_result": "80%",
        "baseline_result": "64.4%"
      },
      {
        "method_name": "Triplet Fingerprinting (TF)",
        "paper_reference": "[35] Sirinam et al.",
        "metric": "Closed-world accuracy under concept drift (5-year gap), 20 labeled samples",
        "their_result": "72%",
        "baseline_result": "51%"
      },
      {
        "method_name": "Deep Fingerprinting (DF)",
        "paper_reference": "[34] Sirinam et al.",
        "metric": "Open-world precision with 5 labeled samples",
        "their_result": "92%",
        "baseline_result": "75%"
      },
      {
        "method_name": "Deep Fingerprinting (DF)",
        "paper_reference": "[34] Sirinam et al.",
        "metric": "Robustness under Blind Adversarial Perturbations (BAP): accuracy drop with 10 labeled samples",
        "their_result": "Accuracy reduced by 4.9%",
        "baseline_result": "Accuracy reduced by 52.3%"
      }
    ],
    "performance_metrics_used": [
      "accuracy",
      "precision"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can augmenting Tor network traces improve WF performance when training and deployment occur under different/unobserved network conditions?",
        "Can semi/self-supervised learning with Tor-specific augmentations reduce labeled data requirements while remaining robust to concept drift?",
        "Does Tor-tailored augmentation increase robustness of WF against defenses (e.g., BAP)?"
      ],
      "gaps_identified": [
        "WF evaluations often assume unrealistic conditions (matching train/test distributions).",
        "Sensitivity to concept drift over time degrades WF accuracy.",
        "Network condition variations (bandwidth/latency/circuits) between training and deployment hurt generalization.",
        "Synthetic crawler traces poorly imitate real user behavior (e.g., subpages, multi-tab).",
        "Large labeled datasets are often required for high WF accuracy."
      ],
      "limitations": [
        "Evaluation is performed on synthetic Tor traces (not genuine user traces) due to privacy and reproducibility constraints.",
        "Training is on ‘superior’ traces and evaluation on ‘inferior’ traces; results may differ with truly heterogeneous real-world user traffic.",
        "Details of model architecture/backbone and compute requirements are not fully specified in the provided text."
      ],
      "future_work": [
        "Extend network-trace augmentation to other types of network traffic analysis.",
        "Further study robustness and transfer across more diverse/unseen network conditions and longer-term concept drift.",
        "Explore integration with other learning paradigms (e.g., domain adaptation) and broader defenses."
      ],
      "motivation": "Address impracticality of prior WF attacks by overcoming lack of longitudinal perspective and distribution mismatch via Tor-tailored augmentation and semi/self-supervised learning.",
      "potential_research_ideas": [
        "Learn augmentation policies for network traces via AutoAugment/RandAugment-style search tailored to Tor bursts and inter-arrival timings.",
        "Domain adaptation with adversarial feature alignment to explicitly remove network-condition factors (bandwidth/latency) from embeddings.",
        "Meta-learning for rapid adaptation to new circuits/conditions with minimal labels (few-shot domain generalization).",
        "Generative modeling (e.g., diffusion models) to synthesize realistic trace variations beyond rule-based augmentation.",
        "Multi-view contrastive learning combining packet directions, timings, and burst-level graphs for richer invariances.",
        "Evaluate and train on genuine Tor traces using privacy-preserving aggregation or synthetic-but-validated benchmarks to close the realism gap.",
        "Adversarial training jointly against WF defenses (e.g., BAP) to harden embeddings without overfitting."
      ],
      "architectural_improvement_recommendations": [
        "Adopt time-series Transformers or temporal CNNs as the backbone encoder within NetCLR; compare against existing CNNs.",
        "Incorporate prototype-based contrastive objectives (supervised contrastive fine-tuning, prototypical loss) to improve few-shot classification.",
        "Use momentum encoders/memory banks (MoCo) for larger negative sets under limited batch sizes.",
        "Condition augmentations on estimated network state (bandwidth/latency) to generate targeted counterfactual views.",
        "Joint training with domain discriminators to encourage domain-invariant features across circuits/conditions.",
        "Calibrate few-shot classifier (e.g., cosine classifiers with temperature scaling) for open-world settings."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/SPIN-UMass/Realistic-Website-Fingerprinting-By-Augmenting-Network-Traces",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Passive local adversary monitoring the link between client and Tor entry relay",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Concept drift over time",
        "Mismatched network conditions between training and deployment",
        "Difficulty collecting labeled data at scale",
        "Synthetic traces may not match real user behavior (e.g., multi-tab, subpages)"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": true,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces NetAugment, a Tor-tailored network-trace augmentation mechanism to simulate unobserved conditions.",
      "Proposes NetCLR, a self-supervised WF attack using contrastive learning with NetAugment, plus semi-supervised instantiations.",
      "Demonstrates superior performance under practical settings with distribution shifts (e.g., 80% vs 64.4% TF in closed-world 5-shot).",
      "Shows improved resilience to concept drift (e.g., 72% vs 51% TF with 5-year gap).",
      "Demonstrates greater robustness against BAP defense (4.9% accuracy drop vs 52.3% for DF)."
    ]
  },
  {
    "arxiv_id": "2310.10660v1",
    "title": "Analysis and Detection against Network Attacks in the Overlapping Phenomenon of Behavior Attribute",
    "authors": "Jiang Xie; Shuhao Li; Yongzheng Zhanga; Peishuai Sun; Hongbo Xu",
    "abstract": "The proliferation of network attacks poses a significant threat. Researchers propose datasets for network attacks to support research in related fields. Then, many attack detection methods based on these datasets are proposed. These detection methods, whether two-classification or multi-classification, belong to single-label learning, i.e., only one label is given to each sample. However, we discover that there is a noteworthy phenomenon of behavior attribute overlap between attacks, The presentation of this phenomenon in a dataset is that there are multiple samples with the same features but different labels. In this paper, we verify the phenomenon in well-known datasets(UNSW-NB15, CCCS-CIC-AndMal-2020) and re-label these data. In addition, detecting network attacks in a multi-label manner can obtain more information, providing support for tracing the attack source and building IDS. Therefore, we propose a multi-label detection model based on deep learning, MLD-Model, in which Wasserstein-Generative-Adversarial- Network-with-Gradient-Penalty (WGAN-GP) with improved loss performs data enhancement to alleviate the class imbalance problem, and Auto-Encoder (AE) performs classifier parameter pre-training. Experimental results demonstrate that MLD-Model can achieve excellent classification performance. It can achieve F1=80.06% in UNSW-NB15 and F1=83.63% in CCCS-CIC-AndMal-2020. Especially, MLD-Model is 5.99%-7.97% higher in F1 compared with the related single-label methods.",
    "published_date": "2023-09-13",
    "pdf_link": "https://arxiv.org/pdf/2310.10660v1",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Multi-label network attack detection under overlapping behavior attributes; re-labeling and detecting attacks with overlapping features",
      "attack_types": [
        "DoS",
        "Fuzzers",
        "Analysis",
        "Backdoor",
        "Exploits",
        "Reconnaissance",
        "Trojan",
        "Zeroday"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "GAN",
        "specific": "WGAN-GP",
        "novel_contribution": "Improved loss adds a penalty encouraging generated samples for class i to be dissimilar from other classes (maintain inter-class separation) and used solely for data augmentation during pre-training"
      },
      {
        "type": "primary",
        "category": "Autoencoder",
        "specific": "Unbalanced Auto-Encoder",
        "novel_contribution": "Asymmetric (encoder>decoder) AE used for unsupervised pre-training on augmented data to initialize classifier parameters"
      },
      {
        "type": "primary",
        "category": "Feedforward NN",
        "specific": "MLP with softmax output",
        "novel_contribution": "Final classifier formed by the pre-trained encoder plus a softmax layer; fine-tuned on raw labeled data"
      },
      {
        "type": "primary",
        "category": "Problem Transformation",
        "specific": "Label Powerset",
        "novel_contribution": "Transforms multi-label attack sets into single-label multi-class targets for training/inference"
      },
      {
        "type": "baseline",
        "category": "Classical/Unspecified Multi-label Methods",
        "specific": null,
        "novel_contribution": "Compared against unspecified single-label and multi-label baselines reported in the paper"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "UNSW-NB15 (processed multi-label version)",
        "type": "public",
        "domain": "network_traffic",
        "link": "https://github.com/BitBrave-Xie/processed-multi-label-dataset",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "CCCS-CIC-AndMal-2020 (processed multi-label version)",
        "type": "public",
        "domain": "network_traffic",
        "link": "https://github.com/BitBrave-Xie/processed-multi-label-dataset",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "UNSW-NB15 (original)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CCCS-CIC-AndMal-2020 (original)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Accuracy",
      "F1"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Does a measurable overlapping behavior-attribute phenomenon exist across classes in widely used network attack datasets?",
        "Can re-labeling and treating detection as a multi-label problem improve informational value and performance over single-label methods?",
        "Can WGAN-GP-based augmentation with an inter-class separation penalty plus AE pre-training mitigate class imbalance and improve multi-label detection?"
      ],
      "gaps_identified": [
        "Traditional intrusion detection methods assume single-label samples and ignore overlapping behavior attributes, limiting accuracy ceilings.",
        "No existing multi-label detection technology tailored for network attacks with overlapping attributes; NLP/image multi-label methods do not transfer directly.",
        "Class imbalance in network attack datasets hampers effective training.",
        "Existing feature extraction may miss mutually exclusive features, contributing to label ambiguity."
      ],
      "limitations": [
        "Generated data from WGAN-GP is only effective for pre-training; using it in fine-tuning interferes with classifier decisions, indicating the generator only partially matches true distributions.",
        "Label Powerset transformation can lead to a large number of classes as label combinations grow (implied by method choice).",
        "Evaluation limited to two datasets (UNSW-NB15 and CCCS-CIC-AndMal-2020); generalization to other domains/datasets not demonstrated."
      ],
      "future_work": [],
      "motivation": "Leverage the observed overlap of behavior attributes among network attacks to perform multi-label detection, yielding richer information for attack source tracing and IDS construction while improving detection performance.",
      "potential_research_ideas": [
        "Develop classifier chains or graph-based multi-label models to explicitly model label correlations beyond Label Powerset.",
        "Introduce hierarchical multi-label taxonomies for attacks (e.g., technique→tactic) and perform hierarchical classification.",
        "Design conditional/disentangled generative models to separate shared (overlapping) vs unique class features, improving augmentation quality.",
        "Apply contrastive/self-supervised pretraining on flows or dynamic traces to learn representations robust to overlapping attributes.",
        "Incorporate temporal sequence modeling (1D CNN/Transformer over flow sequences) to disambiguate overlapping behaviors.",
        "Leverage weak supervision or programmatic labeling to expand multi-label ground truth from raw telemetry.",
        "Calibrate predictive uncertainty for multi-label outputs to support analyst triage and investigation.",
        "Evaluate and adapt methods for streaming/online IDS settings with concept drift handling."
      ],
      "architectural_improvement_recommendations": [
        "Replace Label Powerset with sigmoid multi-label heads optimized with class-balanced BCE/focal loss to avoid class explosion.",
        "Use classifier chains or attention-based cross-label modules to capture inter-label dependencies.",
        "Adopt conditional WGAN-GP or InfoGAN with mutual-information regularization to better control class-conditional generation and disentanglement.",
        "Upgrade AE to denoising or variational autoencoder; consider self-supervised encoders (SimCLR/MoCo) for stronger initialization.",
        "Incorporate sequence encoders (Temporal CNN/Transformer) over packet/flow time series instead of purely tabular MLPs.",
        "Use logit adjustment or re-weighting for long-tail label distributions; combine with MixUp/SMOTE as simpler augmentation baselines.",
        "Add calibration (temperature scaling) and threshold optimization per label for better F1 in multi-label inference."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/BitBrave-Xie/processed-multi-label-dataset",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Ambiguous/overlapping ground truth labels in operational data",
        "Class imbalance across attack types",
        "Quality and realism of synthetic samples for augmentation",
        "Need for ongoing re-labeling and taxonomy alignment in changing threat landscapes"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Identify and formalize the overlapping phenomenon of behavior attributes among network attacks; provide cause analysis.",
      "Quantitatively validate overlap in UNSW-NB15 (avg. 1.689 labels/sample) and CCCS-CIC-AndMal-2020 (avg. 1.413 labels/sample).",
      "Release processed multi-label versions of these datasets and code for the community.",
      "Propose MLD-Model: a deep multi-label detection approach using WGAN-GP with improved loss for augmentation and unbalanced AE for pre-training; Label Powerset for training.",
      "Demonstrate performance: UNSW-NB15 Acc=79.87%, F1=80.06%; CCCS-CIC-AndMal-2020 Acc=83.17%, F1=83.63%; report F1 gains of 5.99%–7.97% over related single-label methods and 1.65%–58.25% over multi-label baselines."
    ]
  },
  {
    "arxiv_id": "2309.16618v1",
    "title": "Revisiting Neural Program Smoothing for Fuzzing",
    "authors": "Maria-Irina Nicolae; Max Eisele; Andreas Zeller",
    "abstract": "Testing with randomly generated inputs (fuzzing) has gained significant traction due to its capacity to expose program vulnerabilities automatically. Fuzz testing campaigns generate large amounts of data, making them ideal for the application of machine learning (ML). Neural program smoothing (NPS), a specific family of ML-guided fuzzers, aims to use a neural network as a smooth approximation of the program target for new test case generation.   In this paper, we conduct the most extensive evaluation of NPS fuzzers against standard gray-box fuzzers (>11 CPU years and >5.5 GPU years), and make the following contributions: (1) We find that the original performance claims for NPS fuzzers do not hold; a gap we relate to fundamental, implementation, and experimental limitations of prior works. (2) We contribute the first in-depth analysis of the contribution of machine learning and gradient-based mutations in NPS. (3) We implement Neuzz++, which shows that addressing the practical limitations of NPS fuzzers improves performance, but that standard gray-box fuzzers almost always surpass NPS-based fuzzers. (4) As a consequence, we propose new guidelines targeted at benchmarking fuzzing based on machine learning, and present MLFuzz, a platform with GPU access for easy and reproducible evaluation of ML-based fuzzers. Neuzz++, MLFuzz, and all our data are public.",
    "published_date": "2023-09-28",
    "pdf_link": "https://arxiv.org/pdf/2309.16618v1",
    "paper_types": [
      "empirical_analysis",
      "reproducibility",
      "benchmark",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Fuzzing",
      "specific_problem": "Evaluation and improvement of neural program smoothing (NPS) for coverage-guided fuzzing; benchmarking ML-guided fuzzers against state-of-the-art gray-box fuzzers",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "MLP",
        "specific": null,
        "novel_contribution": "Neuzz++ integrates an MLP-based neural program smoothing model as an AFL++ custom mutator with on-demand gradient computation, in-memory model, coverage caching, and periodic retraining"
      },
      {
        "type": "primary",
        "category": "Gradient-based optimization",
        "specific": "FGSM-inspired byte-level gradient-guided mutation",
        "novel_contribution": "Gradient computation is performed on demand per test case; gradients are not precomputed or stored, reducing overhead and improving integration with AFL++"
      },
      {
        "type": "baseline",
        "category": "MLP",
        "specific": "Neuzz",
        "novel_contribution": "Original NPS approach approximating edge coverage with an MLP to guide gradient-based mutations"
      },
      {
        "type": "baseline",
        "category": "Multitask Learning",
        "specific": "MTFuzz",
        "novel_contribution": "Joint learning of edge, context-sensitive, and approach-sensitive coverage to rank hot bytes via saliency"
      },
      {
        "type": "baseline",
        "category": "Bandit / Reinforcement Learning",
        "specific": "Havoc MAB (two-layer multi-armed bandit)",
        "novel_contribution": "Bandit formulation of the havoc strategy to select mutation operators"
      },
      {
        "type": "baseline",
        "category": "Saliency/Heuristic-guided mutation",
        "specific": "PreFuzz",
        "novel_contribution": "Neighbor-aware coverage bitmap and probabilistic targeting; combines gradient saliency with havoc on byte segments"
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "23 target programs (benchmark suite used in this study)",
        "type": "public",
        "domain": "program_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "MLFuzz platform (benchmarking framework with GPU access)",
        "type": "public",
        "domain": "benchmark_platform",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "AFL (American Fuzzy Lop)",
        "paper_reference": "AFL [49]",
        "metric": "code coverage, number of bugs found",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "AFL++",
        "paper_reference": "AFL++ [18]",
        "metric": "code coverage, number of bugs found",
        "their_result": "Standard gray-box fuzzers almost always surpass NPS-based fuzzers",
        "baseline_result": null
      },
      {
        "method_name": "Havoc MAB",
        "paper_reference": "Havoc MAB [45]",
        "metric": "code coverage",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Neuzz (original NPS)",
        "paper_reference": "Neuzz [39]",
        "metric": "code coverage, number of bugs found",
        "their_result": "Original performance claims for NPS fuzzers do not hold under rigorous evaluation",
        "baseline_result": null
      },
      {
        "method_name": "PreFuzz",
        "paper_reference": "PreFuzz [46]",
        "metric": "code coverage, number of bugs found",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "code coverage",
      "number of bugs found",
      "execution throughput (execs/sec)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Does gradient-guided mutation in neural program smoothing actually reach the targeted edges?",
        "To what extent does neural network prediction accuracy translate to improved coverage in fuzzing?",
        "What is the contribution of machine learning and gradient-based mutations to the overall efficacy of NPS fuzzers?"
      ],
      "gaps_identified": [
        "NPS model performance does not translate to improved coverage; the model fails to capture rare edge coverage",
        "Incomplete coverage bitmaps (afl-showmap only includes edges seen at least once), preventing explicit targeting of unseen edges",
        "Prior NPS studies did not assess what the model was learning nor whether it achieved its objective",
        "Use of outdated components and impractical build processes hinder reproducibility and adoption",
        "Evaluation flaws: missing comparisons to modern fuzzers (AFL++, libFuzzer), improper target builds, and non-persistent mode leading to questionable claims"
      ],
      "limitations": [
        "Approximation errors from smoothing cause the neural model to differ from the program at discontinuities (branches/jumps)",
        "Uncertain capacity of gradient-based mutations to flip targeted edges on real programs",
        "Dependency on incomplete coverage bitmaps that exclude unseen edges",
        "Previous implementations rely on outdated toolchains and employ undocumented magic numbers",
        "Prior experiments used slower compilation (afl-gcc) and file-based I/O instead of persistent mode, biasing results"
      ],
      "future_work": [
        "Adopt improved benchmarking guidelines for ML-based fuzzing and leverage MLFuzz for reproducible evaluations",
        "Instrument targets to expose unseen edges in coverage bitmaps to enable explicit targeting",
        "Further analyze when and why gradient-guided mutations succeed or fail across program structures"
      ],
      "motivation": "Neural program smoothing has shown promising claims in prior work but lacks practical adoption; this paper rigorously evaluates NPS fuzzers, identifies fundamental and experimental limitations, improves practicality via Neuzz++, and provides a reproducible benchmarking platform (MLFuzz).",
      "potential_research_ideas": [
        "Design coverage prediction models that explicitly handle discontinuities and rare edges (e.g., focal loss, cost-sensitive learning, or rarity-aware objectives)",
        "Integrate static analysis to enumerate potential edges/paths and include them in training targets to overcome incomplete bitmaps",
        "Explore program-structure-aware models (e.g., GNNs over CFG/PDG) for better input-to-coverage generalization",
        "Uncertainty-aware active learning to prioritize training on inputs with high epistemic uncertainty about coverage",
        "Hybrid search combining gradient-guided mutations with learned operator selection (bandits/RL) conditioned on input/program features",
        "Contrastive or metric learning to build robust input–coverage embeddings that better identify impactful byte regions",
        "Grammar- and format-aware differentiable mutation layers using straight-through estimators for discrete bytes",
        "Asynchronous, non-blocking model training/inference pipelines to avoid stalling the fuzzer",
        "Gradient-free optimization (e.g., CMA-ES, evolutionary strategies) as a complementary path when gradients are misleading"
      ],
      "architectural_improvement_recommendations": [
        "Augment the training objective with rarity-aware losses (e.g., focal loss) and class rebalancing to emphasize rare edges",
        "Incorporate static CFG extraction to add placeholders for unseen edges into the model’s output space and training targets",
        "Adopt program-structure-aware encoders (byte-level transformers with attention to taint/coverage cues, or GNNs on CFGs) instead of plain MLPs",
        "Introduce uncertainty estimation (MC dropout/ensembles) to drive active data collection and mutation focus",
        "Co-train a policy for mutation operator selection (bandit/RL) jointly with the coverage predictor",
        "Run model training and gradient inference asynchronously and cache per-seed state to minimize latency",
        "Leverage persistent mode and zero-copy shared memory for faster end-to-end loops; pin the model and use mixed precision on GPU"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "high",
      "computational_requirements": ">11 CPU years and >5.5 GPU years reported for the full experimental campaign; GPU access recommended (MLFuzz provides GPU-enabled benchmarking)"
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Benchmarked on open-source software targets via AFL++/MLFuzz with GPU support",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Outdated dependencies and toolchains in prior NPS implementations",
        "Difficulty building targets with custom instrumentation (e.g., multiple builds in MTFuzz)",
        "High computational overhead of ML training/inference within fuzzing loop",
        "Use of file-based I/O and non-persistent mode in prior work reducing throughput",
        "Incomplete coverage feedback limiting explicit targeting of unseen edges"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Critical analysis of neural program smoothing, uncovering fundamental, implementation, and experimental limitations",
      "First in-depth analysis of the contribution of machine learning and gradient-based mutations in NPS",
      "Extensive benchmark of NPS-guided fuzzers against AFL, AFL++, and Havoc MAB on 23 target programs",
      "Implementation of Neuzz++ as an AFL++ custom mutator addressing practical limitations of NPS and improving performance",
      "Proposal of new benchmarking guidelines for ML-based fuzzing",
      "Introduction of MLFuzz, a GPU-enabled platform for easy and reproducible evaluation of ML-based fuzzers",
      "Public release of Neuzz++, MLFuzz, and all experimental data"
    ]
  },
  {
    "arxiv_id": "2309.08474v1",
    "title": "VulnSense: Efficient Vulnerability Detection in Ethereum Smart Contracts by Multimodal Learning with Graph Neural Network and Language Model",
    "authors": "Phan The Duy; Nghi Hoang Khoa; Nguyen Huu Quyen; Le Cong Trinh; Vu Trung Kien; Trinh Minh Hoang; Van-Hau Pham",
    "abstract": "This paper presents VulnSense framework, a comprehensive approach to efficiently detect vulnerabilities in Ethereum smart contracts using a multimodal learning approach on graph-based and natural language processing (NLP) models. Our proposed framework combines three types of features from smart contracts comprising source code, opcode sequences, and control flow graph (CFG) extracted from bytecode. We employ Bidirectional Encoder Representations from Transformers (BERT), Bidirectional Long Short-Term Memory (BiLSTM) and Graph Neural Network (GNN) models to extract and analyze these features. The final layer of our multimodal approach consists of a fully connected layer used to predict vulnerabilities in Ethereum smart contracts. Addressing limitations of existing vulnerability detection methods relying on single-feature or single-model deep learning techniques, our method surpasses accuracy and effectiveness constraints. We assess VulnSense using a collection of 1.769 smart contracts derived from the combination of three datasets: Curated, SolidiFI-Benchmark, and Smartbugs Wild. We then make a comparison with various unimodal and multimodal learning techniques contributed by GNN, BiLSTM and BERT architectures. The experimental outcomes demonstrate the superior performance of our proposed approach, achieving an average accuracy of 77.96\\% across all three categories of vulnerable smart contracts.",
    "published_date": "2023-09-15",
    "pdf_link": "https://arxiv.org/pdf/2309.08474v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Blockchain Security",
      "subdomain": "Smart Contract Security",
      "specific_problem": "Multiclass vulnerability detection in Ethereum smart contracts using multimodal deep learning over source code, opcodes, and CFG",
      "attack_types": [
        "Reentrancy",
        "Integer Overflow/Underflow (Arithmetic)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "BERT",
        "novel_contribution": "Branch to encode Solidity source code as one modality within a three-branch multimodal architecture"
      },
      {
        "type": "primary",
        "category": "RNN",
        "specific": "BiLSTM",
        "novel_contribution": "Branch to model opcode sequences as a sequential modality"
      },
      {
        "type": "primary",
        "category": "GNN",
        "specific": null,
        "novel_contribution": "Branch to encode CFG extracted from bytecode as a graph modality"
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "BERT",
        "novel_contribution": "Unimodal baseline using only source code"
      },
      {
        "type": "baseline",
        "category": "RNN",
        "specific": "BiLSTM",
        "novel_contribution": "Unimodal baseline using only opcodes"
      },
      {
        "type": "baseline",
        "category": "GNN",
        "specific": null,
        "novel_contribution": "Unimodal baseline using only CFG"
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "SmartBugs Curated",
        "type": "public",
        "domain": "smart_contracts",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "SolidiFI-Benchmark",
        "type": "public",
        "domain": "smart_contracts",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "SmartBugs Wild",
        "type": "public",
        "domain": "smart_contracts",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "BERT-only (source code unimodal)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "BiLSTM-only (opcode unimodal)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "GNN-only (CFG unimodal)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "BERT + BiLSTM (bimodal fusion)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "BERT + GNN (bimodal fusion)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "BiLSTM + GNN (bimodal fusion)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Accuracy"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can multimodal fusion of source code (BERT), opcodes (BiLSTM), and CFG (GNN) improve the accuracy of Ethereum smart contract vulnerability detection compared to unimodal approaches?",
        "Can a multimodal model support multi-class vulnerability detection rather than only binary vulnerable/non-vulnerable classification?"
      ],
      "gaps_identified": [
        "Existing methods often rely on a single feature or a single-model deep learning technique, limiting accuracy and effectiveness.",
        "Prior multimodal approaches used simpler embeddings (e.g., word2vec) with out-of-vocabulary limitations.",
        "Many prior works framed detection as binary classification rather than multi-class vulnerability type classification.",
        "Rule-based static/dynamic tools depend on expert-defined rules, missing paths and being time-consuming, with false negatives."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Address limitations of single-feature and single-model approaches by leveraging complementary modalities (source code, opcodes, CFG) in a unified multimodal framework to improve detection effectiveness and support multi-class classification.",
      "potential_research_ideas": [
        "Extend modalities with dynamic execution traces and transaction-level behaviors to capture runtime patterns associated with vulnerabilities.",
        "Localize vulnerability statements/paths (fine-grained detection) in addition to contract-level classification using multi-task learning.",
        "Pretrain modality-specific encoders on large unlabeled Ethereum corpora (e.g., code-language models, graph contrastive learning on CFGs) and then fine-tune for detection.",
        "Incorporate cross-contract dependency graphs (inter-contract calls, libraries, proxies) to detect vulnerabilities requiring multi-contract context.",
        "Leverage program analysis artifacts (AST, SSA, data-flow/taint graphs) to create a richer heterogeneous graph and use Heterogeneous GNNs.",
        "Use contrastive multimodal alignment (e.g., CLIP-style) to better fuse representations from source, bytecode, and opcodes.",
        "Handle class imbalance via focal loss, reweighting, or synthetic minority oversampling tailored to vulnerability types.",
        "Evaluate robustness to compiler/EVM versions and optimization flags; build domain adaptation techniques across toolchains and Solidity versions."
      ],
      "architectural_improvement_recommendations": [
        "Replace generic BERT with code-aware PLMs (e.g., CodeBERT/GraphCodeBERT) and add token-type features for Solidity-specific constructs.",
        "Adopt Graph Transformers or message-passing with edge types to better capture CFG semantics and control-flow edge labels.",
        "Introduce attention-based fusion (cross-modal attention or gated multimodal transformers) instead of simple concatenation and FC layers.",
        "Add an auxiliary vulnerability-localization head that predicts vulnerable basic blocks/lines to guide representation learning.",
        "Use hierarchical modeling: opcode-level -> basic-block -> function -> contract with pooling at each level.",
        "Apply self-supervised pretraining on opcodes (masked opcode modeling) and CFGs (graph contrastive objectives) before supervised fine-tuning."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Propose VulnSense, a multimodal learning framework combining BERT (source code), BiLSTM (opcodes), and GNN (CFG) for smart contract vulnerability detection.",
      "Extract and fuse three feature types (source code, opcode sequences, CFG from bytecode) to provide a comprehensive representation.",
      "Evaluate on 1,769 contracts combined from SmartBugs Curated, SolidiFI-Benchmark, and SmartBugs Wild; compare against unimodal and multimodal baselines.",
      "Reported result: \"achieving an average accuracy of 77.96% across all three categories of vulnerable smart contracts.\""
    ]
  },
  {
    "arxiv_id": "2310.05956v1",
    "title": "Efficient Network Representation for GNN-based Intrusion Detection",
    "authors": "Hamdi Friji; Alexis Olivereau; Mireille Sarkiss",
    "abstract": "The last decades have seen a growth in the number of cyber-attacks with severe economic and privacy damages, which reveals the need for network intrusion detection approaches to assist in preventing cyber-attacks and reducing their risks. In this work, we propose a novel network representation as a graph of flows that aims to provide relevant topological information for the intrusion detection task, such as malicious behavior patterns, the relation between phases of multi-step attacks, and the relation between spoofed and pre-spoofed attackers activities. In addition, we present a Graph Neural Network (GNN) based framework responsible for exploiting the proposed graph structure to classify communication flows by assigning them a maliciousness score. The framework comprises three main steps that aim to embed nodes features and learn relevant attack patterns from the network representation. Finally, we highlight a potential data leakage issue with classical evaluation procedures and suggest a solution to ensure a reliable validation of intrusion detection systems performance. We implement the proposed framework and prove that exploiting the flow-based graph structure outperforms the classical machine learning-based and the previous GNN-based solutions.",
    "published_date": "2023-09-11",
    "pdf_link": "https://arxiv.org/pdf/2310.05956v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection (NIDS)",
      "specific_problem": "Flow-based malicious traffic detection via graph representation and GNN classification",
      "attack_types": [
        "DoS",
        "DDoS",
        "Botnet",
        "Ransomware",
        "Man-in-the-Middle",
        "Malware",
        "Multi-step attacks",
        "Reconnaissance/Scanning",
        "SQL Injection",
        "IP Spoofing (evasion concern addressed by representation)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "GNN",
        "specific": null,
        "novel_contribution": "End-to-end framework with three modules: Graph Structure-Agnostic embedding layer, Attention-based Feature Extractor, and Spatial Feature Extractor over a new flow-as-nodes, weighted graph"
      },
      {
        "type": "primary",
        "category": "GCN",
        "specific": null,
        "novel_contribution": "Used as Spatial Feature Extractor (SFE) to capture topological patterns among flow-nodes"
      },
      {
        "type": "primary",
        "category": "Attention",
        "specific": null,
        "novel_contribution": "Attention-based Feature Extractor (AFE) aggregates neighbor information with learned importance scores"
      },
      {
        "type": "primary",
        "category": "Feedforward/MLP",
        "specific": null,
        "novel_contribution": "Graph Structure-Agnostic (GSA) layer to embed node (flow) attributes prior to graph message passing"
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "CICIDS 2017",
        "type": "public",
        "domain": "network_traffic",
        "link": "https://www.unb.ca/cic/datasets/ids-2017.html",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "ToN-IoT",
        "type": "public",
        "domain": "network_traffic",
        "link": "https://research.unsw.edu.au/projects/toniot-datasets",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can a new flow-as-nodes, weighted graph representation capture relevant topological information (e.g., multi-step attack relations, iterative malicious routines) to improve intrusion detection performance?",
        "Can a GNN-based framework that combines attention and convolutional graph modules effectively classify flows as malicious or normal?",
        "How should evaluation procedures be designed to avoid data leakage for reliable IDS validation?"
      ],
      "gaps_identified": [
        "Classical user-as-nodes, flow-as-edges graph representations are vulnerable to IP spoofing and provide limited multi-step/distributed attack topology information.",
        "Edge classification with GNNs is less mature and often requires costly line-graph transformations to recast flow classification as node classification.",
        "Knowledge-graph approaches can incur considerable memory consumption and can be evaded when using IP addresses as identifiers.",
        "Common IDS validation procedures risk data leakage; datasets are highly imbalanced, biasing models toward normal traffic."
      ],
      "limitations": [
        "Public IDS datasets (e.g., CICIDS 2017, ToN-IoT) are highly imbalanced, necessitating undersampling.",
        "Details on computational cost and real-time constraints are not provided in the excerpt."
      ],
      "future_work": [],
      "motivation": "Rising frequency and sophistication of cyber-attacks necessitate improved anomaly-based NIDS that can detect unseen attacks while mitigating false positives; existing graph representations are insufficient for multi-step attacks and are susceptible to IP spoofing and data leakage in evaluation.",
      "potential_research_ideas": [
        "Temporal/dynamic graph modeling to capture evolving multi-step attack sequences across time windows.",
        "Self-supervised pretraining on large unlabeled flow graphs (e.g., contrastive learning) to improve generalization and reduce label reliance.",
        "Cross-domain transfer and domain adaptation between enterprise and IoT datasets (CICIDS 2017 ↔ ToN-IoT) to improve robustness.",
        "Uncertainty estimation and calibrated anomaly scoring to assist human analysts in triaging alerts.",
        "Integrate edge features and temporal edge weights (e.g., inter-arrival statistics) for richer message passing.",
        "Graph pooling or subgraph mining to explicitly detect distributed attack motifs (e.g., star-shaped DDoS patterns).",
        "Evaluate and harden against adversarial examples on graphs (evasion/poisoning of flow features and topology)."
      ],
      "architectural_improvement_recommendations": [
        "Replace or complement the GCN SFE with GAT/GIN or edge-aware GNNs (e.g., EGNN, Graph Attention with edge weights) to better leverage similarity-weighted edges.",
        "Introduce temporal GNN layers (TGAT, TGN) to encode time-stamped flows and causal dependencies.",
        "Add an edge-feature channel and learn edge weights end-to-end (rather than fixed cosine similarity) with attention conditioned on edge attributes.",
        "Use hierarchical graph pooling or coarsening to summarize repeating malicious routines and scale to large networks.",
        "Apply class-imbalance handling in training (focal loss, class-weighting, hard negative mining) instead of or in addition to undersampling.",
        "Incorporate calibration layers and conformal prediction for reliable maliciousness scores and decision thresholds.",
        "Design leakage-free evaluation via temporal splits at flow-level and user/session-level isolation with blocked group k-fold."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Network edge/enterprise network (NIDS at firewall internal interface)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Data imbalance leading to biased classifiers",
        "Risk of data leakage in evaluation procedures",
        "Need for robust identifiers resilient to IP spoofing",
        "Graph construction and memory overhead on large traffic volumes"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Novel flow-based graph structure (flows as nodes, similarity-weighted edges) capturing malicious behavior patterns, relations between phases of multi-step attacks, and resilience to IP spoofing.",
      "GNN-based IDS framework with three steps: Graph Structure-Agnostic node feature embedding, Attention-based Feature Extractor, and GCN-based Spatial Feature Extractor, combined for flow classification.",
      "Identification of potential data leakage in classical IDS evaluation procedures and suggestion of a leakage-free validation strategy.",
      "Empirical demonstration that the proposed flow-graph and GNN framework outperform classical ML-based and prior GNN-based solutions on public datasets."
    ]
  },
  {
    "arxiv_id": "2309.02785v1",
    "title": "CVE-driven Attack Technique Prediction with Semantic Information Extraction and a Domain-specific Language Model",
    "authors": "Ehsan Aghaei; Ehab Al-Shaer",
    "abstract": "This paper addresses a critical challenge in cybersecurity: the gap between vulnerability information represented by Common Vulnerabilities and Exposures (CVEs) and the resulting cyberattack actions. CVEs provide insights into vulnerabilities, but often lack details on potential threat actions (tactics, techniques, and procedures, or TTPs) within the ATT&CK framework. This gap hinders accurate CVE categorization and proactive countermeasure initiation. The paper introduces the TTPpredictor tool, which uses innovative techniques to analyze CVE descriptions and infer plausible TTP attacks resulting from CVE exploitation. TTPpredictor overcomes challenges posed by limited labeled data and semantic disparities between CVE and TTP descriptions. It initially extracts threat actions from unstructured cyber threat reports using Semantic Role Labeling (SRL) techniques. These actions, along with their contextual attributes, are correlated with MITRE's attack functionality classes. This automated correlation facilitates the creation of labeled data, essential for categorizing novel threat actions into threat functionality classes and TTPs. The paper presents an empirical assessment, demonstrating TTPpredictor's effectiveness with accuracy rates of approximately 98% and F1-scores ranging from 95% to 98% in precise CVE classification to ATT&CK techniques. TTPpredictor outperforms state-of-the-art language model tools like ChatGPT. Overall, this paper offers a robust solution for linking CVEs to potential attack techniques, enhancing cybersecurity practitioners' ability to proactively identify and mitigate threats.",
    "published_date": "2023-09-06",
    "pdf_link": "https://arxiv.org/pdf/2309.02785v1",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Threat Intelligence",
      "subdomain": "ATT&CK Mapping",
      "specific_problem": "Predicting MITRE ATT&CK techniques (TTPs) from CVE descriptions via functionality classification",
      "attack_types": [
        "Create Account",
        "Create or Upload File",
        "Delete Files",
        "Disable Protections",
        "Install App",
        "Memory Modification",
        "Password Reset",
        "Change Ownership or Permissions",
        "Modify Configuration",
        "Obtain Sensitive Information: Credentials",
        "Obtain Sensitive Information: Other Data",
        "Read From Memory",
        "Read Files",
        "Memory Read",
        "Restart or Reboot",
        "Write To Existing File"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "SecureBERT",
        "novel_contribution": "Fine-tuned a cybersecurity domain-specific BERT (SecureBERT) with a context-oriented classifier that consumes SRL-derived SVO actions and surrounding context to map CVEs to functionalities and ATT&CK techniques."
      },
      {
        "type": "primary",
        "category": "Semantic Role Labeling",
        "specific": "AllenNLP SRL",
        "novel_contribution": "Used SRL to automatically extract threat actions (Subject-Verb-Object and arguments) from unstructured cyber threat reports/CVE descriptions to create labeled data via verb–object to functionality matching."
      },
      {
        "type": "baseline",
        "category": "Large Language Model",
        "specific": "ChatGPT",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Weakly-supervised"
    ],
    "datasets": [
      {
        "name": "Automatically labeled threat action SVO dataset for functionality classification",
        "type": "proprietary",
        "domain": "threat_reports",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "NVD CVE descriptions",
        "type": "public",
        "domain": "vulnerability_text",
        "link": "https://nvd.nist.gov/vuln",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "MITRE ATT&CK knowledge base (techniques/functionality classes)",
        "type": "public",
        "domain": "attack_knowledge_base",
        "link": "https://attack.mitre.org/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "ChatGPT",
        "paper_reference": null,
        "metric": "Accuracy, F1",
        "their_result": "“accuracy rates of approximately 98% and F1-scores ranging from 95% to 98%”",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "F1-score"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "RQ.1 : How can semantic role labeling techniques be effectively employed to generate a labeled dataset in the cybersecurity domain for different tasks?",
        "RQ.2 : To what extent does the utilization of context-oriented classification model design enhance the performance and robustness of the classification approach?",
        "RQ.3 : What are the strengths and limitations of the proposed methodology for the functionality-based classification of CVEs, and how can it be improved using state-of-the-art language models such as ChatGPT?"
      ],
      "gaps_identified": [
        "Lack of labeled datasets mapping CVEs to their corresponding TTPs/functionalities.",
        "Semantic gap between CVE descriptions and ATT&CK TTP descriptions that complicates automated mapping.",
        "Absence of domain-specific resources for this classification task.",
        "Interdependent/overlapping functionalities causing ambiguity (e.g., Install App vs Modify Configuration; Read/Memory Read vs Obtain Sensitive Information)."
      ],
      "limitations": [
        "No ground truth available to reliably distinguish certain highly similar functionalities; authors merged some classes (e.g., Install App and Modify Configuration).",
        "Functionalities exhibit dependencies (inheritance/commonality), making disambiguation difficult.",
        "Extraction captures functionality-related SVOs without awareness of other functionalities in the same report, challenging report-level labeling.",
        "Reliance on SRL and verb–object matching may miss nuanced context or lead to misclassification when context is complex."
      ],
      "future_work": [
        "Improve the methodology using state-of-the-art language models such as ChatGPT.",
        "Expand and refine the labeled dataset to better disambiguate overlapping functionalities.",
        "Model and incorporate functionality dependency structures during training and evaluation."
      ],
      "motivation": "Bridge the gap between CVE descriptions and ATT&CK TTPs to enable accurate CVE categorization and proactive countermeasures.",
      "potential_research_ideas": [
        "Construct a knowledge graph linking CVEs, CWEs, CAPEC, ATT&CK techniques, and extracted SVO actions; use graph neural networks for joint reasoning.",
        "Develop a multi-task model that jointly predicts functionalities and ATT&CK techniques with structured dependency constraints.",
        "Leverage retrieval-augmented transformers that pull evidence from CTI reports/NVD at inference time for improved context reasoning.",
        "Use contrastive learning to align SVO/action phrases with technique labels, improving robustness to phrasing variations.",
        "Incorporate active learning and weak supervision frameworks (e.g., data programming) to scale labeling beyond SRL VO matching.",
        "Evaluate temporal generalization (e.g., training on older CVEs and testing on newer CVEs) to assess real-world readiness."
      ],
      "architectural_improvement_recommendations": [
        "Introduce a dual-encoder architecture: one encoder for SRL-derived SVO graphs and one for full-text context, fused via cross-attention.",
        "Model functionality dependencies explicitly (inheritance/commonality) using a CRF or label graph regularization during classification.",
        "Adopt retrieval-augmented generation (RAG) or Fusion-in-Decoder to integrate external knowledge (ATT&CK pages, vendor advisories).",
        "Apply prompt-tuning or LoRA fine-tuning on strong LLM backbones and compare against SecureBERT, especially for zero-shot/low-shot cases.",
        "Use hierarchical transformers to handle report-level context while keeping sentence-level SRL evidence.",
        "Add calibration and selective prediction (abstention) mechanisms to handle ambiguous cases safely."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "AllenNLP"
      ],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Scarcity of labeled data for CVE-to-TTP mapping.",
        "Semantic disparities between CVE text and ATT&CK technique descriptions.",
        "Ambiguity due to dependencies and overlap among functionalities.",
        "Dependence on SRL quality and verb–object matching for label generation."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces TTPpredictor, a tool to infer ATT&CK techniques from CVE descriptions.",
      "Automated data labeling pipeline using SRL to extract threat actions (SVO) and map them to MITRE-defined functionality classes.",
      "Creation of a comprehensive, iteratively refined dataset of threat action sentences for training.",
      "Fine-tuning of a domain-specific language model (SecureBERT) with a context-oriented classifier combining SVO content and contextual attributes.",
      "Empirical results with approximately 98% accuracy and 95%–98% F1 for CVE-to-ATT&CK technique classification.",
      "Demonstrates that TTPpredictor outperforms state-of-the-art language model tools such as ChatGPT."
    ]
  },
  {
    "arxiv_id": "2310.10655v2",
    "title": "Enhancing Trustworthiness in ML-Based Network Intrusion Detection with Uncertainty Quantification",
    "authors": "Jacopo Talpini; Fabio Sartori; Marco Savi",
    "abstract": "The evolution of Internet and its related communication technologies have consistently increased the risk of cyber-attacks. In this context, a crucial role is played by Intrusion Detection Systems (IDSs), which are security devices designed to identify and mitigate attacks to modern networks. Data-driven approaches based on Machine Learning (ML) have gained more and more popularity for executing the classification tasks required by signature-based IDSs. However, typical ML models adopted for this purpose do not properly take into account the uncertainty associated with their prediction. This poses significant challenges, as they tend to produce misleadingly high classification scores for both misclassified inputs and inputs belonging to unknown classes (e.g. novel attacks), limiting the trustworthiness of existing ML-based solutions. In this paper, we argue that ML-based IDSs should always provide accurate uncertainty quantification to avoid overconfident predictions. In fact, an uncertainty-aware classification would be beneficial to enhance closed-set classification performance, would make it possible to carry out Active Learning, and would help recognize inputs of unknown classes as truly unknowns, unlocking open-set classification capabilities and Out-of-Distribution (OoD) detection. To verify it, we compare various ML-based methods for uncertainty quantification and for open-set classification, either specifically designed for or tailored to the domain of network intrusion detection. Moreover, we develop a custom model based on Bayesian Neural Networks to ensure reliable uncertainty estimates and improve the OoD detection capabilities, thus showing how proper uncertainty quantification can be exploited to significantly enhance the trustworthiness of ML-based IDSs.",
    "published_date": "2023-09-05",
    "pdf_link": "https://arxiv.org/pdf/2310.10655v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Uncertainty-aware ML-based network intrusion detection with open-set classification and Out-of-Distribution (OoD) detection",
      "attack_types": [
        "unknown attacks",
        "zero-day attacks",
        "Out-of-Distribution traffic"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Bayesian Neural Network",
        "specific": "Variational Inference BNN (Bayes-by-Backprop style)",
        "novel_contribution": "Custom BNN-based IDS with a post-training uncertainty recalibration method to enforce high uncertainty for inputs far from training data, improving OoD detection with minimal computational overhead"
      },
      {
        "type": "baseline",
        "category": "Ensemble Trees",
        "specific": "Random Forest (uncertainty-aware via ensemble disagreement/entropy)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Neural Network",
        "specific": "MLP with Softmax (uncertainty-unaware closed-set classifier)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Deterministic Uncertainty",
        "specific": "Deep Deterministic Uncertainty (DDU)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Energy-based model",
        "specific": "Energy-Based OoD Detection (applied to pre-trained classifier)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Energy-based model",
        "specific": "EFC (Energy-based Flow Classifier) for open-set classification",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Calibration/Open-set layer",
        "specific": "OpenMax",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Open-set classification",
      "Uncertainty quantification"
    ],
    "datasets": [
      {
        "name": "",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Uncertainty-unaware NN classifier (Softmax MLP)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Random Forest (uncertainty via ensemble entropy/decomposition)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Deep Deterministic Uncertainty (DDU)",
        "paper_reference": "[43]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Energy-Based OoD Detection",
        "paper_reference": "[38]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "EFC: Energy-based Flow Classifier",
        "paper_reference": "[57]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "OpenMax",
        "paper_reference": "[6]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "accuracy",
      "precision",
      "recall",
      "false positive rate",
      "predictive entropy (total uncertainty)",
      "epistemic uncertainty (information gain)",
      "aleatoric uncertainty (expected entropy)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can uncertainty-aware ML models improve the trustworthiness of ML-based IDS by avoiding overconfident predictions?",
        "How do different uncertainty quantification methods and open-set classifiers perform for network intrusion detection?",
        "Can a BNN-based approach provide reliable uncertainty estimates and enhance OoD detection without substantial computational overhead?"
      ],
      "gaps_identified": [
        "Most ML-based IDS research assumes closed-set conditions and ignores predictive uncertainty.",
        "Existing IDS models can be overconfident on misclassified and OoD inputs.",
        "Uncertainty quantification for IDS, especially to support active learning and open-set classification, is largely unexplored.",
        "Some open-set approaches (e.g., OCD) rely on synthetic unknowns, which may be impractical."
      ],
      "limitations": [
        "Experiments reported on a single open-source dataset [4] (dataset details not provided in the excerpt).",
        "BNNs typically require multiple forward passes at test time for uncertainty estimation (partially mitigated by the proposed recalibration)."
      ],
      "future_work": [
        "Extend evaluation to multiple and more diverse NIDS datasets and real network environments.",
        "Investigate deployment on constrained/edge settings and further reduce latency for uncertainty estimation.",
        "Integrate active learning pipelines leveraging decomposed uncertainties in operational IDS.",
        "Study robustness under distributional shifts across network environments and traffic patterns."
      ],
      "motivation": "Improve the trustworthiness of ML-based IDS by providing accurate uncertainty estimates to prevent overconfident errors, enable active learning, and recognize unknown attacks via open-set classification and OoD detection.",
      "potential_research_ideas": [
        "Combine the proposed BNN uncertainty recalibration with energy-based scoring to create a hybrid OoD detector.",
        "Meta-uncertainty adaptation: online calibration of epistemic uncertainty thresholds based on observed traffic drift.",
        "Few-shot adaptation for newly discovered attack classes guided by high-epistemic-uncertainty samples.",
        "Benchmark and standardize uncertainty metrics for IDS (calibration error, risk-coverage curves) across datasets.",
        "Explore conformal prediction atop BNN outputs for distribution-free coverage guarantees in IDS."
      ],
      "architectural_improvement_recommendations": [
        "Augment the BNN with deep ensembles to further stabilize epistemic estimates while keeping recalibration for far-OoD.",
        "Incorporate density estimators in feature space (e.g., Gaussian Mixture Models) alongside BNN predictive uncertainty.",
        "Use temperature scaling and class-conditional calibration jointly with the proposed recalibration for better closed-set calibration.",
        "Integrate an uncertainty-gated decision module that routes high-uncertainty flows to secondary analysis or human review.",
        "Leverage lightweight stochastic layers (e.g., MC Dropout) at inference for partial epistemic estimation under tight latency."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Overconfident predictions on misclassified and OoD inputs",
        "Handling unknown/zero-day attacks in open-world deployment",
        "Computational overhead of uncertainty estimation (e.g., BNN multiple forward passes)",
        "Labeling cost and data volume for IDS (motivation for active learning)",
        "Operating under resource constraints (e.g., edge environments)"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Critical comparison of uncertainty-aware ML models and open-set classifiers for network intrusion detection.",
      "A custom BNN-based model with a recalibration method to ensure reliable uncertainty estimates and improved OoD detection with minimal overhead.",
      "Demonstration that uncertainty-aware models (NNs, BNNs, RFs) can: (i) provide truthful uncertainty in closed-set classification, (ii) support active learning, and (iii) enhance OoD detection.",
      "Empirical evaluation on an open-source NIDS dataset, showing reduced false positives and higher robustness across OoD experiments versus state-of-the-art methods."
    ]
  },
  {
    "arxiv_id": "2401.01343v2",
    "title": "IoTGeM: Generalizable Models for Behaviour-Based IoT Attack Detection",
    "authors": "Kahraman Kostas; Mike Just; Michael A. Lones",
    "abstract": "Previous research on behavior-based attack detection for networks of IoT devices has resulted in machine learning models whose ability to adapt to unseen data is limited and often not demonstrated. This paper presents IoTGeM, an approach for modeling IoT network attacks that focuses on generalizability, yet also leads to better detection and performance. We first introduce an improved rolling window approach for feature extraction. To reduce overfitting, we then apply a multi-step feature selection process where a Genetic Algorithm (GA) is uniquely guided by exogenous feedback from a separate, independent dataset. To prevent common data leaks that have limited previous models, we build and test our models using strictly isolated train and test datasets. The resulting models are rigorously evaluated using a diverse portfolio of machine learning algorithms and datasets. Our window-based models demonstrate superior generalization compared to traditional flow-based models, particularly when tested on unseen datasets. On these stringent, cross-dataset tests, IoTGeM achieves F1 scores of 99\\% for ACK, HTTP, SYN, MHD, and PS attacks, as well as a 94\\% F1 score for UDP attacks. Finally, we build confidence in the models by using the SHAP (SHapley Additive exPlanations) explainable AI technique, allowing us to identify the specific features that underlie the accurate detection of attacks.",
    "published_date": "2023-10-17",
    "pdf_link": "https://arxiv.org/pdf/2401.01343v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "IoT Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Behaviour-based network intrusion detection for IoT devices using generalizable models evaluated across sessions and datasets (NIDS at network chokepoint)",
      "attack_types": [
        "Mirai Host Discovery (MHD)",
        "Scan Host Discovery (SHD)",
        "OS Version Detection (OSD)",
        "Telnet Brute-Force (BF)",
        "Port Scan (PS)",
        "ARP Spoofing (ARPS)",
        "UDP Flooding (UDPF)",
        "ACK Flooding (ACKF)",
        "HTTP Flooding (HTTPF)",
        "SYN Flooding (SYNF)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Feature Engineering",
        "specific": "Rolling-window statistical features (multi-dimensional: packet size, payload entropy, TCP flag statistics, etc.)",
        "novel_contribution": "Introduces an improved rolling window approach to extract rich, multi-dimensional statistical features aimed at generalization."
      },
      {
        "type": "primary",
        "category": "Genetic Algorithm",
        "specific": "GA-driven feature selection with exogenous feedback",
        "novel_contribution": "Feature subsets are evaluated for fitness on an entirely separate, unseen dataset to optimize for generalizability and reduce overfitting."
      },
      {
        "type": "primary",
        "category": "Tree Ensemble",
        "specific": "ExtraTrees (Extremely Randomized Trees)",
        "novel_contribution": "Used in a multi-step feature elimination stage, combined with Cohen’s Kappa-based validation to avoid overfitting/leakage."
      },
      {
        "type": "primary",
        "category": "Linear Model",
        "specific": "Logistic Regression",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Decision Tree",
        "specific": "CART-style DT",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Naive Bayes",
        "specific": "Gaussian/Multinomial NB (not explicitly specified)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "SVM",
        "specific": "Support Vector Machine",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Tree Ensemble",
        "specific": "Random Forest",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Instance-based",
        "specific": "k-Nearest Neighbours (KNN)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Neural Network",
        "specific": "Multilayer Perceptron (MLP)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Gradient Boosting",
        "specific": "XGBoost (XGB)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Feature Engineering",
        "specific": "Flow-based features (CICFlowMeter)",
        "novel_contribution": "Used as an internal baseline to contrast with rolling-window features under the same pipeline."
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "IoT-NID",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CIC-IoT-2022",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "MazeBolt attack examples (ACK/HTTPS flood)",
        "type": "public",
        "domain": "network_traffic",
        "link": "https://kb.mazebolt.com",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Flow-based features (CICFlowMeter) under same pipeline",
        "paper_reference": null,
        "metric": "F1 score",
        "their_result": "On cross-dataset tests, IoTGeM achieves F1=99% for ACK, HTTP, SYN, MHD, and PS; and 94% for UDP.",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "Precision",
      "Recall",
      "F1 score",
      "Cohen's Kappa"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can feature engineering via an improved rolling-window approach yield models that generalize to unseen sessions and entirely unseen datasets in IoT NIDS?",
        "Does a GA-based multi-step feature selection with exogenous (external dataset) feedback reduce overfitting and improve cross-dataset performance?",
        "How do window-based features compare to traditional flow-based features when evaluated under a strict, leakage-free pipeline?"
      ],
      "gaps_identified": [
        "Heavy reliance on outdated or non-IoT datasets; lack of applicability to modern IoT security.",
        "Use of private datasets hinders reproducibility and external validation.",
        "Many public datasets lack raw PCAPs, limiting novel feature extraction beyond pre-extracted flow features.",
        "Common methodological errors (data leakage, feature overfitting) undermine reported high performance and generalizability.",
        "Inappropriate use of metrics (e.g., accuracy only) on imbalanced data."
      ],
      "limitations": [
        "Data scarcity for some attacks necessitated the use of external sources (e.g., kb.mazebolt.com) for HTTPS and ACK flood variants.",
        "For SHD and MHD there were no equivalent data in other datasets; cross-variant testing (MHD to test SHD and vice versa) was used.",
        "For Brute-Force, training/validation/test used different variants (telnet BF, password attacks, RTSP BF), potentially introducing distribution shift beyond the targeted attack type."
      ],
      "future_work": [],
      "motivation": "To build intrusion detection models for IoT networks that avoid common pitfalls (data leakage, overfitting) and truly generalize to unseen sessions and datasets, using publicly available data and a reproducible pipeline.",
      "potential_research_ideas": [
        "Extend evaluation to additional real, multi-session IoT datasets and broader attack coverage, including encrypted traffic scenarios.",
        "Investigate semi-supervised or self-supervised pretraining on raw PCAPs to enhance generalization with limited labels.",
        "Apply domain adaptation/transfer learning to handle dataset shift between different IoT environments and devices.",
        "Incorporate online/continual learning with drift detection to maintain performance over time.",
        "Explore federated learning across organizations to improve privacy and generalization without sharing raw traffic.",
        "Augment data using realistic traffic generators or generative models to address class imbalance and rare attacks.",
        "Develop multi-attack (multi-label) and hierarchical detection frameworks to jointly model related attack families.",
        "Conduct adversarial robustness studies (e.g., evasion attacks) and defenses for behaviour-based IoT NIDS."
      ],
      "architectural_improvement_recommendations": [
        "Introduce temporal models (e.g., 1D CNNs, TCNs, or Transformers) on rolling-window sequences to capture richer dynamics.",
        "Use multi-objective GA for feature selection optimizing accuracy, generalization (external validation), and feature cost/latency.",
        "Replace or complement GA with Bayesian optimization or evolutionary strategies for joint feature-hyperparameter search.",
        "Add calibration (e.g., Platt scaling) and threshold optimization specific to deployment risk profiles.",
        "Implement robust nested cross-validation for HPO and feature selection to further reduce selection bias.",
        "Integrate drift detection (ADWIN/KS-tests) and online updates for real-world deployment.",
        "Leverage SHAP interaction values and counterfactual explanations to refine features and increase trust."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/kahramankostas/IoTGeM",
      "frameworks": [
        "scikit-learn",
        "XGBoost",
        "SHAP",
        "Scapy",
        "Wireshark",
        "CICFlowMeter"
      ],
      "reproducibility_score": "high",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Network-based IDS at a central chokepoint (gateway/router/SPAN port) monitoring IoT traffic",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Avoiding data leakage and identifying features that do not generalize across devices/sessions.",
        "Limited availability of real, multi-session IoT datasets for certain attack types.",
        "Requirement to capture and process raw PCAP traffic at a network chokepoint."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces an improved rolling-window feature extraction method that produces rich multi-dimensional statistics (e.g., packet size, payload entropy, TCP flag statistics).",
      "Proposes a novel multi-step feature selection process where a Genetic Algorithm is guided by exogenous feedback from an independent dataset to optimize generalizability.",
      "Implements strict train/test isolation to prevent data leakage and evaluates using cross-validation, independent sessions, and cross-dataset tests.",
      "Provides SHAP-based explainability to identify influential features underlying accurate attack detection.",
      "Demonstrates that window-based models generalize better than traditional flow-based models under a controlled pipeline.",
      "Reports high cross-dataset performance: “On these stringent, cross-dataset tests, IoTGeM achieves F1 scores of 99% for ACK, HTTP, SYN, MHD, and PS attacks, as well as a 94% F1 score for UDP attacks.”",
      "Releases modular code and uses publicly available datasets to facilitate validation and extension."
    ]
  },
  {
    "arxiv_id": "2309.09482v1",
    "title": "Spatio-temporal Co-attention Fusion Network for Video Splicing Localization",
    "authors": "Man Lin; Gang Cao; Zijie Lou",
    "abstract": "Digital video splicing has become easy and ubiquitous. Malicious users copy some regions of a video and paste them to another video for creating realistic forgeries. It is significant to blindly detect such forgery regions in videos. In this paper, a spatio-temporal co-attention fusion network (SCFNet) is proposed for video splicing localization. Specifically, a three-stream network is used as an encoder to capture manipulation traces across multiple frames. The deep interaction and fusion of spatio-temporal forensic features are achieved by the novel parallel and cross co-attention fusion modules. A lightweight multilayer perceptron (MLP) decoder is adopted to yield a pixel-level tampering localization map. A new large-scale video splicing dataset is created for training the SCFNet. Extensive tests on benchmark datasets show that the localization and generalization performances of our SCFNet outperform the state-of-the-art. Code and datasets will be available at https://github.com/multimediaFor/SCFNet.",
    "published_date": "2023-09-18",
    "pdf_link": "https://arxiv.org/pdf/2309.09482v1",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Digital Forensics",
      "subdomain": "Video Forensics",
      "specific_problem": "Video splicing (object insertion) tampering localization at pixel level",
      "attack_types": [
        "video splicing",
        "object insertion"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "CNN",
        "specific": "ResNet-101 (three-stream encoder)",
        "novel_contribution": "Three-stream encoder processes frames It-1, It, It+1 with integrated co-attention fusion across stages 2-4 to capture intra-/inter-frame tampering traces."
      },
      {
        "type": "primary",
        "category": "Attention/Co-attention",
        "specific": "Parallel Co-attention Module (PCM) and Cross Co-attention Module (CCM) with global-local attention",
        "novel_contribution": "Novel parallel and cross co-attention fusion to deeply interact and fuse spatio-temporal forensic features across adjacent and three consecutive frames; leverages GAC and GAF for global-local enhancement."
      },
      {
        "type": "primary",
        "category": "MLP Decoder",
        "specific": "All-MLP decoder (SegFormer-style)",
        "novel_contribution": "Lightweight MLP-only decoder aggregates multi-scale features for pixel-level mask prediction."
      },
      {
        "type": "primary",
        "category": "Attention",
        "specific": "Global-Local Attention Context (GAC) and Global-Local Attention Fusion (GAF)",
        "novel_contribution": "Fuse multi-branch features emphasizing local forensic cues and global semantics."
      },
      {
        "type": "baseline",
        "category": "CNN+Attention",
        "specific": "PSCC-Net",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN+Attention/Context",
        "specific": "VideoFact",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Dual-stream CNN",
        "specific": "OVFD",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN+Reinforcement Learning",
        "specific": "MDFFRL",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "VSD (Video Splicing Dataset)",
        "type": "synthetic",
        "domain": "manipulated_videos (video splicing)",
        "link": "https://github.com/multimediaFor/SCFNet",
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "GRIP",
        "type": "public",
        "domain": "tampered_videos (video forgery)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "VideoSham",
        "type": "public",
        "domain": "tampered_videos (video manipulations beyond faces)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "HTVD (HEVC-based Tampered Video Database)",
        "type": "public",
        "domain": "tampered_videos (HEVC)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "YouTube-VOS",
        "type": "public",
        "domain": "video_object_segmentation (source of foreground objects)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "GOT-10k",
        "type": "public",
        "domain": "video_object_tracking (source of foreground objects)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Video-ACID",
        "type": "public",
        "domain": "camera_identification videos (used as backgrounds)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "PSCC-Net",
        "paper_reference": "[6]",
        "metric": "IoU, F1 (GRIP)",
        "their_result": "IoU=0.441, F1=0.545 (fixed thr=0.5)",
        "baseline_result": "IoU=0.135, F1=0.243"
      },
      {
        "method_name": "PSCC-Net",
        "paper_reference": "[6]",
        "metric": "IoU, F1 (VideoSham)",
        "their_result": "IoU=0.241, F1=0.334 (fixed thr=0.5)",
        "baseline_result": "IoU=0.150, F1=0.228"
      },
      {
        "method_name": "PSCC-Net",
        "paper_reference": "[6]",
        "metric": "IoU, F1 (HTVD)",
        "their_result": "IoU=0.124, F1=0.178 (fixed thr=0.5)",
        "baseline_result": "IoU=0.073, F1=0.134"
      },
      {
        "method_name": "VideoFact",
        "paper_reference": "[3]",
        "metric": "IoU, F1 (GRIP)",
        "their_result": "IoU=0.441, F1=0.545 (fixed thr=0.5)",
        "baseline_result": "IoU=0.112, F1=0.221"
      },
      {
        "method_name": "VideoFact",
        "paper_reference": "[3]",
        "metric": "IoU, F1 (VideoSham)",
        "their_result": "IoU=0.241, F1=0.334 (fixed thr=0.5)",
        "baseline_result": "IoU=0.001, F1=0.088"
      },
      {
        "method_name": "VideoFact",
        "paper_reference": "[3]",
        "metric": "IoU, F1 (HTVD)",
        "their_result": "IoU=0.124, F1=0.178 (fixed thr=0.5)",
        "baseline_result": "IoU=0.000, F1=0.045"
      },
      {
        "method_name": "SCFNet (best threshold) vs. OVFD",
        "paper_reference": "[2]",
        "metric": "F1 (GRIP, best threshold)",
        "their_result": "F1=0.664 (IoU=0.572)",
        "baseline_result": "F1=0.641"
      },
      {
        "method_name": "SCFNet (best threshold) vs. MDFFRL",
        "paper_reference": "[12]",
        "metric": "IoU, F1 (GRIP, best threshold)",
        "their_result": "IoU=0.572, F1=0.664",
        "baseline_result": "IoU=0.512, F1=0.623"
      },
      {
        "method_name": "SCFNet (best threshold)",
        "paper_reference": null,
        "metric": "IoU, F1 (VideoSham, best threshold)",
        "their_result": "IoU=0.254, F1=0.351",
        "baseline_result": null
      },
      {
        "method_name": "SCFNet (best threshold)",
        "paper_reference": null,
        "metric": "IoU, F1 (HTVD, best threshold)",
        "their_result": "IoU=0.176, F1=0.242",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Intersection over Union (IoU)",
      "F1-score"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Existing video forensic methods often rely on single-frame information, ignoring temporal correlations, leading to limited generalization.",
        "Video compression weakens forensic traces, causing image-based forensic techniques to fail on videos.",
        "Lack of large-scale forged video datasets forces training on small video sets or image datasets, risking overfitting and missing temporal anomalies."
      ],
      "limitations": [
        "Qualitative examples show a few missed regions remain in complex scenes.",
        "Current work focuses on object insertion (splicing); does not yet address object removal or deepfake scenarios."
      ],
      "future_work": [
        "Expand the proposed method to handle more intricate video forgeries, including object removal and deepfake videos."
      ],
      "motivation": "Blindly and robustly localize spliced regions in videos by exploiting spatio-temporal cues and improving robustness to post-compression, while addressing the scarcity of large-scale video splicing training data.",
      "potential_research_ideas": [
        "Extend SCFNet to multi-type tampering (splicing, removal, inpainting, deepfake) via multi-task learning with shared encoder and type-specific decoders.",
        "Integrate motion compensation/optical flow or learned warping to better align temporal features before co-attention, reducing motion-induced noise.",
        "Self-supervised or contrastive pretraining on large unlabeled video corpora to learn tampering-agnostic temporal features that improve generalization.",
        "Compression- and codec-aware training: simulate diverse codecs/bitrates and incorporate a differentiable compression module to further enhance robustness.",
        "Domain adaptation across cameras/codecs: adversarial or test-time adaptation to mitigate domain shift from acquisition pipelines and compression settings.",
        "Hybrid forensic features: fuse frequency-domain artifacts (e.g., DCT residuals, quantization maps) alongside RGB streams to emphasize compression inconsistencies.",
        "Uncertainty estimation and calibration to provide per-pixel confidence, aiding forensic triage and human-in-the-loop review.",
        "Lightweight real-time variant for on-device screening, exploring knowledge distillation and dynamic inference."
      ],
      "architectural_improvement_recommendations": [
        "Replace or augment ResNet101 with a spatio-temporal transformer (e.g., Swin-T, VideoSwin, MViT) and deformable attention for long-range temporal dependencies.",
        "Add an explicit optical-flow or motion-residual branch and perform cross-attention between appearance and motion streams.",
        "Introduce temporal pyramids (multi-scale temporal context) and cross-frame windowed attention to handle variable motion patterns.",
        "Incorporate frequency-domain branches (e.g., DCT/HEVC bitstream side information) with cross-modal attention to capture compression artifacts.",
        "Enhance decoder with lightweight multi-scale feature aggregation (e.g., HRNet-like lateral paths) while preserving the MLP simplicity.",
        "Apply supervised contrastive losses across frames (tampered vs. authentic regions) to sharpen decision boundaries.",
        "Use calibration/uncertainty losses and focal/BCE hybrid to handle class imbalance in sparse splicing regions."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": "https://github.com/multimediaFor/SCFNet",
      "frameworks": [
        "PyTorch"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Single NVIDIA A800 GPU; input of 3 consecutive frames; batch size 10; frames resized to 512x512; SGD optimizer (lr=1e-4, momentum=0.9, weight decay=1e-5); 12 epochs with LR halved every 2 epochs; BCE loss. Data augmentation includes H.264 compression (CRF ∈ {15,23,30}) on 25% of training data."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes SCFNet, a spatio-temporal co-attention fusion network with a three-stream ResNet101 encoder for video splicing localization.",
      "Introduces novel parallel and cross co-attention fusion modules (with GAC/GAF) to deeply integrate inter-frame and intra-frame forensic features.",
      "Adopts a lightweight All-MLP decoder for pixel-level tampering localization.",
      "Constructs a new large-scale synthetic Video Splicing Dataset (VSD) for supervised training, with automated compositing using masks from VOS/tracking datasets and backgrounds from Video-ACID.",
      "Improves robustness via compression-based data augmentation (H.264 CRF 15/23/30).",
      "Demonstrates superior performance to state-of-the-art across GRIP, VideoSham, and HTVD; shows robustness to H.264 compression (F1 drop 0.545→0.504 from CRF 0→30)."
    ]
  },
  {
    "arxiv_id": "2308.15674v1",
    "title": "Predict And Prevent DDOS Attacks Using Machine Learning and Statistical Algorithms",
    "authors": "Azadeh Golduzian",
    "abstract": "A malicious attempt to exhaust a victim's resources to cause it to crash or halt its services is known as a distributed denial-of-service (DDoS) attack. DDOS attacks stop authorized users from accessing specific services available on the Internet. It targets varying components of a network layer and it is better to stop into layer 4 (transport layer) of the network before approaching a higher layer. This study uses several machine learning and statistical models to detect DDoS attacks from traces of traffic flow and suggests a method to prevent DDOS attacks. For this purpose, we used logistic regression, CNN, XGBoost, naive Bayes, AdaBoostClassifier, KNN, and random forest ML algorithms. In addition, data preprocessing was performed using three methods to identify the most relevant features. This paper explores the issue of improving the DDOS attack detection accuracy using the latest dataset named CICDDoS2019, which has over 50 million records. Because we employed an extensive dataset for this investigation, our findings are trustworthy and practical. Our target class (attack class) was imbalanced. Therefore, we used two techniques to deal with imbalanced data in machine learning. The XGboost machine learning model provided the best detection accuracy of (99.9999%) after applying the SMOTE approach to the target class, outperforming recently developed DDoS detection systems. To the best of our knowledge, no other research has worked on the most recent dataset with over 50 million records, addresses the statistical technique to select the most significant feature, has this high accuracy, and suggests ways to avoid DDOS attackI.",
    "published_date": "2023-08-30",
    "pdf_link": "https://arxiv.org/pdf/2308.15674v1",
    "paper_types": [
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Distributed Denial-of-Service (DDoS) attack detection from network flow traces",
      "attack_types": [
        "UDP Flood",
        "ICMP (Ping) Flood",
        "SYN Flood",
        "Ping of Death",
        "HTTP Flood",
        "DrDoS_UDP",
        "DrDoS_DNS",
        "DrDoS_LDAP",
        "DrDoS_SSDP",
        "DrDoS_MSSQL",
        "DrDoS_NetBIOS",
        "UDP-lag",
        "DrDoS_NTP",
        "TDTP"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Gradient Boosted Trees",
        "specific": "XGBoost",
        "novel_contribution": "Identified as best-performing classifier after SMOTE oversampling on CICDDoS2019, reporting 99.9999% accuracy."
      },
      {
        "type": "baseline",
        "category": "Random Forest",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Boosting",
        "specific": "AdaBoostClassifier",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Linear Model",
        "specific": "Logistic Regression",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Probabilistic",
        "specific": "Naive Bayes",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Distance-based",
        "specific": "k-Nearest Neighbors (KNN)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "Convolutional Neural Network",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Feature Selection",
        "specific": "ANOVA F-test (filter)",
        "novel_contribution": "Used as part of a three-pronged feature selection pipeline alongside correlation heatmap and embedded methods."
      },
      {
        "type": "primary",
        "category": "Feature Selection",
        "specific": "ExtraTreesClassifier (embedded importance)",
        "novel_contribution": "Used to rank top 30/20/10 features; identified key features (e.g., Inbound, URG Flag Count, Destination Port, Avg Bwd Segment Size, min-seg-size-forward, Source Port)."
      },
      {
        "type": "primary",
        "category": "Feature Selection",
        "specific": "Logistic regression p-value based elimination",
        "novel_contribution": "Used to eliminate features with non-significant p-values; combined with heatmap and ExtraTrees to avoid relying solely on p-values."
      },
      {
        "type": "primary",
        "category": "Imbalanced Learning",
        "specific": "SMOTE (Synthetic Minority Over-sampling Technique)",
        "novel_contribution": "Applied to balance attack/benign classes to ~50/50; reported as most effective among tested balancing strategies."
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Imbalanced learning (resampling via SMOTE)"
    ],
    "datasets": [
      {
        "name": "CICDDoS2019",
        "type": "public",
        "domain": "network_traffic",
        "link": "https://www.unb.ca/cic/datasets/ddos-2019.html",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Naive Bayes",
        "paper_reference": "[6,7,8] (related works using Naive Bayes for DDoS detection)",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Support Vector Machine",
        "paper_reference": "[9,10,11] (related works using SVM for DDoS detection)",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Decision Tree",
        "paper_reference": "[12,13] (related works using decision trees for DDoS detection)",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Deep Neural Network (DNN)",
        "paper_reference": "[14] Zeeshan Ahmad et al. on CICDDoS2019",
        "metric": "Accuracy",
        "their_result": "99.9999% (XGBoost after SMOTE)",
        "baseline_result": "94.57% (DNN on CICDDoS2019)"
      },
      {
        "method_name": "Random Forest (internal baseline)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "AdaBoost (internal baseline)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Logistic Regression (internal baseline)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Naive Bayes (internal baseline)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "KNN (internal baseline)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "CNN (internal baseline)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "Precision",
      "Recall"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Which machine learning model achieves the highest DDoS detection accuracy on CICDDoS2019?",
        "Which features are most relevant for DDoS detection to maintain high accuracy and low false-positive rate?",
        "How can class imbalance in CICDDoS2019 be addressed to improve detection performance?"
      ],
      "gaps_identified": [
        "Limited prior work using the most recent CICDDoS2019 dataset with over 50 million records.",
        "Insufficient use of statistical feature selection to identify the most significant features.",
        "Existing systems report lower accuracy on CICDDoS2019 compared to the claimed results.",
        "Few works suggest concrete methods to avoid/prevent DDoS beyond detection."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Improve DDoS detection accuracy using a large, recent dataset (CICDDoS2019), identify key features statistically, and propose prevention strategies.",
      "potential_research_ideas": [
        "Evaluate generalization across networks by cross-dataset validation (e.g., train on CICDDoS2019, test on CIC-IDS2017/UNSW-NB15) to assess overfitting and robustness.",
        "Develop a real-time streaming DDoS detector with concept drift handling and latency/throughput characterization.",
        "Explore cost-sensitive learning and focal loss as alternatives to SMOTE for imbalanced DDoS detection.",
        "Investigate adversarial robustness of tree ensembles and CNNs against evasion attacks on flow features.",
        "Add explainability (e.g., SHAP for XGBoost) to surface feature attributions for security analysts and rule synthesis.",
        "Extend to multi-class attack-type classification and hierarchical detection (family and subtype).",
        "Incorporate temporal/sequential models (1D CNN/TCN, Transformer) over flow sequences instead of single-flow tabular features.",
        "Perform ablation on feature preprocessing to detect potential data leakage and confirm causality of key features.",
        "Design a lightweight model for deployment at L4/L7 with memory/CPU budgeting for edge gateways."
      ],
      "architectural_improvement_recommendations": [
        "Replace SMOTE with class-weighted XGBoost or Balanced Random Forest to reduce synthetic sample bias.",
        "Integrate SHAP-based feature selection to stabilize selected features and align with model behavior.",
        "Adopt stacked generalization (stacking) combining tree ensembles and linear models for improved calibration.",
        "Use calibrated probabilities (Platt/Isotonic) and threshold optimization to control false positives.",
        "Implement streaming feature computation with windowed aggregation and incremental learning support.",
        "Add drift detection (ADWIN/DSD) to trigger retraining on changing traffic patterns."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Capacity issues for businesses that provide defense systems to stop DDoS attacks"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Analysis of the CICDDoS2019 dataset with over 50 million records and 88 features.",
      "Use of statistical and machine learning techniques to select the most relevant features (ANOVA, ExtraTrees, logistic regression, correlation heatmap).",
      "Evaluation of multiple ML models (logistic regression, CNN, XGBoost, Naive Bayes, AdaBoost, KNN, Random Forest) for DDoS detection.",
      "Addressing class imbalance with SMOTE; finding SMOTE most effective and balancing classes to ~50/50.",
      "Reported best accuracy of 99.9999% with XGBoost after SMOTE, claiming to outperform recent DDoS detection systems.",
      "Suggestion of methods to prevent DDoS attacks (high-level guidance)."
    ]
  },
  {
    "arxiv_id": "2309.03040v1",
    "title": "Automated CVE Analysis for Threat Prioritization and Impact Prediction",
    "authors": "Ehsan Aghaei; Ehab Al-Shaer; Waseem Shadid; Xi Niu",
    "abstract": "The Common Vulnerabilities and Exposures (CVE) are pivotal information for proactive cybersecurity measures, including service patching, security hardening, and more. However, CVEs typically offer low-level, product-oriented descriptions of publicly disclosed cybersecurity vulnerabilities, often lacking the essential attack semantic information required for comprehensive weakness characterization and threat impact estimation. This critical insight is essential for CVE prioritization and the identification of potential countermeasures, particularly when dealing with a large number of CVEs. Current industry practices involve manual evaluation of CVEs to assess their attack severities using the Common Vulnerability Scoring System (CVSS) and mapping them to Common Weakness Enumeration (CWE) for potential mitigation identification. Unfortunately, this manual analysis presents a major bottleneck in the vulnerability analysis process, leading to slowdowns in proactive cybersecurity efforts and the potential for inaccuracies due to human errors. In this research, we introduce our novel predictive model and tool (called CVEDrill) which revolutionizes CVE analysis and threat prioritization. CVEDrill accurately estimates the CVSS vector for precise threat mitigation and priority ranking and seamlessly automates the classification of CVEs into the appropriate CWE hierarchy classes. By harnessing CVEDrill, organizations can now implement cybersecurity countermeasure mitigation with unparalleled accuracy and timeliness, surpassing in this domain the capabilities of state-of-the-art tools like ChaptGPT.",
    "published_date": "2023-09-06",
    "pdf_link": "https://arxiv.org/pdf/2309.03040v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Vulnerability Management",
      "subdomain": "Vulnerability Scoring and Classification",
      "specific_problem": "Automated prediction of CVSS v3 base metrics and classification of CVEs into CWE classes from CVE descriptions",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "RoBERTa (domain-adapted SecureBERT)",
        "novel_contribution": "Fine-tuning SecureBERT on CVE descriptions for eight separate CVSS v3 base-metric classifiers; uses [CLS] representation for classification"
      },
      {
        "type": "primary",
        "category": "Feature Engineering / Fusion",
        "specific": "Customized TF-IDF signature-word vectors",
        "novel_contribution": "TF-IDF vectors derived from minority-class signature words are concatenated with SecureBERT outputs to mitigate severe class imbalance across CVSS metric values"
      },
      {
        "type": "primary",
        "category": "Feed-forward Neural Network",
        "specific": null,
        "novel_contribution": "Output-dense layer over fused SecureBERT+[TF-IDF] representation for per-metric multi-class prediction"
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "NVD CVE corpus with CVSS v3 base metric labels",
        "type": "public",
        "domain": "vulnerability_text",
        "link": "https://nvd.nist.gov/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "NVD CVE-to-CWE mappings",
        "type": "public",
        "domain": "vulnerability_text",
        "link": "https://nvd.nist.gov/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "ChatGPT",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can CVSS v3 base metric values be automatically predicted from unstructured CVE descriptions?",
        "Can CVEs be automatically mapped to appropriate CWE hierarchy classes?",
        "How can severe class imbalance across CVSS metric values be addressed effectively in training?"
      ],
      "gaps_identified": [
        "CVE descriptions often lack attack semantic details needed for comprehensive weakness characterization and threat impact estimation.",
        "Manual CVSS scoring and CWE mapping are labor-intensive, slow, and error-prone.",
        "Existing works do not effectively address the strong class imbalance present in labeled CVSS datasets."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Automate CVE analysis to enable timely and accurate vulnerability prioritization by predicting CVSS vectors and mapping CVEs to CWE classes, reducing reliance on slow and inconsistent manual processes.",
      "potential_research_ideas": [
        "Extend prediction beyond base metrics to include CVSS temporal and environmental metrics for full vector automation.",
        "Multi-task learning to jointly predict all CVSS base metrics and CWE classes with shared representations.",
        "Integrate external knowledge (e.g., CWE hierarchy, CAPEC/ATT&CK, exploit databases) via knowledge-graph embeddings to enrich context.",
        "Calibrated uncertainty estimation for per-metric predictions to guide analyst review and triage.",
        "Data augmentation and weak supervision using vendor advisories and security bulletins to increase minority classes.",
        "Active learning and continual learning to adapt models to newly published CVEs.",
        "Instruction-tuned or retrieval-augmented domain LLMs for explanation-backed predictions.",
        "Robustness studies against adversarial or noisy CVE texts and mitigation strategies."
      ],
      "architectural_improvement_recommendations": [
        "Adopt a unified multi-task architecture with shared SecureBERT backbone and metric-specific heads, plus a hierarchical head for CWE.",
        "Replace simple concatenation with attention-based or gating fusion between PLM embeddings and TF-IDF/keyword features.",
        "Use class-balanced or focal losses, logit adjustment, and re-sampling to complement TF-IDF-based imbalance handling.",
        "Leverage contrastive pretraining on CVE–CWE and CVE–CVSS pairs to align representations.",
        "Incorporate token-level NER for indicators (e.g., protocol, privilege terms) and aggregate with pooling for classification.",
        "Add retrieval augmentation from NVD/CWE entries to provide explicit definitions/examples during inference.",
        "Semi-/self-supervised learning using large unlabeled CVE corpora to improve minority-class representations."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Severe class imbalance in labeled CVSS data across metric values",
        "CVE descriptions may omit critical attack semantics, limiting prediction fidelity"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "CVEDrill: a tool that predicts CVSS v3 base metric values and classifies CVEs into CWE classes from their descriptions.",
      "A domain-specific approach leveraging SecureBERT (a RoBERTa-based cybersecurity language model) for CVE understanding.",
      "Fusion of SecureBERT representations with customized TF-IDF signature-word vectors to mitigate class imbalance.",
      "Eight independently trained classifiers, one per CVSS base metric, enabling granular prediction.",
      "Claims to surpass state-of-the-art tools like ChatGPT for CVE analysis and threat prioritization."
    ]
  },
  {
    "arxiv_id": "2309.04700v4",
    "title": "From Programming Bugs to Multimillion-Dollar Scams: An Analysis of Trapdoor Tokens on Uniswap",
    "authors": "Phuong Duy Huynh; Thisal De Silva; Son Hoang Dau; Xiaodong Li; Iqbal Gondal; Emanuele Viterbo",
    "abstract": "We investigate in this work a recently emerged type of scam ERC-20 token called Trapdoor, which has cost investors billions of US dollars on Uniswap, the largest decentralised exchange on Ethereum, from 2020 to 2023. In essence, Trapdoor tokens allow users to buy but preventing them from selling by embedding logical bugs and/or owner-only features in their smart contracts. By manually inspecting a number of Trapdoor samples, we established the first systematic classification of Trapdoor tokens and a comprehensive list of techniques that scammers used to embed and conceal malicious codes, accompanied by a detailed analysis of representative scam contracts. In particular, we developed TrapdoorAnalyser, a fine-grained detection tool that generates and crosschecks the error-log of a buy-and-sell test and the list of embedded Trapdoor indicators from a contract-semantic check to reliably identify a Trapdoor token. TrapdoorAnalyser not only outperforms the state-of-the-art commercial tool GoPlus in accuracy, but also provides traces of malicious code with a full explanation, which most of the existing tools lack. Using TrapdoorAnalyser, we constructed the very first dataset of about 30,000 Trapdoor and non-Trapdoor tokens on UniswapV2, which allows us to train several machine learning algorithms that can detect with very high accuracy even Trapdoor tokens with no available Solidity source codes.",
    "published_date": "2023-09-09",
    "pdf_link": "https://arxiv.org/pdf/2309.04700v4",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Blockchain Security",
      "subdomain": "DeFi/Smart Contract Security",
      "specific_problem": "Detection and analysis of Trapdoor scam ERC-20 tokens on UniswapV2",
      "attack_types": [
        "Trapdoor (sell restriction)",
        "Rug pull (liquidity withdrawal)",
        "Whitelist/Blacklist based selling restriction",
        "Fee manipulation (owner-only)",
        "Logic bugs (e.g., always-false conditions)",
        "Numerical exceptions (e.g., division by zero)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Rule-based program analysis (static + dynamic)",
        "specific": "TrapdoorAnalyser",
        "novel_contribution": "Cross-checks error-log from a buy-and-sell dynamic test with a contract-semantic indicator list to reliably identify Trapdoor tokens and provide explainable traces."
      },
      {
        "type": "primary",
        "category": "Classical ML (unspecified algorithms)",
        "specific": null,
        "novel_contribution": "Supervised models trained on features from on-chain transactions and token bytecode opcodes to detect Trapdoor tokens when Solidity source code is unavailable."
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "UniswapV2 liquidity pools dataset",
        "type": "public",
        "domain": "dex_pools",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "UniswapV2 tokens dataset",
        "type": "public",
        "domain": "smart_contracts",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Ground-truth Trapdoor/Non-Trapdoor tokens (UniswapV2)",
        "type": "public",
        "domain": "smart_contracts_and_blockchain_transactions",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "Sampled Trapdoor tokens for technique taxonomy (1,859)",
        "type": "public",
        "domain": "smart_contracts",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "GoPlus (commercial Trapdoor/Honeypot detection)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "F1 score",
      "Accuracy"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "What techniques do Trapdoor tokens use to trap investors and how are these techniques concealed in smart contracts?",
        "Can Trapdoor tokens be reliably detected by cross-validating dynamic buy/sell error logs with semantic indicators from static analysis?",
        "Can supervised ML models using transaction and opcode features detect Trapdoor tokens without Solidity source code?",
        "What is the scale and financial impact of Trapdoor scams on Uniswap from 2020–2023?"
      ],
      "gaps_identified": [
        "Lack of a systematic and comprehensive study of Trapdoor tokens on Uniswap.",
        "Existing Rug-pull detection approaches either work post-event or cannot distinguish Trapdoor type scams.",
        "State-of-the-art tools (e.g., GoPlus) lack explainability and miss advanced Trapdoor techniques.",
        "About 23% of ERC-20 tokens on UniswapV2 lack public source code, hindering source-based analysis."
      ],
      "limitations": [
        "The proposed cross-validated semantic + buy/sell test approach only works on tokens with publicly available Solidity source code."
      ],
      "future_work": [],
      "motivation": "Trapdoor tokens on Uniswap have caused massive investor losses; there is no comprehensive taxonomy or reliable, explainable detector, especially for tokens without source code.",
      "potential_research_ideas": [
        "Generalize Trapdoor detection to other DEX versions (UniswapV3) and EVM-compatible chains (BSC, Polygon) and cross-chain forks.",
        "Real-time, pre-trade Trapdoor risk scoring in the mempool using bytecode-only features and fast emulation.",
        "Use graph neural networks over control/data-flow graphs extracted from bytecode to detect concealed Trapdoor patterns.",
        "Combine symbolic execution and greybox fuzzing with the current buy/sell simulator to uncover more evasive sell-blocking logic.",
        "Semi-supervised or PU learning to leverage large unlabeled token populations and reduce labeling cost.",
        "Develop an on-chain warning/oracle service that publishes signed Trapdoor indicators for wallets/aggregators to consume.",
        "Adversarial-evasion study where scammers adapt logic to bypass detectors; develop robustness benchmarks."
      ],
      "architectural_improvement_recommendations": [
        "Augment TrapdoorAnalyser with symbolic execution and selective fuzzing to exercise hidden branches and time-gated conditions.",
        "Build a bytecode decompiler + IR pipeline and extract CFG/DFG features; train a GNN/Transformer model on graphs/opcode sequences.",
        "Feature expansion: liquidity events, LP token mint/burn patterns, creator behavioral features, and temporal transaction motifs.",
        "Ensemble ML with calibrations (e.g., stacking tree models with Transformer embeddings of bytecode) and conformal prediction for risk-aware outputs.",
        "Streaming architecture for incremental learning from newly listed tokens and continual adaptation to novel Trapdoor tactics.",
        "Integrate explanation generation for ML predictions via attention/feature attribution aligned to specific bytecode locations/indicators."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Offline analysis of UniswapV2 (Ethereum) tokens and pools; applicable to EVM chains",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Dynamic buy/sell test may incur gas costs and is constrained by on-chain conditions (e.g., black/white lists).",
        "Evasion via obfuscation or time/role-gated logic paths not covered by simple test scenarios.",
        "Coverage of tokens without source code requires robust bytecode-only analysis.",
        "Generalizing beyond UniswapV2 and across chains requires handling different router/pair semantics.",
        "High churn of newly listed tokens demands streaming-scale data ingestion and feature computation."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "First systematic classification of Trapdoor tokens and techniques, including concealment maneuvers (with real-world examples).",
      "TrapdoorAnalyser: a fine-grained detection tool that “generates and crosschecks the error-log of a buy-and-sell test and the list of embedded Trapdoor indicators from a contract-semantic check.”",
      "Outperforms the state-of-the-art commercial tool GoPlus in accuracy and provides explainable traces of malicious code.",
      "Construction of the first ground-truth dataset of ≈30,000 UniswapV2 tokens (11,943 Trapdoor; 18,548 non-Trapdoor).",
      "Supervised ML models using transaction and opcode features to detect Trapdoor tokens without Solidity source code, achieving very high F1 scores.",
      "Comprehensive impact analysis: Trapdoor scams on Uniswap cost investors roughly a billion USD (2020–2023) involving ~267k investor addresses."
    ]
  },
  {
    "arxiv_id": "2309.08019v2",
    "title": "CRYPTO-MINE: Cryptanalysis via Mutual Information Neural Estimation",
    "authors": "Benjamin D. Kim; Vipindev Adat Vasudevan; Jongchan Woo; Alejandro Cohen; Rafael G. L. D'Oliveira; Thomas Stahlbuhk; Muriel Médard",
    "abstract": "The use of Mutual Information (MI) as a measure to evaluate the efficiency of cryptosystems has an extensive history. However, estimating MI between unknown random variables in a high-dimensional space is challenging. Recent advances in machine learning have enabled progress in estimating MI using neural networks. This work presents a novel application of MI estimation in the field of cryptography. We propose applying this methodology directly to estimate the MI between plaintext and ciphertext in a chosen plaintext attack. The leaked information, if any, from the encryption could potentially be exploited by adversaries to compromise the computational security of the cryptosystem. We evaluate the efficiency of our approach by empirically analyzing multiple encryption schemes and baseline approaches. Furthermore, we extend the analysis to novel network coding-based cryptosystems that provide individual secrecy and study the relationship between information leakage and input distribution.",
    "published_date": "2023-09-14",
    "pdf_link": "https://arxiv.org/pdf/2309.08019v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Cryptography",
      "subdomain": "Cryptanalysis",
      "specific_problem": "Estimating plaintext–ciphertext mutual information to detect information leakage and assess cryptosystem strength (including network-coding-based schemes with individual secrecy) under chosen-plaintext settings",
      "attack_types": [
        "Chosen-plaintext attack (CPA)",
        "Information leakage analysis"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Neural MI Estimator (critic network / MLP)",
        "specific": "MINE (Mutual Information Neural Estimation) with Donsker–Varadhan KL bound",
        "novel_contribution": "Application of MINE directly to plaintext–ciphertext pairs for cryptanalysis; provides a practical MI-based leakage test for cryptosystems and network-coding-based cryptosystems (HUNCC)."
      },
      {
        "type": "primary",
        "category": "Regularization/Stabilization for MI estimation",
        "specific": "Regularized MINE per Choi & Lee (2020)",
        "novel_contribution": "Uses a stabilization term to mitigate high-variance large-MI estimates, enabling more stable convergence on high-dimensional plaintext–ciphertext data."
      },
      {
        "type": "baseline",
        "category": "Classical Information Theory",
        "specific": "Shannon entropy calculations for sanity checks (e.g., theoretical I(X;Y) in no-encryption case)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "Uniform random plaintext–ciphertext pairs (16-byte blocks)",
        "type": "synthetic",
        "domain": "plaintext_ciphertext_pairs",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Gilbert–Elliott generated input sequences (varying uniformity, 8 channels × 128 bits)",
        "type": "synthetic",
        "domain": "plaintext_sequences_for_network_coding_and_AES",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "HUNCC setting data (random linear encoding over 8 links; first link AES-128-ECB encrypted)",
        "type": "synthetic",
        "domain": "network_coding_cryptosystem_io",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "No encryption (X=Y)",
        "paper_reference": null,
        "metric": "Estimated MI (nats)",
        "their_result": "INO(X;Y) = 9.7 nats",
        "baseline_result": "Theoretical upper bound H(X) ≃ 11.09 nats (uniform 16-byte input)"
      },
      {
        "method_name": "XOR with repeating key",
        "paper_reference": null,
        "metric": "Estimated MI (nats)",
        "their_result": "IXOR(X;Y) = 7.8 nats",
        "baseline_result": null
      },
      {
        "method_name": "One-Time Pad (OTP, perfect secrecy)",
        "paper_reference": "Shannon 1949",
        "metric": "Estimated MI (nats)",
        "their_result": "≈ 0.05 nats",
        "baseline_result": "True MI = 0 nats"
      },
      {
        "method_name": "OTP with key provided to estimator (sanity)",
        "paper_reference": null,
        "metric": "Estimated MI (nats)",
        "their_result": null,
        "baseline_result": "High MI expected as correlation becomes learnable (value not numerically reported)"
      },
      {
        "method_name": "AES-ECB (uniform inputs)",
        "paper_reference": "Daemen & Rijmen (AES proposal)",
        "metric": "Estimated MI (nats)",
        "their_result": "I(X;Y) = 0.07 nats",
        "baseline_result": null
      },
      {
        "method_name": "AES-CTR (uniform inputs)",
        "paper_reference": "NIST SP 800-38G modes",
        "metric": "Estimated MI (nats)",
        "their_result": "I(X;Y) ≈ 0.07 nats (very low)",
        "baseline_result": null
      },
      {
        "method_name": "AES-ECB (non-uniform, correlated inputs)",
        "paper_reference": null,
        "metric": "Estimated MI (nats)",
        "their_result": "I(X;Y) = 2.1 nats",
        "baseline_result": null
      },
      {
        "method_name": "Single SPN block cipher",
        "paper_reference": null,
        "metric": "Estimated MI (nats)",
        "their_result": "I(X;Y) = 1.1 nats",
        "baseline_result": null
      },
      {
        "method_name": "Caesar cipher (stream cipher)",
        "paper_reference": null,
        "metric": "Estimated MI (nats)",
        "their_result": "I(X;Y) ≈ 3.1 nats",
        "baseline_result": null
      },
      {
        "method_name": "HUNCC (8 links; 1 link AES-ECB), α=0.01 (very non-uniform inputs)",
        "paper_reference": "Cohen et al. (HUNCC)",
        "metric": "Estimated MI (nats)",
        "their_result": "7.2542",
        "baseline_result": "AES-CTR: 0.0384; AES-ECB: 0.0536"
      },
      {
        "method_name": "HUNCC, α=0.02",
        "paper_reference": null,
        "metric": "Estimated MI (nats)",
        "their_result": "5.1399",
        "baseline_result": "AES-CTR: 0.0411; AES-ECB: 0.0411"
      },
      {
        "method_name": "HUNCC, α=0.03",
        "paper_reference": null,
        "metric": "Estimated MI (nats)",
        "their_result": "3.1801",
        "baseline_result": "AES-CTR: 0.0395; AES-ECB: 0.0384"
      },
      {
        "method_name": "HUNCC, α=0.05",
        "paper_reference": null,
        "metric": "Estimated MI (nats)",
        "their_result": "1.1455",
        "baseline_result": "AES-CTR: 0.0374; AES-ECB: 0.0263"
      },
      {
        "method_name": "HUNCC, α=0.075",
        "paper_reference": null,
        "metric": "Estimated MI (nats)",
        "their_result": "0.1447",
        "baseline_result": "AES-CTR: 0.0354; AES-ECB: 0.0281"
      },
      {
        "method_name": "HUNCC, α=0.10",
        "paper_reference": null,
        "metric": "Estimated MI (nats)",
        "their_result": "0.0319",
        "baseline_result": "AES-CTR: 0.0452; AES-ECB: 0.0298"
      },
      {
        "method_name": "HUNCC, α=0.15",
        "paper_reference": null,
        "metric": "Estimated MI (nats)",
        "their_result": "0.0302",
        "baseline_result": "AES-CTR: 0.0258; AES-ECB: 0.0371"
      },
      {
        "method_name": "HUNCC, α=0.20",
        "paper_reference": null,
        "metric": "Estimated MI (nats)",
        "their_result": "0.0233",
        "baseline_result": "AES-CTR: 0.0377; AES-ECB: 0.0334"
      },
      {
        "method_name": "HUNCC, α=0.50 (uniform inputs)",
        "paper_reference": null,
        "metric": "Estimated MI (nats)",
        "their_result": "0.0270",
        "baseline_result": "AES-CTR: 0.0359; AES-ECB: 0.0324"
      }
    ],
    "performance_metrics_used": [
      "Mutual Information (nats)",
      "Shannon entropy of inputs (nats)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can neural MI estimation (MINE) reveal information leakage between plaintext and ciphertext for various cryptosystems under chosen-plaintext access?",
        "How does input distribution (degree of uniformity/correlation) affect MI leakage, particularly for network-coding-based cryptosystems that provide individual secrecy (HUNCC)?",
        "Can MINE be used as a practical tool to evaluate cryptosystem strength and model a computationally limited adversary?"
      ],
      "gaps_identified": [
        "Direct application of MI between plaintext and ciphertext for chosen-plaintext cryptanalysis has not been widely explored.",
        "Estimating MI between unknown, high-dimensional variables is challenging due to variance and stability issues.",
        "Schemes assuming perfectly uniform inputs may exhibit leakage for non-uniform inputs; practical guidance is limited."
      ],
      "limitations": [
        "Estimator provides a lower bound and can have high variance for large MI; requires stabilization.",
        "The approach shows \"limitations in learning about stronger systems such as AES.\"",
        "Experiments rely on synthetic data and chosen-plaintext access; no key recovery is attempted—only leakage estimation.",
        "Large sample sizes (100k–500k) and long training (2000–5000 epochs) are needed for stable estimates."
      ],
      "future_work": [
        "Extend MI-based analysis to broader classes of cryptosystems and physical-layer secure schemes.",
        "Systematize input uniformization (e.g., compression) and quantify its effect across workloads and ciphers.",
        "Investigate tighter/alternative MI bounds and estimators for improved stability and sample-efficiency on cryptographic data."
      ],
      "motivation": "Provide a practical MI-based cryptanalysis tool to detect leakage directly between plaintext and ciphertext and to assess the security of cryptosystems (including individual secrecy schemes) under realistic input distributions.",
      "potential_research_ideas": [
        "Design and benchmark multiple MI estimators (MINE, InfoNCE/NCE, NWJ, CLUB, variational bounds) on cryptographic IO to assess stability, sample-efficiency, and detectability thresholds.",
        "Link MI estimates to concrete key-search complexity reductions to quantify real security impact under CPA.",
        "Develop adaptive input uniformization pipelines (e.g., compression + randomness extraction) and measure leakage across applications and traffic types.",
        "Construct a public cryptographic MI-leakage benchmark suite with standardized generators, ciphers, and evaluation protocols.",
        "Apply representation learning to localize which parts of plaintext/ciphertext contribute most to leakage (feature attribution for ciphers).",
        "Evaluate MI leakage under constrained samples/online settings and with noisy or partially observed ciphertexts.",
        "Integrate MI-minimization (adversarial training) as a defense: train encoding/encryption parameters to minimize estimated MI leakage."
      ],
      "architectural_improvement_recommendations": [
        "Upgrade the critic to deeper/wider MLPs or sequence models (e.g., CNN/Transformer) to better capture byte-level dependencies.",
        "Adopt alternative divergence estimators (InfoNCE with temperature, NWJ, DV with moving-average baseline) and gradient penalties to stabilize training.",
        "Use joint encoders for X and Y with shared embeddings and multi-scale critics to capture local/global correlations.",
        "Apply variance reduction (importance sampling, EMA of log-sum-exp term) and minibatch stratification.",
        "Perform rigorous ablations on batch size, learning rate schedules, and number of negatives for contrastive estimators."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Neural MI estimator with two hidden layers of 100 nodes (ReLU); inputs of 32–256 nodes depending on setting; batch size 10,000; learning rate 1e-4; 2,000–5,000 epochs; 100,000–500,000 samples per experiment."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Requires chosen-plaintext access to gather plaintext–ciphertext pairs.",
        "Training stability and variance issues for large MI; needs many samples and epochs.",
        "Limited utility against strong ciphers (e.g., AES) with uniform inputs; leakage largely undetectable.",
        "Input distribution strongly affects leakage; non-uniformity can induce MI in some schemes.",
        "Potential increase in MI with key reuse over large files (noted risk)."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces CRYPTO-MINE: a practical use of neural MI estimation directly between plaintext and ciphertext to detect leakage under CPA.",
      "Empirical evaluation across baseline scenarios (no encryption, OTP, XOR), classical ciphers (Caesar, SPN), and AES modes (ECB/CTR).",
      "Extends analysis to HUNCC (network coding + partial encryption) and quantifies how input non-uniformity induces leakage, with detailed MI vs. input randomness (Gilbert–Elliott α).",
      "Provides practical guidance: minimal input randomization or lossless compression can sufficiently uniformize inputs so that HUNCC leakage matches AES.",
      "Documents training setup and stabilization required for MI estimation on high-dimensional cryptographic data."
    ]
  },
  {
    "arxiv_id": "2310.05952v1",
    "title": "Mitigating Denial of Service Attacks in Fog-Based Wireless Sensor Networks Using Machine Learning Techniques",
    "authors": "Ademola Abidoye; Ibidun Obagbuwa; Nureni Azeez",
    "abstract": "Wireless sensor networks are considered to be among the most significant and innovative technologies in the 21st century due to their wide range of industrial applications. Sensor nodes in these networks are susceptible to a variety of assaults due to their special qualities and method of deployment. In WSNs, denial of service attacks are common attacks in sensor networks. It is difficult to design a detection and prevention system that would effectively reduce the impact of these attacks on WSNs. In order to identify assaults on WSNs, this study suggests using two machine learning models: decision trees and XGBoost. The WSNs dataset was the subject of extensive tests to identify denial of service attacks. The experimental findings demonstrate that the XGBoost model, when applied to the entire dataset, has a higher true positive rate (98.3%) than the Decision tree approach (97.3%) and a lower false positive rate (1.7%) than the Decision tree technique (2.7%). Like this, with selected dataset assaults, the XGBoost approach has a higher true positive rate (99.01%) than the Decision tree technique (97.50%) and a lower false positive rate (0.99%) than the Decision tree technique (2.50%).",
    "published_date": "2023-09-10",
    "pdf_link": "https://arxiv.org/pdf/2310.05952v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "IoT Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Detection and mitigation of Denial-of-Service (DoS) attacks in fog-based Wireless Sensor Networks (WSNs)",
      "attack_types": [
        "Denial of Service (DoS)",
        "Flooding",
        "Gray hole",
        "Black hole",
        "Selective Forwarding"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Ensemble/Gradient Boosted Trees",
        "specific": "XGBoost",
        "novel_contribution": "Applied to detect DoS attacks in fog-based WSNs; achieves higher TPR and lower FPR than Decision Trees on the used WSN dataset."
      },
      {
        "type": "primary",
        "category": "Decision Tree",
        "specific": null,
        "novel_contribution": "Used as a comparative classical model for DoS detection in WSNs; evaluated with entropy-based splitting."
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "WSNs dataset",
        "type": "",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      }
    ],
    "baselines": [
      {
        "method_name": "Decision Tree",
        "paper_reference": null,
        "metric": "True Positive Rate (full dataset)",
        "their_result": "98.3%",
        "baseline_result": "97.3%"
      },
      {
        "method_name": "Decision Tree",
        "paper_reference": null,
        "metric": "False Positive Rate (full dataset)",
        "their_result": "1.7%",
        "baseline_result": "2.7%"
      },
      {
        "method_name": "Decision Tree",
        "paper_reference": null,
        "metric": "True Positive Rate (selected attack subset)",
        "their_result": "99.01%",
        "baseline_result": "97.50%"
      },
      {
        "method_name": "Decision Tree",
        "paper_reference": null,
        "metric": "False Positive Rate (selected attack subset)",
        "their_result": "0.99%",
        "baseline_result": "2.50%"
      }
    ],
    "performance_metrics_used": [
      "True Positive Rate (TPR)",
      "False Positive Rate (FPR)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Traditional ML techniques like SVM are not ideal for classification on large datasets due to limited capacity to learn from many features and high computational cost.",
        "Classical ML performs better for known attacks than for unknown (zero-day) attacks; false alarm rates increase with network growth.",
        "Some prior works lacked comprehensive comparisons with related methods.",
        "Some deep learning studies evaluated on small numbers of sensor nodes, raising scalability concerns.",
        "Using raw features without robust representation learning can reduce detection accuracy."
      ],
      "limitations": [
        "Only Flooding and Gray hole attack detection were implemented and evaluated; Black hole and Selective Forwarding are stated as targeted but not empirically demonstrated.",
        "Dataset details (source, availability, features) are not specified, limiting reproducibility."
      ],
      "future_work": [],
      "motivation": "Design an effective detection and prevention system to mitigate DoS attacks in resource-constrained WSNs by leveraging a fog-based architecture and supervised learning to achieve high detection rates and low false positives.",
      "potential_research_ideas": [
        "Extend evaluation to additional WSN attack types (Black hole, Selective Forwarding) with labeled datasets and scenario diversity.",
        "Develop online/streaming learning on fog nodes to handle concept drift and evolving attack patterns.",
        "Incorporate transfer learning or meta-learning to improve generalization to unseen WSN deployments and zero-day attacks.",
        "Investigate federated learning across fog nodes to preserve data locality and privacy while improving global models.",
        "Augment training with realistic adversarial examples and simulation-in-the-loop to enhance robustness.",
        "Design lightweight, energy-aware models tailored to sensor and fog constraints with adaptive complexity."
      ],
      "architectural_improvement_recommendations": [
        "Integrate a feature engineering pipeline at fog nodes (e.g., time-windowed traffic statistics, node behavior sequences) and perform model inference locally with edge acceleration.",
        "Adopt gradient-boosted tree variants with monotonic constraints or calibrated probability outputs to reduce false positives.",
        "Introduce an ensemble combining XGBoost with simple anomaly detection (e.g., one-class models) to capture unseen attacks.",
        "Implement hierarchical detection: quick lightweight screening at sensors, refined classification at fog, and correlation/forensics in the cloud.",
        "Add explainability tooling (e.g., SHAP for tree models) to support operator trust and incident response."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Fog-based WSN with three-layer architecture: sensor nodes (edge), fog nodes (intermediate), and cloud data centers (top layer).",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Energy constraints and limited computation on sensor nodes.",
        "Vulnerability of multi-hop wireless communication to various attacks.",
        "Potential overhead and latency trade-offs in hierarchical architectures."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes a three-layer fog-based architecture for WSN security and data processing.",
      "Design targets prevention of Selective Forwarding, Black hole, Flooding, and Gray hole attacks in WSNs.",
      "Implements detection of Flooding and Gray hole attacks using a WSNs dataset.",
      "Employs Decision Tree and XGBoost classifiers to identify DoS attacks from sensor node behavior.",
      "Demonstrates that XGBoost achieves higher TPR (up to 99.01%) and lower FPR (down to 0.99%) than Decision Trees on the evaluated datasets."
    ]
  },
  {
    "arxiv_id": "2308.06695v1",
    "title": "Helion: Enabling Natural Testing of Smart Homes",
    "authors": "Prianka Mandal; Sunil Manandhar; Kaushal Kafle; Kevin Moran; Denys Poshyvanyk; Adwait Nadkarni",
    "abstract": "Prior work has developed numerous systems that test the security and safety of smart homes. For these systems to be applicable in practice, it is necessary to test them with realistic scenarios that represent the use of the smart home, i.e., home automation, in the wild. This demo paper presents the technical details and usage of Helion, a system that uses n-gram language modeling to learn the regularities in user-driven programs, i.e., routines developed for the smart home, and predicts natural scenarios of home automation, i.e., event sequences that reflect realistic home automation usage. We demonstrate the HelionHA platform, developed by integrating Helion with the popular Home Assistant smart home platform. HelionHA allows an end-to-end exploration of Helion's scenarios by executing them as test cases with real and virtual smart home devices.",
    "published_date": "2023-08-13",
    "pdf_link": "https://arxiv.org/pdf/2308.06695v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "IoT Security",
      "subdomain": "Smart Home Security",
      "specific_problem": "Generating realistic smart home automation event scenarios to test security and safety analyses/systems",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Language Model (n-gram)",
        "specific": "n-gram LM with interpolated smoothing (order n≥3, typically 3- or 4-gram)",
        "novel_contribution": "Applies statistical n-gram language modeling to user-driven smart home routines to learn regularities and generate natural (and intentionally unnatural) automation scenarios for testing security/safety systems"
      }
    ],
    "learning_paradigm": [
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "HOME corpus (Helion)",
        "type": "public",
        "domain": "smart_home_event_sequences",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Evaluations of smart home security systems often use random permutations of events that do not reflect realistic usage",
        "Collecting real execution traces from end-user homes is privacy-invasive and may include superfluous, platform/device-specific noise",
        "Need for synthetically generated yet realistic home automation scenarios for effective, practical testing"
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Enable practical and realistic testing of smart home security and safety systems by learning natural usage patterns from user-driven routines and generating executable scenarios.",
      "potential_research_ideas": [
        "Replace or augment n-gram models with neural sequence models (e.g., Transformers) to capture longer-range dependencies across events",
        "Condition scenario generation on richer context (household profiles, occupancy, time-of-day, location, season) to increase realism",
        "Incorporate privacy-preserving data collection (federated analytics, differential privacy) for scaling beyond survey-derived routines",
        "Develop quantitative realism metrics and human-in-the-loop validation for generated scenarios; compare to real (opt-in) traces where feasible",
        "Coverage-guided or property-driven scenario generation targeting specific security/safety policies and known vulnerability classes",
        "Adversarial/constrained scenario generation to elicit unsafe states or policy violations under platform/device constraints",
        "Integrate Helion with formal policy/spec checkers to automatically label scenarios as safe/unsafe and close the loop for guided generation",
        "Cross-platform adapters (SmartThings, Google Home, Apple Home) to generalize beyond Home Assistant and evaluate portability",
        "Augment tokens with device topology and interdependencies (e.g., rooms, hubs) and learn graph-conditioned sequences",
        "Online learning from execution feedback to refine the model in deployed environments while respecting privacy"
      ],
      "architectural_improvement_recommendations": [
        "Adopt state-of-the-art smoothing (e.g., modified Kneser-Ney) and backoff strategies; compare with neural LMs (Transformer LM) using constrained decoding",
        "Introduce conditional generation heads (time/day/frequency indicators) with beam search, nucleus sampling, and diversity controls",
        "Encode device capabilities and constraints as a grammar or schema to ensure only feasible events are generated",
        "Add uncertainty estimates and calibration to rank scenario realism and prioritize tests",
        "Use reinforcement learning or bandit optimization to generate scenarios that maximize policy violation detection or coverage metrics",
        "Implement plug-in interfaces for policy checkers and simulators; support coverage metrics (state, rule, and device coverage)",
        "Extend tokenization to include environment/context tokens (presence, weather) and hierarchical sequences (routines, subroutines)"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/Secure-Platforms-Lab-W-M/Helion-on-Home-Assistant",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Home Assistant-based smart home with real and virtual devices",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": true,
      "fairness_bias": false
    },
    "contributions": [
      "Helion: a framework that models user-driven routines with n-gram language models to generate natural and unnatural smart home automation scenarios",
      "HelionHA: integration with the Home Assistant platform enabling end-to-end generation and execution of scenarios on real/virtual devices",
      "User-driven data collection and construction of a HOME corpus (30,518 events from 40 month-long sequences; 273 routines, 233 unique) derived from a 40-user survey with execution indicators",
      "Public release of code (and anonymized datasets per prior study) enabling others to reproduce and use Helion for testing"
    ]
  },
  {
    "arxiv_id": "2309.08158v1",
    "title": "A Testbed for Automating and Analysing Mobile Devices and their Applications",
    "authors": "Lachlan Simpson; Kyle Millar; Adriel Cheng; Hong Gunn Chew; Cheng-Chew Lim",
    "abstract": "The need for improved network situational awareness has been highlighted by the growing complexity and severity of cyber-attacks. Mobile phones pose a significant risk to network situational awareness due to their dynamic behaviour and lack of visibility on a network. Machine learning techniques enhance situational awareness by providing administrators insight into the devices and activities which form their network. Developing machine learning techniques for situational awareness requires a testbed to generate and label network traffic. Current testbeds, however, are unable to automate the generation and labelling of realistic network traffic. To address this, we describe a testbed which automates applications on mobile devices to generate and label realistic traffic. From this testbed, two labelled datasets of network traffic have been created. We provide an analysis of the testbed automation reliability and benchmark the datasets for the task of application classification.",
    "published_date": "2023-09-15",
    "pdf_link": "https://arxiv.org/pdf/2309.08158v1",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "benchmark",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Traffic Classification",
      "specific_problem": "Mobile application identification/classification from encrypted network flows for situational awareness",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "baseline",
        "category": "Support Vector Machine",
        "specific": "SVM with RBF kernel",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Ensemble (Tree-based)",
        "specific": "Random Forest",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Instance-based",
        "specific": "k-Nearest Neighbors (k=1)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Testbed-Captures Capture 1",
        "type": "public",
        "domain": "network_traffic",
        "link": "https://github.com/lachieS/Testbed-Captures",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "Testbed-Captures Capture 2",
        "type": "public",
        "domain": "network_traffic",
        "link": "https://github.com/lachieS/Testbed-Captures",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "Dataset in [22] (VPN vs non-VPN mobile traffic, single device)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Random Forest",
        "paper_reference": null,
        "metric": "Macro-F1",
        "their_result": "0.936",
        "baseline_result": null
      },
      {
        "method_name": "SVM (RBF)",
        "paper_reference": null,
        "metric": "Macro-F1",
        "their_result": "0.931",
        "baseline_result": null
      },
      {
        "method_name": "KNN (k=1)",
        "paper_reference": null,
        "metric": "Macro-F1",
        "their_result": "0.859",
        "baseline_result": null
      },
      {
        "method_name": "Random Forest",
        "paper_reference": null,
        "metric": "Macro-Recall",
        "their_result": "0.920",
        "baseline_result": null
      },
      {
        "method_name": "SVM (RBF)",
        "paper_reference": null,
        "metric": "Macro-Recall",
        "their_result": "0.932",
        "baseline_result": null
      },
      {
        "method_name": "KNN (k=1)",
        "paper_reference": null,
        "metric": "Macro-Recall",
        "their_result": "0.865",
        "baseline_result": null
      },
      {
        "method_name": "Random Forest",
        "paper_reference": null,
        "metric": "Macro-Precision",
        "their_result": "0.956",
        "baseline_result": null
      },
      {
        "method_name": "SVM (RBF)",
        "paper_reference": null,
        "metric": "Macro-Precision",
        "their_result": "0.931",
        "baseline_result": null
      },
      {
        "method_name": "KNN (k=1)",
        "paper_reference": null,
        "metric": "Macro-Precision",
        "their_result": "0.854",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Macro-Precision",
      "Macro-Recall",
      "Macro-F1",
      "Execution Failure (EF)",
      "Launching Failure (LF)",
      "Random Forest Feature Importance"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can a testbed automate realistic user actions on Android and iOS to generate and label realistic application traffic, including background applications?",
        "How reliable is the automation framework (Appium-based) across devices and apps (measured via EF and LF)?",
        "How well do standard supervised classifiers perform on the generated flow-based datasets for application classification?"
      ],
      "gaps_identified": [
        "Current testbeds are unable to automate the generation and labelling of realistic network traffic.",
        "Existing experimental and automated testbeds fail to capture and label background applications.",
        "Prior automated testbeds often rely on UI fuzzing, which may not produce realistic traffic and lacks controllability.",
        "Many testbeds are purpose-built for specific apps/devices and do not scale to new devices or operating systems (notably iOS).",
        "Operational network data is privacy-sensitive and difficult to label with reliable ground truth."
      ],
      "limitations": [
        "Limited set of physical devices and OS versions; keeping hardware/OS up-to-date is challenging.",
        "Appium-based automation on iOS shows higher failure rates (e.g., iPhone 6 LF 1.02%, EF 0.409%; iPhone 7 LF 0.288%).",
        "Some app automations failed on Android as well (e.g., YouTube EF 8%, SoundCloud EF 5%).",
        "Assumption of uniform app usage patterns (e.g., fixed-length emails, no attachments), which may not reflect real user behavior.",
        "Intermittent connection issues with Appium server (e.g., iPhone 6 lost connection three times)."
      ],
      "future_work": [
        "Determine whether Appium, iOS versions, or specific apps (e.g., Meta apps) cause higher EF and LF on iOS.",
        "Incorporate emulated mobile devices to keep pace with current OS and apps.",
        "Leverage categorical features in classification (left to future work in this paper)."
      ],
      "motivation": "Improve network situational awareness by enabling realistic, labeled mobile application traffic generation that preserves privacy and supports ML development; overcome limitations of existing testbeds (UI fuzzing, lack of background app labels, limited OS coverage).",
      "potential_research_ideas": [
        "Develop domain adaptation or transfer learning methods to bridge testbed-to-real-network distribution shifts for application classification.",
        "Model richer user behavior (e.g., attachments in email, varied session lengths, multi-tab browsing) and evaluate its impact on classifier generalization.",
        "Extend the testbed to include VPN, QUIC/HTTP3, and cellular scenarios to study protocol impacts on app identification.",
        "Investigate multi-task learning that jointly predicts operating system, application, and device family from flows.",
        "Add longitudinal updates (app/OS updates over time) to evaluate temporal robustness and continual learning approaches.",
        "Incorporate synthetic or simulation-based user models (e.g., Markov models, reinforcement learning agents) to generate diverse yet realistic action sequences.",
        "Use self-supervised representation learning on flows to reduce reliance on labeled data while maintaining performance.",
        "Augment with side-channel telemetry (e.g., battery, CPU, app lifecycle events) to improve labeling fidelity and support multimodal models.",
        "Assess adversarial robustness of traffic classifiers (evasion via traffic shaping, padding, timing obfuscation) and design defenses.",
        "Expand to security-specific tasks (malware/PUA traffic, DLP policy violations) leveraging the same automation and labeling pipeline."
      ],
      "architectural_improvement_recommendations": [
        "Replace or augment Appium on iOS with direct instrumentation (e.g., WebDriverAgent enhancements) and add robust watchdog/heartbeat to reconnect on crashes.",
        "Introduce a scenario generator that parameterizes user behavior (attachments, media uploads, browsing depth) and stochastically varies sequences and timings.",
        "Automate co-running background workloads with explicit labels to stress-test multi-app interference and background traffic labeling.",
        "Capture optional PCAPs alongside flows with a privacy-preserving export (e.g., payload stripping, differential privacy on metadata) for advanced methods.",
        "Standardize and publish a feature schema with versioning; include categorical features in released baselines and provide preprocessing pipelines.",
        "Integrate continuous integration for device/OS/app updates using emulators and containerized capture/labeling components.",
        "Add QUIC/HTTP3-specific features (e.g., connection IDs, packet number spaces) and TLS fingerprinting (JA3/JA4) where permissible.",
        "Cross-validate labels by correlating OS-level UIDs/RVI with app logs or accessibility events to detect mislabeling."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "Python",
        "Appium",
        "UIAutomator2 (Android)",
        "XCUITest (iOS)",
        "tcpdump",
        "Tranalyzer"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Laboratory mobile testbed subnet with physical smartphones connected via router and managed switch (port mirroring)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Maintaining reliability of Appium-based automation on iOS (higher LF/EF).",
        "Device/OS/app version churn requiring frequent updates.",
        "Ensuring realistic user behavior beyond uniform scripted actions.",
        "Managing stable connections to the automation server.",
        "Scaling to diverse devices while preserving accurate labels for background apps."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": true,
      "fairness_bias": false
    },
    "contributions": [
      "A novel cross-OS (Android and iOS) mobile device testbed that automates application use without UI fuzzing to generate realistic traffic.",
      "An automation framework that models realistic sequences of user actions and labels both foreground and background application flows.",
      "Two publicly released, labeled flow datasets (~40 hours each; ~24,000 flows total) covering 15 applications across five devices (3 Android, 2 iOS).",
      "Analysis of automation reliability across apps and devices using execution failure (EF) and launching failure (LF) metrics.",
      "Baseline benchmarks for mobile application classification using supervised ML (Random Forest, SVM-RBF, KNN) with macro-averaged metrics.",
      "Feature importance analysis indicating packet statistics, bytes, duration, and TCP window size as most informative features."
    ]
  },
  {
    "arxiv_id": "2308.14693v1",
    "title": "Hybrid PLS-ML Authentication Scheme for V2I Communication Networks",
    "authors": "Hala Amin; Jawaher Kaldari; Nora Mohamed; Waqas Aman; Saif Al-Kuwari",
    "abstract": "Vehicular communication networks are rapidly emerging as vehicles become smarter. However, these networks are increasingly susceptible to various attacks. The situation is exacerbated by the rise in automated vehicles complicates, emphasizing the need for security and authentication measures to ensure safe and effective traffic management. In this paper, we propose a novel hybrid physical layer security (PLS)-machine learning (ML) authentication scheme by exploiting the position of the transmitter vehicle as a device fingerprint. We use a time-of-arrival (ToA) based localization mechanism where the ToA is estimated at roadside units (RSUs), and the coordinates of the transmitter vehicle are extracted at the base station (BS).Furthermore, to track the mobility of the moving legitimate vehicle, we use ML model trained on several system parameters. We try two ML models for this purpose, i.e., support vector regression and decision tree. To evaluate our scheme, we conduct binary hypothesis testing on the estimated positions with the help of the ground truths provided by the ML model, which classifies the transmitter node as legitimate or malicious. Moreover, we consider the probability of false alarm and the probability of missed detection as performance metrics resulting from the binary hypothesis testing, and mean absolute error (MAE), mean square error (MSE), and coefficient of determination $\\text{R}^2$ to further evaluate the ML models. We also compare our scheme with a baseline scheme that exploits the angle of arrival at RSUs for authentication. We observe that our proposed position-based mechanism outperforms the baseline scheme significantly in terms of missed detections.",
    "published_date": "2023-08-28",
    "pdf_link": "https://arxiv.org/pdf/2308.14693v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Cyber-Physical Systems Security",
      "subdomain": "Vehicular Network Security / Physical Layer Authentication",
      "specific_problem": "Authentication of transmitters in V2I networks using position as a physical-layer fingerprint with ML-based mobility tracking",
      "attack_types": [
        "impersonation",
        "spoofing"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "SVM (Regression)",
        "specific": "Support Vector Regression (SVR)",
        "novel_contribution": "Used to predict the next (x,y) position of the legitimate vehicle to provide ground-truth for PLA hypothesis testing in a dynamic V2I setting"
      },
      {
        "type": "primary",
        "category": "Decision Tree (Regression)",
        "specific": "Decision Tree Regressor",
        "novel_contribution": "Alternative regressor for mobility tracking to generate ground-truth positions for hypothesis testing"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Regression"
    ],
    "datasets": [
      {
        "name": "Simulated V2I ToA/position dataset (authors)",
        "type": "synthetic",
        "domain": "vehicular_v2i_localization",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Angle-of-Arrival (AoA)-based PLA at RSUs",
        "paper_reference": "Ref [9] in paper (AoA-based authentication for V2X)",
        "metric": "Probability of false alarm (Pfa), Probability of missed detection (Pmd)",
        "their_result": "“We observe that our proposed position-based mechanism outperforms the baseline scheme significantly in terms of missed detections.”",
        "baseline_result": "Baseline achieves low false alarm but ‘collapses’ on missed detection when fingerprints are close; error increases with LQ for Pmd per qualitative plots."
      }
    ],
    "performance_metrics_used": [
      "Probability of false alarm (Pfa)",
      "Probability of missed detection (Pmd)",
      "ROC (Pd vs Pfa)",
      "MAE",
      "MSE",
      "RMSE",
      "R^2"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Prior AoA-based PLA assumes transmitter location ground truth is available; ‘no mechanism for ground truth tracking, and the effect of mobility is not discussed.’",
        "Closed-form PDFs for TS|H0 and TS|H1 are ‘very challenging to find’; probabilities are computed empirically.",
        "Existing position-based PLA reported only for stationary nodes (underwater), not dynamic vehicular environments."
      ],
      "limitations": [
        "Evaluation is simulation-only (MATLAB) with synthetic data.",
        "Assumptions include LoS RSUs within 400 m, single transmitter per time slot (no collision), and error-free secured RSU–BS links.",
        "Empirical computation of Pfa and Pmd due to intractable TS distributions.",
        "Scenario considers one legitimate and one malicious vehicle; malicious follows legitimate’s speed and direction.",
        "2D V2I setting; RSUs at fixed positions; threshold selection impacts trade-off between Pfa and Pmd."
      ],
      "future_work": [
        "Study with more realistic and non-linear mobility models.",
        "Try more ML models for better accuracy.",
        "Incorporate multiple legitimate and malicious vehicles."
      ],
      "motivation": "PLA can overcome limitations of higher-layer cryptographic authentication and brute-force risks by exploiting physical-layer fingerprints; dynamic V2I needs mobility-aware authentication without pre-known ground truth.",
      "potential_research_ideas": [
        "Fuse multiple physical-layer fingerprints (ToA + AoA + CSI/PL) for joint hypothesis testing to reduce missed detection while controlling false alarms.",
        "Adopt sequential/probabilistic mobility tracking (Kalman/Extended/Unscented/Particle filters, Bayesian SVR, Gaussian Processes) with uncertainty estimates to set adaptive thresholds.",
        "Leverage road-map constraints and lane geometry (map-matching) to regularize predicted positions and reduce ML errors.",
        "Explore sequence models (LSTM/GRU/Temporal CNN/Transformer) for trajectory prediction under varying speeds and maneuvers.",
        "Develop robust NLoS/NLOS-aware ToA estimation and multipath mitigation to relax the LoS assumption.",
        "Design distributed edge inference at RSUs with federated/continual learning for online adaptation to traffic dynamics.",
        "Extend to multi-vehicle, multi-attacker settings with data association and joint multi-target tracking.",
        "Formalize and derive approximations/bounds for TS distributions to enable analytical ROC predictions.",
        "Investigate adversarial spoofing of ToA/RSU timing and propose timing-consistency and cross-RSU sanity checks.",
        "Integrate privacy-preserving mechanisms for location handling (e.g., secure enclaves or homomorphic checks) to protect driver privacy."
      ],
      "architectural_improvement_recommendations": [
        "Replace point prediction with probabilistic mobility tracking (e.g., EKF/UKF/Particle filter or Bayesian SVR) and propagate uncertainty into hypothesis testing via adaptive thresholds.",
        "Use ensemble regressors (e.g., Gradient Boosting/XGBoost/Random Forest) and compare to SVR/DT for robustness.",
        "Introduce map-matching and physical constraints (speed/acceleration bounds, lane constraints) into the prediction pipeline.",
        "Incorporate multi-fingerprint fusion (ToA, AoA, CSI/PL) using a learned feature-level or score-level fusion model.",
        "Implement adaptive thresholding using ROC-driven or Neyman–Pearson criteria conditioned on current LQ and predicted uncertainty.",
        "Augment ToA estimation with improved synchronization methods and robust estimators to reduce bias/variance under noise.",
        "Evaluate 3D localization with RSU height and elevation angles to improve separability when lateral separation is small."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "MATLAB"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "V2I with RSUs connected to a base station (simulated)",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Assumes LoS between transmitter and RSUs within 400 m",
        "Single transmitter per time slot (no collision domain)",
        "Assumes error-free secured RSU–BS backhaul",
        "Performance sensitive to threshold selection and mobility speed",
        "Relies on accurate ToA estimation at RSUs"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes a hybrid physical layer security–machine learning authentication scheme using transmitter position as a device fingerprint for V2I.",
      "Estimates ToA at RSUs via maximum likelihood and extracts transmitter coordinates at the BS via least squares.",
      "Constructs a test statistic on estimated coordinates and performs binary hypothesis testing to classify legitimate vs malicious transmitters.",
      "Introduces ML-based mobility tracking (SVR and DT) to predict the legitimate vehicle’s next position as ground truth for hypothesis testing.",
      "Empirically evaluates Pfa and Pmd versus link quality, speed, and threshold; presents ROC analysis.",
      "Compares against an AoA-based authentication baseline and reports significantly lower missed detection for the proposed position-based scheme."
    ]
  },
  {
    "arxiv_id": "2310.07958v5",
    "title": "Towards Causal Deep Learning for Vulnerability Detection",
    "authors": "Md Mahbubur Rahman; Ira Ceka; Chengzhi Mao; Saikat Chakraborty; Baishakhi Ray; Wei Le",
    "abstract": "Deep learning vulnerability detection has shown promising results in recent years. However, an important challenge that still blocks it from being very useful in practice is that the model is not robust under perturbation and it cannot generalize well over the out-of-distribution (OOD) data, e.g., applying a trained model to unseen projects in real world. We hypothesize that this is because the model learned non-robust features, e.g., variable names, that have spurious correlations with labels. When the perturbed and OOD datasets no longer have the same spurious features, the model prediction fails. To address the challenge, in this paper, we introduced causality into deep learning vulnerability detection. Our approach CausalVul consists of two phases. First, we designed novel perturbations to discover spurious features that the model may use to make predictions. Second, we applied the causal learning algorithms, specifically, do-calculus, on top of existing deep learning models to systematically remove the use of spurious features and thus promote causal based prediction. Our results show that CausalVul consistently improved the model accuracy, robustness and OOD performance for all the state-of-the-art models and datasets we experimented. To the best of our knowledge, this is the first work that introduces do calculus based causal learning to software engineering models and shows it's indeed useful for improving the model accuracy, robustness and generalization. Our replication package is located at https://figshare.com/s/0ffda320dcb96c249ef2.",
    "published_date": "2023-10-12",
    "pdf_link": "https://arxiv.org/pdf/2310.07958v5",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Vulnerability Detection",
      "specific_problem": "Improving robustness and out-of-distribution generalization for source-code vulnerability detection by removing spurious features via do-calculus-based causal learning",
      "attack_types": [
        "Software vulnerabilities (various CWEs)",
        "Memory leak (example)",
        "API misuse (example)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Causal Learning",
        "specific": "do-calculus and backdoor criterion",
        "novel_contribution": "Applies do-calculus-based causal intervention to deep code models to marginalize out known spurious features (e.g., variable and API names) at inference time for vulnerability detection"
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "CodeBERT",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "GraphCodeBERT",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "UniXcoder",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Causal inference (interventional adjustment)"
    ],
    "datasets": [
      {
        "name": "Devign",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Big-Vul",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Devign-PerturbVar",
        "type": "synthetic",
        "domain": "source_code",
        "link": "https://figshare.com/s/0ffda320dcb96c249ef2",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "Devign-PerturbAPI",
        "type": "synthetic",
        "domain": "source_code",
        "link": "https://figshare.com/s/0ffda320dcb96c249ef2",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "Devign-PerturbJoint",
        "type": "synthetic",
        "domain": "source_code",
        "link": "https://figshare.com/s/0ffda320dcb96c249ef2",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "Big-Vul-PerturbVar",
        "type": "synthetic",
        "domain": "source_code",
        "link": "https://figshare.com/s/0ffda320dcb96c249ef2",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "Big-Vul-PerturbAPI",
        "type": "synthetic",
        "domain": "source_code",
        "link": "https://figshare.com/s/0ffda320dcb96c249ef2",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "Big-Vul-PerturbJoint",
        "type": "synthetic",
        "domain": "source_code",
        "link": "https://figshare.com/s/0ffda320dcb96c249ef2",
        "is_new_contribution": true,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "CodeBERT (vanilla fine-tuning)",
        "paper_reference": null,
        "metric": "F1",
        "their_result": "CausalVul improves overall in-distribution performance by +7% (Devign) and +6% (Big-Vul) over SOTA",
        "baseline_result": "Devign F1=0.61; Big-Vul F1=0.36 (baseline rows reported in tables)"
      },
      {
        "method_name": "GraphCodeBERT (vanilla fine-tuning)",
        "paper_reference": null,
        "metric": "F1",
        "their_result": "CausalVul improves over GraphCodeBERT on both datasets (exact F1 not specified in excerpt)",
        "baseline_result": "Devign F1=0.62; Big-Vul F1=0.37 (baseline rows reported)"
      },
      {
        "method_name": "UniXcoder (vanilla fine-tuning)",
        "paper_reference": null,
        "metric": "F1",
        "their_result": "CausalVul improves over UniXcoder on both datasets (exact F1 not specified in excerpt)",
        "baseline_result": "Devign F1=0.63; Big-Vul F1=0.38 (baseline rows reported)"
      },
      {
        "method_name": "Cross-project OOD generalization (SOTA vanilla)",
        "paper_reference": null,
        "metric": "F1 (OOD)",
        "their_result": "“improving the performance on the Devign dataset trained model up to 100% and Big-Vul dataset trained model up to 200%.”",
        "baseline_result": "Vanilla SOTA OOD F1 (not numerically specified in excerpt)"
      },
      {
        "method_name": "Robustness under perturbations (SOTA vanilla)",
        "paper_reference": null,
        "metric": "F1 on perturbed sets",
        "their_result": "“improves the performance up to 62% on Devign and 100% on Big-Vul on the perturbed data we constructed for robustness testing.”",
        "baseline_result": "Example degradations: CodeBERT Devign 0.61→0.52 (PerturbAPI), Big-Vul 0.36→0.10; Joint perturbations down to Devign 0.33, Big-Vul 0.06 for some settings"
      }
    ],
    "performance_metrics_used": [
      "F1 score",
      "OOD F1 under cross-project testing",
      "F1 under robustness perturbations"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Do current deep learning vulnerability detectors rely on spurious features such as variable names and API names?",
        "Can causal learning (do-calculus/backdoor adjustment) systematically remove the influence of such spurious features?",
        "Does promoting causal features improve accuracy, robustness under perturbations, and OOD generalization?"
      ],
      "gaps_identified": [
        "Deep models for vulnerability detection are not robust to perturbations and generalize poorly to OOD data.",
        "Models learn non-robust, spurious correlations (e.g., variable names) rather than causal features.",
        "Lack of a systematic study for discovering spurious features in vulnerability detection models.",
        "Retraining large pretrained models with data augmentation to mitigate spurious features is costly."
      ],
      "limitations": [
        "Focuses on two spurious feature types (variable names and API names) as a proof of concept.",
        "Causal intervention relies on prior knowledge of spurious features discovered via designed perturbations."
      ],
      "future_work": [
        "Extend discovery and mitigation beyond variable/API names to additional spurious features.",
        "Evaluate on more datasets, languages, and project settings for broader OOD scenarios."
      ],
      "motivation": "Deep learning vulnerability detectors fail under semantic-preserving perturbations and on unseen projects because they exploit spurious correlations; introducing causal learning aims to remove spurious features and improve robustness and generalization.",
      "potential_research_ideas": [
        "Automated causal discovery of spurious features in code (beyond identifiers/APIs) using invariant risk minimization or stability selection.",
        "Counterfactual data augmentation for code (identifier renaming, API substitutions, formatting, and AST rewrites) aligned with a structural causal model.",
        "Causal pretraining objectives that enforce identifier/API invariance in code-language models.",
        "Integrate program analysis (dataflow/control-flow) with causal adjustment to better capture causal mechanisms behind vulnerabilities.",
        "Extend causal debiasing to vulnerability repair and root-cause explanation generation.",
        "Cross-language, cross-ecosystem causal generalization for multi-language vulnerability detection."
      ],
      "architectural_improvement_recommendations": [
        "Disentangle causal and spurious factors via a two-branch encoder: a spurious-feature encoder trained with identifier/API prediction and a causal encoder trained adversarially against spurious signals.",
        "Backdoor-adjusted inference combined with adversarial training that penalizes performance when predictions change under semantics-preserving renamings.",
        "Token-type-aware masking to downweight identifier tokens and API surface forms while emphasizing dataflow/semantic edges (e.g., hybrid Transformer+GNN).",
        "Explicit structural causal model over program features (identifiers, API calls, dataflow edges) enabling targeted interventions during training and inference.",
        "Leverage contrastive learning across counterfactual pairs (original vs. perturbed) to enforce invariance."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://figshare.com/s/0ffda320dcb96c249ef2",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Generalization to unseen projects (cross-project OOD).",
        "Sensitivity to semantics-preserving perturbations of identifiers and API usage.",
        "High retraining cost for large pretrained models if relying solely on data augmentation."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": true,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "“We discovered and experimentally demonstrated that variable names and API names are used as spurious features in the current deep learning vulnerability detection models.”",
      "“We formulate deep learning vulnerability detection using causality and applied causal deep learning to remove spurious features in the models.”",
      "“We experimentally demonstrated that causal deep learning can improve model accuracy, robustness and generalization.”",
      "Introduces CausalVul: a two-phase approach that (i) discovers spurious features via semantic-preserving perturbations (PerturbVar, PerturbAPI, PerturbJoint) and (ii) applies do-calculus-based backdoor adjustment at inference to marginalize out spurious features."
    ]
  },
  {
    "arxiv_id": "2401.02960v1",
    "title": "Forensic Video Analytic Software",
    "authors": "Anton Jeran Ratnarajah; Sahani Goonetilleke; Dumindu Tissera; Kapilan Balagopalan; Ranga Rodrigo",
    "abstract": "Law enforcement officials heavily depend on Forensic Video Analytic (FVA) Software in their evidence extraction process. However present-day FVA software are complex, time consuming, equipment dependent and expensive. Developing countries struggle to gain access to this gateway to a secure haven. The term forensic pertains the application of scientific methods to the investigation of crime through post-processing, whereas surveillance is the close monitoring of real-time feeds.   The principle objective of this Final Year Project was to develop an efficient and effective FVA Software, addressing the shortcomings through a stringent and systematic review of scholarly research papers, online databases and legal documentation. The scope spans multiple object detection, multiple object tracking, anomaly detection, activity recognition, tampering detection, general and specific image enhancement and video synopsis.   Methods employed include many machine learning techniques, GPU acceleration and efficient, integrated architecture development both for real-time and postprocessing. For this CNN, GMM, multithreading and OpenCV C++ coding were used. The implications of the proposed methodology would rapidly speed up the FVA process especially through the novel video synopsis research arena. This project has resulted in three research outcomes Moving Object Based Collision Free Video Synopsis, Forensic and Surveillance Analytic Tool Architecture and Tampering Detection Inter-Frame Forgery.   The results include forensic and surveillance panel outcomes with emphasis on video synopsis and Sri Lankan context. Principal conclusions include the optimization and efficient algorithm integration to overcome limitations in processing power, memory and compromise between real-time performance and accuracy.",
    "published_date": "2023-09-17",
    "pdf_link": "https://arxiv.org/pdf/2401.02960v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Digital Forensics",
      "subdomain": "Multimedia Forensics",
      "specific_problem": "Integrated forensic and surveillance video analytics for CCTV, including video synopsis, tampering detection (inter-frame, intra-frame, camera tampering), anomaly detection, and activity recognition",
      "attack_types": [
        "Video/image evidence tampering (inter-frame insertion/deletion/repetition)",
        "Copy-move forgery",
        "Image splicing (double JPEG compression)",
        "Camera tampering (redirection, shaking, blocking)",
        "Trespass/intrusion",
        "Abnormal/violent activities (e.g., fighting, robbery)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Video synopsis heuristics",
        "specific": "Moving-object tube clustering and re-arrangement (online collision-free scheduling)",
        "novel_contribution": "Proposes an online, moving object-based collision-free video synopsis using cluster selection and tube re-arrangement, avoiding costly energy minimization"
      },
      {
        "type": "primary",
        "category": "Mixture Model",
        "specific": "Gaussian Mixture Model (GMM) for background subtraction",
        "novel_contribution": "Used to detect moving objects for tube generation in synopsis"
      },
      {
        "type": "primary",
        "category": "One-Stage Detector",
        "specific": "YOLO (You Only Look Once)",
        "novel_contribution": "Used for real-time object detection within surveillance pipeline"
      },
      {
        "type": "primary",
        "category": "Multi-object Tracking",
        "specific": "SORT (Kalman Filter + Hungarian Algorithm)",
        "novel_contribution": "Used to track YOLO-detected objects in real-time and to generate tubes for synopsis"
      },
      {
        "type": "primary",
        "category": "Optical Flow",
        "specific": "Dense optical flow + clustering",
        "novel_contribution": "Used for anomaly detection by clustering motion patterns and for inter-frame forgery via optical flow variance analysis"
      },
      {
        "type": "primary",
        "category": "Feature descriptors",
        "specific": "BRISK + FREAK",
        "novel_contribution": "Feature extraction feeding Bag-of-Features representation for activity recognition"
      },
      {
        "type": "primary",
        "category": "Bag-of-Features + SVM",
        "specific": "SVM classifier on BoF of BRISK/FREAK",
        "novel_contribution": "Used for activity recognition of crowd/individual behaviors"
      },
      {
        "type": "primary",
        "category": "Copy-move forgery detection",
        "specific": "SIFT-based keypoint matching",
        "novel_contribution": "Detects intra-frame copy-move tampering"
      },
      {
        "type": "primary",
        "category": "JPEG Forensics",
        "specific": "DCT histogram analysis for double quantization",
        "novel_contribution": "Detects image splicing via double JPEG compression artifacts"
      },
      {
        "type": "primary",
        "category": "Deconvolution",
        "specific": "Wiener filtering with PSF and SNR estimation",
        "novel_contribution": "Textual enhancement to remove motion blur and Gaussian noise"
      },
      {
        "type": "primary",
        "category": "Super-resolution",
        "specific": "Linear interpolation followed by super-resolution (CNN-based, unspecified)",
        "novel_contribution": "Face enhancement across multiple frames"
      },
      {
        "type": "primary",
        "category": "Classical image processing",
        "specific": "Contrast/exposure/HSV/temperature/noise reduction/shadow-highlight/sharpening",
        "novel_contribution": "User-guided general image enhancement for forensic ROI"
      },
      {
        "type": "primary",
        "category": "CNN",
        "specific": null,
        "novel_contribution": "CNNs referenced for parts of the pipeline (e.g., super-resolution/face enhancement), implementation details not specified"
      },
      {
        "type": "baseline",
        "category": "Video Synopsis",
        "specific": "Prior methods from [2] and [3]",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Energy Minimization",
        "specific": "Collision avoidance via energy minimization",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised",
      "Rule-based"
    ],
    "datasets": [
      {
        "name": "GRAM dataset",
        "type": "public",
        "domain": "surveillance_videos",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Video synopsis datasets from prior work [2] and [3] (6 videos total)",
        "type": "public",
        "domain": "surveillance_videos",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Synopsis dataset (as per Table 4.3)",
        "type": "public",
        "domain": "surveillance_videos",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "M-30-HD video",
        "type": "public",
        "domain": "surveillance_videos",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "M-30 video",
        "type": "public",
        "domain": "surveillance_videos",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Sri Lankan CCTV context videos (faces, number plates, anomalies)",
        "type": "proprietary",
        "domain": "surveillance_videos",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Prior video synopsis method [2]",
        "paper_reference": "[2]",
        "metric": "Spatiotemporal reduction / frame reduction rate; qualitative visual quality",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Prior video synopsis method [3]",
        "paper_reference": "[3]",
        "metric": "Spatiotemporal reduction / frame reduction rate; qualitative visual quality",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Existing methods (unspecified) for tracking/synopsis",
        "paper_reference": null,
        "metric": "Average Precision; Precision-Recall",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Precision-Recall curves (tracking)",
      "Average Precision",
      "Frames Per Second (FPS)",
      "Frame Reduction rate (FR)",
      "Number of synopsis frames",
      "H.264 compressed file size (bytes)",
      "Confusion matrix (activity recognition)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can an integrated forensic and surveillance video analytic software efficiently process CCTV feeds in real-time and for post-processing?",
        "Can a moving-object-based collision-free video synopsis summarize long CCTV videos into short clips while preserving all human activities?",
        "How to detect inter-frame and intra-frame forgery and camera tampering in CCTV evidence?",
        "How to enhance faces and text in low-quality CCTV footage for reliable evidence extraction?",
        "How to detect anomalies (including trespass) and recognize activities in real-time on resource-limited systems?"
      ],
      "gaps_identified": [
        "Present-day FVA software are complex, time-consuming, equipment-dependent and expensive, limiting access in developing countries",
        "Scarcity of human resources to manually review continuous CCTV footage; need for effective video synopsis",
        "Lack of self-learning anomaly detection in user-defined environments in the market",
        "Need to verify tampering of presented CCTV evidence (video/image forgery detection)",
        "Poor quality CCTV impedes recognition of faces and number plates; need for targeted enhancement",
        "Unavailability of software to assess reliability of evidence produced in court",
        "Lack of integrated tools that support both surveillance (real-time) and forensic (post-processing) workflows"
      ],
      "limitations": [
        "Compromise between real-time performance and accuracy",
        "Constraints in processing power and memory; need for optimization and GPU acceleration",
        "Failure cases discussed for synopsis (e.g., dependence on cluster size; collisions if not scheduled properly)",
        "Energy-minimization approaches avoided due to time complexity; heuristic scheduling may miss global optimum"
      ],
      "future_work": [],
      "motivation": "Provide an efficient, cost-effective, and integrated forensic video analytic software for law enforcement, addressing practical constraints in developing countries and enabling both real-time surveillance and post-event forensic analysis.",
      "potential_research_ideas": [
        "Replace classical background subtraction with modern instance segmentation (e.g., Mask-based detectors) to produce cleaner object tubes for synopsis",
        "Develop self-supervised/online anomaly detection tailored to specific cameras/environments to address the noted market gap",
        "Integrate multi-camera spatiotemporal correlation for cross-camera tracking and synopsis across a site",
        "Adopt deep-learning-based forensic detectors for copy-move/splicing (e.g., transformer backbones, noise/PRNU consistency) and compare with DCT histogram methods",
        "Evaluate on broader public benchmarks (e.g., UCF-Crime, Avenue, ShanghaiTech, UCSD Ped2) for anomaly and activity recognition",
        "Incorporate robust super-resolution and deblurring (state-of-the-art CNNs) specialized for faces and license plates under compression and motion blur",
        "Add authenticity verification via sensor pattern noise (PRNU) and camera model identification to strengthen evidentiary reliability",
        "Design a streaming, GPU-accelerated pipeline (e.g., GStreamer/DeepStream) for scalable multi-channel deployment"
      ],
      "architectural_improvement_recommendations": [
        "Upgrade object detection/tracking stack to modern real-time components (e.g., YOLOv8/RT-DETR + ByteTrack/OC-SORT/StrongSORT) for improved recall/ID consistency",
        "Implement tube generation with re-identification features to maintain object identity gaps and improve synopsis continuity",
        "Use differentiable or learned scheduling for tube rearrangement to reduce collisions while maintaining efficiency",
        "Adopt transformer-based video models for activity recognition; augment with temporal modeling (e.g., TCNs) for improved accuracy",
        "Introduce adaptive, camera-specific background modeling and motion segmentation to reduce false anomalies",
        "Modularize the pipeline with message queues and batch inference to maximize GPU utilization and throughput",
        "Provide an evidence provenance and audit trail module (hashing, chain-of-custody metadata) within the software"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "OpenCV (C++)",
        "CUDA/GPU (unspecified)",
        "YOLO",
        "SORT (Kalman Filter + Hungarian Algorithm)",
        "SVM"
      ],
      "reproducibility_score": "low",
      "computational_requirements": "GPU acceleration and multithreading on a workstation; optimization needed due to processing power and memory constraints (no exact specs provided)"
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Workstation with GPU connected to CCTV camera; tested in Sri Lankan law-enforcement/security context",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Limited processing power and memory on target systems",
        "Trade-off between real-time performance and accuracy",
        "Low-quality CCTV footage (motion blur, compression, low resolution)",
        "Need for integrated workflows across forensic and surveillance use-cases",
        "Camera tampering impacting reliability of surveillance feeds"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Moving Object Based Collision-Free Video Synopsis with online tube generation and rearrangement",
      "Integrated Forensic and Surveillance Analytic Tool Architecture with GPU-accelerated modules",
      "Tampering Detection across inter-frame forgery (optical flow variance), copy-move (SIFT), splicing (DCT histograms), and real-time camera tampering (redirection/shaking/blocking)",
      "Real-time surveillance modules: object detection (YOLO) + tracking (SORT) + anomaly/trespass detection + activity recognition (BRISK/FREAK BoF + SVM)",
      "Forensic image enhancement suite: general enhancement controls, face enhancement via super-resolution, textual enhancement via Wiener deconvolution",
      "Empirical evaluation on datasets from prior work (including GRAM) and Sri Lankan context, reporting PR curves, average precision, FPS, frame reduction rate, and file size"
    ]
  },
  {
    "arxiv_id": "2309.13467v1",
    "title": "SUDS: Sanitizing Universal and Dependent Steganography",
    "authors": "Preston K. Robinette; Hanchen D. Wang; Nishan Shehadeh; Daniel Moyer; Taylor T. Johnson",
    "abstract": "Steganography, or hiding messages in plain sight, is a form of information hiding that is most commonly used for covert communication. As modern steganographic mediums include images, text, audio, and video, this communication method is being increasingly used by bad actors to propagate malware, exfiltrate data, and discreetly communicate. Current protection mechanisms rely upon steganalysis, or the detection of steganography, but these approaches are dependent upon prior knowledge, such as steganographic signatures from publicly available tools and statistical knowledge about known hiding methods. These dependencies render steganalysis useless against new or unique hiding methods, which are becoming increasingly common with the application of deep learning models. To mitigate the shortcomings of steganalysis, this work focuses on a deep learning sanitization technique called SUDS that is not reliant upon knowledge of steganographic hiding techniques and is able to sanitize universal and dependent steganography. SUDS is tested using least significant bit method (LSB), dependent deep hiding (DDH), and universal deep hiding (UDH). We demonstrate the capabilities and limitations of SUDS by answering five research questions, including baseline comparisons and an ablation study. Additionally, we apply SUDS to a real-world scenario, where it is able to increase the resistance of a poisoned classifier against attacks by 1375%.",
    "published_date": "2023-09-23",
    "pdf_link": "https://arxiv.org/pdf/2309.13467v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Multimedia Security",
      "subdomain": "Steganography Defense/Sanitization",
      "specific_problem": "Sanitization of image-based steganography (universal and dependent) to prevent secret recovery while preserving image quality",
      "attack_types": [
        "Least Significant Bit (LSB) image steganography",
        "Dependent Deep Hiding (DDH)",
        "Universal Deep Hiding (UDH)",
        "Data poisoning (demonstration of increased classifier resistance)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Autoencoder",
        "specific": "Variational Autoencoder (VAE)",
        "novel_contribution": "VAE-based sanitization model (SUDS) trained only on clean images to sanitize both universal and dependent image steganography without prior knowledge of hiding methods; also explored latent-space size effects and latent-space-based detection signals."
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": null,
        "novel_contribution": "CNN-based hider/revealer networks used to implement DDH and UDH for generating stego containers (from referenced codebase)."
      }
    ],
    "learning_paradigm": [
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "MNIST",
        "type": "public",
        "domain": "images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CIFAR-10",
        "type": "public",
        "domain": "images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Gaussian noise sanitization (µ=0, σ=0.02)",
        "paper_reference": null,
        "metric": "MSE (S vs ˆS), PSNR (S vs ˆS) on LSB containers",
        "their_result": "SUDS: ˆS MSE 62.72; PSNR 30.17",
        "baseline_result": "Noise: ˆS MSE 30.04; PSNR 33.37"
      },
      {
        "method_name": "Gaussian noise sanitization (µ=0, σ=0.02)",
        "paper_reference": null,
        "metric": "MSE (S vs ˆS), PSNR (S vs ˆS) on DDH containers",
        "their_result": "SUDS: ˆS MSE 73.45; PSNR 29.47",
        "baseline_result": "Noise: ˆS MSE 38.10; PSNR 32.33"
      },
      {
        "method_name": "Gaussian noise sanitization (µ=0, σ=0.02)",
        "paper_reference": null,
        "metric": "MSE (S vs ˆS), PSNR (S vs ˆS) on UDH containers",
        "their_result": "SUDS: ˆS MSE 88.54; PSNR 28.66",
        "baseline_result": "Noise: ˆS MSE 57.58; PSNR 30.53"
      }
    ],
    "performance_metrics_used": [
      "MSE",
      "PSNR"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "RQ1: Ability to sanitize steganography — Is SUDS able to remove the presence of potential secrets while preserving the integrity of the cover?",
        "RQ2: Comparison to noise baseline — How does SUDS compare to noise baseline techniques?",
        "RQ3: Flexibility of the latent dimension — Does latent dimension size affect performance?",
        "RQ4: Ability to detect steganography — In addition to sanitization, can SUDS be used to detect steganography?",
        "RQ5: Scalability — Does SUDS scale to RGB images?"
      ],
      "gaps_identified": [
        "Existing steganalysis approaches depend on prior knowledge (signatures/statistics) of known hiding methods, making them ineffective against new or unique (especially deep-learning-based) steganography.",
        "Lack of a sanitization framework that can handle traditional, dependent deep, and universal deep hiding methods without knowledge of the hiding technique."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Mitigate shortcomings of steganalysis that relies on prior knowledge by introducing a learning-based sanitization approach that removes hidden messages across diverse steganographic methods while preserving image quality.",
      "potential_research_ideas": [
        "Extend SUDS to higher-resolution images and additional modalities (audio, video, text) and evaluate cross-modality sanitization.",
        "Develop a joint sanitize-and-detect pipeline that uses latent-space statistics for automatic stego detection and routing to sanitization.",
        "Adversarially train the sanitizer against adaptive hider/revealer networks to improve robustness against unseen stego methods.",
        "Integrate perceptual losses (e.g., LPIPS) and task-aware constraints to better preserve downstream utility while sanitizing.",
        "Explore diffusion-model or denoising autoencoder variants as sanitizers and compare to VAE in both sanitization strength and image fidelity.",
        "Apply SUDS in data pipelines to mitigate data poisoning and backdoor attacks embedded via steganography, with formal robustness guarantees."
      ],
      "architectural_improvement_recommendations": [
        "Introduce attention/residual blocks in the encoder/decoder to improve reconstruction fidelity at fixed latent sizes.",
        "Use conditional or beta-VAE variants tuning KL weight to balance information bottleneck vs. fidelity for stronger sanitization.",
        "Train SUDS adversarially with a learned revealer (minimax) to explicitly minimize secret recoverability across a family of hiding networks.",
        "Incorporate multi-scale features and perceptual (feature-space) reconstruction losses to better preserve semantics while removing stego signals.",
        "Add explicit cover-prior regularization (e.g., normalizing flows on latent space) to further constrain decodable outputs away from secrets.",
        "Calibrate latent dimensionality with automatic capacity control (e.g., KL annealing, information bottleneck measurement) for optimal sanitize/fidelity trade-off."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/pkrobinette/suds-ecai-2023",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Experiments conducted on macOS Monterey 12.5.1 with 2.3 GHz 8-Core Intel Core i9 CPU and 16 GB 2667 MHz DDR4 RAM."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Implementation of a robust sanitizer (SUDS) capable of sanitizing traditional (LSB), dependent deep (DDH), and universal deep (UDH) steganography.",
      "Demonstration of capabilities via five research questions (sanitization ability, comparison to noise, latent dimension ablation, detection signals, and scalability).",
      "Case study showing SUDS increases resistance of a poisoned classifier against attacks by 1375%."
    ]
  },
  {
    "arxiv_id": "2309.03739v1",
    "title": "Detecting unknown HTTP-based malicious communication behavior via generated adversarial flows and hierarchical traffic features",
    "authors": "Xiaochun Yun; Jiang Xie; Shuhao Li; Yongzheng Zhang; Peishuai Sun",
    "abstract": "Malicious communication behavior is the network communication behavior generated by malware (bot-net, spyware, etc.) after victim devices are infected. Experienced adversaries often hide malicious information in HTTP traffic to evade detection. However, related detection methods have inadequate generalization ability because they are usually based on artificial feature engineering and outmoded datasets. In this paper, we propose an HTTP-based Malicious Communication traffic Detection Model (HMCD-Model) based on generated adversarial flows and hierarchical traffic features. HMCD-Model consists of two parts. The first is a generation algorithm based on WGAN-GP to generate HTTP-based malicious communication traffic for data enhancement. The second is a hybrid neural network based on CNN and LSTM to extract hierarchical spatial-temporal features of traffic. In addition, we collect and publish a dataset, HMCT-2020, which consists of large-scale malicious and benign traffic during three years (2018-2020). Taking the data in HMCT-2020(18) as the training set and the data in other datasets as the test set, the experimental results show that the HMCD-Model can effectively detect unknown HTTP-based malicious communication traffic. It can reach F1 = 98.66% in the dataset HMCT-2020(19-20), F1 = 90.69% in the public dataset CIC-IDS-2017, and F1 = 83.66% in the real traffic, which is 20+% higher than other representative methods on average. This validates that HMCD-Model has the ability to discover unknown HTTP-based malicious communication behavior.",
    "published_date": "2023-09-07",
    "pdf_link": "https://arxiv.org/pdf/2309.03739v1",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Detection of unknown HTTP-based malicious communication (C2) in HTTP traffic",
      "attack_types": [
        "Malware command-and-control (C2) over HTTP",
        "Botnet communication",
        "Spyware exfiltration/communication",
        "Zero-day/previously unseen malicious HTTP flows"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "GAN",
        "specific": "WGAN-GP",
        "novel_contribution": "Generation of HTTP-based Generated Adversarial Flows (GAFs) with maliciousness, compliance, covertness, and multiformity for data augmentation to improve generalization"
      },
      {
        "type": "primary",
        "category": "CNN",
        "specific": null,
        "novel_contribution": "Packet-level spatial feature extraction by converting HTTP packets to 2D byte images and learning hierarchical spatial features"
      },
      {
        "type": "primary",
        "category": "RNN",
        "specific": "LSTM",
        "novel_contribution": "Flow-level temporal modeling of packet sequences to learn hierarchical temporal features"
      },
      {
        "type": "primary",
        "category": "Feature Engineering",
        "specific": null,
        "novel_contribution": "Hierarchical statistical features at packet level (e.g., counts/lengths of fields, payload length) and flow level (e.g., counts of request types, packet counts), while discarding misleading attributes (ip, url, etc.)"
      },
      {
        "type": "baseline",
        "category": "SVM",
        "specific": "Chi-square feature selection + N-gram + SVM (Wang et al. [44])",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Ensemble",
        "specific": "IG-PCA-Ensemble (Salo et al. [33]) combining SVM, IBK, MLP",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "k-NN",
        "specific": "IBK (k-NN)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Neural Network",
        "specific": "Multilayer Perceptron (MLP)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Dimensionality Reduction",
        "specific": "PCA; Information Gain (IG)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised (GAN-based data generation/augmentation)"
    ],
    "datasets": [
      {
        "name": "HMCT-2020",
        "type": "public",
        "domain": "network_traffic",
        "link": "https://github.com/BitBrave-Xie/HMCD-Model",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "HMCT-2020(18)",
        "type": "public",
        "domain": "network_traffic",
        "link": "https://github.com/BitBrave-Xie/HMCD-Model",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "HMCT-2020(19-20)",
        "type": "public",
        "domain": "network_traffic",
        "link": "https://github.com/BitBrave-Xie/HMCD-Model",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CIC-IDS-2017",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Real-world HTTP traffic (proprietary)",
        "type": "proprietary",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "N-gram + Chi-square feature selection + SVM (Wang et al. [44])",
        "paper_reference": "[44]",
        "metric": "F1 (reported), FPR",
        "their_result": "F1≈98.66% (HMCT-2020(19-20)); F1≈90.69% (CIC-IDS-2017); F1≈83.66% (real traffic)",
        "baseline_result": null
      },
      {
        "method_name": "IG-PCA-Ensemble (SVM + IBK + MLP) (Salo et al. [33])",
        "paper_reference": "[33]",
        "metric": "F1 (reported), FPR",
        "their_result": "F1≈98.66% (HMCT-2020(19-20)); F1≈90.69% (CIC-IDS-2017); F1≈83.66% (real traffic)",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "F1",
      "FPR"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "How to detect unknown HTTP-based malicious communication traffic that closely mimics benign HTTP behavior?",
        "Can GAN-generated adversarial HTTP flows improve generalization of detectors across years, public datasets, and real-world traffic?",
        "Do hierarchical spatial-temporal features at packet and flow levels provide better generalization than prior feature-engineered approaches?"
      ],
      "gaps_identified": [
        "Prior methods rely on artificial feature engineering and unstable features (ip, url, etc.), limiting generalization to unknown attacks.",
        "Existing datasets are outmoded or small-scale/private, with limited timeliness and coverage for future forms of malicious HTTP communication.",
        "Traffic generation work often targets flow-level statistics or specific datasets, not real HTTP-based malicious communication that can be transmitted on networks.",
        "Hierarchical spatial-temporal features for HTTP malicious communication under adversarial conditions have been under-explored."
      ],
      "limitations": [
        "Preprocessing discards flows if packet/field counts exceed thresholds (e.g., packet count >50; field lines >18), potentially losing information.",
        "Miss-leading attributes (ip, url, etc.) are discarded; while reducing bias, this could omit useful context in some cases."
      ],
      "future_work": [],
      "motivation": "Improve generalization for detecting unknown HTTP-based malicious communication by augmenting data with adversarially generated flows and learning hierarchical spatial-temporal features; provide a timely, large-scale dataset.",
      "potential_research_ideas": [
        "Extend from HTTP to multi-protocol C2 detection (HTTPS, HTTP/2, DNS-over-HTTPS, QUIC) with protocol-agnostic representations.",
        "Self-supervised pretraining on massive unlabeled network traffic (contrastive learning on packet/flow sequences) to reduce reliance on labeled data.",
        "Protocol-constrained generative models (conditional diffusion or constrained GANs) that enforce HTTP grammar and semantics during generation.",
        "Domain generalization/robustness strategies (e.g., invariant risk minimization, adversarial domain adaptation) across time, networks, and organizations.",
        "Incorporate side-channel timing features and inter-flow correlations (session graphs) via graph neural networks for C2 campaign-level detection.",
        "Calibrated uncertainty estimation and open-set recognition to better handle unknown/novel malicious behaviors.",
        "Causality-inspired feature learning to disentangle benign browsing patterns from adversarial manipulations."
      ],
      "architectural_improvement_recommendations": [
        "Replace or augment LSTM with Transformers for long-range packet/flow dependencies; include relative position encodings for variable-length flows.",
        "Multi-view fusion: jointly encode raw byte images, parsed header tokens, and statistical features using cross-attention.",
        "Use lightweight 1D CNNs on byte streams instead of 2D images to preserve sequential locality and reduce compute.",
        "Introduce hard protocol constraints via a finite-state automaton layer or masked decoding during generation to ensure compliant HTTP semantics.",
        "Conditional generation (class/behavior-conditioned WGAN/ diffusion) to synthesize specific C2 behaviors and hard-negative benign lookalikes.",
        "Train with domain-adversarial objectives to learn features invariant to network/site/organization domains.",
        "Apply curriculum learning with progressively harder GAFs (increasing covertness) to improve robustness.",
        "Add out-of-distribution (OOD) detectors and confidence calibration (e.g., energy-based scores, temperature scaling)."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": "https://github.com/BitBrave-Xie/HMCD-Model",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "WGAN-GP-based generation of HTTP Generated Adversarial Flows (GAFs) exhibiting maliciousness, compliance, covertness, and multiformity for data augmentation.",
      "Hybrid CNN+LSTM model extracting hierarchical spatial-temporal features from packet-level byte images and flow-level sequences, with hierarchical statistical features and removal of misleading attributes.",
      "New large-scale dataset HMCT-2020 (2018–2020) with ~76,760 malicious and ~4,798,110 benign HTTP flows; publicly released.",
      "Demonstrated generalization: train on HMCT-2020(18) and test on HMCT-2020(19–20), CIC-IDS-2017, and real traffic. Reported results: F1≈98.66% (HMCT-2020(19–20)), F1≈90.69% (CIC-IDS-2017), F1≈83.66% (real traffic); HMCT-2020 full: F1=99.46% and FPR=0.48%.",
      "Empirical evidence that the approach is 20%+ higher on average than representative methods across evaluations."
    ]
  },
  {
    "arxiv_id": "2310.01152v2",
    "title": "Large Language Model-Powered Smart Contract Vulnerability Detection: New Perspectives",
    "authors": "Sihao Hu; Tiansheng Huang; Fatih İlhan; Selim Furkan Tekin; Ling Liu",
    "abstract": "This paper provides a systematic analysis of the opportunities, challenges, and potential solutions of harnessing Large Language Models (LLMs) such as GPT-4 to dig out vulnerabilities within smart contracts based on our ongoing research. For the task of smart contract vulnerability detection, achieving practical usability hinges on identifying as many true vulnerabilities as possible while minimizing the number of false positives. Nonetheless, our empirical study reveals contradictory yet interesting findings: generating more answers with higher randomness largely boosts the likelihood of producing a correct answer but inevitably leads to a higher number of false positives. To mitigate this tension, we propose an adversarial framework dubbed GPTLens that breaks the conventional one-stage detection into two synergistic stages $-$ generation and discrimination, for progressive detection and refinement, wherein the LLM plays dual roles, i.e., auditor and critic, respectively. The goal of auditor is to yield a broad spectrum of vulnerabilities with the hope of encompassing the correct answer, whereas the goal of critic that evaluates the validity of identified vulnerabilities is to minimize the number of false positives. Experimental results and illustrative examples demonstrate that auditor and critic work together harmoniously to yield pronounced improvements over the conventional one-stage detection. GPTLens is intuitive, strategic, and entirely LLM-driven without relying on specialist expertise in smart contracts, showcasing its methodical generality and potential to detect a broad spectrum of vulnerabilities. Our code is available at: https://github.com/git-disl/GPTLens.",
    "published_date": "2023-10-02",
    "pdf_link": "https://arxiv.org/pdf/2310.01152v2",
    "paper_types": [
      "position",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Blockchain Security",
      "subdomain": "Smart Contract Security / Auditing",
      "specific_problem": "LLM-powered smart contract vulnerability detection with reduced false positives via a two-stage adversarial (generation + discrimination) framework",
      "attack_types": [
        "re-entrancy",
        "integer overflow/underflow",
        "access control risk",
        "condition logic error",
        "incorrect constructor name",
        "arbitrary balance manipulation"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Large Language Model / Prompting",
        "specific": "GPT-4 (LLM) used in two roles: Auditor (generator) and Critic (discriminator)",
        "novel_contribution": "GPTLens: a two-stage adversarial, entirely LLM-driven framework with open-ended prompting, diverse high-temperature sampling for generation, and LLM-based discrimination ranking by correctness, severity, and profitability"
      },
      {
        "type": "primary",
        "category": "Sampling / Self-consistency-style prompting",
        "specific": null,
        "novel_contribution": "Use of multiple auditors with higher randomness (temperature) to increase likelihood of generating correct vulnerabilities, then refined by an LLM critic"
      },
      {
        "type": "baseline",
        "category": "Large Language Model / Prompting",
        "specific": "One-stage open-ended LLM detection (single-pass generation without discrimination)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Zero-shot",
      "Prompt-based",
      "In-context learning"
    ],
    "datasets": [
      {
        "name": "13 real-world CVE-labeled smart contracts",
        "type": "public",
        "domain": "smart_contract_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CVE-2018-13836 smart contract",
        "type": "public",
        "domain": "smart_contract_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CVE-2018-10666 smart contract",
        "type": "public",
        "domain": "smart_contract_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CVE-2018-11411 smart contract",
        "type": "public",
        "domain": "smart_contract_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CVE-2019-15079 smart contract",
        "type": "public",
        "domain": "smart_contract_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "52 DeFi attacks dataset (from prior study [10])",
        "type": "public",
        "domain": "smart_contract_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Dataset for 9 most common vulnerability categories (from prior study [8])",
        "type": "public",
        "domain": "smart_contract_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "HumanEval (referenced for sampling effect in Codex paper)",
        "type": "public",
        "domain": "code_generation_benchmarks",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Conventional one-stage LLM detection (single-stage open-ended prompting)",
        "paper_reference": null,
        "metric": "Contract-level top-1 success rate",
        "their_result": "76.9% (GPTLens)",
        "baseline_result": "38.5% (one-stage)"
      },
      {
        "method_name": "Conventional one-stage LLM detection (single-stage open-ended prompting)",
        "paper_reference": null,
        "metric": "Trial-level top-1 accuracy",
        "their_result": "59.0% (GPTLens)",
        "baseline_result": "33.3% (one-stage)"
      },
      {
        "method_name": "GPT-4 (binary/close-ended style across 52 DeFi attacks) [10]",
        "paper_reference": "[10]",
        "metric": "Precision",
        "their_result": null,
        "baseline_result": "4.15% precision (32/73 true across 52 DeFi attacks with 740 false positives)"
      },
      {
        "method_name": "Claude-1.3 (close-ended on 52 DeFi attacks) [10]",
        "paper_reference": "[10]",
        "metric": "Precision / Recall",
        "their_result": null,
        "baseline_result": "4.3% precision; 35.6% recall"
      },
      {
        "method_name": "GPT-4 (close-ended on 9 common vulnerability categories) [8]",
        "paper_reference": "[8]",
        "metric": "Precision",
        "their_result": null,
        "baseline_result": "22.6% precision"
      },
      {
        "method_name": "GPT-3.5 (close-ended on 9 common vulnerability categories) [8]",
        "paper_reference": "[8]",
        "metric": "Precision",
        "their_result": null,
        "baseline_result": "19.7% precision"
      }
    ],
    "performance_metrics_used": [
      "Contract-level top-1 success rate",
      "Trial-level top-1 accuracy",
      "Top-k ranking (selection by critic)",
      "Precision (discussed from prior work)",
      "Recall (discussed from prior work)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How can LLMs identify more true smart contract vulnerabilities while minimizing false positives?",
        "Can open-ended prompting enable detection of unknown or uncategorized vulnerabilities compared to close-ended prompting?",
        "Does a two-stage adversarial (auditor-critic) framework outperform conventional one-stage LLM detection?",
        "How should detected vulnerabilities be ranked considering correctness, severity, and profitability to reflect real-world exploitability?"
      ],
      "gaps_identified": [
        "LLMs produce a large number of false positives leading to low precision and high manual verification cost.",
        "Close-ended (binary/multi-class) prompting restricts detection to predefined categories, missing unknown/uncategorized vulnerabilities.",
        "LLMs miss many true vulnerabilities (false negatives), due to both cognitive limitations and randomness of generation.",
        "No oracle in real-world settings to pick the best sample among diverse generations; naive high-temperature sampling increases false positives.",
        "Detection should account for severity and profitability, not just correctness, to prioritize practical risks."
      ],
      "limitations": [
        "Hard cases beyond the cognitive ability of current LLMs remain undetected.",
        "LLMs are sensitive to syntactic details (e.g., modifiers, require/assert/revert, events) especially with large inputs, causing factual errors.",
        "Naive diverse sampling increases false positives without an oracle to choose the correct sample.",
        "Evaluation conducted on 13 CVE-labeled contracts; broader generalization not yet empirically validated in large-scale benchmarks."
      ],
      "future_work": [
        "Use more powerful LLMs or more sophisticated designs to handle hard cases beyond current capabilities.",
        "Explore more complicated designs (beyond the simple two-stage approach) to further reduce false positives and false negatives.",
        "Extend to broader application scenarios and larger-scale evaluations."
      ],
      "motivation": "Achieve practically usable LLM-powered smart contract auditing by maximizing true vulnerability detection while minimizing false positives, resolving the tension between diverse generation (recall) and precision.",
      "potential_research_ideas": [
        "Integrate static analysis, symbolic execution, or fuzzing with the LLM critic to automatically validate exploitability and reduce false positives.",
        "Train a lightweight verifier model (e.g., fine-tuned classifier or re-ranker) on labeled auditor outputs to replace or augment the LLM critic.",
        "Cross-model adjudication: use multiple heterogeneous LLMs as critics and aggregate scores for robust ranking.",
        "Program analysis–aware prompting: feed function-level slices, CFG/DFG summaries, or AST-based features to reduce syntactic misinterpretation.",
        "Automatic exploit/test generation (e.g., Foundry/Hardhat) to confirm severity/profitability and feedback to the critic.",
        "Active sampling and temperature scheduling: adapt generation temperature and number of samples based on critic uncertainty.",
        "Human-in-the-loop triage interfaces that surface critic rationales and uncertainty to auditors for efficient verification.",
        "Domain retrieval augmentation: retrieve similar historical CVEs/attacks to guide auditor reasoning and critic calibration."
      ],
      "architectural_improvement_recommendations": [
        "Replace the single critic with an ensemble of critics (LLM + trained verifier) and use calibrated score fusion for ranking.",
        "Incorporate tool use: have the critic call static analyzers (e.g., Slither) and on-chain simulators to validate claims before scoring.",
        "Structure-aware chunking and hierarchical prompting (contract-level to function-level) to mitigate token-limit-induced errors.",
        "Self-consistency with majority voting across auditor samples combined with critic scoring to improve robustness.",
        "Uncertainty-aware ranking: use the critic’s confidence to decide when to generate additional samples.",
        "Exploitability simulation: integrate gas/MEV/profit estimators to quantify profitability in ranking."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/git-disl/GPTLens",
      "frameworks": [
        "OpenAI API"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Offline smart contract auditing of real-world CVE-labeled contracts",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "High false positive rate without discrimination increases manual verification burden",
        "Potential false negatives due to LLM cognitive limits and generation randomness",
        "Sensitivity to Solidity syntax details and token-length constraints",
        "Trade-off between diversity (recall) and precision without an oracle",
        "Cost and rate limits of LLM API usage for multi-sample generation"
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Systematic analysis of opportunities and challenges for LLM-powered smart contract vulnerability detection",
      "GPTLens: an adversarial two-stage framework with LLM as auditor (generator) and critic (discriminator) to balance recall and precision",
      "Demonstrated improvements over one-stage detection on 13 real-world CVE contracts (contract-level top-1 success: 38.5% -> 76.9%; trial-level top-1 accuracy: 33.3% -> 59.0%)",
      "Advocacy and design of open-ended prompting for generality beyond predefined vulnerability categories",
      "Released code for the proposed framework"
    ]
  },
  {
    "arxiv_id": "2309.03292v2",
    "title": "Scalable Learning of Intrusion Responses through Recursive Decomposition",
    "authors": "Kim Hammar; Rolf Stadler",
    "abstract": "We study automated intrusion response for an IT infrastructure and formulate the interaction between an attacker and a defender as a partially observed stochastic game. To solve the game we follow an approach where attack and defense strategies co-evolve through reinforcement learning and self-play toward an equilibrium. Solutions proposed in previous work prove the feasibility of this approach for small infrastructures but do not scale to realistic scenarios due to the exponential growth in computational complexity with the infrastructure size. We address this problem by introducing a method that recursively decomposes the game into subgames which can be solved in parallel. Applying optimal stopping theory we show that the best response strategies in these subgames exhibit threshold structures, which allows us to compute them efficiently. To solve the decomposed game we introduce an algorithm called Decompositional Fictitious Self-Play (DFSP), which learns Nash equilibria through stochastic approximation. We evaluate the learned strategies in an emulation environment where real intrusions and response actions can be executed. The results show that the learned strategies approximate an equilibrium and that DFSP significantly outperforms a state-of-the-art algorithm for a realistic infrastructure configuration.",
    "published_date": "2023-09-06",
    "pdf_link": "https://arxiv.org/pdf/2309.03292v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Response / Active Cyber Defense",
      "specific_problem": "Automated learning of defender intrusion-response strategies against dynamic attackers under partial observability",
      "attack_types": [
        "Reconnaissance",
        "Brute-force attacks",
        "Code execution exploits"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Reinforcement Learning (Multi-agent)",
        "specific": "Decompositional Fictitious Self-Play (DFSP)",
        "novel_contribution": "Recursive decomposition of a partially observed stochastic game into parallelizable subgames with efficient best-response learning; stochastic-approximation self-play to learn Nash equilibria."
      },
      {
        "type": "primary",
        "category": "Game-theoretic learning",
        "specific": "Fictitious Self-Play (with decomposition)",
        "novel_contribution": "Learns equilibrium strategies via stochastic approximation over decomposed subgames."
      },
      {
        "type": "primary",
        "category": "Optimal Control / Optimal Stopping",
        "specific": null,
        "novel_contribution": "Proves best responses in subgames have threshold structures enabling efficient computation via optimal stopping."
      },
      {
        "type": "primary",
        "category": "Partially Observed Stochastic Game (POSG) modeling",
        "specific": "PO-POSG with public observations",
        "novel_contribution": "Formulates intrusion response as a stationary, finite, zero-sum PO-POSG with public observations and establishes existence of mixed Nash equilibria."
      }
    ],
    "learning_paradigm": [
      "Reinforcement Learning",
      "Self-Play",
      "Game-theoretic Learning"
    ],
    "datasets": [
      {
        "name": "Digital twin emulation logs/statistics of target IT infrastructure",
        "type": "synthetic",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Discounted cumulative utility J (workflow utility minus intrusion cost)",
      "Workflow utility (proportional to active nodes in workflow subtrees)",
      "Intrusion cost (binary compromise indicator plus action cost)",
      "Approximation to Nash equilibrium (qualitative claim)"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can automated intrusion response be learned as equilibrium strategies against dynamic attackers when modeled as a partially observed stochastic game?",
        "Can recursive decomposition of the game enable scalability to realistic infrastructures without exponential complexity?",
        "Do best-response strategies in decomposed subgames admit threshold structures that allow efficient computation?",
        "Does the proposed DFSP algorithm learn strategies that approximate Nash equilibria and outperform existing methods in realistic configurations?"
      ],
      "gaps_identified": [
        "Prior approaches scale poorly due to exponential growth with infrastructure size.",
        "Much prior work assumes static attackers or decision-theoretic (non-game) settings, not dynamic attackers and Nash equilibria.",
        "Some prior decomposition-based works lack partial observability or dynamic attackers, or rely only on simulation-based evaluation rather than emulation on a digital twin."
      ],
      "limitations": [
        "Results rely on assumptions that enable decomposition (e.g., workflows isolated; workflow graphs with tree structure; public observations; stationary client processes; conditional independence of observations given state).",
        "Evaluation is conducted in an emulation (digital twin) rather than a live production environment.",
        "Game model parameters are estimated from emulation logs; performance depends on fidelity of the digital twin and estimation."
      ],
      "future_work": [],
      "motivation": "Scale automated intrusion response learning to realistic IT infrastructures by reducing computational complexity while handling dynamic attackers under partial observability.",
      "potential_research_ideas": [
        "Relax decomposition assumptions to support overlapping/non-isolated workflows and general (non-tree) dependency graphs.",
        "Incorporate richer observation models (multi-sensor alerts, continuous signals) and learn observation models jointly with strategies.",
        "Online adaptation of thresholds and policies under non-stationary client traffic and attacker behavior.",
        "Combine decomposition with opponent modeling or population-based training to accelerate convergence.",
        "Investigate transfer learning/meta-learning of subgame policies across infrastructures or workflows.",
        "Quantify equilibrium approximation (e.g., exploitability) and develop guarantees for DFSP convergence under partial observability.",
        "Integrate detection and response co-learning to jointly optimize alerting and mitigation actions."
      ],
      "architectural_improvement_recommendations": [
        "Augment DFSP with PSRO-style policy management to curate and reuse subgame best responses across iterations.",
        "Introduce hierarchical policies: high-level scheduler over workflows with low-level per-node optimal-stopping controllers.",
        "Use function approximation (e.g., shallow NN or GBDT) to learn threshold mappings from beliefs to actions for complex subgames while retaining interpretability.",
        "Add uncertainty-aware belief updates (e.g., Bayesian filters) and robust optimization for model-mismatch tolerance.",
        "Parallelize subgame learning with asynchronous updates and prioritized replay of rare critical states."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Digital twin emulation of an enterprise IT infrastructure segmented into zones with workflows/microservices",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Requires partitioning infrastructure into isolated workflows; assumption may not hold universally.",
        "Assumes workflow graphs with tree structure to exploit optimal substructure.",
        "Depends on accurate estimation of infrastructure statistics from logs to instantiate simulations.",
        "Assumes public observations and stationary client-driven traffic processes."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Formulates intrusion response as a PO-POSG and proves existence of mixed Nash equilibria and non-empty best-response correspondences.",
      "Introduces a recursive decomposition into workflow and node subgames with optimal substructure, enabling parallel and efficient computation.",
      "Shows best responses in subgames have threshold structures via optimal stopping, enabling efficient learning of best responses.",
      "Designs Decompositional Fictitious Self-Play (DFSP) to learn Nash equilibria through stochastic approximation over decomposed games.",
      "Evaluates learned strategies on a digital twin emulation with real intrusions and responses, showing approximate equilibrium behavior and significant outperformance over a state-of-the-art algorithm for a realistic configuration."
    ]
  },
  {
    "arxiv_id": "2309.09826v2",
    "title": "Efficient Avoidance of Vulnerabilities in Auto-completed Smart Contract Code Using Vulnerability-constrained Decoding",
    "authors": "André Storhaug; Jingyue Li; Tianyuan Hu",
    "abstract": "Auto-completing code enables developers to speed up coding significantly. Recent advances in transformer-based large language model (LLM) technologies have been applied to code synthesis. However, studies show that many of such synthesized codes contain vulnerabilities. We propose a novel vulnerability-constrained decoding approach to reduce the amount of vulnerable code generated by such models. Using a small dataset of labeled vulnerable lines of code, we fine-tune an LLM to include vulnerability labels when generating code, acting as an embedded classifier. Then, during decoding, we deny the model to generate these labels to avoid generating vulnerable code. To evaluate the method, we chose to automatically complete Ethereum Blockchain smart contracts (SCs) as the case study due to the strict requirements of SC security. We first fine-tuned the 6-billion-parameter GPT-J model using 186,397 Ethereum SCs after removing the duplication from 2,217,692 SCs. The fine-tuning took more than one week using ten GPUs. The results showed that our fine-tuned model could synthesize SCs with an average BLEU (BiLingual Evaluation Understudy) score of 0.557. However, many codes in the auto-completed SCs were vulnerable. Using the code before the vulnerable line of 176 SCs containing different types of vulnerabilities to auto-complete the code, we found that more than 70% of the auto-completed codes were insecure. Thus, we further fine-tuned the model on other 941 vulnerable SCs containing the same types of vulnerabilities and applied vulnerability-constrained decoding. The fine-tuning took only one hour with four GPUs. We then auto-completed the 176 SCs again and found that our approach could identify 62% of the code to be generated as vulnerable and avoid generating 67% of them, indicating the approach could efficiently and effectively avoid vulnerabilities in the auto-completed code.",
    "published_date": "2023-09-18",
    "pdf_link": "https://arxiv.org/pdf/2309.09826v2",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Smart Contract Security",
      "specific_problem": "Avoiding generation of vulnerable code during smart contract auto-completion by LLMs via vulnerability-constrained decoding",
      "attack_types": [
        "Integer Overflow/Underflow (IOU)",
        "Multiple smart contract vulnerability types (10 total; unspecified)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer (decoder-only)",
        "specific": "GPT-J-6B",
        "novel_contribution": "Fine-tuned for Solidity smart contract auto-completion; achieves high BLEU on SC synthesis"
      },
      {
        "type": "primary",
        "category": "Constrained/Guided Decoding",
        "specific": "Vulnerability-constrained decoding (forbidden-token decoding)",
        "novel_contribution": "Embed vulnerability labels during fine-tuning and forbid these tokens at decode time to steer model away from vulnerable generations"
      },
      {
        "type": "baseline",
        "category": "Decoding Strategy",
        "specific": "Greedy search (illustrative); compatible with Beam Search",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Self-supervised (causal language modeling)",
      "Supervised (fine-tuning with line-level vulnerability labels)"
    ],
    "datasets": [
      {
        "name": "Verified Smart Contracts dataset (Etherscan-derived, deduplicated)",
        "type": "proprietary",
        "domain": "smart_contract_source_code",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "Vulnerable Smart Contracts dataset (dataset1 in [16])",
        "type": "public",
        "domain": "smart_contract_source_code",
        "link": "https://doi.org/10.5281/zenodo.7744053",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Line-labeled vulnerable SC overlaps (created in this paper)",
        "type": "proprietary",
        "domain": "smart_contract_source_code",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "Vulnerability-tuning training subset (941 vulnerable SCs, same types as eval)",
        "type": "proprietary",
        "domain": "smart_contract_source_code",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "Evaluation set for constrained decoding (176 vulnerable SCs)",
        "type": "proprietary",
        "domain": "smart_contract_source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "The Pile (pretraining corpus for GPT-J)",
        "type": "public",
        "domain": "mixed_text_and_code_corpus",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Pre-trained GPT-J-6B",
        "paper_reference": "EleutherAI GPT-J (2021)",
        "metric": "BLEU (whitespace tokenization)",
        "their_result": "0.557 (fine-tuned)",
        "baseline_result": "0.258 (pre-trained)"
      },
      {
        "method_name": "Pre-trained GPT-J-6B",
        "paper_reference": "EleutherAI GPT-J (2021)",
        "metric": "BLEU (Solidity lexer)",
        "their_result": "0.557 (fine-tuned)",
        "baseline_result": "0.414 (pre-trained)"
      },
      {
        "method_name": "Pre-trained GPT-J-6B",
        "paper_reference": "EleutherAI GPT-J (2021)",
        "metric": "CrystalBLEU (whitespace tokenization; K=500, maxN=4)",
        "their_result": "0.481 (fine-tuned)",
        "baseline_result": "0.188 (pre-trained)"
      },
      {
        "method_name": "Pre-trained GPT-J-6B",
        "paper_reference": "EleutherAI GPT-J (2021)",
        "metric": "CrystalBLEU (Solidity lexer; K=500, maxN=4)",
        "their_result": "0.481 (fine-tuned)",
        "baseline_result": "0.291 (pre-trained)"
      },
      {
        "method_name": "Unconstrained decoding (fine-tuned model) vs vulnerability-constrained decoding",
        "paper_reference": null,
        "metric": "Insecure completion rate on 176 SCs",
        "their_result": "Constrained decoding avoided generating 67% of the vulnerable code it identified; identified 62% of to-be-generated code as vulnerable",
        "baseline_result": ">70% of auto-completed codes were insecure without constraints"
      }
    ],
    "performance_metrics_used": [
      "BLEU",
      "CrystalBLEU",
      "accuracy",
      "perplexity",
      "percentage_insecure_generations",
      "vulnerability_identification_rate",
      "vulnerability_avoidance_rate"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How to efficiently fine-tune the transformer model to avoid vulnerabilities in the auto-complete code?"
      ],
      "gaps_identified": [
        "LLM-generated code often contains vulnerabilities (e.g., ~40% in prior work).",
        "Retraining large LLMs to remove vulnerable patterns is slow and impractical for rapid updates.",
        "Post-generation vulnerability scanning can be slow and typically requires complete functions/bytecode, conflicting with real-time autocomplete needs.",
        "Existing SC vulnerability detectors require whole functions or bytecode, not partial code typical of autocomplete."
      ],
      "limitations": [
        "Greedy decoding used for illustration is not optimal; approach compatible with other strategies but not evaluated in-depth.",
        "Method relies on availability and quality of labeled vulnerable lines and is limited to known vulnerability types encoded as labels.",
        "No hyperparameter optimization performed for fine-tuning; potential performance not fully explored.",
        "Generalization beyond the evaluated 10 vulnerability types and to other languages/frameworks is not demonstrated."
      ],
      "future_work": [],
      "motivation": "Reduce vulnerable code produced by LLM-based autocomplete efficiently without full retraining by steering generation away from patterns labeled as vulnerable.",
      "potential_research_ideas": [
        "Extend vulnerability-constrained decoding to dynamic, detector-guided decoding where static analysis signals guide beam scoring in real time.",
        "Learn a token-level vulnerability risk scorer (auxiliary head) to enable soft constraints or risk-aware decoding rather than hard label blocking.",
        "Apply the approach to broader programming languages and vulnerability taxonomies (e.g., CWE categories) and study cross-language transfer.",
        "Use parameter-efficient fine-tuning (LoRA/IA3/adapters) to rapidly update vulnerability knowledge with minimal compute.",
        "Combine constrained decoding with reinforcement learning using static-analysis-based rewards to optimize secure generation end-to-end.",
        "Develop a public, line-level labeled SC dataset with standardized splits to benchmark secure code generation methods.",
        "Investigate uncertainty-aware stopping strategies that halt generation when risk surpasses a threshold and prompt for developer input.",
        "Integrate control/data-flow representations to condition decoding with structural security constraints (e.g., taint-propagation-aware decoding)."
      ],
      "architectural_improvement_recommendations": [
        "Add an auxiliary classification head for vulnerability type prediction conditioned on context to guide logits during decoding (logit fusion).",
        "Adopt beam search with risk-penalized scoring (e.g., subtract risk score for paths likely to include vulnerable labels).",
        "Use parameter-efficient fine-tuning (LoRA) for rapid updates and A/B hot-swapping of vulnerability adapters.",
        "Introduce contrastive decoding or mixture-of-experts where a \"secure expert\" biases against insecure patterns.",
        "Calibrate and threshold vulnerability label probabilities to enable soft blocking and reduce false positives impacting code quality."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "HuggingFace Transformers",
        "DeepSpeed"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Initial fine-tuning of GPT-J-6B on 186,397 SCs: 2 epochs, 10× NVIDIA A100 40GB GPUs, ~7 days 4 hours; validation ACC 0.917, perplexity 1.510. Vulnerability-tuning on 941 vulnerable SCs: ~1 hour on 4 GPUs."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Requires curated, line-level vulnerability labels; coverage limited to known types.",
        "Potential trade-offs between blocking tokens and functional correctness/usability of generated code.",
        "Integration into IDEs must preserve low-latency autocomplete; detector-guided approaches may introduce overhead.",
        "Generalization to unseen vulnerabilities or languages is uncertain without additional data."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes vulnerability-constrained decoding: fine-tune with vulnerability labels and forbid these tokens at decode time to avoid vulnerable code.",
      "Demonstrates fine-tuned GPT-J-6B can auto-complete Solidity smart contracts with BLEU 0.557 and CrystalBLEU 0.481.",
      "Creates a large deduplicated dataset of 186,397 verified Ethereum smart contracts for training autocomplete models.",
      "Shows empirical security impact: >70% insecure completions without constraints on 176 SCs; method identifies 62% and avoids 67% of vulnerable generations it flags.",
      "Provides an efficient update path: vulnerability-tuning completed in ~1 hour on 4 GPUs versus >1 week for full fine-tune."
    ]
  },
  {
    "arxiv_id": "2309.13496v1",
    "title": "Stratosphere: Finding Vulnerable Cloud Storage Buckets",
    "authors": "Jack Cable; Drew Gregory; Liz Izhikevich; Zakir Durumeric",
    "abstract": "Misconfigured cloud storage buckets have leaked hundreds of millions of medical, voter, and customer records. These breaches are due to a combination of easily-guessable bucket names and error-prone security configurations, which, together, allow attackers to easily guess and access sensitive data. In this work, we investigate the security of buckets, finding that prior studies have largely underestimated cloud insecurity by focusing on simple, easy-to-guess names. By leveraging prior work in the password analysis space, we introduce Stratosphere, a system that learns how buckets are named in practice in order to efficiently guess the names of vulnerable buckets. Using Stratosphere, we find wide-spread exploitation of buckets and vulnerable configurations continuing to increase over the years. We conclude with recommendations for operators, researchers, and cloud providers.",
    "published_date": "2023-09-23",
    "pdf_link": "https://arxiv.org/pdf/2309.13496v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Cloud Security",
      "subdomain": "Configuration and Access Control",
      "specific_problem": "Discovery of misconfigured public cloud object storage buckets via learned bucket-name generation and large-scale measurement",
      "attack_types": [
        "Data exposure due to public-read listings",
        "Unauthorized write/delete via misconfigured ACLs",
        "Public ACL modification",
        "Exploitation of misconfigured buckets (observed in the wild)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Probabilistic Grammar",
        "specific": "Token-level PCFG",
        "novel_contribution": "Learns human bucket-naming patterns from passive DNS and code-repository leaks to generate realistic candidate bucket names; best-performing generator in Stratosphere"
      },
      {
        "type": "primary",
        "category": "Probabilistic Grammar",
        "specific": "Character-level PCFG",
        "novel_contribution": "Character-level grammar model for bucket name generation"
      },
      {
        "type": "primary",
        "category": "RNN",
        "specific": "LSTM language model",
        "novel_contribution": "Sequence model trained on bucket names to generate realistic names at scale"
      },
      {
        "type": "primary",
        "category": "N-gram",
        "specific": "Token bigrams",
        "novel_contribution": "Token-level bigram model for bucket name generation"
      },
      {
        "type": "primary",
        "category": "N-gram",
        "specific": "Character 5-grams",
        "novel_contribution": "Character-level 5-gram model for bucket name generation"
      },
      {
        "type": "baseline",
        "category": "Heuristic/Rule-based",
        "specific": "zxcvbn password-strength estimator",
        "novel_contribution": "Used to measure name guessability/complexity and decompose names into corpus vs random tokens; not used for generation"
      }
    ],
    "learning_paradigm": [
      "Unsupervised",
      "Heuristic"
    ],
    "datasets": [
      {
        "name": "Farsight passive DNS",
        "type": "public",
        "domain": "dns_records",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "VirusTotal passive DNS",
        "type": "public",
        "domain": "dns_records",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Zetalytics passive DNS",
        "type": "public",
        "domain": "dns_records",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Bing search (Azure Cognitive Services API) for bucket hostnames",
        "type": "public",
        "domain": "web_search_index",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "GitHub BigQuery dataset (bucket URLs in code)",
        "type": "public",
        "domain": "code_repositories",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Grayhat Warfare repository of buckets",
        "type": "public",
        "domain": "cloud_storage_indexes",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Stratosphere active scanning outputs (AWS S3, GCS, Alibaba OSS)",
        "type": "proprietary",
        "domain": "cloud_storage_metadata",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "35 AWS S3 honeypot buckets with varying name complexity",
        "type": "synthetic",
        "domain": "cloud_storage_honeypots",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Continella et al. scanner (random 3–4 char sequences with edits/concats)",
        "paper_reference": "[34]",
        "metric": "Hit rate (%Valid)",
        "their_result": "Token PCFG: 2.0% valid; Character PCFG: 1.1%; LSTM: 1.6%; Character 5-grams: 0.53%; Token bigrams: 0.64%",
        "baseline_result": "Continella: 4.6%"
      },
      {
        "method_name": "Continella et al. scanner",
        "paper_reference": "[34]",
        "metric": "Public buckets found (absolute)",
        "their_result": "Token PCFG: 286,610; Character PCFG: 185,978; LSTM: 259,352; Character 5-grams: 215,525; Token bigrams: 65,864",
        "baseline_result": "Continella: 125,712"
      },
      {
        "method_name": "Continella et al. scanner",
        "paper_reference": "[34]",
        "metric": "Sensitive buckets found (absolute)",
        "their_result": "Token PCFG: 300; LSTM: 219; Character PCFG: 107; Token bigrams: 66; Character 5-grams: 110",
        "baseline_result": "Continella: 208"
      },
      {
        "method_name": "Continella et al. scanner",
        "paper_reference": "[34]",
        "metric": "Misconfigured buckets found (absolute)",
        "their_result": "Token PCFG: 1.5K; LSTM: ~1.0K; Character PCFG: 543; Token bigrams: 327; Character 5-grams: 528",
        "baseline_result": "Continella: 635"
      },
      {
        "method_name": "Random alphanumeric scanner (3–64 chars)",
        "paper_reference": null,
        "metric": "Hit rate (%Valid)",
        "their_result": "Token PCFG: 2.0%; LSTM: 1.6%; Character PCFG: 1.1%; Character 5-grams: 0.53%; Token bigrams: 0.64%",
        "baseline_result": "Random: 0.1%"
      },
      {
        "method_name": "Grayhat Warfare repository",
        "paper_reference": null,
        "metric": "Name complexity distribution vs. Stratosphere and passive sources",
        "their_result": "Stratosphere finds names closer to passive DNS distribution (longer, higher entropy, harder-to-guess)",
        "baseline_result": "Grayhat names are on average an order of magnitude shorter and easier to guess than passive DNS names"
      },
      {
        "method_name": "Prior techniques (aggregate, per paper)",
        "paper_reference": "[34]",
        "metric": "Relative improvement",
        "their_result": "“find 2.7 times more public buckets, 5.8 times more misconfigured buckets (i.e., full write and delete access) and 5.3 times more buckets that host potentially sensitive documents compared to prior techniques [34]”",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Hit rate (%Valid)",
      "% Unique",
      "Public buckets found (count)",
      "Sensitive buckets found (count)",
      "Misconfigured buckets found (count)",
      "Valid public buckets per day",
      "Name length distribution",
      "Shannon entropy of name",
      "Guessability (log10 of zxcvbn-estimated guesses)",
      "ACL misconfiguration categories (public write/delete/ACL modification)"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Have prior studies underestimated the prevalence of vulnerable cloud storage buckets by focusing on easy-to-guess names?",
        "Can we learn real-world bucket naming patterns to efficiently guess names of vulnerable buckets?",
        "What is the correlation between bucket name guessability and vulnerability?",
        "What is the current state of misconfiguration and exploitation across major cloud storage providers?"
      ],
      "gaps_identified": [
        "Prior scanners bias toward short, simple names, underestimating global vulnerability by “500%”.",
        "Passive DNS gives a privileged but incomplete view and may become less effective as DoH/DoT deploys.",
        "Cloud provider customer notifications of misconfigurations are slow (months), leaving windows for exploitation."
      ],
      "limitations": [
        "Passive DNS and search sources are not complete representations of all real-world buckets.",
        "Ethical constraints: analysis of file metadata only; no content downloads except index pages on Alibaba websites.",
        "Grayhat Warfare’s methodology is a black box; comparisons rely on observable outputs only."
      ],
      "future_work": [
        "Recommendations for operators, researchers, and cloud providers (details in paper’s conclusion)."
      ],
      "motivation": "Misconfigured cloud storage buckets have caused massive data leaks; the enormous name space makes global assessment and notification difficult; attackers actively scan while provider notifications are slow.",
      "potential_research_ideas": [
        "Train modern transformer-based character/subword language models for bucket-name generation to further improve coverage of hard-to-guess names.",
        "Conditioned generation using auxiliary context (e.g., domains observed in TLS certs/logs, organization names in code repositories) to focus scanning efforts.",
        "Active learning loop that prioritizes model updates from newly discovered valid/vulnerable buckets to adapt to evolving naming conventions.",
        "Defensive tooling: provider-side pre-deployment name audits, real-time misconfiguration validators, and canary token insertion to detect illicit access.",
        "Cross-cloud generalization: extend to additional providers and services (e.g., Azure Blob, DigitalOcean Spaces) and compare security postures.",
        "Risk scoring that combines name guessability, ACLs, and object metadata signals to triage disclosures."
      ],
      "architectural_improvement_recommendations": [
        "Adopt a hybrid generator that ensembles token-PCFG with a transformer LM and character-level models using weighted sampling to balance hit rate and vulnerability yield.",
        "Use subword tokenization (e.g., Byte-Pair Encoding) to better capture l33t variants and mixed alphanumeric tokens.",
        "Implement importance sampling guided by zxcvbn guessability to target higher-risk name strata while maintaining coverage.",
        "Integrate a feedback controller for API rate limits and error codes to dynamically tune generation pace per provider.",
        "Add a classifier to predict public/vulnerable likelihood from candidate names prior to probing, reducing unnecessary requests."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": null,
      "frameworks": [
        "ZMap",
        "ZGrab",
        "zxcvbn"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Internet-scale active scanning of AWS S3, Google Cloud Storage, and Alibaba OSS; AWS S3 honeypots for measurement",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Passive DNS visibility may decline with DoH/DoT adoption",
        "Cloud provider notification delays hinder rapid remediation",
        "Ethical and legal considerations limit content access",
        "Provider-side interface changes (e.g., removal of side channels used by prior tools)"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces Stratosphere, a system that learns real-world bucket naming patterns to efficiently guess vulnerable bucket names.",
      "Demonstrates prior scanners’ bias toward short names and underestimation of global vulnerability by approximately 500%.",
      "Shows Stratosphere finds “2.7× more public buckets, 5.8× more misconfigured buckets, and 5.3× more buckets with potentially sensitive documents” compared to prior techniques.",
      "Builds multiple generators (Token-PCFG, Character-PCFG, LSTM, token bigrams, character 5-grams) and analyzes their performance.",
      "Analyzes 2.1M buckets across AWS, GCS, and Alibaba; identifies sensitive data in 10.6% of public buckets and evidence of exploitation in 3% of vulnerable buckets.",
      "Reveals worsening security trends in AWS S3 (recently updated buckets more likely to be vulnerable; up to 5% allowing unauthenticated ACL modification).",
      "Deploys 35 AWS honeypot buckets, observing unsolicited access within 24 hours; contrasts with slow provider notifications (months).",
      "Releases Stratosphere as an open-source tool and provides recommendations for operators, researchers, and cloud providers."
    ]
  },
  {
    "arxiv_id": "2308.09578v1",
    "title": "An AI-Driven VM Threat Prediction Model for Multi-Risks Analysis-Based Cloud Cybersecurity",
    "authors": "Deepika Saxena; Ishu Gupta; Rishabh Gupta; Ashutosh Kumar Singh; Xiaoqing Wen",
    "abstract": "Cloud virtualization technology, ingrained with physical resource sharing, prompts cybersecurity threats on users' virtual machines (VM)s due to the presence of inevitable vulnerabilities on the offsite servers. Contrary to the existing works which concentrated on reducing resource sharing and encryption and decryption of data before transfer for improving cybersecurity which raises computational cost overhead, the proposed model operates diversely for efficiently serving the same purpose. This paper proposes a novel Multiple Risks Analysis based VM Threat Prediction Model (MR-TPM) to secure computational data and minimize adversary breaches by proactively estimating the VMs threats. It considers multiple cybersecurity risk factors associated with the configuration and management of VMs, along with analysis of users' behaviour. All these threat factors are quantified for the generation of respective risk score values and fed as input into a machine learning based classifier to estimate the probability of threat for each VM. The performance of MR-TPM is evaluated using benchmark Google Cluster and OpenNebula VM threat traces. The experimental results demonstrate that the proposed model efficiently computes the cybersecurity risks and learns the VM threat patterns from historical and live data samples. The deployment of MR-TPM with existing VM allocation policies reduces cybersecurity threats up to 88.9%.",
    "published_date": "2023-08-18",
    "pdf_link": "https://arxiv.org/pdf/2308.09578v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Cloud Security",
      "subdomain": "Virtualization Security",
      "specific_problem": "Proactive prediction of VM cyberthreats arising from VM/hypervisor misconfiguration and insecure allocation in multi-tenant clouds",
      "attack_types": [
        "Side-channel attacks",
        "Unauthorized data access",
        "Co-residency attacks",
        "Hypervisor vulnerability exploitation",
        "Network cascading effects",
        "Data leakage",
        "Account hijacking"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Gradient Boosted Trees",
        "specific": "XGBoost",
        "novel_contribution": "MR-TPM integrates quantified multi-risk factors (VM vulnerability, hypervisor vulnerability, side-channel risk, network cascading risk, and user behavior) into an online XGBoost classifier for proactive VM threat prediction."
      },
      {
        "type": "primary",
        "category": "Neural Network",
        "specific": null,
        "novel_contribution": "Neural-network-based workload predictor used to identify active VMs and guide feature computation for the next time window."
      },
      {
        "type": "primary",
        "category": "Feature Selection",
        "specific": "Recursive Feature Elimination (RFE)",
        "novel_contribution": "Used to filter useful risk features prior to training to improve accuracy and reduce computation time."
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Online Learning"
    ],
    "datasets": [
      {
        "name": "Google Cluster traces",
        "type": "public",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      },
      {
        "name": "OpenNebula VM threat traces",
        "type": "public",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Threat reduction (%)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How to proactively estimate VM security threats in real-time on multi-tenant clouds without harming VM management efficiency?",
        "How to quantify multiple security risk factors (user behavior, VM vulnerability, hypervisor vulnerability, side-channel risk, network cascading effects) into a training-ready knowledge base?",
        "How to integrate online learning so the threat predictor adapts to historical and live data?"
      ],
      "gaps_identified": [
        "Existing works focus on reducing resource sharing or encrypting/decrypting data, which raises computational cost overhead.",
        "Rigorous control over VM-centered cybercrimes is in its infancy; lack of proactive real-time VM threat estimation.",
        "Misconfiguration and insecure VM allocation remain key untreated sources of threats."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Cloud VM and hypervisor misconfiguration/mismanagement enable co-residency and side-channel risks; need a low-overhead, proactive ML approach to predict and mitigate VM threats.",
      "potential_research_ideas": [
        "Develop a graph-based threat predictor leveraging VM–server–network topology (e.g., GNNs) to model co-residency and network cascading risks explicitly.",
        "Incorporate uncertainty quantification and calibrated probabilities (e.g., conformal prediction) for risk-aware decision-making and thresholding.",
        "Add concept-drift detection and adaptive online learning to handle evolving attacker behaviors and workload patterns.",
        "Combine supervised prediction with unsupervised anomaly detection for zero-day threat discovery (e.g., autoencoders, isolation forests).",
        "Use reinforcement learning to co-optimize threat mitigation with VM placement/migration costs under SLAs and power constraints.",
        "Investigate privacy-preserving or federated training across clusters to utilize sensitive logs without centralizing data.",
        "Create a standardized benchmark for VM threat prediction with labeled multi-tenant traces and public baselines."
      ],
      "architectural_improvement_recommendations": [
        "Model temporal dependencies with sequence models (e.g., Temporal Convolutional Networks or LSTMs) jointly with XGBoost to capture time-evolving risks.",
        "Adopt a heterogeneous ensemble (stacking) that blends tree-based models with neural temporal models for improved generalization.",
        "Integrate SHAP/feature-attribution pipelines for explainability and operator feedback loops.",
        "Implement cost-sensitive learning and class imbalance handling (focal loss or reweighting) to better detect rare threats.",
        "Add adversarial training or robustness evaluation to withstand data poisoning or evasion on telemetry inputs.",
        "Use online feature stores and streaming pipelines for low-latency inference and continual learning."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Cloud datacenter setting (evaluation on Google Cluster and OpenNebula traces; integration with VM allocation policies)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes a Multiple Risks Analysis based VM Threat Prediction Model (MR-TPM) that quantifies configuration- and allocation-related risk factors plus user behavior to predict VM threats.",
      "Introduces quantification and assessment of VM vulnerability, hypervisor vulnerability, side-channel risk, network cascading risk, and user behavior into risk scores for model training.",
      "Implements an AI-driven predictor (XGBoost) with online/periodic retraining using historical and live samples; includes a neural workload predictor and RFE-based feature selection.",
      "Demonstrates compatibility of MR-TPM with existing VM allocation policies and reports threat reduction “up to 88.9%.”"
    ]
  },
  {
    "arxiv_id": "2310.10664v2",
    "title": "Nebula: Self-Attention for Dynamic Malware Analysis",
    "authors": "Dmitrijs Trizna; Luca Demetrio; Battista Biggio; Fabio Roli",
    "abstract": "Dynamic analysis enables detecting Windows malware by executing programs in a controlled environment and logging their actions. Previous work has proposed training machine learning models, i.e., convolutional and long short-term memory networks, on homogeneous input features like runtime APIs to either detect or classify malware, neglecting other relevant information coming from heterogeneous data like network and file operations. To overcome these issues, we introduce Nebula, a versatile, self-attention Transformer-based neural architecture that generalizes across different behavioral representations and formats, combining diverse information from dynamic log reports. Nebula is composed by several components needed to tokenize, filter, normalize and encode data to feed the transformer architecture. We firstly perform a comprehensive ablation study to evaluate their impact on the performance of the whole system, highlighting which components can be used as-is, and which must be enriched with specific domain knowledge. We perform extensive experiments on both malware detection and classification tasks, using three datasets acquired from different dynamic analyses platforms, show that, on average, Nebula outperforms state-of-the-art models at low false positive rates, with a peak of 12% improvement. Moreover, we showcase how self-supervised learning pre-training matches the performance of fully-supervised models with only 20% of training data, and we inspect the output of Nebula through explainable AI techniques, pinpointing how attention is focusing on specific tokens correlated to malicious activities of malware families. To foster reproducibility, we open-source our findings and models at https://github.com/dtrizna/nebula.",
    "published_date": "2023-09-19",
    "pdf_link": "https://arxiv.org/pdf/2310.10664v2",
    "paper_types": [
      "empirical_analysis",
      "reproducibility",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Malware Analysis",
      "subdomain": "Dynamic Analysis",
      "specific_problem": "Windows malware detection and family classification from dynamic behavioral log reports",
      "attack_types": [
        "malware",
        "malware family classification"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "Transformer encoder with self-attention and positional encoding",
        "novel_contribution": "Nebula: a versatile Transformer-based architecture that tokenizes, filters, normalizes, and encodes heterogeneous dynamic log data (API, file, network, registry) and fuses them via self-attention."
      },
      {
        "type": "primary",
        "category": "Tokenization/Embedding",
        "specific": "Byte-Pair Encoding (BPE) adapted to include raw bytes and UTF-8 characters",
        "novel_contribution": "Domain-adapted BPE vocabulary and normalization placeholders (e.g., <sha1>, <domain>, IP-class placeholders) to control vocabulary explosion and improve generalization."
      },
      {
        "type": "baseline",
        "category": "CNN + LSTM + Attention",
        "specific": "Neurlux",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Gated CNN",
        "specific": "Zhang et al. (Gated CNN)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "Quo.Vadis dynamic component (1D CNN on API names)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "HMIL + MLP",
        "specific": "JSONGrinder (JsonGrinder.jl + Mill.jl)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN + LSTM",
        "specific": "CurParamer (parameter-assisted API labeling and sensitivity-inspired embedding)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Self-supervised (pretraining)"
    ],
    "datasets": [
      {
        "name": "Speakeasy emulated behavioral reports (from Quo.Vadis)",
        "type": "public",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "JSONGrinder malware family classification dataset",
        "type": "public",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Neurlux",
        "paper_reference": "Jindal et al. [3]",
        "metric": "TPR at FPR = 1e-3 (low-FPR regime)",
        "their_result": "\"on average, Nebula outperforms state-of-the-art models at low false positive rates, with a peak of 12% improvement.\"",
        "baseline_result": null
      },
      {
        "method_name": "Gated CNN",
        "paper_reference": "Zhang et al. [7]",
        "metric": "TPR at FPR = 1e-3 (low-FPR regime)",
        "their_result": "\"on average, Nebula outperforms state-of-the-art models at low false positive rates, with a peak of 12% improvement.\"",
        "baseline_result": null
      },
      {
        "method_name": "Quo.Vadis (dynamic CNN)",
        "paper_reference": "Trizna [2]",
        "metric": "TPR at FPR = 1e-3 (low-FPR regime)",
        "their_result": "\"on average, Nebula outperforms state-of-the-art models at low false positive rates, with a peak of 12% improvement.\"",
        "baseline_result": null
      },
      {
        "method_name": "JSONGrinder (HMIL + MLP)",
        "paper_reference": "Bosansky et al. [13]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "CurParamer (CNN + LSTM with API labeling/embedding)",
        "paper_reference": "Chen et al. [4]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "True Positive Rate (TPR) at fixed False Positive Rate (FPR)",
      "Low-FPR operating point (FPR = 1e-3)"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can a self-attention Transformer (Nebula) generalize across heterogeneous dynamic behavioral representations (API, file, network, registry) to improve malware detection and family classification?",
        "Which components of the dynamic-analysis pipeline (tokenization, filtering, normalization, encoding) can be used as-is versus requiring domain-specific knowledge?",
        "Does self-supervised pretraining reduce labeled data needs while maintaining detection performance?",
        "Can attention mechanisms provide interpretable focus on tokens correlated with malicious behaviors/families?"
      ],
      "gaps_identified": [
        "Prior work focuses on homogeneous inputs (mostly API calls), neglecting heterogeneous behavioral signals (network, file, registry).",
        "CNNs capture only local patterns; LSTMs struggle with long-range dependencies in lengthy behavioral sequences.",
        "Lack of reproducibility: source code, data, and pre-trained models often unavailable in prior work.",
        "Vocabulary explosion and low epistemic density in machine logs without domain-informed normalization and field filtering.",
        "Limited explainability in prior dynamic models regarding what behaviors drive decisions."
      ],
      "limitations": [
        "Evaluation is scoped to offline sandbox/emulation reports; real-time EDR/AV settings are excluded as \"unfair\" to compare.",
        "Domain knowledge is needed for effective normalization (placeholders for IPs, hashes, domains, paths), which may require ongoing curation across environments.",
        "Concrete runtime/inference speed and on-host resource footprint are not reported in the provided text.",
        "Cross-sandbox/domain generalization is discussed conceptually and via datasets, but deployment in live enterprise environments is not evaluated in the provided text."
      ],
      "future_work": [],
      "motivation": "Improve dynamic malware analysis by leveraging self-attention to fuse heterogeneous behavioral signals, enhance low-FPR performance, reduce labeled data needs via self-supervised pretraining, and increase reproducibility and interpretability.",
      "potential_research_ideas": [
        "Domain-adaptive pretraining across multiple sandboxes (contrastive or masked-token objectives) to enhance cross-platform generalization.",
        "Hierarchical multi-field modeling: per-field encoders (API/file/network/registry) with cross-attention fusion to better exploit modality-specific structure.",
        "Temporal and causal modeling of event sequences (time-aware positional encodings or Hawkes/transformer hybrids) to capture timing semantics.",
        "Adversarially robust training against behavior obfuscation and sandbox evasion (e.g., adversarial augmentations of arguments/paths/domains).",
        "Online/streaming transformer variants for near-real-time behavioral detection in EDR settings with early-exit mechanisms.",
        "Graph-augmented modeling (process–file–network–registry interaction graphs) integrated with GNN-transformer hybrids.",
        "Automated domain-informed normalization via learned token mappers that recognize and canonicalize entities (IPs, hashes, domains, paths)."
      ],
      "architectural_improvement_recommendations": [
        "Introduce token-type and field-specific embeddings with multi-branch encoders and cross-attention fusion layers.",
        "Adopt efficient long-sequence transformers (e.g., Performer/Longformer/FlashAttention) to scale to very long logs.",
        "Use time-delta embeddings and event-bucketing to encode temporal dynamics beyond standard positional encodings.",
        "Leverage masked language modeling or span-masking pretraining tailored to behavior fields (API names/args, paths, domains).",
        "Add contrastive objectives to align semantically similar behaviors across different sandboxes (domain adaptation).",
        "Incorporate calibration layers and cost-sensitive training explicitly targeting ultra-low FPR operation.",
        "Integrate explanation-consistency regularization (attention supervision or attribution alignment) to stabilize interpretability."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/dtrizna/nebula",
      "frameworks": [],
      "reproducibility_score": "high",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Offline sandbox/emulation dynamic analysis pipeline",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Maintaining ultra-low FPR (e.g., 1e-3) while preserving high TPR for deployment-grade use.",
        "Handling heterogeneous, high-volume logs and vocabulary explosion without overfitting.",
        "Need for domain-specific normalization rules across varying environments/sandboxes.",
        "Portability and cross-sandbox/domain shift issues.",
        "Operational constraints for near-real-time EDR deployment not evaluated in the provided text."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Nebula: a Transformer-based architecture for dynamic malware detection and classification from heterogeneous behavioral logs.",
      "Domain-informed data cleaning and normalization (field filtering and placeholder mapping) to control vocabulary and improve generalization.",
      "Comprehensive ablation study on tokenization, filtering, normalization, and encoding components.",
      "Extensive experiments on malware detection and classification across datasets from multiple dynamic analysis platforms; best average performance at low FPR with a \"peak of 12% improvement.\"",
      "Self-supervised pretraining that matches fully supervised performance using only 20% of labeled training data.",
      "Explainable AI analysis showing attention focuses on tokens correlated with malicious activities and families.",
      "Open-sourcing code, models, and re-implementations of previously closed-source methods to foster reproducibility."
    ]
  },
  {
    "arxiv_id": "2308.11834v1",
    "title": "Performance Comparison and Implementation of Bayesian Variants for Network Intrusion Detection",
    "authors": "Tosin Ige; Christopher Kiekintveld",
    "abstract": "Bayesian classifiers perform well when each of the features is completely independent of the other which is not always valid in real world application. The aim of this study is to implement and compare the performances of each variant of Bayesian classifier (Multinomial, Bernoulli, and Gaussian) on anomaly detection in network intrusion, and to investigate whether there is any association between each variant assumption and their performance. Our investigation showed that each variant of Bayesian algorithm blindly follows its assumption regardless of feature property, and that the assumption is the single most important factor that influences their accuracy. Experimental results show that Bernoulli has accuracy of 69.9% test (71% train), Multinomial has accuracy of 31.2% test (31.2% train), while Gaussian has accuracy of 81.69% test (82.84% train). Going deeper, we investigated and found that each Naive Bayes variants performances and accuracy is largely due to each classifier assumption, Gaussian classifier performed best on anomaly detection due to its assumption that features follow normal distributions which are continuous, while multinomial classifier have a dismal performance as it simply assumes discreet and multinomial distribution.",
    "published_date": "2023-08-22",
    "pdf_link": "https://arxiv.org/pdf/2308.11834v1",
    "paper_types": [
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Anomaly-based network intrusion detection using Naive Bayes variants",
      "attack_types": [
        "DoS slowloris",
        "DoS Slowhttptest",
        "DoS Hulk",
        "DoS GoldenEye",
        "Heartbleed",
        "BENIGN"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Naive Bayes",
        "specific": "Gaussian Naive Bayes",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Naive Bayes",
        "specific": "Bernoulli Naive Bayes",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Naive Bayes",
        "specific": "Multinomial Naive Bayes",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Feature Selection",
        "specific": "Chi-square test (supervised)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "KDD dataset (from Kaggle)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Bernoulli Naive Bayes",
        "paper_reference": null,
        "metric": "Test accuracy",
        "their_result": "81.69% (Gaussian NB)",
        "baseline_result": "69.9% (Bernoulli NB)"
      },
      {
        "method_name": "Multinomial Naive Bayes",
        "paper_reference": null,
        "metric": "Test accuracy",
        "their_result": "81.69% (Gaussian NB)",
        "baseline_result": "31.2% (Multinomial NB)"
      }
    ],
    "performance_metrics_used": [
      "Accuracy (test)",
      "Accuracy (train)",
      "Confusion matrix"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Implement and compare the performances of Multinomial, Bernoulli, and Gaussian Naive Bayes on anomaly detection in network intrusion.",
        "Investigate whether there is an association between each variant’s assumption and its performance."
      ],
      "gaps_identified": [
        "Existing comparisons of Bayesian classifiers often ignore that each variant operates under different assumptions which affect performance depending on the task/dataset."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Understand how each Naive Bayes variant performs on intrusion datasets and why some intrusions are missed when an inappropriate variant is adopted.",
      "potential_research_ideas": [
        "Evaluate Naive Bayes variants across multiple standard IDS datasets (e.g., NSL-KDD, UNSW-NB15, CIC-IDS2017) to validate generality.",
        "Analyze per-attack-class performance (precision/recall/F1) and class imbalance effects; incorporate resampling or cost-sensitive learning.",
        "Investigate semi-naive extensions (e.g., Tree-Augmented Naive Bayes, feature-dependency modeling) to relax independence assumptions.",
        "Explore distribution-matching preprocessing (e.g., quantile/Box-Cox transforms, discretization strategies) tailored to each variant’s assumptions.",
        "Combine Naive Bayes with ensemble or stacking frameworks for IDS (e.g., NB as generative base learner with discriminative meta-learner).",
        "Assess robustness to concept drift in streaming network traffic and perform online/ incremental NB updating.",
        "Evaluate robustness to adversarial evasion attacks against IDS and develop defenses (e.g., adversarial training, randomized smoothing)."
      ],
      "architectural_improvement_recommendations": [
        "Apply appropriate feature transformations per variant: Gaussianization for GNB; frequency-based count features for Multinomial NB; calibrated binarization thresholds for Bernoulli NB.",
        "Use Laplace/additive smoothing and hyperparameter tuning for MNB/BNB; evaluate different binarization schemes.",
        "Adopt dependency-aware Naive Bayes (e.g., TAN, AODE) to capture limited feature correlations.",
        "Perform rigorous cross-validation, stratification by attack class, and report precision/recall/F1/AUC alongside accuracy.",
        "Incorporate class balancing (SMOTE/undersampling) or class weights to mitigate skew across attack types."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Empirically shows that Gaussian Naive Bayes achieved highest accuracy on the used intrusion dataset: \"Gaussian has accuracy of 81.69% test (82.84% train).\"",
      "Finds that each Naive Bayes variant’s core distributional assumption dominates its performance; Gaussian performs best due to continuous, approximately normal feature/label distributions; Multinomial performs poorly under discrete/multinomial assumption; Bernoulli is intermediate.",
      "Implements a preprocessing and supervised Chi-square feature selection pipeline and compares three Naive Bayes variants on a Kaggle-provided KDD dataset."
    ]
  },
  {
    "arxiv_id": "2308.12086v2",
    "title": "Out of the Cage: How Stochastic Parrots Win in Cyber Security Environments",
    "authors": "Maria Rigaki; Ondřej Lukáš; Carlos A. Catania; Sebastian Garcia",
    "abstract": "Large Language Models (LLMs) have gained widespread popularity across diverse domains involving text generation, summarization, and various natural language processing tasks. Despite their inherent limitations, LLM-based designs have shown promising capabilities in planning and navigating open-world scenarios. This paper introduces a novel application of pre-trained LLMs as agents within cybersecurity network environments, focusing on their utility for sequential decision-making processes.   We present an approach wherein pre-trained LLMs are leveraged as attacking agents in two reinforcement learning environments. Our proposed agents demonstrate similar or better performance against state-of-the-art agents trained for thousands of episodes in most scenarios and configurations. In addition, the best LLM agents perform similarly to human testers of the environment without any additional training process. This design highlights the potential of LLMs to efficiently address complex decision-making tasks within cybersecurity.   Furthermore, we introduce a new network security environment named NetSecGame. The environment is designed to eventually support complex multi-agent scenarios within the network security domain. The proposed environment mimics real network attacks and is designed to be highly modular and adaptable for various scenarios.",
    "published_date": "2023-08-23",
    "pdf_link": "https://arxiv.org/pdf/2308.12086v2",
    "paper_types": [
      "benchmark",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Attack Simulation / Penetration Testing Automation",
      "specific_problem": "Using pre-trained LLMs as autonomous attacking agents to plan and execute multi-step network intrusions and data exfiltration in simulated enterprise networks with/without a defender",
      "attack_types": [
        "Network scanning/reconnaissance",
        "Service exploitation",
        "Lateral movement/control of hosts",
        "Data discovery",
        "Data exfiltration",
        "Evasion of defender/detection avoidance"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer LLM (prompted agent)",
        "specific": "GPT-4",
        "novel_contribution": "Design and use of a pre-trained LLM as a sequential decision-making attacker agent in security RL environments (no fine-tuning), achieving “similar or better performance against state-of-the-art agents trained for thousands of episodes” and comparable to humans in most scenarios"
      },
      {
        "type": "primary",
        "category": "Transformer LLM (prompted agent)",
        "specific": "GPT-3.5-turbo",
        "novel_contribution": "Evaluated as a pre-trained LLM attacker agent; found to underperform GPT-4 and be less stable"
      },
      {
        "type": "baseline",
        "category": "Tabular RL",
        "specific": "Q-learning",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Reinforcement Learning (environment formalism and RL baselines)",
      "Zero-shot / In-context prompting (LLM agents)"
    ],
    "datasets": [
      {
        "name": "NetSecGame",
        "type": "public",
        "domain": "simulated_security_environment",
        "link": "https://github.com/stratosphereips/NetSecGame",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "Microsoft CyberBattleSim",
        "type": "public",
        "domain": "simulated_security_environment",
        "link": "https://github.com/microsoft/CyberBattleSim",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Q-learning (NetSecGame baseline)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "RL agents provided/used with CyberBattleSim",
        "paper_reference": null,
        "metric": null,
        "their_result": "“similar or better performance against state-of-the-art agents trained for thousands of episodes in most scenarios and configurations.”",
        "baseline_result": null
      },
      {
        "method_name": "Human testers",
        "paper_reference": null,
        "metric": "win rate / task success",
        "their_result": "“the best LLM agents perform similarly to human testers of the environment without any additional training process.”",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "win rate",
      "episode success (goal reached without detection)",
      "qualitative stability across runs",
      "(implied) cumulative reward"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can pre-trained LLMs, without additional training, act as effective attacker agents for sequential decision-making in network security environments?",
        "Do LLM agents match or outperform RL agents trained for thousands of episodes in security games?",
        "How do different pre-trained LLMs (e.g., GPT-4 vs GPT-3.5-turbo) compare as attacker agents?",
        "Can an LLM attacker perform comparably to human testers?"
      ],
      "gaps_identified": [
        "Existing security RL environments vary widely in goals, rewards, and defender modeling with limited justification, hindering realism and comparability.",
        "Many environments expose full action spaces or privileged information to agents, unlike real attacks.",
        "Vectorized state/action designs (e.g., adjacency matrices) do not scale well to enterprise networks.",
        "Few environments include realistic defenders; attacker goals often unrealistic (e.g., ‘control half the network’)."
      ],
      "limitations": [
        "Pre-trained LLMs can struggle with long-term planning and may hallucinate actions (noted as general LLM limitations).",
        "NetSecGame is not fully Gym-compatible because actions/parameters are not enumerated to the agent.",
        "Defender model in NetSecGame is omnipresent and stochastic with thresholds (simplified, not a learning blue agent).",
        "Results are from simulated environments; no real-network execution is reported.",
        "Reliance on proprietary LLM APIs (costs, nondeterminism)."
      ],
      "future_work": [
        "Extend NetSecGame to complex multi-agent settings (offense and defense).",
        "Run agents in real networks in the future using NetSecGame’s design intent.",
        "Explore richer defenders (learning-based blue agents) and varied detection models.",
        "Investigate more advanced prompting/planning frameworks (ReAct, Reflexion, DEPS) for improved long-horizon planning.",
        "Systematically evaluate more LLMs and tool-augmented agents."
      ],
      "motivation": "Assess whether LLMs can serve as effective planning agents for cyber attacks, potentially outperforming RL agents and approaching human performance, and provide a more realistic, modular environment (NetSecGame) to study such interactions.",
      "potential_research_ideas": [
        "Multi-agent red-vs-blue with both sides powered by LLM+RL hybrids; study emergent strategies and equilibria.",
        "Tool-augmented LLM attacker that executes real scanners/exploit scripts in a sandbox via tool-use APIs; measure end-to-end success.",
        "Graph-aware LLM planning with a learned GNN world model of the network to reduce hallucinations and improve long-horizon reasoning.",
        "Memory-augmented attacker (episodic + semantic retrieval) using Reflexion-style self-critique across episodes to improve sample efficiency.",
        "Curriculum learning in NetSecGame: progressively harder topologies, variable defender policies, randomized goals.",
        "Distill the LLM policy into a smaller supervised/RL policy for cost and latency reduction (policy distillation).",
        "Uncertainty-aware POMDP formulation with Bayesian belief updates combined with LLM planning for partial observability.",
        "Benchmarking suite additions: standardized scenarios, metrics, seeds, and blue-team variants for reproducible comparisons across works."
      ],
      "architectural_improvement_recommendations": [
        "Adopt ReAct/Reflexion/DEPS prompting with explicit tool-use and self-critique loops to handle long-horizon tasks.",
        "Introduce structured memory (short-term and long-term) with retrieval augmentation from prior episodes.",
        "Integrate a graph-structured state representation and a GNN-based critic to guide LLM action selection.",
        "Use Monte Carlo Tree Search or lookahead planning guided by the LLM for better exploration/exploitation balance.",
        "Add safety guards and action validation layers to prevent invalid actions and reduce hallucination-induced loops.",
        "Employ reward-modeling and offline RL to fine-tune a lightweight policy from successful LLM trajectories (LLM-to-RL distillation)."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/stratosphereips/NetSecGame",
      "frameworks": [
        "Python",
        "OpenAI API (GPT-3.5/4)",
        "CYST simulation engine"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "No GPU required for LLM agent (API-based). RL baselines are lightweight (e.g., Q-learning). Requires API access to GPT-3.5/4; stochastic runs due to environment randomization and LLM nondeterminism."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Simulated enterprise network environments (NetSecGame; CyberBattleSim)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Bridging from simulated actions to real tool execution in live networks.",
        "Cost and rate limits of LLM APIs for large-scale evaluations.",
        "Nondeterminism and stability of LLM outputs across runs and versions.",
        "Simplified defender modeling vs. complex real SOC behaviors.",
        "Action validation and safety when moving to real infrastructure."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Design and evaluation of pre-trained LLM attacker agents for cybersecurity sequential decision-making in RL environments.",
      "Introduction of NetSecGame: a new modular, realistic network security environment with a stochastic defender and data-exfiltration goal.",
      "Empirical comparison across two environments (NetSecGame and Microsoft CyberBattleSim), showing “similar or better performance against state-of-the-art agents” and LLM agents performing similarly to human testers.",
      "Analysis of LLM choice: “GPT-4 outperforms GPT-3.5-turbo significantly and at the same time exhibits high stability.”",
      "Environment design choices closer to real attacks: hidden action parameters, realistic goal (data exfiltration), defender, and generic rewards."
    ]
  },
  {
    "arxiv_id": "2310.05953v2",
    "title": "Classification of Spam URLs Using Machine Learning Approaches",
    "authors": "Omar Husni Odeh; Anas Arram; Murad Njoum",
    "abstract": "The Internet is used by billions of users every day because it offers fast and free communication tools and platforms. Nevertheless, with this significant increase in usage, huge amounts of spam are generated every second, which wastes internet resources and, more importantly, users' time. This study investigates the use of machine learning models to classify URLs as spam or nonspam. We first extract the features from the URL as it has only one feature, and then we compare the performance of several models, including k nearest neighbors, bagging, random forest, logistic regression, and others. Experimental results demonstrate that bagging outperformed other models and achieved the highest accuracy of 98.64%. In addition, bagging outperformed the current state-of-the-art approaches which emphasize its effectiveness in addressing spam-related challenges on the Internet. This suggests that bagging is a promising approach for URL spam classification.",
    "published_date": "2023-09-10",
    "pdf_link": "https://arxiv.org/pdf/2310.05953v2",
    "paper_types": [
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Web Security",
      "subdomain": "Spam Detection",
      "specific_problem": "URL spam classification (spam vs. non-spam)",
      "attack_types": [
        "spam"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Ensemble (Bagging)",
        "specific": "scikit-learn BaggingClassifier",
        "novel_contribution": "Empirically shows Bagging achieves 98.64% accuracy on the Kaggle URL spam dataset and outperforms previously reported results."
      },
      {
        "type": "baseline",
        "category": "Ensemble (Random Forest)",
        "specific": "RandomForestClassifier",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Ensemble (Stacking)",
        "specific": "scikit-learn StackingClassifier",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Instance-based",
        "specific": "K-Nearest Neighbors (KNN)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Boosting",
        "specific": "AdaBoostClassifier",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Gradient Boosting Decision Trees",
        "specific": "GradientBoostingClassifier",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "GBDT (LightGBM)",
        "specific": "LightGBM",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Decision Tree",
        "specific": "DecisionTreeClassifier",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Linear Model",
        "specific": "Logistic Regression",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Neural Network",
        "specific": "MLPClassifier (feed-forward neural network)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Naive Bayes",
        "specific": "MultinomialNB",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Naive Bayes",
        "specific": "BernoulliNB",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Spam URLs Classification Dataset (Kaggle)",
        "type": "public",
        "domain": "urls",
        "link": "https://www.kaggle.com/shivamb/spam-url-prediction",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "KNeighborsClassifier",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "98.64%",
        "baseline_result": "97.63%"
      },
      {
        "method_name": "RandomForestClassifier",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "98.64%",
        "baseline_result": "97.55%"
      },
      {
        "method_name": "GradientBoostingClassifier",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "98.64%",
        "baseline_result": "97.60%"
      },
      {
        "method_name": "StackingClassifier",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "98.64%",
        "baseline_result": "98.45%"
      },
      {
        "method_name": "LightGBM",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "98.64%",
        "baseline_result": "96.88%"
      },
      {
        "method_name": "DecisionTreeClassifier",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "98.64%",
        "baseline_result": "94.66%"
      },
      {
        "method_name": "KNN, RF, Stacking (AUC)",
        "paper_reference": null,
        "metric": "ROC-AUC",
        "their_result": "Bagging AUC=1 (perfect) (implied alongside RF and Stacking)",
        "baseline_result": "Random Forest AUC=1; Stacking AUC=1"
      },
      {
        "method_name": "LogisticRegression",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "98.64%",
        "baseline_result": "82.89%"
      },
      {
        "method_name": "MLPClassifier",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "98.64%",
        "baseline_result": "87.04%"
      },
      {
        "method_name": "AdaBoostClassifier",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "98.64%",
        "baseline_result": "84.40%"
      },
      {
        "method_name": "MultinomialNB",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "98.64%",
        "baseline_result": "73.05%"
      },
      {
        "method_name": "BernoulliNB",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "98.64%",
        "baseline_result": "77.75%"
      },
      {
        "method_name": "DistilBERT (external SOTA)",
        "paper_reference": "[31]",
        "metric": "Accuracy (10-fold)",
        "their_result": "98.64%",
        "baseline_result": "95.02%"
      },
      {
        "method_name": "Random Forest (external SOTA)",
        "paper_reference": "[32]",
        "metric": "Accuracy (10-fold)",
        "their_result": "98.64%",
        "baseline_result": "97.39%"
      },
      {
        "method_name": "Random Forest (external SOTA)",
        "paper_reference": "[33]",
        "metric": "Accuracy (10-fold)",
        "their_result": "98.64%",
        "baseline_result": "93.77%"
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "10-fold cross-validation score",
      "Precision",
      "Recall",
      "F1 score",
      "ROC-AUC",
      "Confusion matrix",
      "R2"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can machine learning models effectively classify URLs as spam or non-spam using only features extracted from the URL string?",
        "Which machine learning algorithm achieves the highest performance for URL spam classification on the given dataset?"
      ],
      "gaps_identified": [
        "Traditional blacklisting is insufficient due to the rapid increase and evolution of malicious/spam URLs.",
        "Enumerating all ranking elements for spam detection is infeasible, risking omission of important features.",
        "Prior work often relies on page content and ranking-related features; URL-only approaches need systematic evaluation."
      ],
      "limitations": [
        "Experiments use only URL-derived features; no webpage content, script-level, or body features were used.",
        "Evaluation is on a single dataset (newsletter-derived URLs), which may limit generalizability.",
        "No analysis of adversarial robustness or concept drift.",
        "No runtime, resource usage, or deployment evaluation is provided.",
        "No code or detailed hyperparameters are released for full reproducibility."
      ],
      "future_work": [
        "Train on more models including deep learning models.",
        "Extract more features from the website of the URL itself such as the body size of the web page and features related to the script used."
      ],
      "motivation": "Investigate machine learning techniques for classifying URLs as spam or non-spam and identify the best-performing model using features extracted from the URL string.",
      "potential_research_ideas": [
        "Combine lexical URL features with host-based (WHOIS, ASN, DNS), temporal, and popularity signals to improve generalization.",
        "Design adversarially robust URL classifiers resilient to obfuscations (e.g., homograph, percent-encoding, subdomain padding).",
        "Develop self-supervised or contrastive pretraining for URL strings and fine-tune for spam detection.",
        "Online/streaming learning with concept-drift detection for evolving spam campaigns from newsletters and social media.",
        "Cross-dataset and cross-distribution evaluation (newsletters vs. email vs. social media vs. web crawl) with domain adaptation.",
        "Graph-based modeling of domains/links (GNNs over host graphs) to capture campaign structure behind spam URLs.",
        "Active learning with human-in-the-loop labeling to reduce false positives in production filters.",
        "Cost-sensitive and calibrated classification to meet operational precision/recall tradeoffs under imbalance."
      ],
      "architectural_improvement_recommendations": [
        "Build a heterogeneous stacking ensemble that mixes tree ensembles (RF/GBDT), linear models, and neural models with calibration (Platt/Isotonic).",
        "Adopt character-level CNN/Transformer models for raw URLs and compare with boosted trees on engineered features.",
        "Engineer richer features: tokenized domain/path/query n-grams, TLD/entropy profiles, WHOIS age, DNS MX/NS, TLS cert features, URL shortening expansion.",
        "Apply robust training: adversarial perturbations on URL strings, data augmentation (insert/delete/replace tokens).",
        "Use nested cross-validation and report PR-AUC in addition to ROC-AUC for imbalanced settings; tune classification thresholds for cost-sensitive metrics.",
        "Perform feature selection/importance analysis (e.g., SHAP for tree ensembles) and deploy post-hoc calibration (temperature/Isotonic)."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "scikit-learn",
        "LightGBM"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Feature engineering from raw URL strings into multiple lexical features (e.g., length, digits count, HTTPS flag, words count, entropy, parameters, fragments, subdomains, presence of subscribe, etc.).",
      "Comprehensive empirical comparison of 12 classifiers with random-search hyperparameter tuning and 10-fold cross-validation.",
      "BaggingClassifier achieved the highest accuracy of 98.64% and outperformed reported state-of-the-art results.",
      "Provided analysis via ROC–AUC, confusion matrices, and standard classification metrics (precision, recall, F1)."
    ]
  },
  {
    "arxiv_id": "2309.07461v2",
    "title": "Detecting Unknown Attacks in IoT Environments: An Open Set Classifier for Enhanced Network Intrusion Detection",
    "authors": "Yasir Ali Farrukh; Syed Wali; Irfan Khan; Nathaniel D. Bastian",
    "abstract": "The widespread integration of Internet of Things (IoT) devices across all facets of life has ushered in an era of interconnectedness, creating new avenues for cybersecurity challenges and underscoring the need for robust intrusion detection systems. However, traditional security systems are designed with a closed-world perspective and often face challenges in dealing with the ever-evolving threat landscape, where new and unfamiliar attacks are constantly emerging. In this paper, we introduce a framework aimed at mitigating the open set recognition (OSR) problem in the realm of Network Intrusion Detection Systems (NIDS) tailored for IoT environments. Our framework capitalizes on image-based representations of packet-level data, extracting spatial and temporal patterns from network traffic. Additionally, we integrate stacking and sub-clustering techniques, enabling the identification of unknown attacks by effectively modeling the complex and diverse nature of benign behavior. The empirical results prominently underscore the framework's efficacy, boasting an impressive 88\\% detection rate for previously unseen attacks when compared against existing approaches and recent advancements. Future work will perform extensive experimentation across various openness levels and attack scenarios, further strengthening the adaptability and performance of our proposed solution in safeguarding IoT environments.",
    "published_date": "2023-09-14",
    "pdf_link": "https://arxiv.org/pdf/2309.07461v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Open Set Recognition for IoT NIDS using packet-level image representations to detect unknown/zero-day attacks",
      "attack_types": [
        "DoS Hulk",
        "DoS Slowloris",
        "DoS Slowhttptest",
        "Web Attack – SQL Injection",
        "Bot"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "CNN",
        "specific": "Deep concatenated CNN base learners (binary: cluster vs. rest) on packet-image inputs",
        "novel_contribution": "Stacked ensemble of N CNN base learners trained on K-means sub-clusters of benign traffic to model diverse benign prototypes for OSR"
      },
      {
        "type": "primary",
        "category": "Clustering",
        "specific": "K-means (N=7 on t-SNE 2D embedding)",
        "novel_contribution": "Sub-clusters benign traffic to create multiple benign prototypes used to train base learners"
      },
      {
        "type": "primary",
        "category": "Dimensionality Reduction",
        "specific": "t-SNE",
        "novel_contribution": "Used to project serialized RGB packet images to 2D for visualization and as the space for K-means clustering"
      },
      {
        "type": "primary",
        "category": "Ensemble",
        "specific": "Stacking (meta-learning) with majority voting",
        "novel_contribution": "Combines N base-learner probabilities as meta-features; uses an ensemble of meta-classifiers with voting for final OSR decision"
      },
      {
        "type": "primary",
        "category": "Tree Ensemble",
        "specific": "Random Forest (meta-classifier)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Linear Model",
        "specific": "Logistic Regression (meta-classifier)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Gradient Boosting",
        "specific": "XGBoost (meta-classifier)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Gradient Boosting",
        "specific": "LightGBM (meta-classifier)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Outlier Detection",
        "specific": "ECOD",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Deep Neural Network",
        "specific": "PReNet (PreNet)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "SVM",
        "specific": "W-SVM (CAP model with EVT calibration)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Autoencoder Ensemble",
        "specific": "Auto-encoder Ensemble",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Survival Analysis",
        "specific": "Survival analysis-based method",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Deep Semi-Supervised",
        "specific": "DeepSAD",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised",
      "Meta-learning",
      "Ensemble Learning"
    ],
    "datasets": [
      {
        "name": "CIC-IDS2017 (packet-based PCAPs, converted to serialized RGB packet images; labeled via Payload-Byte tool)",
        "type": "public",
        "domain": "network_traffic",
        "link": "https://www.unb.ca/cic/datasets/ids-2017.html",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "ECOD",
        "paper_reference": "Li et al. (ECOD) [15]",
        "metric": "Specificity (benign detection rate)",
        "their_result": "“our approach outperforms ECOD by 37%” (specificity)",
        "baseline_result": null
      },
      {
        "method_name": "PReNet (PreNet)",
        "paper_reference": "Pang et al. (PreNet) [14]",
        "metric": "Sensitivity/Recall (unknown attack detection rate)",
        "their_result": "“leads by 17% in sensitivity” over PReNet",
        "baseline_result": null
      },
      {
        "method_name": "Overall unknown attack detection",
        "paper_reference": null,
        "metric": "Sensitivity/Recall (unknown attack detection rate)",
        "their_result": "“boasting an impressive 88% detection rate for previously unseen attacks”",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Sensitivity (Recall) for unknown attack detection",
      "Specificity (benign detection rate)"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Most ML-based NIDS operate in closed-world settings and fail to detect previously unseen attacks in open-set scenarios.",
        "Flow-based data lacks actual packet payloads, limiting detection of payload-dependent attacks.",
        "Benign traffic is more dispersed and overlaps with attack distributions, making discrimination from unknown attacks difficult."
      ],
      "limitations": [
        "Evaluation limited to a single dataset (CIC-IDS2017) and one openness configuration with five unseen attack classes.",
        "Paper serves as a foundational study; lacks comprehensive experimentation across varying openness levels and broader attack scenarios.",
        "No runtime, resource, or deployment evaluations provided."
      ],
      "future_work": [
        "“perform extensive experimentation across various openness levels and attack scenarios”",
        "Further validation and fine-tuning of the framework’s adaptability and performance in IoT environments."
      ],
      "motivation": "IoT proliferation increases attack surface; closed-world NIDS are ineffective for evolving threats. Need robust open-set classifiers that can detect unknown/zero-day attacks using richer packet-level information.",
      "potential_research_ideas": [
        "Evaluate the framework across multiple IoT/NIDS datasets (e.g., BoT-IoT, TON_IoT, CIC-IoT 2023) and real traffic to assess generalization.",
        "Replace t-SNE-based clustering with learned embeddings from the CNN (or an autoencoder) and apply deep clustering to define benign prototypes.",
        "Incorporate self-supervised pretraining on raw packet bytes (contrastive or masked modeling) to improve representations for OSR.",
        "Fuse packet-level and flow-level features in a multi-view architecture and study their complementarity for OSR.",
        "Adopt EVT-based calibration (e.g., OpenMax/CROSR-like tails) or energy-based OOD scoring on meta-features.",
        "Explore sequence models (1D CNNs/Transformers) over ordered packet bytes or short packet sequences to capture temporal patterns.",
        "Study threshold calibration and cost-sensitive optimization to jointly maximize sensitivity and specificity under varying openness.",
        "Assess robustness to adversarial perturbations in packet payloads and apply adversarial training or certified defenses."
      ],
      "architectural_improvement_recommendations": [
        "Learn the benign sub-clusters in the latent space of the CNN (end-to-end deep clustering) instead of K-means on t-SNE projections.",
        "Use a stronger image/byte backbone (e.g., ResNet/ConvNeXt for images or Byte-level Transformers) for base learners.",
        "Add a dedicated OOD head using energy scores or Mahalanobis distance on penultimate features to complement the meta-classifier.",
        "Calibrate meta-classifier outputs with temperature scaling and conformal prediction for better uncertainty estimation.",
        "Introduce hard-negative mining using known attacks and synthetic anomalies (e.g., byte shuffling) to sharpen the decision boundary.",
        "Replace majority voting with a learned stacking combiner (e.g., logistic meta-learner) with cross-validated base predictions to avoid leakage."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "XGBoost",
        "LightGBM"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes an open-set classification framework for IoT NIDS operating on packet-level data serialized into RGB images.",
      "Introduces sub-clustering of benign traffic (K-means, N=7) to model diverse benign prototypes and train N binary CNN base learners.",
      "Uses stacking: base-learner probabilities become meta-features for multiple meta-classifiers (RF, LR, XGBoost, LightGBM) with voting.",
      "Achieves “88% detection rate for previously unseen attacks” and outperforms ECOD and PReNet on specificity/sensitivity, respectively.",
      "Demonstrates packet-level, image-based representations can capture spatial-temporal patterns beneficial for OSR in NIDS."
    ]
  },
  {
    "arxiv_id": "2309.06643v1",
    "title": "Semi-supervised Classification of Malware Families Under Extreme Class Imbalance via Hierarchical Non-Negative Matrix Factorization with Automatic Model Selection",
    "authors": "Maksim E. Eren; Manish Bhattarai; Robert J. Joyce; Edward Raff; Charles Nicholas; Boian S. Alexandrov",
    "abstract": "Identification of the family to which a malware specimen belongs is essential in understanding the behavior of the malware and developing mitigation strategies. Solutions proposed by prior work, however, are often not practicable due to the lack of realistic evaluation factors. These factors include learning under class imbalance, the ability to identify new malware, and the cost of production-quality labeled data. In practice, deployed models face prominent, rare, and new malware families. At the same time, obtaining a large quantity of up-to-date labeled malware for training a model can be expensive. In this paper, we address these problems and propose a novel hierarchical semi-supervised algorithm, which we call the HNMFk Classifier, that can be used in the early stages of the malware family labeling process. Our method is based on non-negative matrix factorization with automatic model selection, that is, with an estimation of the number of clusters. With HNMFk Classifier, we exploit the hierarchical structure of the malware data together with a semi-supervised setup, which enables us to classify malware families under conditions of extreme class imbalance. Our solution can perform abstaining predictions, or rejection option, which yields promising results in the identification of novel malware families and helps with maintaining the performance of the model when a low quantity of labeled data is used. We perform bulk classification of nearly 2,900 both rare and prominent malware families, through static analysis, using nearly 388,000 samples from the EMBER-2018 corpus. In our experiments, we surpass both supervised and semi-supervised baseline models with an F1 score of 0.80.",
    "published_date": "2023-09-12",
    "pdf_link": "https://arxiv.org/pdf/2309.06643v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Malware Analysis",
      "subdomain": "Malware Family Classification",
      "specific_problem": "Semi-supervised classification of Windows PE malware families under extreme class imbalance with reject-option for novel family identification",
      "attack_types": [
        "Windows PE malware",
        "Novel/previously unseen malware families"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Matrix Factorization",
        "specific": "Hierarchical Non-Negative Matrix Factorization (HNMF) with NMFk (automatic model selection)",
        "novel_contribution": "HNMFk Classifier: hierarchical semi-supervised classification with automatic estimation of the number of clusters (via NMFk) and abstaining predictions (reject option) to handle extreme class imbalance and novel families"
      },
      {
        "type": "baseline",
        "category": "Gradient Boosted Decision Trees",
        "specific": "XGBoost",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Gradient Boosted Decision Trees",
        "specific": "LightGBM",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Semi-supervised Wrapper",
        "specific": "Self-training (applied to XGBoost/LightGBM)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Neural Network",
        "specific": "Multilayer Perceptron (MLP)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Semi-supervised",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "EMBER-2018",
        "type": "public",
        "domain": "malware_binaries",
        "link": "https://github.com/elastic/ember",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "XGBoost",
        "paper_reference": "Chen and Guestrin, 2016",
        "metric": "F1",
        "their_result": "“In our experiments, we surpass both supervised and semi-supervised baseline models with an F1 score of 0.80.”",
        "baseline_result": null
      },
      {
        "method_name": "LightGBM",
        "paper_reference": "Ke et al., 2017",
        "metric": "F1",
        "their_result": "F1 = 0.80 (HNMFk Classifier)",
        "baseline_result": null
      },
      {
        "method_name": "Self-training + XGBoost",
        "paper_reference": "SelfTrain [73]",
        "metric": "F1",
        "their_result": "F1 = 0.80 (HNMFk Classifier)",
        "baseline_result": null
      },
      {
        "method_name": "Self-training + LightGBM",
        "paper_reference": "SelfTrain [73]",
        "metric": "F1",
        "their_result": "F1 = 0.80 (HNMFk Classifier)",
        "baseline_result": null
      },
      {
        "method_name": "Multilayer Perceptron (MLP)",
        "paper_reference": "Goodfellow et al. (general reference) / [33] in paper",
        "metric": "F1",
        "their_result": "F1 = 0.80 (HNMFk Classifier)",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "F1"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How to classify malware families under extreme class imbalance while identifying novel families?",
        "Can a semi-supervised hierarchical NMF with automatic model selection provide robust classification with limited labeled data?",
        "Can abstaining (reject-option) predictions help maintain performance and enable detection of novel families?"
      ],
      "gaps_identified": [
        "Prior work often ignores realistic evaluation factors: severe class imbalance, presence of novel malware, and scarcity/cost of labeled data.",
        "Most studies limit to top/populous families or collapse rare families into an 'other' class, inflating apparent performance and reducing real-world utility.",
        "Semi-supervised learning for malware family classification is underexplored despite potential benefits with limited labeled data.",
        "Supervised methods generalize poorly to rare families."
      ],
      "limitations": [
        "Designed for early-stage labeling and bulk classification; not intended for real-time deployment.",
        "Focuses on Windows PE malware and static analysis features (metadata and PE headers) only.",
        "Assumes inputs are malware (does not perform benign vs. malware detection).",
        "Results reported on EMBER-2018; cross-dataset generalization not demonstrated in provided text."
      ],
      "future_work": [],
      "motivation": "Improve practicality of malware family classification by addressing class imbalance, novel family discovery, and limited labeled data through a semi-supervised, hierarchical NMF-based approach with reject option.",
      "potential_research_ideas": [
        "Extend HNMFk to incorporate dynamic behavior features and hybrid static-dynamic representations for improved family discrimination.",
        "Develop open-set calibration methods (e.g., conformal prediction) to control abstention rates and error trade-offs per deployment constraints.",
        "Investigate incremental/online hierarchical NMF with automatic rank selection to handle stream data and concept drift.",
        "Combine HNMFk embeddings with self-supervised pretraining (e.g., contrastive learning on PE representations) before hierarchical factorization.",
        "Evaluate cross-dataset generalization and domain adaptation from EMBER-2018 to newer corpora; build robustness to vendor label noise.",
        "Integrate AVClass/label harmonization pipelines to standardize family names and improve ground-truth quality.",
        "Adversarially robust variants of NMF or robust statistics to mitigate packing/obfuscation and evasion tactics.",
        "Graph-augmented HNMFk using call-graph/import graph features with matrix- or tensor-factorization to capture structural patterns.",
        "GPU-accelerated and distributed NMFk for million-scale corpora with rigorous scalability benchmarks."
      ],
      "architectural_improvement_recommendations": [
        "Incorporate semi-supervised constraints into NMF (e.g., must-link/cannot-link or Laplacian regularization) to use limited labels more directly.",
        "Add a calibrated decision function atop factorization coefficients (H) with threshold learning for reject-option optimization.",
        "Use deep NMF or NMF-initialized autoencoders to capture non-linearities while retaining parts-based interpretability.",
        "Employ class-prior-aware weighting during factorization or cost-sensitive post-processing to better handle extreme imbalance.",
        "Ensemble multiple NMFk runs with diverse perturbation schemes and consensus clustering to improve stability.",
        "Integrate label propagation over sample-similarity graphs induced by H to refine pseudo-labels before final assignment."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Extreme class imbalance across thousands of families",
        "Scarcity and cost of high-quality labeled malware",
        "Presence of novel/previously unseen families in deployment",
        "Label noise and inconsistency in family naming across vendors",
        "Concept drift as malware evolves"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces the HNMFk Classifier: a novel semi-supervised hierarchical bulk classifier for malware family labeling.",
      "Identifies Windows malware families using static features (metadata and PE headers) under extreme class imbalance.",
      "Utilizes abstaining prediction (reject option) to help identify novel families and maintain performance with few labels.",
      "Performs large-scale classification over 2,898 malware families (≈388k samples) from EMBER-2018, surpassing supervised and semi-supervised baselines with an F1 score of 0.80.",
      "First to classify malware families over EMBER-2018 under realistic conditions including rare and novel families, with about 29× more family classes than prior largest-class study."
    ]
  },
  {
    "arxiv_id": "2309.03294v1",
    "title": "MALITE: Lightweight Malware Detection and Classification for Constrained Devices",
    "authors": "Sidharth Anand; Barsha Mitra; Soumyadeep Dey; Abhinav Rao; Rupsa Dhar; Jaideep Vaidya",
    "abstract": "Today, malware is one of the primary cyberthreats to organizations. Malware has pervaded almost every type of computing device including the ones having limited memory, battery and computation power such as mobile phones, tablets and embedded devices like Internet-of-Things (IoT) devices. Consequently, the privacy and security of the malware infected systems and devices have been heavily jeopardized. In recent years, researchers have leveraged machine learning based strategies for malware detection and classification. Malware analysis approaches can only be employed in resource constrained environments if the methods are lightweight in nature. In this paper, we present MALITE, a lightweight malware analysis system, that can classify various malware families and distinguish between benign and malicious binaries. MALITE converts a binary into a gray scale or an RGB image and employs low memory and battery power consuming as well as computationally inexpensive malware analysis strategies. We have designed MALITE-MN, a lightweight neural network based architecture and MALITE-HRF, an ultra lightweight random forest based method that uses histogram features extracted by a sliding window. We evaluate the performance of both on six publicly available datasets (Malimg, Microsoft BIG, Dumpware10, MOTIF, Drebin and CICAndMal2017), and compare them to four state-of-the-art malware classification techniques. The results show that MALITE-MN and MALITE-HRF not only accurately identify and classify malware but also respectively consume several orders of magnitude lower resources (in terms of both memory as well as computation capabilities), making them much more suitable for resource constrained environments.",
    "published_date": "2023-09-06",
    "pdf_link": "https://arxiv.org/pdf/2309.03294v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Malware Analysis",
      "subdomain": "Malware Detection and Classification",
      "specific_problem": "Lightweight malware family classification and benign-vs-malicious detection on resource-constrained devices using image representations of binaries",
      "attack_types": [
        "general malware",
        "Windows malware families (e.g., Gatak, Lollipop, Vundo, Ramnit, Simda, Obfuscator.ACY, Kelihos_ver1, Kelihos_ver3, Tracur)",
        "Android malware (Adware, Ransomware, Scareware, SMS malware)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "CNN (depthwise separable convolutions)",
        "specific": "MobileNetV2-style residual bottleneck blocks (k=3, expansion factor t=6 except first)",
        "novel_contribution": "MALITE-MN: a lightweight CNN tailored for 256×256 byteplot images with low parameter count and computational cost while maintaining accuracy"
      },
      {
        "type": "primary",
        "category": "Random Forest",
        "specific": null,
        "novel_contribution": "MALITE-HRF: ultra-lightweight RF classifier on 1024-d histogram features extracted from 16 overlapping 32×256 patches (64-bin intensity histograms) of 256×256 images; restricted number/depth of trees for low memory/compute"
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Malimg",
        "type": "public",
        "domain": "malware_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Microsoft BIG (Microsoft Malware Classification Challenge)",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Dumpware10",
        "type": "public",
        "domain": "malware_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "MOTIF",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Drebin",
        "type": "public",
        "domain": "android_apps",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CICAndMal2017",
        "type": "public",
        "domain": "android_apps",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "3C2D",
        "paper_reference": "[42]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "DTMIC",
        "paper_reference": "[35]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Wong et al. method",
        "paper_reference": "[61]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "MalConv2",
        "paper_reference": "[50]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "accuracy",
      "precision",
      "model size",
      "parameter count",
      "number of multiply-add operations (computational overhead)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can malware binaries converted to grayscale/RGB images be accurately classified using methods that are lightweight enough for constrained devices?",
        "Can patchwise histogram features with a small Random Forest provide accurate malware family classification at ultra-low compute/memory?",
        "Can a MobileNetV2-style lightweight CNN achieve comparable or better accuracy than heavier SoTA while drastically reducing parameters and operations?"
      ],
      "gaps_identified": [
        "Few existing malware analysis approaches explicitly account for memory, battery, and computational overheads for deployment on constrained devices.",
        "Existing works rarely quantify lightweightness via memory consumed, parameter count, and number of operations performed."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Deploy effective malware detection/classification on resource-constrained devices by minimizing memory and computational cost without sacrificing accuracy.",
      "potential_research_ideas": [
        "Adversarial robustness assessments and defenses for image-based malware classifiers (both RF-histogram and lightweight CNN).",
        "On-device deployment study (Android/IoT) with energy profiling and latency measurements; integrate on-device accelerators.",
        "Quantization-aware training and post-training quantization of MALITE-MN; integer-only inference and microcontroller deployment.",
        "Self-supervised or contrastive pretraining on unlabeled malware bytes/images to improve accuracy with minimal compute.",
        "Adaptive/learned histogram feature extractors (e.g., learnable binning via 1×1 convs) to replace fixed bins in MALITE-HRF.",
        "Multimodal fusion of byteplot images with metadata/API-call features under tight resource budgets.",
        "Open-set and incremental learning for emerging/unknown malware families with lightweight updates.",
        "Hardware-aware neural architecture search constrained by parameter/operation budgets specific to target devices.",
        "Robustness to obfuscation/packing through data augmentation (byte shuffling, section perturbations) and invariance learning.",
        "Federated or on-device continual learning for evolving malware, minimizing privacy leakage and bandwidth."
      ],
      "architectural_improvement_recommendations": [
        "Apply 8-bit or lower precision quantization to MALITE-MN; evaluate latency/energy on ARM CPUs/NPUs.",
        "Knowledge distillation from a larger teacher to MALITE-MN to boost accuracy at same compute.",
        "Replace fixed histogram bins with learnable bins or depthwise separable conv-based feature pooling to keep compute low while improving discriminativeness.",
        "Use dynamic inference (early-exit classifiers) to save compute on easy samples.",
        "Prune RF estimators and constrain tree depth via cost-aware hyperparameter tuning; consider Extremely Randomized Trees for faster inference.",
        "Calibrate outputs (e.g., temperature scaling) and add uncertainty estimation for open-set handling with minimal overhead.",
        "Optimize image preprocessing (byteplot conversion, resizing) with streaming and zero-copy pipelines on-device."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Reported: “MALITE-MN requires between 226 to 2 times lesser computational overhead while being between 375 to 6 times smaller in size than these existing methods… MALITE-HRF … requires between 528611 to 5598 times lesser computational overhead while being between 6761 to 107 times smaller in size …” Specific GPU/CPU not stated."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Intended for mobile phones, tablets, and IoT (resource-constrained) devices",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes MALITE, an image-based lightweight malware analysis framework for benign-vs-malicious and family classification.",
      "Introduces two methods: MALITE-HRF (ultra-lightweight RF on patchwise histogram features) and MALITE-MN (lightweight CNN with residual bottleneck blocks).",
      "Defines efficient byteplot conversion and fixed 256×256 image pipeline; patchwise 32×256 with 50% overlap, 64-bin histograms (1024-d feature).",
      "Evaluates on six public datasets: Malimg, Microsoft BIG, Dumpware10, MOTIF, Drebin, CICAndMal2017.",
      "Compares against four SoTA methods (3C2D, DTMIC, Wong et al., MalConv2) and demonstrates comparable or better accuracy with orders-of-magnitude lower compute/memory.",
      "Quantifies lightweightness via parameter count, multiply-add operations, and model size."
    ]
  },
  {
    "arxiv_id": "2308.10776v1",
    "title": "A Modular and Adaptive System for Business Email Compromise Detection",
    "authors": "Jan Brabec; Filip Šrajer; Radek Starosta; Tomáš Sixta; Marc Dupont; Miloš Lenoch; Jiří Menšík; Florian Becker; Jakub Boros; Tomáš Pop; Pavel Novák",
    "abstract": "The growing sophistication of Business Email Compromise (BEC) and spear phishing attacks poses significant challenges to organizations worldwide. The techniques featured in traditional spam and phishing detection are insufficient due to the tailored nature of modern BEC attacks as they often blend in with the regular benign traffic. Recent advances in machine learning, particularly in Natural Language Understanding (NLU), offer a promising avenue for combating such attacks but in a practical system, due to limitations such as data availability, operational costs, verdict explainability requirements or a need to robustly evolve the system, it is essential to combine multiple approaches together. We present CAPE, a comprehensive and efficient system for BEC detection that has been proven in a production environment for a period of over two years. Rather than being a single model, CAPE is a system that combines independent ML models and algorithms detecting BEC-related behaviors across various email modalities such as text, images, metadata and the email's communication context. This decomposition makes CAPE's verdicts naturally explainable. In the paper, we describe the design principles and constraints behind its architecture, as well as the challenges of model design, evaluation and adapting the system continuously through a Bayesian approach that combines limited data with domain knowledge. Furthermore, we elaborate on several specific behavioral detectors, such as those based on Transformer neural architectures.",
    "published_date": "2023-08-21",
    "pdf_link": "https://arxiv.org/pdf/2308.10776v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Email Security",
      "subdomain": "Business Email Compromise (BEC) and Spear Phishing Detection",
      "specific_problem": "Modular, explainable detection of Business Email Compromise in enterprise email using multi-modal signals and communication context",
      "attack_types": [
        "Business Email Compromise (BEC)",
        "Spear phishing",
        "Impersonation (e.g., CEO fraud, vendor impersonation, attorney impersonation)",
        "Account compromise/takeover",
        "Data theft via social engineering",
        "Wire transfer fraud",
        "Cryptocurrency payment requests",
        "Credential phishing",
        "Open redirect misuse"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Probabilistic/Bayesian Modeling",
        "specific": "Bayesian evidence aggregation and updating",
        "novel_contribution": "Combines weak detector signals with domain priors to maintain high precision under data scarcity; supports continual adaptation of the system."
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "Transformer-based NLU detectors (BERT-like)",
        "novel_contribution": "Behavioral detectors (e.g., text-based signals) using Transformer architectures as part of a modular, explainable system."
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Probabilistic/Bayesian",
      "Hybrid (rules + ML)"
    ],
    "datasets": [
      {
        "name": "Enron Email Dataset",
        "type": "public",
        "domain": "email_corpus",
        "link": "https://www.cs.cmu.edu/~enron/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CAPE Production Email Telemetry (enterprise deployment)",
        "type": "proprietary",
        "domain": "email_corpus",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Indicators of Compromise (IOC) feeds (URLs, attachment hashes)",
        "type": "proprietary",
        "domain": "threat_intel_indicators",
        "link": null,
        "is_new_contribution": false,
        "availability": "available_on_request"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Precision",
      "Recall",
      "False Positive Rate (FPR)",
      "Conviction-rate",
      "Latency/processing time distribution"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Severe data sensitivity limiting research access, storage size, and retention for emails.",
        "Extreme class imbalance (BEC prevalence ~1e-4 to 1e-5) requiring extremely low FPR for high precision.",
        "Lack of representative labeled datasets for BEC; public corpora are outdated or focused on other threats.",
        "Indicators of Compromise (IOC) feeds have nontrivial false positives and are less helpful for BEC, which often lacks such IOCs.",
        "Fully end-to-end ML models risk shortcut learning and poor generalization under limited, non-representative data.",
        "Operational constraints: high throughput, latency bounds, and inference cost limit indiscriminate use of large NLU/LLMs.",
        "Inability to directly measure recall in production; must use proxies (e.g., conviction-rate at fixed precision).",
        "Need for explainable decisions for SOC workflows and investigations."
      ],
      "limitations": [
        "No broad research access to benign traffic; limited and sensitive data hinder comprehensive supervised training.",
        "High precision requirement at very low FPR constrains model choices and thresholds.",
        "Cost envelope prevents running large NLU/LLMs on all emails; necessitates staged pipeline and early exits.",
        "Recall cannot be directly measured in production; reliance on proxy metrics.",
        "Public datasets provide limited utility for BEC; domain shift from production traffic is significant."
      ],
      "future_work": [],
      "motivation": "Design an explainable, modular, and cost-aware BEC detection system that integrates multi-modal signals and context, and adapts over time via a Bayesian approach under severe data scarcity, privacy, and operational constraints.",
      "potential_research_ideas": [
        "Privacy-preserving collaboration across organizations (e.g., federated learning or secure aggregation) to mitigate data scarcity while respecting data sensitivity.",
        "Self-supervised and contrastive pretraining on unlabeled enterprise email to improve behavioral detectors with minimal labels.",
        "Synthetic data generation and simulation frameworks for BEC (e.g., controllable LLM-based attack emulation with strict safety/validation) to augment rare classes.",
        "Graph-based anomaly detection on the Mail Graph (e.g., rare communication patterns) leveraging temporal dynamics and role hierarchies.",
        "Active learning and human-in-the-loop SOC feedback integration with uncertainty sampling to prioritize labeling effort.",
        "Causal inference or structured probabilistic models to disentangle benign urgency/solicitation from malicious intent.",
        "Robustness research against adversarial text obfuscation (e.g., homographs, style transfer, image-text phishing) and link/redirect evasion.",
        "Efficient multimodal fusion (text, images, headers, links, graph context) with cost-aware gating and early-exit policies.",
        "Uncertainty calibration and risk-aware decision policies to maintain precision while safely expanding recall.",
        "Continuous evaluation frameworks to better approximate recall (e.g., canary injections, shadow deployments, red-teaming)."
      ],
      "architectural_improvement_recommendations": [
        "Introduce a graph learning module (e.g., GNN or temporal graph models) over the Mail Graph for richer context features and anomaly scoring.",
        "Adopt cost-aware cascades with learned gates and early exits to allocate Transformer inference only to hard cases.",
        "Use multi-task Transformer detectors (urgency, solicitation, credential theft cues) with shared encoders and lightweight heads for efficiency.",
        "Apply OCR + vision transformers for image/text-in-image phishing cues and integrate with link/redirect analysis.",
        "Leverage weak supervision (e.g., programmatic labeling, Snorkel) to bootstrap detectors from IOC feeds and heuristics while controlling noise.",
        "Add Bayesian calibration layers (e.g., hierarchical priors per tenant/sector) to better adapt thresholds and priors to customer-specific distributions.",
        "Distill large language models into compact domain-specialized text encoders for production latency budgets.",
        "Implement robust URL/redirect analysis with homograph detection and lexical + hosting metadata features combined via calibrated ensembles."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": "Cost-aware staged pipeline; 50% emails processed in ~50 ms, 75% <100 ms, 99% <1 s; maximum allowed per-email latency is in the range of multiple seconds."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Enterprise email security product (cloud-based pipeline processing large volumes of mailboxes)",
      "scalability_discussed": true,
      "inference_time": "~50 ms p50; <100 ms p75; <1 s p99; upper bound multiple seconds for hard cases",
      "deployment_challenges": [
        "Stringent precision at extremely low FPR due to rarity of BEC.",
        "Severe data sensitivity and privacy constraints limit data access and retention.",
        "High throughput and latency constraints necessitate cost-aware design and early exits.",
        "Explainability requirements for SOC workflows.",
        "Class imbalance and limited positive labels; difficulty measuring recall in production."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Design and description of CAPE: a modular, production-proven BEC detection system operating for over two years.",
      "Multi-modal detector layer covering text, images, metadata, and communication context, producing explainable behavioral signals.",
      "Mail Graph component modeling identities/relationships at global, customer, and user levels to supply fast context features.",
      "Bayesian aggregation and continual adaptation framework to combine weak signals with domain knowledge under limited data.",
      "Cost-aware, staged pipeline with early benign detectors and budget allocation to hard emails; latency distribution reported.",
      "Operational methodology and efficacy tracking, including precision as a primary constraint and conviction-rate proxy for recall.",
      "Description of specific behavioral detectors, including Transformer-based NLU detectors and examples like urgency and open-redirect detection."
    ]
  },
  {
    "arxiv_id": "2310.09810v1",
    "title": "ChatGPT for Vulnerability Detection, Classification, and Repair: How Far Are We?",
    "authors": "Michael Fu; Chakkrit Tantithamthavorn; Van Nguyen; Trung Le",
    "abstract": "Large language models (LLMs) like ChatGPT (i.e., gpt-3.5-turbo and gpt-4) exhibited remarkable advancement in a range of software engineering tasks associated with source code such as code review and code generation. In this paper, we undertake a comprehensive study by instructing ChatGPT for four prevalent vulnerability tasks: function and line-level vulnerability prediction, vulnerability classification, severity estimation, and vulnerability repair. We compare ChatGPT with state-of-the-art language models designed for software vulnerability purposes. Through an empirical assessment employing extensive real-world datasets featuring over 190,000 C/C++ functions, we found that ChatGPT achieves limited performance, trailing behind other language models in vulnerability contexts by a significant margin. The experimental outcomes highlight the challenging nature of vulnerability prediction tasks, requiring domain-specific expertise. Despite ChatGPT's substantial model scale, exceeding that of source code-pre-trained language models (e.g., CodeBERT) by a factor of 14,000, the process of fine-tuning remains imperative for ChatGPT to generalize for vulnerability prediction tasks. We publish the studied dataset, experimental prompts for ChatGPT, and experimental results at https://github.com/awsm-research/ChatGPT4Vul.",
    "published_date": "2023-10-15",
    "pdf_link": "https://arxiv.org/pdf/2310.09810v1",
    "paper_types": [
      "empirical_analysis",
      "benchmark"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Vulnerability Analysis and Management",
      "specific_problem": "Function- and line-level vulnerability prediction, CWE-ID classification, CVSS severity estimation, and automated vulnerability repair for C/C++ source code",
      "attack_types": [
        "CWE weaknesses (91 types) in Big-Vul"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer (LLM) with prompting",
        "specific": "ChatGPT (gpt-3.5-turbo, gpt-4)",
        "novel_contribution": "Prompt designs/templates for four vulnerability tasks (function/line-level detection, CWE-ID classification with constrained label set, CVSS regression, patch-generation using example-based templates)"
      },
      {
        "type": "baseline",
        "category": "Transformer Encoder",
        "specific": "CodeBERT",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer Encoder + Code/Dataflow",
        "specific": "GraphCodeBERT",
        "novel_contribution": "Incorporates data flow graph signals in addition to code tokens"
      },
      {
        "type": "baseline",
        "category": "Model Distillation (Transformer-based)",
        "specific": "VulExplainer",
        "novel_contribution": "Hierarchical distillation to mitigate data imbalance for CWE-ID classification"
      },
      {
        "type": "baseline",
        "category": "Seq2Seq Transformer (T5)",
        "specific": "VulRepair",
        "novel_contribution": "T5-based automated vulnerability repair; integrated by AIBugHunter for repair"
      },
      {
        "type": "baseline",
        "category": "System (multi-model pipeline)",
        "specific": "AIBugHunter",
        "novel_contribution": "Practical tool aggregating fine-tuned models for detection, classification, severity estimation, and repair"
      }
    ],
    "learning_paradigm": [
      "Prompt-based (zero-shot/few-shot)",
      "Supervised",
      "Knowledge distillation"
    ],
    "datasets": [
      {
        "name": "Big-Vul",
        "type": "public",
        "domain": "source_code (C/C++ functions with vulnerability labels, CWE-ID, CVSS)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CVEFixes",
        "type": "public",
        "domain": "source_code (open-source projects with CVEs and fixes)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Big-Vul + CVEFixes (pre-processed by Chen et al. for repair)",
        "type": "public",
        "domain": "source_code (paired vulnerable functions and repair patches)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CodeSearchNet (pretraining corpus mentioned for baselines)",
        "type": "public",
        "domain": "source_code (multi-language code corpus for model pretraining)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "ChatGPT4Vul (studied dataset, prompts, and results)",
        "type": "public",
        "domain": "source_code (derived splits/prompts/results for vulnerability tasks)",
        "link": "https://github.com/awsm-research/ChatGPT4Vul",
        "is_new_contribution": true,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "AIBugHunter",
        "paper_reference": "[10]",
        "metric": "Function-level F1",
        "their_result": "29% (gpt-4)",
        "baseline_result": "94%"
      },
      {
        "method_name": "CodeBERT",
        "paper_reference": "[7]",
        "metric": "Function-level F1",
        "their_result": "29% (gpt-4)",
        "baseline_result": "87%"
      },
      {
        "method_name": "GraphCodeBERT",
        "paper_reference": "[12]",
        "metric": "Function-level F1",
        "their_result": "29% (gpt-4)",
        "baseline_result": "35%"
      },
      {
        "method_name": "AIBugHunter",
        "paper_reference": "[10]",
        "metric": "Line-level Top-10 Accuracy",
        "their_result": "65% (gpt-4) / 25% (gpt-3.5-turbo)",
        "baseline_result": "99%"
      },
      {
        "method_name": "GraphCodeBERT",
        "paper_reference": "[12]",
        "metric": "Line-level Top-10 Accuracy",
        "their_result": "65% (gpt-4)",
        "baseline_result": "87%"
      },
      {
        "method_name": "VulExplainer",
        "paper_reference": "[8]",
        "metric": "CWE-ID Multiclass Accuracy",
        "their_result": "20% (gpt-4) / 13% (gpt-3.5-turbo)",
        "baseline_result": "65%"
      },
      {
        "method_name": "AIBugHunter",
        "paper_reference": "[10]",
        "metric": "CWE-ID Multiclass Accuracy",
        "their_result": "20% (gpt-4)",
        "baseline_result": "65%"
      },
      {
        "method_name": "CodeBERT",
        "paper_reference": "[7]",
        "metric": "CWE-ID Multiclass Accuracy",
        "their_result": "20% (gpt-4)",
        "baseline_result": "63%"
      },
      {
        "method_name": "GraphCodeBERT",
        "paper_reference": "[12]",
        "metric": "CWE-ID Multiclass Accuracy",
        "their_result": "20% (gpt-4)",
        "baseline_result": "62%"
      },
      {
        "method_name": "CodeBERT",
        "paper_reference": "[7]",
        "metric": "CVSS Severity MSE",
        "their_result": "5.85 (gpt-4) / 5.4 (gpt-3.5-turbo)",
        "baseline_result": "1.8"
      },
      {
        "method_name": "GraphCodeBERT",
        "paper_reference": "[12]",
        "metric": "CVSS Severity MSE",
        "their_result": "5.85 (gpt-4)",
        "baseline_result": "1.85"
      },
      {
        "method_name": "AIBugHunter",
        "paper_reference": "[10]",
        "metric": "CVSS Severity MSE",
        "their_result": "5.85 (gpt-4)",
        "baseline_result": "1.86"
      },
      {
        "method_name": "CodeBERT",
        "paper_reference": "[7]",
        "metric": "CVSS Severity MAE",
        "their_result": "1.84 (gpt-3.5-turbo)",
        "baseline_result": "0.83"
      },
      {
        "method_name": "AIBugHunter (uses VulRepair)",
        "paper_reference": "[10], [11]",
        "metric": "Automated Repair % Perfect Predictions (%PP)",
        "their_result": "0% (both ChatGPT variants)",
        "baseline_result": "30%"
      },
      {
        "method_name": "CodeBERT (fine-tuned for repair in prior work)",
        "paper_reference": "[7], [3]",
        "metric": "Automated Repair % Perfect Predictions (%PP)",
        "their_result": "0%",
        "baseline_result": "7%"
      },
      {
        "method_name": "GraphCodeBERT (fine-tuned for repair in prior work)",
        "paper_reference": "[12], [3]",
        "metric": "Automated Repair % Perfect Predictions (%PP)",
        "their_result": "0%",
        "baseline_result": "9%"
      }
    ],
    "performance_metrics_used": [
      "F1",
      "Precision",
      "Recall",
      "Top-10 Accuracy (line-level localization)",
      "Multiclass Accuracy",
      "Mean Squared Error (MSE)",
      "Mean Absolute Error (MAE)",
      "% Perfect Predictions (%PP)",
      "BLEU",
      "METEOR"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How accurate is ChatGPT for function and line-level vulnerability predictions?",
        "How accurate is ChatGPT for vulnerability types classification?",
        "How accurate is ChatGPT for vulnerability severity estimation?",
        "How accurate is ChatGPT for automated vulnerability repair?"
      ],
      "gaps_identified": [
        "Prompted ChatGPT substantially underperforms fine-tuned code models across all vulnerability tasks despite its scale.",
        "Vulnerability prediction requires domain-specific knowledge and fine-tuning to generalize.",
        "CWE-ID classification is impacted by class imbalance; generic LLM pretraining does not capture specific vulnerability-to-CWE mappings.",
        "Automated vulnerability repair is particularly challenging; ChatGPT failed to produce any correct patches under the evaluated setup.",
        "Proprietary nature of ChatGPT prevents task-specific fine-tuning."
      ],
      "limitations": [
        "ChatGPT parameters are proprietary; fine-tuning was not possible.",
        "Evaluation restricted to C/C++ functions (Big-Vul and CVEFixes).",
        "ChatGPT prompting only (no integration with program analysis or external tools).",
        "Greedy decoding used for repair to ensure fair comparison.",
        "Results tied to specific datasets and splits; broader generalization not assessed in this paper."
      ],
      "future_work": [
        "Fine-tune large language models with domain-specific vulnerability data to improve generalization.",
        "Explore improved prompting strategies and task formulations for vulnerability tasks.",
        "Investigate integration of program analysis signals (AST, data-flow) with LLMs.",
        "Extend evaluation to additional languages, datasets, and real-world settings."
      ],
      "motivation": "Assess how far general-purpose LLMs (ChatGPT) can go across the end-to-end vulnerability workflow and benchmark them against fine-tuned, code-focused models.",
      "potential_research_ideas": [
        "Fine-tune open-source LLMs (e.g., Code Llama, StarCoder2) on Big-Vul/CVEFixes with multi-task objectives covering detection, CWE classification, severity regression, and repair.",
        "Retrieval-augmented prompting/fine-tuning that conditions LLMs on CWE definitions, known patterns, and CVSS guidelines to improve explainability and accuracy.",
        "Hybrid neuro-symbolic systems combining static analysis (data/taint flow, AST, SSA) with LLM reasoning for both localization and patch generation.",
        "Curriculum or contrastive learning on vulnerability pattern families (e.g., buffer overflow vs. use-after-free) to sharpen CWE discrimination.",
        "Constrained decoding for repair (syntax/type constraints, patch templates) plus unit-test or fuzz-driven patch validation loops.",
        "Self-consistency and iterative patch refinement with execution feedback (e.g., compile/test) to increase %PP.",
        "Data augmentation via synthetic vulnerable/benign pairs and counterfactual edits to mitigate class imbalance.",
        "Uncertainty calibration and confidence scoring for safe triaging in practical pipelines."
      ],
      "architectural_improvement_recommendations": [
        "Adapter/LoRA-based fine-tuning of LLMs on vulnerability corpora to retain general knowledge while specializing to security tasks.",
        "Multi-modal code representations that fuse token embeddings with graph-based program representations (CFG/DFG) via cross-attention.",
        "Joint multi-task heads (detection, CWE-ID, CVSS) with shared encoders to exploit task synergies.",
        "RAG pipeline that indexes CWE and CVE knowledge bases and injects retrieved passages into prompts or encoder memory.",
        "Patch generation with constrained decoding and verification-in-the-loop (compile, tests, static checks) to gate outputs.",
        "Imbalance-aware training (focal loss, reweighting, distillation) specifically for CWE-ID classification."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/awsm-research/ChatGPT4Vul",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Experiments run on Linux with AMD Ryzen 9 5950X CPU, 64 GB RAM, NVIDIA RTX 3090 GPU; ChatGPT prompting via paid OpenAI API."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "LLMs like ChatGPT require domain-specific fine-tuning to generalize to vulnerability tasks.",
        "ChatGPT is proprietary and cannot be fine-tuned by users for these tasks.",
        "Automated repair is especially difficult; zero correct patches under evaluated prompting setup."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Comprehensive empirical evaluation of ChatGPT (gpt-3.5-turbo and gpt-4) on four vulnerability tasks spanning detection, classification, severity estimation, and repair.",
      "Head-to-head benchmark against fine-tuned, code-focused models (AIBugHunter, CodeBERT, GraphCodeBERT, VulExplainer) on large real-world datasets (>190k C/C++ functions).",
      "Prompt templates for each task and analysis showing that prompting alone is insufficient without fine-tuning for vulnerability tasks.",
      "Release of studied dataset splits, prompts, and experimental results at https://github.com/awsm-research/ChatGPT4Vul."
    ]
  },
  {
    "arxiv_id": "2308.06924v1",
    "title": "FedEdge AI-TC: A Semi-supervised Traffic Classification Method based on Trusted Federated Deep Learning for Mobile Edge Computing",
    "authors": "Pan Wang; Zeyi Li; Mengyi Fu; Zixuan Wang; Ze Zhang; MinYao Liu",
    "abstract": "As a typical entity of MEC (Mobile Edge Computing), 5G CPE (Customer Premise Equipment)/HGU (Home Gateway Unit) has proven to be a promising alternative to traditional Smart Home Gateway. Network TC (Traffic Classification) is a vital service quality assurance and security management method for communication networks, which has become a crucial functional entity in 5G CPE/HGU. In recent years, many researchers have applied Machine Learning or Deep Learning (DL) to TC, namely AI-TC, to improve its performance. However, AI-TC faces challenges, including data dependency, resource-intensive traffic labeling, and user privacy concerns. The limited computing resources of 5G CPE further complicate efficient classification. Moreover, the \"black box\" nature of AI-TC models raises transparency and credibility issues. The paper proposes the FedEdge AI-TC framework, leveraging Federated Learning (FL) for reliable Network TC in 5G CPE. FL ensures privacy by employing local training, model parameter iteration, and centralized training. A semi-supervised TC algorithm based on Variational Auto-Encoder (VAE) and convolutional neural network (CNN) reduces data dependency while maintaining accuracy. To optimize model light-weight deployment, the paper introduces XAI-Pruning, an AI model compression method combined with DL model interpretability. Experimental evaluation demonstrates FedEdge AI-TC's superiority over benchmarks in terms of accuracy and efficient TC performance. The framework enhances user privacy and model credibility, offering a comprehensive solution for dependable and transparent Network TC in 5G CPE, thus enhancing service quality and security.",
    "published_date": "2023-08-14",
    "pdf_link": "https://arxiv.org/pdf/2308.06924v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Traffic Classification",
      "specific_problem": "Federated semi-supervised network traffic classification on 5G CPE/HGU with privacy protection, edge deployment, and model interpretability",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Federated Learning",
        "specific": "Horizontal FL with FedAvg-style secure aggregation",
        "novel_contribution": "Trusted FL framework tailored for 5G CPE/HGU, using local training, encrypted parameter upload, and centralized aggregation for privacy-preserving traffic classification"
      },
      {
        "type": "primary",
        "category": "Hybrid (VAE + CNN)",
        "specific": "Variational Auto-Encoder for semi-supervised representation + CNN classifier",
        "novel_contribution": "Semi-supervised VAE+CNN algorithm to reduce labeled data dependency within the FL setting"
      },
      {
        "type": "primary",
        "category": "Pruning/Model Compression",
        "specific": "XAI-Pruning",
        "novel_contribution": "Compression method that combines model interpretability (global and local explanations) to guide pruning for lightweight edge deployment"
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": null,
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Semi-supervised",
      "Supervised",
      "Federated Learning"
    ],
    "datasets": [
      {
        "name": "ISCIX",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "UNSW-NB15",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "MIRAGE",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Self-built real network dataset (home network traffic on 5G CPE/HGU)",
        "type": "proprietary",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Accuracy",
      "Precision",
      "Recall",
      "F1-Score",
      "MSE"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How to achieve efficient and trusted traffic classification on a weak-computing-power gateway device (5G CPE/HGU) while protecting user privacy?",
        "How to reduce model data dependency and costly traffic labeling for AI-based traffic classification?",
        "How to improve transparency and credibility of AI-TC models for network operators?"
      ],
      "gaps_identified": [
        "High data dependency and costly labeling for traffic classification",
        "User privacy concerns when centralizing sensitive home network data",
        "Limited computing resources on 5G CPE/HGU hindering efficient classification",
        "\"Black box\" nature of AI-TC models reduces transparency and credibility",
        "Potential non-IID data across edge devices in home networks"
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Provide a dependable, privacy-preserving, and transparent traffic classification solution for smart home/edge gateways via federated, semi-supervised learning with lightweight and interpretable models.",
      "potential_research_ideas": [
        "Integrate formal differential privacy or secure aggregation enhancements into the FL pipeline to provide quantifiable privacy guarantees",
        "Develop personalized FL methods for non-IID home network traffic (e.g., FedProx/SCAFFOLD-style corrections) and evaluate personalization vs. global performance",
        "Explore contrastive and consistency-based semi-supervised learning (e.g., FixMatch, Mean Teacher) for traffic data within FL",
        "Investigate adversarial robustness in federated semi-supervised traffic classifiers against poisoning and evasion attacks",
        "Open-world and zero-day traffic classification with out-of-distribution detection in the FL setting",
        "Knowledge distillation from the global model to ultra-tiny edge models for faster inference on resource-constrained CPE",
        "Communication-efficient FL via gradient compression/quantization and client selection tailored to variable CPE connectivity",
        "Multi-task learning combining application classification and intrusion detection in a unified federated model",
        "Online/streaming continual learning to handle rapid evolution of home applications and protocols"
      ],
      "architectural_improvement_recommendations": [
        "Augment CNN with temporal modules (1D-CNN + LSTM/Transformer) to capture sequence dynamics of flows",
        "Leverage graph neural networks for modeling relationships among flows/sessions within and across clients",
        "Adopt robust FL aggregation (e.g., Trimmed Mean, Krum) to mitigate byzantine/poisoned clients",
        "Apply quantization-aware training in conjunction with XAI-Pruning to further reduce model size with minimal accuracy loss",
        "Use client-level personalization layers or adapters to handle heterogeneous non-IID distributions",
        "Employ self-supervised contrastive pretraining on unlabeled flows before semi-supervised fine-tuning",
        "Incorporate uncertainty estimation and XAI methods (e.g., SHAP/Integrated Gradients) to enhance trust and guide pruning",
        "Asynchronous FL and intelligent client selection to scale to large fleets of 5G CPEs with intermittent connectivity"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "5G CPE/HGU (smart home edge gateway) with limited CPU/memory/flash; central aggregator/server",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Edge device resource constraints (CPU, memory, storage)",
        "Data privacy and security for home network users",
        "Non-IID traffic distributions across clients",
        "Model transparency requirements for operators"
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": true,
      "fairness_bias": false
    },
    "contributions": [
      "Federated learning-based traffic classification framework (local training, parameter iteration, centralized aggregation) to protect user data privacy",
      "Semi-supervised traffic classification algorithm based on VAE and CNN to reduce dependence on labeled data",
      "XAI-Pruning: pruning-based model compression combined with global and local interpretability to improve transparency and enable lightweight deployment",
      "Experimental validation on public benchmarks and real network datasets showing higher accuracy and efficient inference on resource-limited 5G CPE/HGU"
    ]
  },
  {
    "arxiv_id": "2308.05034v3",
    "title": "Kairos: Practical Intrusion Detection and Investigation using Whole-system Provenance",
    "authors": "Zijun Cheng; Qiujian Lv; Jinyuan Liang; Yan Wang; Degang Sun; Thomas Pasquier; Xueyuan Han",
    "abstract": "Provenance graphs are structured audit logs that describe the history of a system's execution. Recent studies have explored a variety of techniques to analyze provenance graphs for automated host intrusion detection, focusing particularly on advanced persistent threats. Sifting through their design documents, we identify four common dimensions that drive the development of provenance-based intrusion detection systems (PIDSes): scope (can PIDSes detect modern attacks that infiltrate across application boundaries?), attack agnosticity (can PIDSes detect novel attacks without a priori knowledge of attack characteristics?), timeliness (can PIDSes efficiently monitor host systems as they run?), and attack reconstruction (can PIDSes distill attack activity from large provenance graphs so that sysadmins can easily understand and quickly respond to system intrusion?). We present KAIROS, the first PIDS that simultaneously satisfies the desiderata in all four dimensions, whereas existing approaches sacrifice at least one and struggle to achieve comparable detection performance.   Kairos leverages a novel graph neural network-based encoder-decoder architecture that learns the temporal evolution of a provenance graph's structural changes to quantify the degree of anomalousness for each system event. Then, based on this fine-grained information, Kairos reconstructs attack footprints, generating compact summary graphs that accurately describe malicious activity over a stream of system audit logs. Using state-of-the-art benchmark datasets, we demonstrate that Kairos outperforms previous approaches.",
    "published_date": "2023-08-09",
    "pdf_link": "https://arxiv.org/pdf/2308.05034v3",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Host/Endpoint Security",
      "subdomain": "Provenance-based Intrusion Detection",
      "specific_problem": "Whole-system provenance graph anomaly detection and automatic attack reconstruction for APT-style intrusions using streaming system audit logs",
      "attack_types": [
        "Advanced Persistent Threat (APT)",
        "Zero-day exploits",
        "Privilege escalation",
        "Command-and-control (C2) communication",
        "Code injection",
        "Port scanning"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "GNN",
        "specific": "Graph autoencoder (encoder-decoder) for dynamic graphs",
        "novel_contribution": "Edge-level anomaly scoring by reconstructing each new edge using a GNN-based encoder-decoder that learns temporal evolution and spatial neighborhood structure of provenance graphs; node states capture histories of neighborhood changes."
      },
      {
        "type": "primary",
        "category": "Anomaly Detection",
        "specific": "Reconstruction-error-based anomaly detection with streaming time-window queues",
        "novel_contribution": "Queueing of overlapping suspicious time windows and queue-level anomaly scoring for timely alerts in streaming settings."
      },
      {
        "type": "primary",
        "category": "Feature Engineering/Hashing",
        "specific": "Hierarchical feature hashing",
        "novel_contribution": "Encodes node attributes (e.g., file paths, IPs) at multiple hierarchical granularities to preserve structural similarity in a compact vector space for graph learning."
      },
      {
        "type": "primary",
        "category": "Graph Mining",
        "specific": null,
        "novel_contribution": "Community discovery over highly anomalous edges and causality-based correlation to build compact attack summary graphs for investigation."
      }
    ],
    "learning_paradigm": [
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "DARPA E3-THEIA",
        "type": "public",
        "domain": "provenance_graphs (system audit logs)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "DARPA benchmark datasets simulating APT campaigns (unspecified names)",
        "type": "public",
        "domain": "provenance_graphs (system audit logs)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Unicorn",
        "paper_reference": "[2] (as cited in the paper)",
        "metric": null,
        "their_result": "\"KAIROS achieves high detection accuracy, outperforming both systems while incurring low computational and memory overhead.\"",
        "baseline_result": null
      },
      {
        "method_name": "ThreaTrace",
        "paper_reference": "[8] (as cited in the paper)",
        "metric": null,
        "their_result": "\"KAIROS achieves high detection accuracy, outperforming both systems while incurring low computational and memory overhead.\"",
        "baseline_result": null
      },
      {
        "method_name": "Holmes",
        "paper_reference": "[3] (as cited in the paper)",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Winnower",
        "paper_reference": "[5] (as cited in the paper)",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "SIGL",
        "paper_reference": "[14] (as cited in the paper)",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Poirot",
        "paper_reference": "[17] (as cited in the paper)",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "RapSheet",
        "paper_reference": "[22] (as cited in the paper)",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Detection accuracy",
      "Computational overhead",
      "Memory overhead",
      "Attack reconstruction graph size reduction (edges/nodes)"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can provenance-based IDS detect modern attacks that infiltrate across application boundaries (whole-system scope)?",
        "Can PIDS detect novel/zero-day attacks without a priori knowledge (attack agnosticity)?",
        "Can PIDS efficiently monitor hosts as they run (timeliness/streaming)?",
        "Can PIDS distill attack activity from large provenance graphs for quick human investigation (attack reconstruction)?"
      ],
      "gaps_identified": [
        "Existing PIDSes sacrifice at least one of scope, attack-agnostic detection, timeliness, or attack reconstruction.",
        "Anomaly-based prior work detects anomalies but provides little support for attack reconstruction, leaving heavy manual graph forensics.",
        "Template/single-application approaches do not scale to whole-system APT scenarios and miss inter-process information flows.",
        "Signature/graph-matching systems are not attack-agnostic and are too slow for streaming detection.",
        "Node-only anomaly methods fail to provide edge-level causal context for reconstruction."
      ],
      "limitations": [
        "Does not consider hardware, side-channel, or covert-channel attacks, since they are not captured by kernel-level audit.",
        "Assumes training data is benign and that benign behavior is thoroughly observed; concept drift is excluded from threat model (mitigation shown empirically elsewhere).",
        "Trusted computing base includes OS, audit framework, and Kairos analysis code; kernel/audit compromise is out of scope.",
        "Assumes integrity of audit logs; relies on secure provenance/tamper-evident logging.",
        "May miss some attack-related activities (paper’s Figure 1 shows dashed pink nodes/edges the system missed per ground truth)."
      ],
      "future_work": [],
      "motivation": "Design a provenance-based IDS that simultaneously achieves whole-system scope, attack-agnostic detection, timely streaming operation, and automated attack reconstruction, overcoming trade-offs in prior PIDS designs.",
      "potential_research_ideas": [
        "Incorporate continual/online learning to address concept drift and evolving benign workloads without labeled anomalies.",
        "Integrate federated/centralized cross-host correlation to reason about multi-host APT campaigns with global context.",
        "Develop adversarially robust training against poisoning/obfuscation of provenance logs and mimicry attacks on reconstruction-error signals.",
        "Leverage contrastive/self-supervised pretext tasks on dynamic provenance graphs to improve embeddings beyond reconstruction loss.",
        "Human-in-the-loop active learning to refine reconstruction with minimal analyst feedback and reduce false positives.",
        "Extend reconstruction to multi-stage campaign narratives with temporal logic summarization and natural-language explanations.",
        "Combine anomaly detection with lightweight signature priors to improve precision while keeping attack agnosticity.",
        "Evaluate and optimize privacy-preserving provenance collection (e.g., selective capture, on-host redaction) while maintaining detection fidelity."
      ],
      "architectural_improvement_recommendations": [
        "Replace or augment the encoder with temporal GNNs for dynamic graphs (e.g., attention-based temporal message passing) to better capture fine-grained evolution.",
        "Add attention mechanisms to highlight most influential neighbors/edge types, producing more interpretable anomaly rationales.",
        "Use multi-task objectives (reconstruction + contrastive/forecasting of next-edge) to stabilize training and improve generalization.",
        "Calibrate anomaly scores (e.g., EVT/score normalization per-entity) to handle non-stationarity across entities and time windows.",
        "Introduce type-aware decoders that model edge semantics (read/write/exec/send/receive) explicitly for better reconstruction fidelity.",
        "Employ streaming graph partitioning and GPU-accelerated mini-batch processing to scale to tens of millions of edges with bounded latency.",
        "Incorporate change-point detection to trigger dynamic threshold adaptation and reduce alert fatigue.",
        "Automate summary graph pruning via subgraph-explanation methods to produce minimal yet faithful causal subgraphs."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/ProvenanceAnalytics/kairos",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Host endpoints with system-wide audit (e.g., Windows ETW, Linux Audit, CamFlow) across a network of systems",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Requires a clean benign training period; susceptible to concept drift without continual adaptation.",
        "Depends on integrity and availability of kernel-level audit/provenance logs; audit overhead and storage must be managed.",
        "Trusted OS/audit stack is required; kernel/audit compromise is out of scope.",
        "Integration with SOC workflows and visualization for large enterprises; summarization quality impacts analyst trust."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces KAIROS, a provenance-based IDS that simultaneously addresses scope, attack agnosticity, timeliness, and attack reconstruction.",
      "Proposes a novel GNN-based encoder-decoder that learns temporal-spatial evolution of provenance graphs and yields edge-level anomaly scores via reconstruction error.",
      "Implements streaming detection with time-window queues and overlapping suspicious-node correlation for timely alerts.",
      "Automates attack investigation by constructing compact, meaningful provenance summary graphs from anomalous edges using information-flow causality and community discovery.",
      "Empirically evaluates on DARPA APT benchmark datasets and shows that \"KAIROS outperforms previous approaches\" including Unicorn and ThreaTrace while incurring low computational and memory overhead.",
      "Releases open-source implementation (https://github.com/ProvenanceAnalytics/kairos)."
    ]
  },
  {
    "arxiv_id": "2309.08485v1",
    "title": "XFedHunter: An Explainable Federated Learning Framework for Advanced Persistent Threat Detection in SDN",
    "authors": "Huynh Thai Thi; Ngo Duc Hoang Son; Phan The Duy; Nghi Hoang Khoa; Khoa Ngo-Khanh; Van-Hau Pham",
    "abstract": "Advanced Persistent Threat (APT) attacks are highly sophisticated and employ a multitude of advanced methods and techniques to target organizations and steal sensitive and confidential information. APT attacks consist of multiple stages and have a defined strategy, utilizing new and innovative techniques and technologies developed by hackers to evade security software monitoring. To effectively protect against APTs, detecting and predicting APT indicators with an explanation from Machine Learning (ML) prediction is crucial to reveal the characteristics of attackers lurking in the network system. Meanwhile, Federated Learning (FL) has emerged as a promising approach for building intelligent applications without compromising privacy. This is particularly important in cybersecurity, where sensitive data and high-quality labeling play a critical role in constructing effective machine learning models for detecting cyber threats. Therefore, this work proposes XFedHunter, an explainable federated learning framework for APT detection in Software-Defined Networking (SDN) leveraging local cyber threat knowledge from many training collaborators. In XFedHunter, Graph Neural Network (GNN) and Deep Learning model are utilized to reveal the malicious events effectively in the large number of normal ones in the network system. The experimental results on NF-ToN-IoT and DARPA TCE3 datasets indicate that our framework can enhance the trust and accountability of ML-based systems utilized for cybersecurity purposes without privacy leakage.",
    "published_date": "2023-09-15",
    "pdf_link": "https://arxiv.org/pdf/2309.08485v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Explainable federated APT detection in SDN combining NIDS (NetFlow) and PIDS (provenance graph)",
      "attack_types": [
        "Advanced Persistent Threat (APT)",
        "Network intrusions (general)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Federated Learning",
        "specific": "FedAvg",
        "novel_contribution": "Federated training of combined NIDS (CNN-GRU) and PIDS (GNN) models across collaborators for APT detection in SDN"
      },
      {
        "type": "primary",
        "category": "Hybrid CNN+RNN",
        "specific": "CNN-GRU (variant inspired by DeepFed)",
        "novel_contribution": "Applied to NetFlow-based NIDS within a federated learning and explainability framework"
      },
      {
        "type": "primary",
        "category": "GNN",
        "specific": "E-GraphSAGE",
        "novel_contribution": "Applied to provenance graph-based PIDS within FL; paired with SHAP explanations for interpretability"
      },
      {
        "type": "primary",
        "category": "XAI",
        "specific": "SHAP (KernelSHAP for NIDS, GradientSHAP for PIDS)",
        "novel_contribution": "Integrated SHAP to interpret FL-based APT detection decisions in both NetFlow and provenance graph modalities"
      },
      {
        "type": "primary",
        "category": "Post-hoc analysis",
        "specific": null,
        "novel_contribution": "Proposed decision quality checking using penultimate layer outputs over malicious detection metrics histories to determine correctness of model predictions"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Federated"
    ],
    "datasets": [
      {
        "name": "NF-ToN-IoT",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "DARPA TCE3",
        "type": "public",
        "domain": "provenance_graph",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "NSL-KDD",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "UNSW-NB15",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Limited number of studies employing federated learning in the context of provenance-based IDS (PIDS)",
        "DL-based IDS often lack interpretability; few works thoroughly explore explainability for IDS integrated with federated learning",
        "Existing XAI work for IDS typically model-specific (e.g., Grad-CAM for CNN) or limited to certain models (e.g., decision trees), lacking model-agnostic approaches across modalities",
        "Lack of research interpreting predictions of FL-based IDS and handling cases with mixed explanations for true and false predictions; no methodology to address this issue",
        "Provenance graph complexity challenges effective analysis; organizational growth exacerbates analysis pressure",
        "Data scarcity and privacy concerns hinder centralized training for APT detection"
      ],
      "limitations": [
        "KernelSHAP can be computationally intensive and requires background datasets; SHAP’s need for consistent interaction with the IDS may significantly impede system performance, necessitating a separate replica model for explanation",
        "Provenance graph complexity may pose scalability/analysis challenges (noted contextually)"
      ],
      "future_work": [],
      "motivation": "APT attacks are multi-stage, stealthy, and hard to detect with traditional systems; need privacy-preserving collaborative learning and interpretable ML to detect and understand APT indicators in SDN environments.",
      "potential_research_ideas": [
        "Adversarially robust FL-based IDS (e.g., defenses against model poisoning/backdoor attacks in cross-silo APT detection)",
        "Integrate differential privacy and secure aggregation to strengthen privacy guarantees while preserving detection performance",
        "Personalized FL for heterogeneous organizations (e.g., FedPer/FedRep) to adapt models to site-specific traffic and provenance patterns",
        "Unified multimodal graph learning that fuses NetFlow sequences and provenance graphs into a heterogeneous temporal graph for end-to-end APT detection",
        "Online/continual learning to track evolving APT tactics, techniques, and procedures (TTPs) with concept drift handling",
        "Causality-aware explanations linking features/subgraphs to MITRE ATT&CK techniques to aid analyst triage",
        "Explanation-guided active learning to query labels for ambiguous events identified by SHAP-derived uncertainty signatures",
        "Knowledge distillation to lightweight student models enabling real-time SDN deployment with minimal SHAP overhead",
        "Benchmarking explainability across XAI methods (SHAP, IG, CXPlain, GraphLIME) for both NetFlow and provenance graph models in FL settings",
        "Automated root cause analysis by tracing high-attribution features/subgraphs across clients to reconstruct attack kill-chains collaboratively"
      ],
      "architectural_improvement_recommendations": [
        "Adopt FL algorithms robust to non-IID data (FedProx, SCAFFOLD, FedNova) to stabilize training across diverse organizations",
        "Add secure aggregation and differential privacy to mitigate membership/model inversion risks",
        "Replace/augment E-GraphSAGE with attention-based GNNs (e.g., GAT, temporal graph networks, graph transformers) to better capture temporal multi-step APT behavior",
        "Introduce cross-modal attention to fuse CNN-GRU outputs with GNN embeddings for joint decisions",
        "Employ explanation caching/surrogate models to reduce SHAP computation cost; e.g., train local linear surrogates or TreeSHAP-compatible proxies",
        "Use explanation-aware training (regularize for explanation stability) to improve robustness and analyst trust",
        "Implement anomaly detection heads (autoencoder/contrastive) alongside supervised classifiers to capture unseen APT stages",
        "Utilize gradient/hessian-based SHAP accelerations and batched background datasets; tune background size for speed-accuracy trade-offs",
        "Incorporate poisoning detection at the server (e.g., norm clipping, Krum, FLTrust)",
        "Leverage model distillation and quantization for SDN controller deployment constraints"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Software-Defined Networking (SDN) with SIEM; centralized FL server coordinating collaborators",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "SHAP computation and model interaction may impede IDS performance; need for a replica model for explanations",
        "KernelSHAP is computationally intensive and requires suitable background datasets",
        "Complexity and scale of provenance graphs challenge real-time analysis",
        "Heterogeneous, non-IID client data and label scarcity across organizations",
        "Privacy concerns with collaborative training and potential leakage via model updates",
        "Integration within SDN controller/data plane paths without impacting latency"
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": true,
      "fairness_bias": false
    },
    "contributions": [
      "Proposed XFedHunter: an explainable federated learning framework for APT detection in SDN, combining NIDS (CNN-GRU) and PIDS (GNN E-GraphSAGE)",
      "Integrated model-agnostic XAI (SHAP: KernelSHAP for NIDS, GradientSHAP for PIDS) to analyze and explain detector decisions",
      "Designed a mechanism to determine the correctness of model predictions using the penultimate layer’s outputs of malicious detection metrics histories"
    ]
  },
  {
    "arxiv_id": "2309.01586v1",
    "title": "Automatic Scam-Baiting Using ChatGPT",
    "authors": "Piyush Bajaj; Matthew Edwards",
    "abstract": "Automatic scam-baiting is an online fraud countermeasure that involves automated systems responding to online fraudsters in order to waste their time and deplete their resources, diverting attackers away from real potential victims. Previous work has demonstrated that text generation systems are capable of engaging with attackers as automatic scam-baiters, but the fluency and coherence of generated text may be a limit to the effectiveness of such systems.   In this paper, we report on the results of a month-long experiment comparing the effectiveness of two ChatGPT-based automatic scam-baiters to a control measure. Within our results, with engagement from over 250 real email fraudsters, we find that ChatGPT-based scam-baiters show a marked increase in scammer response rate and conversation length relative to the control measure, outperforming previous approaches. We discuss the implications of these results and practical considerations for wider deployment of automatic scam-baiting.",
    "published_date": "2023-09-04",
    "pdf_link": "https://arxiv.org/pdf/2309.01586v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Email Security",
      "subdomain": "Social Engineering Defense",
      "specific_problem": "Automatic scam-baiting to waste fraudsters’ time via email",
      "attack_types": [
        "Investment scams",
        "Online dating/romance fraud",
        "Tech support scams",
        "Employment scams",
        "Lottery scams",
        "Inheritance scams",
        "Advanced fee fraud"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Large Language Model (LLM)",
        "specific": "GPT-3.5-turbo (ChatGPT)",
        "novel_contribution": "Two prompt-based reply strategies for scam-baiting: (1) zero-shot behavioral instruction prompt (Chat Replier 1) and (2) few-shot prompt seeded with human scam-baiting conversation exemplars (Chat Replier 2)."
      },
      {
        "type": "baseline",
        "category": "Transformer (DistilBERT) + Rule-based templates",
        "specific": "DistilBERT classifier with random template selection (from Chen et al. [5])",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Zero-shot prompting",
      "Few-shot prompting",
      "Rule-based/templates (baseline)"
    ],
    "datasets": [
      {
        "name": "Scam-baiting conversations (this study)",
        "type": "public",
        "domain": "email_conversations",
        "link": "https://github.com/an19352/scam-baiting-conversations",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "Human scam-baiting dataset (Chen et al. [5], extended from Edwards et al. [11])",
        "type": "public",
        "domain": "email_conversations",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Crawled scammer email addresses from online forums (this study)",
        "type": "proprietary",
        "domain": "email_addresses",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Classifier & Random Template (control in this study)",
        "paper_reference": "Chen et al. [5]",
        "metric": "Average replies per conversation",
        "their_result": "Chat Replier 1: 5.38",
        "baseline_result": "Control: 3.78"
      },
      {
        "method_name": "Classifier & Random Template (control in this study)",
        "paper_reference": "Chen et al. [5]",
        "metric": "Max conversation rounds",
        "their_result": "Chat Replier 1: 36",
        "baseline_result": "Control: 31"
      },
      {
        "method_name": "Classifier & Random Template (control in this study)",
        "paper_reference": "Chen et al. [5]",
        "metric": "Average distraction time",
        "their_result": "Chat Replier 1: 2.8 days",
        "baseline_result": "Control: 1.3 days"
      },
      {
        "method_name": "Classifier & Random Template (control in this study)",
        "paper_reference": "Chen et al. [5]",
        "metric": "Max distraction time",
        "their_result": "Chat Replier 1: 27.5 days",
        "baseline_result": "Control: 12.9 days"
      },
      {
        "method_name": "Classifier & Random Template (control in this study)",
        "paper_reference": "Chen et al. [5]",
        "metric": "Average replies per conversation",
        "their_result": "Chat Replier 2: 3.56",
        "baseline_result": "Control: 3.78"
      },
      {
        "method_name": "Text Generator B (best prior in Chen et al.)",
        "paper_reference": "Chen et al. [5]",
        "metric": "Total replies and conversations over one month",
        "their_result": "Chat Replier 1: 501 replies across 93 conversations",
        "baseline_result": "Text Generator B: 68 replies across 17 conversations"
      },
      {
        "method_name": "Prior auto scam-baiters (GPT-Neo generation in Chen et al.)",
        "paper_reference": "Chen et al. [5]",
        "metric": "Scammer response rate",
        "their_result": "This study (all strategies combined): ≈35% of addresses contacted",
        "baseline_result": "Chen et al.: 15–25% elicited responses"
      }
    ],
    "performance_metrics_used": [
      "Scammer response rate",
      "Number of conversations initiated (% of approaches)",
      "Total replies received",
      "Average replies per conversation",
      "Median replies per conversation",
      "Max replies per conversation",
      "Average distraction time",
      "Max distraction time"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Whether and to what degree ChatGPT-based scam-baiting systems outperform previous approaches.",
        "Whether ChatGPT-based systems given examples of human scam-baiting conversations outperform systems given only scam-baiting prompt instructions."
      ],
      "gaps_identified": [
        "Previous GPT-Neo text generation produced low-quality, incoherent emails that limited effectiveness.",
        "Lack of conversational memory can both help (by prompting repeated questions) and hurt (risk of being detected as a bot).",
        "Control measure performance variability across studies undermines its use for indexing performance; potential seasonality effects.",
        "LLM sometimes breaks instructions (e.g., calls out scams or fabricates/shares personal information).",
        "Autoresponder detection/noise in collected scammer replies complicates evaluation.",
        "Limited prompt context window for few-shot examples."
      ],
      "limitations": [
        "During response generation, systems were not given full conversation context; only the immediate email was provided.",
        "Instructions to not call out scams or share personal info were not always followed.",
        "Potential misclassification of autoresponder conversations; 32 discarded conversations may include human content.",
        "Ethical and operational constraints (IRB oversight) limit some deployment choices.",
        "Observed uplift in control performance vs prior work suggests confounders (e.g., seasonality), complicating cross-study comparisons."
      ],
      "future_work": [
        "Practical considerations and recommendations for wider deployment of automatic scam-baiting."
      ],
      "motivation": "Improve the effectiveness of automated scam-baiting as an active defense by leveraging more capable text generation (ChatGPT) to increase scammer response rate and conversation length, reducing scammers’ available time to target real victims.",
      "potential_research_ideas": [
        "Design a memory-augmented scam-baiter that retains selective conversation state while intentionally exhibiting controlled forgetfulness to sustain engagement without revealing bot-like behavior.",
        "Formulate and optimize a reinforcement learning objective that directly maximizes \"time wasted\" and conversation length under ethical and safety constraints.",
        "Develop classifiers to detect autoresponders and scammer bot behavior online to filter and adapt engagement strategies in real time.",
        "Personalize scam-baiter personas (age, profession, communication style) to different fraud types and evaluate A/B at scale.",
        "Incorporate temporal behavior (delays, time-of-day patterns) and typos to mimic human emailing and reduce bot detectability.",
        "Create a red-team/blue-team framework where adversarial scammers attempt to detect the bot, and the system adapts via adversarial training.",
        "Investigate multilingual scam-baiting to engage non-English fraud campaigns.",
        "Estimate disruption impact on scammer operations (e.g., opportunity cost modeling, campaign-level metrics) beyond conversation-level metrics."
      ],
      "architectural_improvement_recommendations": [
        "Use a retrieval-augmented memory store with policies for what to remember/forget to avoid repeating questions excessively and to reduce bot detection.",
        "Add a safety/PII guardrail layer (regex + LLM-based) to block any sharing or fabrication of personal information before sending emails.",
        "Implement a fraud-type classifier (e.g., lightweight transformer) to route emails to specialized prompts/personas per scam category.",
        "Introduce stochastic humanization: variable delays, typographical errors, and style perturbations to improve realism.",
        "Maintain a bot-detection avoidance module that monitors scammer cues and switches tactics/personas when suspicion is detected.",
        "Automate autoresponder detection and de-duplication to prevent metric inflation and adjust strategy.",
        "Leverage function calling/tools to manage attachments or handle structured forms when scammers request proofs or documents."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/an19352/scambaiter back",
      "frameworks": [
        "OpenAI ChatGPT API",
        "DistilBERT (via Chen et al.’s framework)"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Email honeypot/scam-baiting mail server engaging real scammers",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Ethical oversight required for interacting with human subjects (approval code 13904).",
        "Email addresses may become invalid due to provider enforcement.",
        "Autoresponders and duplicated messages complicate engagement and evaluation.",
        "LLM instruction-following failures (calling out scams, sharing invented PII).",
        "Lack of conversation memory can cause repetition and suspicion.",
        "Potential seasonality and external factors affecting scammer engagement.",
        "Risk of addresses being placed on “sucker lists,” attracting additional unsolicited scam emails."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Design and evaluation of two ChatGPT-based automatic scam-baiters (zero-shot behavioral prompt and few-shot with human examples).",
      "Month-long real-world experiment engaging 819 scammers and receiving responses from 286 (≈35%), with 254 valid conversations.",
      "Demonstrated improved engagement vs. control and prior approaches: highest avg replies per conversation (5.38), max rounds (36), and max distraction time (27.5 days) for Chat Replier 1.",
      "Released implementation modules integrating ChatGPT into existing scam-baiting framework.",
      "Released a public dataset of conversations between the automatic scam-baiters and scammers.",
      "Qualitative insights on the trade-offs of limited conversational memory and tactic shifts in scammers."
    ]
  },
  {
    "arxiv_id": "2309.01350v1",
    "title": "MalwareDNA: Simultaneous Classification of Malware, Malware Families, and Novel Malware",
    "authors": "Maksim E. Eren; Manish Bhattarai; Kim Rasmussen; Boian S. Alexandrov; Charles Nicholas",
    "abstract": "Malware is one of the most dangerous and costly cyber threats to national security and a crucial factor in modern cyber-space. However, the adoption of machine learning (ML) based solutions against malware threats has been relatively slow. Shortcomings in the existing ML approaches are likely contributing to this problem. The majority of current ML approaches ignore real-world challenges such as the detection of novel malware. In addition, proposed ML approaches are often designed either for malware/benign-ware classification or malware family classification. Here we introduce and showcase preliminary capabilities of a new method that can perform precise identification of novel malware families, while also unifying the capability for malware/benign-ware classification and malware family classification into a single framework.",
    "published_date": "2023-09-04",
    "pdf_link": "https://arxiv.org/pdf/2309.01350v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Malware Analysis",
      "subdomain": "Static Malware Classification and Family Classification",
      "specific_problem": "Simultaneous malware/benign detection, malware family classification, and identification of novel (unseen) malware families via open-set recognition (reject option)",
      "attack_types": [
        "adposhel",
        "emotet",
        "zusy",
        "ramnit (treated as unseen/novel family)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Matrix Factorization",
        "specific": "Hierarchical Non-negative Matrix Factorization (NMF) with automatic model determination (NMFk)",
        "novel_contribution": "Builds an archive of latent signatures; automatic selection of latent dimensionality; hierarchical refactorization guided by cluster label uniformity to separate mixed signatures."
      },
      {
        "type": "primary",
        "category": "Clustering",
        "specific": "Custom H-clustering with Silhouette statistics (via NMFk)",
        "novel_contribution": "Semi-supervised cluster uniformity test to accept clusters as labeled signatures or hierarchically refactor mixed clusters."
      },
      {
        "type": "primary",
        "category": "Least Squares",
        "specific": "Non-negative Least Squares (NNLS) projection with cosine similarity",
        "novel_contribution": "Real-time inference by projecting samples onto signature archive; similarity-threshold-based classification with reject-option for novel family identification."
      },
      {
        "type": "primary",
        "category": "Open-set Recognition",
        "specific": "Reject-option thresholding (t=1.0) with Risk-Coverage evaluation",
        "novel_contribution": "Abstaining predictions to identify novel malware families without training on rare/unknown specimens."
      },
      {
        "type": "baseline",
        "category": "Gradient Boosted Decision Trees",
        "specific": "XGBoost",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Gradient Boosted Decision Trees",
        "specific": "LightGBM",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Semi-supervised Self-training",
        "specific": "SelfTrain (Yarowsky) applied to XGBoost/LightGBM",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Semi-supervised",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "EMBER-2018",
        "type": "public",
        "domain": "malware_binaries (static PE features)",
        "link": "https://github.com/elastic/ember",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "XGBoost",
        "paper_reference": "[16]",
        "metric": "F1",
        "their_result": "0.975 (at 84.3% coverage)",
        "baseline_result": "0.416"
      },
      {
        "method_name": "XGBoost",
        "paper_reference": "[16]",
        "metric": "Precision",
        "their_result": "0.975",
        "baseline_result": "0.699"
      },
      {
        "method_name": "XGBoost",
        "paper_reference": "[16]",
        "metric": "Recall",
        "their_result": "0.977",
        "baseline_result": "0.510"
      },
      {
        "method_name": "LightGBM",
        "paper_reference": "[17]",
        "metric": "F1",
        "their_result": "0.975 (at 84.3% coverage)",
        "baseline_result": "0.297"
      },
      {
        "method_name": "LightGBM",
        "paper_reference": "[17]",
        "metric": "Precision",
        "their_result": "0.975",
        "baseline_result": "0.749"
      },
      {
        "method_name": "LightGBM",
        "paper_reference": "[17]",
        "metric": "Recall",
        "their_result": "0.977",
        "baseline_result": "0.338"
      },
      {
        "method_name": "XGBoost+SelfTrain",
        "paper_reference": "[16],[18]",
        "metric": "F1",
        "their_result": "0.975 (at 84.3% coverage)",
        "baseline_result": "0.096"
      },
      {
        "method_name": "XGBoost+SelfTrain",
        "paper_reference": "[16],[18]",
        "metric": "Precision",
        "their_result": "0.975",
        "baseline_result": "0.258"
      },
      {
        "method_name": "XGBoost+SelfTrain",
        "paper_reference": "[16],[18]",
        "metric": "Recall",
        "their_result": "0.977",
        "baseline_result": "0.108"
      },
      {
        "method_name": "XGBoost+SelfTrain",
        "paper_reference": "[16],[18]",
        "metric": "AURC",
        "their_result": "0.02",
        "baseline_result": "0.654"
      },
      {
        "method_name": "LightGBM+SelfTrain",
        "paper_reference": "[17],[18]",
        "metric": "F1",
        "their_result": "0.975 (at 84.3% coverage)",
        "baseline_result": "0.096"
      },
      {
        "method_name": "LightGBM+SelfTrain",
        "paper_reference": "[17],[18]",
        "metric": "Precision",
        "their_result": "0.975",
        "baseline_result": "0.078"
      },
      {
        "method_name": "LightGBM+SelfTrain",
        "paper_reference": "[17],[18]",
        "metric": "Recall",
        "their_result": "0.977",
        "baseline_result": "0.197"
      },
      {
        "method_name": "LightGBM+SelfTrain",
        "paper_reference": "[17],[18]",
        "metric": "AURC",
        "their_result": "0.02",
        "baseline_result": "0.651"
      },
      {
        "method_name": "XGBoost+SelfTrain",
        "paper_reference": "[16],[18]",
        "metric": "Rejection Novel (True rejections of unseen family)",
        "their_result": "100.00%",
        "baseline_result": "18.09%"
      },
      {
        "method_name": "LightGBM+SelfTrain",
        "paper_reference": "[17],[18]",
        "metric": "Rejection Novel (True rejections of unseen family)",
        "their_result": "100.00%",
        "baseline_result": "17.14%"
      },
      {
        "method_name": "XGBoost+SelfTrain",
        "paper_reference": "[16],[18]",
        "metric": "Rejection Seen (False rejections on known classes)",
        "their_result": "15.70%",
        "baseline_result": "4.34%"
      },
      {
        "method_name": "LightGBM+SelfTrain",
        "paper_reference": "[17],[18]",
        "metric": "Rejection Seen (False rejections on known classes)",
        "their_result": "15.70%",
        "baseline_result": "2.89%"
      }
    ],
    "performance_metrics_used": [
      "Risk-Coverage (RC) curve",
      "Area Under Risk-Coverage (AURC)",
      "F1",
      "Precision",
      "Recall",
      "Coverage",
      "Rejection Seen",
      "Rejection Novel",
      "0/1-loss"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can a single semi-supervised framework simultaneously perform malware/benign detection, malware family classification, and identify novel malware families?",
        "Does hierarchical NMF with automatic model selection yield robust latent signatures for malware classification?",
        "Can a reject-option mechanism provide precise novel family identification without training on rare/unknown specimens?"
      ],
      "gaps_identified": [
        "Most ML approaches ignore real-world challenge of detecting novel malware.",
        "Systems are typically designed separately for malware/benign detection and for family classification, increasing operational complexity.",
        "Prior works often treat benignware as a class and lump rare/unknown families into an 'others' class, leading to poor generalization to new specimens.",
        "Semi-supervised learning has been underexplored in malware classification despite potential for better generalization."
      ],
      "limitations": [
        "Preliminary capabilities and results demonstrated on a small subset: three known malware families plus one unseen family from EMBER-2018.",
        "Evaluation uses static features only; no dynamic analysis considered.",
        "Fixed threshold t=1.0 for novelty decision in experiments (no calibration study reported)."
      ],
      "future_work": [],
      "motivation": "Improve adoption of ML for malware by unifying detection and family classification while enabling identification of novel malware families; reduce operational complexity and better handle continuous emergence of new malware.",
      "potential_research_ideas": [
        "Scale evaluation to full EMBER and additional datasets (e.g., BIG 2015, SOREL-20M) with multiple unseen families to validate open-set performance.",
        "Integrate dynamic analysis or multi-view features (static + dynamic + strings) to enrich signatures and improve generalization.",
        "Calibrate and learn data-dependent reject thresholds (e.g., via conformal prediction or calibrated uncertainty) to optimize the coverage–risk trade-off.",
        "Online/streaming update of the signature archive using incremental/online NMF to handle concept drift and emerging families.",
        "Incorporate metric learning or contrastive objectives to improve separability of signature space prior to clustering.",
        "Combine with active learning to query labels for ambiguous clusters and accelerate discovery of new families.",
        "Assess and harden against adversarial manipulation (obfuscation/packing) using robust NMF (e.g., beta-divergence, sparsity, outlier-robust losses).",
        "Map latent signatures to semantic behaviors/APIs to improve analyst interpretability and triage utility."
      ],
      "architectural_improvement_recommendations": [
        "Replace fixed cosine-threshold reject-option with calibrated confidence (temperature scaling, Platt scaling) or conformal prediction p-values.",
        "Adopt robust/regularized NMF variants (e.g., beta-divergence, sparsity constraints, ARD) to mitigate noise and packing artifacts.",
        "Introduce an embedding layer with contrastive learning prior to NMF to yield more discriminative nonnegative factors.",
        "Implement online/incremental NMF and periodic reclustering to maintain the signature archive under data drift.",
        "Learn a similarity metric (e.g., Mahalanobis in NNLS space) instead of pure cosine to better match class manifolds.",
        "Fuse multi-modal features (byte histograms, section metadata, imports, n-grams, dynamic traces) with late fusion in the signature archive."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "pydnmfk (NMFk)",
        "XGBoost",
        "LightGBM",
        "Optuna",
        "SmartTensors (referenced platform)"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces MalwareDNA, a semi-supervised framework that unifies malware/benign detection, malware family classification, and novel family identification.",
      "Leverages hierarchical NMFk with automatic model determination to build an archive of latent malware and benign signatures.",
      "Implements a reject-option via NNLS projection and cosine similarity to precisely identify novel malware families without training on rare specimens.",
      "Demonstrates preliminary results on EMBER-2018 subset: AURC=0.02; at 84.3% coverage, F1=0.975 and 100% true rejections for the unseen family (ramnit).",
      "Benchmarks against strong baselines (XGBoost/LightGBM with/without SelfTrain), showing large performance gaps on the unified and open-set task."
    ]
  },
  {
    "arxiv_id": "2310.05939v1",
    "title": "Learning Cyber Defence Tactics from Scratch with Multi-Agent Reinforcement Learning",
    "authors": "Jacob Wiebe; Ranwa Al Mallah; Li Li",
    "abstract": "Recent advancements in deep learning techniques have opened new possibilities for designing solutions for autonomous cyber defence. Teams of intelligent agents in computer network defence roles may reveal promising avenues to safeguard cyber and kinetic assets. In a simulated game environment, agents are evaluated on their ability to jointly mitigate attacker activity in host-based defence scenarios. Defender systems are evaluated against heuristic attackers with the goals of compromising network confidentiality, integrity, and availability. Value-based Independent Learning and Centralized Training Decentralized Execution (CTDE) cooperative Multi-Agent Reinforcement Learning (MARL) methods are compared revealing that both approaches outperform a simple multi-agent heuristic defender. This work demonstrates the ability of cooperative MARL to learn effective cyber defence tactics against varied threats.",
    "published_date": "2023-08-25",
    "pdf_link": "https://arxiv.org/pdf/2310.05939v1",
    "paper_types": [
      "benchmark",
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Autonomous Cyber Defence (ACD)",
      "specific_problem": "Learning cooperative multi-agent defensive tactics for host-based detection and mitigation of lateral movement in an enterprise network simulation",
      "attack_types": [
        "lateral movement",
        "confidentiality breach (data exfiltration/espionage)",
        "integrity violation (file tampering)",
        "availability attack (Denial of Service via malware process)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Multi-Agent Reinforcement Learning (CTDE, value-based)",
        "specific": "QMIX",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Multi-Agent Reinforcement Learning (Independent Learning, value-based)",
        "specific": "Independent Q-Learning with RNN-DQN (PyMARL2 implementation)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Recurrent Neural Networks in value-based RL",
        "specific": "RNN within DQN-style agents (experience replay, epsilon-greedy)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Reinforcement Learning",
      "Multi-Agent Reinforcement Learning",
      "Model-free",
      "Value-based",
      "Partially Observable (POMDP)"
    ],
    "datasets": [
      {
        "name": "CyMARL (Cyber MARL) environment",
        "type": "synthetic",
        "domain": "simulated_network_game",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "CybORG simulator",
        "type": "public",
        "domain": "cyber_operations_simulator",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CAGE Challenge 2-inspired scenarios",
        "type": "synthetic",
        "domain": "benchmark_simulation",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Simple multi-agent heuristic defender",
        "paper_reference": null,
        "metric": "cumulative return (average episodic return)",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "cumulative reward/return per episode",
      "weighted host compromise score (by host importance)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can cooperative MARL learn effective cyber defence tactics in host-based enterprise network scenarios?",
        "How do value-based Independent Learning and CTDE (QMIX) cooperative MARL methods compare for ACD?",
        "Can cooperative MARL adapt to varied attacker objectives (confidentiality, integrity, availability)?"
      ],
      "gaps_identified": [
        "Lack of cooperative MARL environments tailored to enterprise network defence tasks",
        "Need to coordinate multi-agent tactical decision-making under partial observability and non-stationarity",
        "Simulation-to-reality gap for validating RL-based cyber defence systems"
      ],
      "limitations": [
        "Evaluation limited to simulation (CybORG-based) with heuristic attackers",
        "Scope restricted to model-free, value-based MARL (IQL and QMIX) for comparison",
        "No self-play against RL-trained attackers; adversarial attacks on defenders outside scope",
        "Partial observability and sample efficiency constraints; many iterations required to learn policies"
      ],
      "future_work": [],
      "motivation": "Enable autonomous, tactical-level decision-making in cyber defence to mitigate human limitations in attention, cognitive load, and reaction time, and to coordinate actions across large network search spaces.",
      "potential_research_ideas": [
        "Introduce learned inter-agent communication (message passing) to improve coordination under partial observability",
        "Adopt self-play to co-evolve defenders against RL-capable attackers and study robustness",
        "Bridge sim-to-real via emulation-based validation and domain randomization of host/process behaviors",
        "Design hierarchical MARL (high-level tasking + low-level host actions) for complex playbooks",
        "Incorporate opponent modeling and attacker intent inference into CTDE critics",
        "Explore alternative value decomposition (e.g., QTRAN, QPLEX, VDN) and contrast with QMIX",
        "Augment observations with richer host telemetry (process trees, file hashes) and study feature ablations",
        "Curriculum learning over network size/topology and attacker sophistication to improve generalization"
      ],
      "architectural_improvement_recommendations": [
        "Replace or complement QMIX with QPLEX/QTRAN/VDN and attention-based mixers for better credit assignment",
        "Use centralized critics with recurrent/transformer encoders for longer temporal credit assignment",
        "Add differentiable communication channels (e.g., CommNet, TarMAC) under CTDE",
        "Apply curriculum learning and reward shaping aligned with host importance and incident response priorities",
        "Leverage parameter sharing with role specialization (user-subnet vs operational-subnet) and role-conditioned policies",
        "Integrate opponent modeling modules to predict attacker next actions and enable anticipatory defense",
        "Evaluate off-policy corrections and prioritized experience replay tailored to multi-agent non-stationarity"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "PyMARL2"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Simulation-to-reality gap; emulation required for validation is resource-intensive",
        "Non-stationarity and coordination challenges in multi-agent settings",
        "High sample complexity and training time for partially observable environments"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "CyMARL is introduced as the first cooperative MARL training environment for enterprise network defence tasks; it extends CybORG with new game types, actions, network topologies, and a PyMARL interface.",
      "Comparison of cooperative MARL approaches (independent learning vs centrally-learned policies via CTDE) in an ACD context.",
      "Demonstration of adaptability of cooperative MARL to multiple attacker types and action sets.",
      "Both QMIX and IQL approaches outperform a simple multi-agent heuristic defender: \"both approaches outperform a simple multi-agent heuristic defender.\""
    ]
  },
  {
    "arxiv_id": "2308.07925v1",
    "title": "Domain-Adaptive Device Fingerprints for Network Access Authentication Through Multifractal Dimension Representation",
    "authors": "Benjamin Johnson; Bechir Hamdaoui",
    "abstract": "RF data-driven device fingerprinting through the use of deep learning has recently surfaced as a potential solution for automated network access authentication. Traditional approaches are commonly susceptible to the domain adaptation problem where a model trained on data from one domain performs badly when tested on data from a different domain. Some examples of a domain change include varying the device location or environment and varying the time or day of data collection. In this work, we propose using multifractal analysis and the variance fractal dimension trajectory (VFDT) as a data representation input to the deep neural network to extract device fingerprints that are domain generalizable. We analyze the effectiveness of the proposed VFDT representation in detecting device-specific signatures from hardware-impaired IQ signals, and evaluate its robustness in real-world settings, using an experimental testbed of 30 WiFi-enabled Pycom devices under different locations and at different scales. Our results show that the VFDT representation improves the scalability, robustness and generalizability of the deep learning models significantly compared to when using raw IQ data.",
    "published_date": "2023-08-15",
    "pdf_link": "https://arxiv.org/pdf/2308.07925v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Wireless Security",
      "subdomain": "Physical-layer device authentication (RF fingerprinting)",
      "specific_problem": "Domain-adaptive RF device fingerprinting for network access authentication",
      "attack_types": [
        "impersonation"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "CNN",
        "specific": null,
        "novel_contribution": "Introduces a new input representation for RF IQ signals — the Variance Fractal Dimension Trajectory (VFDT) of I and Q components — fed as a 2x1024 input to a CNN to obtain domain-generalizable device fingerprints."
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": null,
        "novel_contribution": "Baseline uses the same CNN architecture but with raw IQ samples as input (no VFDT)."
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Pycom30 WiFi RF IQ dataset (authors' testbed)",
        "type": "proprietary",
        "domain": "rf_iq",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "LoPy10 VFDT separability dataset (authors' collection)",
        "type": "proprietary",
        "domain": "rf_iq",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Simulated RF impairment datasets (PA nonlinearity, IQ imbalance, phase noise)",
        "type": "synthetic",
        "domain": "rf_iq_simulated",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "CNN with raw IQ input (train 1m, test 1m)",
        "paper_reference": null,
        "metric": "Testing Accuracy (%)",
        "their_result": null,
        "baseline_result": "97.1%"
      },
      {
        "method_name": "CNN with raw IQ input (train 1m, test 2m)",
        "paper_reference": null,
        "metric": "Testing Accuracy (%)",
        "their_result": null,
        "baseline_result": "16.4%"
      },
      {
        "method_name": "CNN with raw IQ input (train 1m, test 3m)",
        "paper_reference": null,
        "metric": "Testing Accuracy (%)",
        "their_result": null,
        "baseline_result": "6.4%"
      },
      {
        "method_name": "CNN with raw IQ input (train 1m, test Random location 1)",
        "paper_reference": null,
        "metric": "Testing Accuracy (%)",
        "their_result": null,
        "baseline_result": "14.8%"
      },
      {
        "method_name": "CNN with raw IQ input (train 1m, test Random location 2)",
        "paper_reference": null,
        "metric": "Testing Accuracy (%)",
        "their_result": null,
        "baseline_result": "19.5%"
      }
    ],
    "performance_metrics_used": [
      "Accuracy"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can multifractal analysis (VFDT) of I/Q signals capture device-specific hardware impairments that are robust across domains?",
        "How do individual hardware impairments (PA nonlinearity, IQ imbalance, phase noise) affect the VFDT behavior and separability?",
        "Does using VFDT as the input representation improve scalability, robustness, and generalizability of device classification compared to raw IQ data?"
      ],
      "gaps_identified": [
        "Deep learning models for RF fingerprinting often fail to generalize across domains (channel, time, receiver, location).",
        "Feature attribution is unclear in deep models; they may latch onto channel conditions rather than hardware impairments.",
        "Prior multifractal or preprocessing approaches did not examine domain adaptation effects or scalability to many devices."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Address the domain adaptation/generalization problem in RF device fingerprinting by introducing a representation that encodes hardware impairments while reducing reliance on channel-specific features.",
      "potential_research_ideas": [
        "Combine VFDT with domain generalization frameworks (e.g., invariant risk minimization, adversarial domain adaptation) to further improve cross-domain robustness.",
        "Self-supervised or contrastive pretraining on VFDT sequences to reduce labeled data needs and improve generalization.",
        "Evaluate and adapt VFDT-based models across broader domains (outdoor, NLoS, mobility, cross-buildings, cross-days) and across protocols (WiFi, Bluetooth, LoRa, 5G NR).",
        "Integrate multi-antenna/multi-receiver fusion of VFDT features to mitigate channel effects and enhance fingerprint stability.",
        "Adversarial robustness studies: assess vulnerability to RF spoofing/replay or over-the-air adversarial perturbations targeting VFDT features.",
        "Hardware impairment estimation as auxiliary tasks (multi-task learning) jointly with identification to regularize features."
      ],
      "architectural_improvement_recommendations": [
        "Use a dual-branch network with shared weights for I and Q VFDT streams plus cross-attention or late fusion.",
        "Replace or augment CNN with temporal models (1D CNN-TCN, transformers) to capture longer-range structure in VFDT trajectories.",
        "Make VFDT window size/offset differentiable and learnable (learned feature extraction) within an end-to-end pipeline.",
        "Apply domain-adversarial layers (DANN/gradient reversal) on VFDT features to explicitly remove domain information.",
        "Employ strong RF-specific data augmentation (frequency/phase jitter, channel simulation) at the VFDT level for better generalization."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "PyTorch",
        "MATLAB",
        "GNU Radio"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Not specified. Data collected with USRP B210; examples mention 45 MS/s sampling and CNN input size 2x1024 VFDT windows."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Indoor lab WiFi testbed with 30 Pycom devices (LoPy/Fipy) and a USRP B210 receiver across multiple locations/distances (1m, 2m, 3m, random).",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Domain shift across locations and channel conditions.",
        "Selection/tuning of VFDT window length and offset parameters.",
        "Requirement for labeled RF data per device for supervised training.",
        "Environmental variability (multipath, mobility) not fully characterized."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes a multifractal analysis-based input representation (Variance Fractal Dimension Trajectory, VFDT) for RF device fingerprinting.",
      "Analyzes how key hardware impairments (PA nonlinearity, IQ imbalance, phase noise) affect VFDT and enable separability (via MATLAB simulations).",
      "Implements and evaluates a CNN classifier that consumes I/Q VFDT sequences, demonstrating improved robustness, generalizability, and scalability over raw IQ inputs.",
      "Builds a real-world testbed with 30 WiFi-enabled Pycom devices across multiple locations/distances and shows baseline degradation with domain shift (e.g., 97% to 16% when training at 1m and testing at 2m)."
    ]
  },
  {
    "arxiv_id": "2309.03660v1",
    "title": "Learning from Limited Heterogeneous Training Data: Meta-Learning for Unsupervised Zero-Day Web Attack Detection across Web Domains",
    "authors": "Peiyang Li; Ye Wang; Qi Li; Zhuotao Liu; Ke Xu; Ju Ren; Zhiying Liu; Ruilin Lin",
    "abstract": "Recently unsupervised machine learning based systems have been developed to detect zero-day Web attacks, which can effectively enhance existing Web Application Firewalls (WAFs). However, prior arts only consider detecting attacks on specific domains by training particular detection models for the domains. These systems require a large amount of training data, which causes a long period of time for model training and deployment. In this paper, we propose RETSINA, a novel meta-learning based framework that enables zero-day Web attack detection across different domains in an organization with limited training data. Specifically, it utilizes meta-learning to share knowledge across these domains, e.g., the relationship between HTTP requests in heterogeneous domains, to efficiently train detection models. Moreover, we develop an adaptive preprocessing module to facilitate semantic analysis of Web requests across different domains and design a multi-domain representation method to capture semantic correlations between different domains for cross-domain model training. We conduct experiments using four real-world datasets on different domains with a total of 293M Web requests. The experimental results demonstrate that RETSINA outperforms the existing unsupervised Web attack detection methods with limited training data, e.g., RETSINA needs only 5-minute training data to achieve comparable detection performance to the existing methods that train separate models for different domains using 1-day training data. We also conduct real-world deployment in an Internet company. RETSINA captures on average 126 and 218 zero-day attack requests per day in two domains, respectively, in one month.",
    "published_date": "2023-09-07",
    "pdf_link": "https://arxiv.org/pdf/2309.03660v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Web Application Security",
      "subdomain": "Web Attack Detection / Intrusion Detection for Web Applications",
      "specific_problem": "Unsupervised zero-day web attack detection across multiple heterogeneous domains with limited training data via meta-learning",
      "attack_types": [
        "zero-day web attacks",
        "SQL injection"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Meta-learning",
        "specific": "MAML-style two-loop meta-learning with first-order update/approximation",
        "novel_contribution": "Learns a universal initial model from auxiliary domains and adapts it to target domains with limited data to enable cross-domain unsupervised web attack detection"
      },
      {
        "type": "primary",
        "category": "Sequence-to-sequence Autoencoder",
        "specific": "Seq2seq encoder-decoder with embedding layer and generator (as in ZeroWall)",
        "novel_contribution": "Used as the base unsupervised detector within the meta-learning framework to model benign request patterns"
      },
      {
        "type": "primary",
        "category": "Representation Learning / Embedding Alignment",
        "specific": "Orthogonal transformation-based token alignment (Procrustes-like) to a base domain",
        "novel_contribution": "Multi-domain representation mapping tokens from heterogeneous domains into a shared feature space via weighted sums over a universal token set"
      },
      {
        "type": "primary",
        "category": "Adaptive Tokenization / Preprocessing",
        "specific": "Automatic regex-based token merging and low-frequency token handling",
        "novel_contribution": "Domain-specific automatically generated merging strategies to eliminate inessential tokens and standardize semantics across domains"
      },
      {
        "type": "baseline",
        "category": "Unsupervised Autoencoder",
        "specific": "ZeroWall",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Supervised Learning",
        "specific": null,
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Unsupervised",
      "Meta-learning"
    ],
    "datasets": [
      {
        "name": "Domain A web request dataset",
        "type": "proprietary",
        "domain": "web_requests",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Domain B web request dataset",
        "type": "proprietary",
        "domain": "web_requests",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Domain C web request dataset",
        "type": "proprietary",
        "domain": "web_requests",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Domain D web request dataset",
        "type": "proprietary",
        "domain": "web_requests",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Production deployment logs (two domains, one month)",
        "type": "proprietary",
        "domain": "web_requests",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "352 Web vulnerabilities from a GitHub project [6] (used for coverage analysis)",
        "type": "public",
        "domain": "web_vulnerabilities",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "ZeroWall (unsupervised web attack detection)",
        "paper_reference": "[60]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Knowledge-sharing multi-task security framework",
        "paper_reference": "[69]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Supervised web attack detection method",
        "paper_reference": "[74]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Supervised web attack detection method",
        "paper_reference": "[81]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Per-domain unsupervised detectors (e.g., ZeroWall) trained separately per domain",
        "paper_reference": null,
        "metric": "Training data duration required to reach comparable detection performance",
        "their_result": "“RETSINA needs only 5-minute training data to achieve comparable detection performance...”",
        "baseline_result": "Existing methods require “1-day training data” to reach comparable performance"
      }
    ],
    "performance_metrics_used": [],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can meta-learning enable robust unsupervised zero-day web attack detection across heterogeneous domains with only limited training data per target domain?",
        "How to construct a shared feature space to capture semantic correlations between tokens from different web domains?",
        "Can adaptive preprocessing automatically standardize heterogeneous HTTP requests for effective cross-domain training and detection?"
      ],
      "gaps_identified": [
        "Prior systems train per-domain models that do not generalize across domains and require large training data.",
        "Frequent web service updates necessitate frequent retraining, but collecting sufficient production data is slow and may raise privacy concerns.",
        "Lexical features of HTTP requests are not in the same semantic space across domains, hindering knowledge sharing.",
        "Transferring knowledge between domains is hard because model structures and domain semantics vary."
      ],
      "limitations": [
        "Focuses only on URLs and bodies with content type x-www-form-urlencoded; other body types are not considered.",
        "HTTP headers are skipped, relying on traditional WAF rules to detect header anomalies.",
        "Multi-domain alignment assumes some overlapping tokens and shared development specifications; limited overlap or heavy semantic overloading may reduce alignment quality.",
        "Adaptive preprocessing relies on auto-selected regex sets; mis-specification may merge informative tokens or fail to generalize to novel formats."
      ],
      "future_work": [],
      "motivation": "Enable rapid, effective zero-day web attack detection across multiple, frequently updated domains with limited data by sharing knowledge across domains while reducing the burden of per-domain data collection and training.",
      "potential_research_ideas": [
        "Extend the approach to handle JSON, multipart/form-data, and other body types with learned parsers or schema inference.",
        "Incorporate contrastive cross-domain representation learning to strengthen token/field alignment without relying solely on overlapping tokens.",
        "Develop federated meta-learning to share knowledge across organizations while preserving data privacy.",
        "Investigate robustness to data poisoning and adversarial evasion in the unsupervised/meta-learning setting.",
        "Add probabilistic calibration and uncertainty estimation for adaptive thresholding and WAF triage.",
        "Combine semantic parsing with program analysis to better capture API parameter semantics and constraints.",
        "Online/continual meta-learning to adapt to rapid service updates without catastrophic forgetting.",
        "Leverage large language models to assist adaptive preprocessing and semantic normalization of heterogeneous parameters."
      ],
      "architectural_improvement_recommendations": [
        "Replace or augment the seq2seq autoencoder with Transformer-based sequence models for better long-range dependencies.",
        "Use variational or denoising autoencoders and masked language modeling objectives to improve anomaly sensitivity.",
        "Introduce contrastive alignment losses (e.g., InfoNCE) between domains alongside orthogonal mapping to enforce semantic proximity.",
        "Adopt second-order MAML or implicit meta-gradients where computationally feasible to improve adaptation quality.",
        "Implement a learnable, differentiable tokenizer or byte-level modeling to reduce reliance on hand-crafted regex merging.",
        "Incorporate domain adversarial training to promote domain-invariant representations.",
        "Calibrate anomaly scores with extreme value theory or conformal prediction to control false positives across domains.",
        "Integrate lightweight PEFT/LoRA adapters for fast per-domain adaptation with minimal data."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Production deployment in an Internet company alongside existing WAFs",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Heterogeneity of domains and parameters requires robust preprocessing and alignment.",
        "Frequent service updates necessitate rapid re-training/adaptation with limited data.",
        "Data collection at scale may raise preprocessing overhead and privacy concerns.",
        "Integration with WAF workflows and alert triage in production."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "RETSINA: first unsupervised meta-learning framework for zero-day web attack detection across multiple heterogeneous domains with limited data.",
      "Adaptive preprocessing that automatically generates domain-specific strategies to convert raw HTTP requests into structured token sequences.",
      "Multi-domain representation with unified feature space via token alignment and embedding mapping across domains.",
      "Extensive evaluation on 4 real-world domains with 293M requests showing that 5 minutes of training data can match per-domain baselines trained on 1 day of data.",
      "Real-world production deployment detecting on average 126 and 218 zero-day attack requests per day in two domains over one month."
    ]
  },
  {
    "arxiv_id": "2308.05247v2",
    "title": "TUBERAIDER: Attributing Coordinated Hate Attacks on YouTube Videos to their Source Communities",
    "authors": "Mohammad Hammas Saeed; Kostantinos Papadamou; Jeremy Blackburn; Emiliano De Cristofaro; Gianluca Stringhini",
    "abstract": "Alas, coordinated hate attacks, or raids, are becoming increasingly common online. In a nutshell, these are perpetrated by a group of aggressors who organize and coordinate operations on a platform (e.g., 4chan) to target victims on another community (e.g., YouTube). In this paper, we focus on attributing raids to their source community, paving the way for moderation approaches that take the context (and potentially the motivation) of an attack into consideration. We present TUBERAIDER, an attribution system achieving over 75% accuracy in detecting and attributing coordinated hate attacks on YouTube videos. We instantiate it using links to YouTube videos shared on 4chan's /pol/ board, r/The_Donald, and 16 Incels-related subreddits. We use a peak detector to identify a rise in the comment activity of a YouTube video, which signals that an attack may be occurring. We then train a machine learning classifier based on the community language (i.e., TF-IDF scores of relevant keywords) to perform the attribution. We test TUBERAIDER in the wild and present a few case studies of actual aggression attacks identified by it to showcase its effectiveness.",
    "published_date": "2023-08-09",
    "pdf_link": "https://arxiv.org/pdf/2308.05247v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Online Safety and Platform Integrity",
      "subdomain": "Coordinated Harassment and Raid Detection",
      "specific_problem": "Attribution of coordinated hate raids on YouTube videos to their originating communities",
      "attack_types": [
        "coordinated harassment",
        "brigading",
        "hate speech raids"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Bag-of-Words / TF-IDF + Similarity-based Classification",
        "specific": null,
        "novel_contribution": "Community-level TF-IDF language modeling combined with peak detection to attribute raids to source communities"
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "Sentence-BERT (SBERT) via SentenceTransformer",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Signal Processing",
        "specific": "Peak detection and cross-correlation for lag estimation",
        "novel_contribution": "Use of cross-correlation to control for lag between source thread activity and YouTube comment peaks to improve attribution accuracy"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "4chan /pol/ dataset (Papasavva et al.)",
        "type": "public",
        "domain": "social_media_posts",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Reddit Pushshift dump (filtered to r/The_Donald and 16 Incels-related subreddits)",
        "type": "public",
        "domain": "social_media_posts",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "YouTube comments via YouTube Data API for linked videos",
        "type": "proprietary",
        "domain": "social_media_comments",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Sentence-BERT cosine similarity",
        "paper_reference": "Reimers and Gurevych (SBERT)",
        "metric": "Accuracy",
        "their_result": "TF-IDF correctly attributes 790/2,242 videos; SBERT correctly attributes 756/2,242 videos overall",
        "baseline_result": "Per-community with SBERT: /pol/: 624/1,176 (53%); r/The_Donald: 97/985 (10%); Incels: 35/81 (43%)"
      }
    ],
    "performance_metrics_used": [
      "Accuracy"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Do YouTube videos exhibit a spike in commenting activity after being linked on source communities, indicating a potential coordinated attack?",
        "Can the characteristic language of a source community be modeled to attribute the origin of coordinated hate attacks on YouTube videos?"
      ],
      "gaps_identified": [
        "Past research focused on detecting hateful content or users, but largely neglected attribution of attacks to their source communities.",
        "Bot detection techniques are ineffective for human-coordinated raids."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Enable moderation approaches that incorporate context and motivation by attributing coordinated hate attacks to the organizing community.",
      "potential_research_ideas": [
        "Fine-tune a domain-adapted transformer on source-community language for attribution and compare against TF-IDF at scale.",
        "Develop early-warning models that predict likely raids before peaks fully materialize using thread dynamics and link-sharing features.",
        "Extend attribution to multi-origin scenarios (multi-label) when videos are shared across multiple communities with overlapping timelines.",
        "Cross-platform generalization: apply the framework to other targets (e.g., Twitter/X, TikTok, Twitch) and sources (e.g., Telegram, Gab, KiwiFarms).",
        "Incorporate user-level temporal interaction networks to improve attribution beyond language-only signals.",
        "Robustness evaluation under adversarial language obfuscation (code words, intentional misspellings) and coordinated timing shifts.",
        "Active learning pipeline with human-in-the-loop for borderline attributions and emerging communities/lingo.",
        "Automated case triaging for moderators using contextual summaries and risk scoring combining toxicity markers with attribution confidence."
      ],
      "architectural_improvement_recommendations": [
        "Replace static TF-IDF prototypes with community-specific transformer encoders fine-tuned on historical thread/comment pairs.",
        "Temporal sequence models (e.g., TCNs or Transformers with time embeddings) over comment/thread activity to jointly learn timing and language patterns.",
        "Multi-view learning combining language, temporal cross-correlation, and metadata features (e.g., poster activity, link position in thread).",
        "Continual learning to handle language drift with periodic re-training on recent data while retaining past community-specific jargon.",
        "Calibrated confidence estimation (e.g., temperature scaling) to support safe moderation workflows.",
        "Multi-label classifier with attention over time windows to handle overlapping attacks from multiple communities.",
        "Lightweight approximate nearest-neighbor over learned embeddings for scalable retrieval-based attribution."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "PyTorch",
        "SentenceTransformer"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Cross-platform setting with source communities (4chan /pol/, Reddit subreddits) and YouTube as target platform",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "TUBERAIDER: a system to attribute coordinated hate attacks on YouTube videos to their source communities using peak detection and community language modeling.",
      "Demonstrates that community-specific language modeled via TF-IDF can attribute raids with accuracy above 75% overall and improved per-community performance after controlling for lag.",
      "Empirical analysis on large-scale cross-platform data linking 4chan /pol/, r/The_Donald, and 16 Incels-related subreddits to YouTube videos and comments.",
      "In-the-wild evaluation identifying 700 likely raided videos and case studies showing increased toxicity, abuse, and targeted hate.",
      "Comparison showing a TF-IDF-based approach outperforms a Sentence-BERT cosine similarity baseline for attribution."
    ]
  },
  {
    "arxiv_id": "2310.06951v2",
    "title": "Sanitizing Hidden Information with Diffusion Models",
    "authors": "Preston K. Robinette; Daniel Moyer; Taylor T. Johnson",
    "abstract": "Information hiding is the process of embedding data within another form of data, often to conceal its existence or prevent unauthorized access. This process is commonly used in various forms of secure communications (steganography) that can be used by bad actors to propagate malware, exfiltrate victim data, and discreetly communicate. Recent work has utilized deep neural networks to remove this hidden information in a defense mechanism known as sanitization. Previous deep learning works, however, are unable to scale efficiently beyond the MNIST dataset. In this work, we present a novel sanitization method called DM-SUDS that utilizes a diffusion model framework to sanitize/remove hidden information from image-into-image universal and dependent steganography from CIFAR-10 and ImageNet datasets. We evaluate DM-SUDS against three different baselines using MSE, PSNR, SSIM, and NCC metrics and provide further detailed analysis through an ablation study. DM-SUDS outperforms all three baselines and significantly improves image preservation MSE by 50.44%, PSNR by 12.69%, SSIM by 11.49%, and NCC by 3.26% compared to previous deep learning approaches. Additionally, we introduce a novel evaluation specification that considers the successful removal of hidden information (safety) as well as the resulting quality of the sanitized image (utility). We further demonstrate the versatility of this method with an application in an audio case study, demonstrating its broad applicability to additional domains.",
    "published_date": "2023-10-10",
    "pdf_link": "https://arxiv.org/pdf/2310.06951v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Multimedia and Data Security",
      "subdomain": "Steganography and Watermarking Defense",
      "specific_problem": "Blind sanitization (active steganalysis) to remove hidden information from media while preserving utility",
      "attack_types": [
        "LSB steganography",
        "Dependent Deep Hiding (DDH)",
        "Universal Deep Hiding (UDH)",
        "JPEG-resistant deep steganography (PRIS)",
        "Invisible watermarking (applicability noted)",
        "Text-into-audio steganography (case study)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Diffusion Model",
        "specific": "DDPM-style denoising diffusion with U-Net denoiser; OpenAI improved-diffusion pretrained model",
        "novel_contribution": "First blind diffusion-based sanitizer (DM-SUDS) that denoises potential containers to remove hidden information while preserving image quality; introduces timestep-controlled sanitization and a direct-denoise variant; paired with a new safety-utility evaluation specification."
      },
      {
        "type": "baseline",
        "category": "Variational Autoencoder",
        "specific": "SUDS (prior VAE-based sanitizer)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Noise-based transform",
        "specific": "Gaussian Noise (spatial domain)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Noise-based transform",
        "specific": "Gaussian Noise in DCT domain (DCT-Noise)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Self-supervised (denoising diffusion pretraining)",
      "Transfer Learning"
    ],
    "datasets": [
      {
        "name": "CIFAR-10",
        "type": "public",
        "domain": "images_natural",
        "link": "https://www.cs.toronto.edu/~kriz/cifar.html",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "ImageNet (subset, 128x128)",
        "type": "public",
        "domain": "images_natural",
        "link": "https://image-net.org/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "SUDS (VAE-based sanitizer)",
        "paper_reference": "[26] (authors' prior work)",
        "metric": "MSE, PSNR, SSIM, NCC; sanitization success IP NCC ≥ 0.95 and SE NCC ≤ 0.30",
        "their_result": "“DM-SUDS ... improves image preservation MSE by 50.44%, PSNR by 12.69%, SSIM by 11.49%, and NCC by 3.26% compared to previous deep learning approaches.” Also the only method to successfully sanitize all three unseen hiding methods while preserving image quality.",
        "baseline_result": "SUDS removes secrets (low SE NCC) but degrades image quality (low IP NCC), failing the combined sanitization criterion on CIFAR-10."
      },
      {
        "method_name": "Gaussian Noise (spatial)",
        "paper_reference": null,
        "metric": "MSE, PSNR, SSIM, NCC; sanitization success criterion",
        "their_result": "DM-SUDS outperforms Gaussian Noise on image preservation and meets the sanitization criterion on LSB, DDH, UDH.",
        "baseline_result": "Gaussian Noise either under-sanitizes or over-degrades images; fails combined criterion in cases where DM-SUDS succeeds."
      },
      {
        "method_name": "DCT-Noise (frequency domain)",
        "paper_reference": null,
        "metric": "MSE, PSNR, SSIM, NCC; sanitization success criterion",
        "their_result": "DM-SUDS outperforms DCT-Noise on image preservation and meets the sanitization criterion on LSB, DDH, UDH.",
        "baseline_result": "DCT-Noise tends to degrade image quality and/or insufficiently removes secrets; fails combined criterion in cases where DM-SUDS succeeds."
      }
    ],
    "performance_metrics_used": [
      "MSE",
      "PSNR",
      "SSIM",
      "NCC",
      "Sanitization success definition: IP NCC ≥ 0.95 and SE NCC ≤ 0.30",
      "Ablation over diffusion timesteps t (25–1000)",
      "Runtime (ms) reported per method (table)"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "RQ1: Is DM-SUDS able to successfully sanitize hidden information and how does it compare to previous sanitization methods (SUDS, Gaussian Noise, DCT-Noise) on CIFAR-10 LSB, DDH, UDH?",
        "RQ2: Does the number of diffusion timesteps affect sanitization performance (t ∈ {25,50,...,1000})?",
        "RQ3: Is added noise necessary for the sanitization process (direct denoise vs. noisy input)?",
        "RQ4: Does DM-SUDS extend to JPEG-resistant steganography and to the ImageNet dataset (DDH, PRIS)?"
      ],
      "gaps_identified": [
        "Non-blind steganalysis/sanitization fails on novel hiding techniques.",
        "Prior deep learning sanitizers did not scale beyond MNIST; image preservation deteriorated on more complex datasets (e.g., CIFAR-10 for VAE-based SUDS).",
        "PixelSteganalysis is inefficient (minutes per CIFAR-10 image) and requires prior knowledge of high-frequency areas.",
        "Existing evaluations often considered only secret elimination but not preservation/utility."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Develop a blind, scalable, and utility-preserving sanitizer to remove hidden information from media using diffusion models, overcoming preservation/scalability issues of prior VAE-based methods and non-blind approaches.",
      "potential_research_ideas": [
        "Design a guided diffusion sanitizer that explicitly optimizes a dual-objective: minimize NCC(secret) while maximizing similarity to cover, using a learned or proxy secret detector for guidance.",
        "Automate timestep selection via reinforcement learning or Bayesian optimization to balance safety and utility per-image.",
        "Extend DM-SUDS to video, 3D, and document/PDF steganography, including text and table content protection.",
        "Develop a cross-modal sanitizer framework (images↔audio↔video) with shared backbones and modality-specific adapters.",
        "Investigate distillation or consistency models for real-time sanitization with few reverse steps, enabling deployment on edge devices.",
        "Combine diffusion with frequency-domain branches to better target transform-domain hiding and watermarks (e.g., JPEG-resistant methods).",
        "Formalize robustness guarantees: certify secret elimination under bounded perturbations or compression transforms.",
        "Incorporate adversarial training against adaptive hiders trained end-to-end to evade the sanitizer."
      ],
      "architectural_improvement_recommendations": [
        "Add a classifier-free guidance or plug-and-play guidance term using a stego-detector to drive denoising away from secret content without harming utility.",
        "Introduce a dual-branch UNet operating in spatial and DCT wavelet domains, with cross-attention to suppress frequency-localized secrets.",
        "Use latent diffusion (VAE encoder + UNet) to preserve high-frequency details while enabling faster inference.",
        "Learn an adaptive timestep schedule per-image (or early stopping) based on predicted safety/utility signals to minimize compute and over-smoothing.",
        "Apply diffusion distillation or consistency models to reduce reverse steps (e.g., from 1000 to 10–20) while maintaining sanitization.",
        "Train with a multi-objective loss that directly penalizes secret recoverability (e.g., low NCC to ground-truth secret) when available in synthetic training, improving targeted removal."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/pkrobinette/dmsuds_steg",
      "frameworks": [
        "PyTorch"
      ],
      "reproducibility_score": "high",
      "computational_requirements": "GPU recommended; diffusion reverse with t up to 1000 steps (commonly t=500 in experiments); experiments on 1000 CIFAR-10 image pairs and 500 ImageNet containers; uses OpenAI improved-diffusion U-Net."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": "Varies with diffusion steps; ablations run t from 25 to 1000; runtime reported in ms per method in tables.",
      "deployment_challenges": [
        "Computational cost of diffusion reverse steps for high-resolution images.",
        "Selecting/controlling timestep t to balance safety vs. utility.",
        "Potential sensitivity to different compression pipelines and imaging artifacts beyond tested settings.",
        "Integrating into content pipelines without degrading benign media quality.",
        "Adapting to evolving, adversarial deep-hiding techniques."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Implementation of a novel blind diffusion-based sanitization framework (DM-SUDS) for image-into-image steganography.",
      "Demonstration of capabilities: comparisons to three baselines, timestep flexibility (ablation), direct denoising, scalability, and robustness (including JPEG-resistant PRIS and ImageNet).",
      "Development of a novel sanitization specification combining safety (secret elimination) and utility (image preservation) with NCC thresholds.",
      "Application to audio: first deep learning sanitizer to remove information hidden in audio containers (text-into-audio)."
    ]
  },
  {
    "arxiv_id": "2308.09413v1",
    "title": "A Graph-based Stratified Sampling Methodology for the Analysis of (Underground) Forums",
    "authors": "Giorgio Di Tizio; Gilberto Atondo Siu; Alice Hutchings; Fabio Massacci",
    "abstract": "[Context] Researchers analyze underground forums to study abuse and cybercrime activities. Due to the size of the forums and the domain expertise required to identify criminal discussions, most approaches employ supervised machine learning techniques to automatically classify the posts of interest. [Goal] Human annotation is costly. How to select samples to annotate that account for the structure of the forum? [Method] We present a methodology to generate stratified samples based on information about the centrality properties of the population and evaluate classifier performance. [Result] We observe that by employing a sample obtained from a uniform distribution of the post degree centrality metric, we maintain the same level of precision but significantly increase the recall (+30%) compared to a sample whose distribution is respecting the population stratification. We find that classifiers trained with similar samples disagree on the classification of criminal activities up to 33% of the time when deployed on the entire forum.",
    "published_date": "2023-08-18",
    "pdf_link": "https://arxiv.org/pdf/2308.09413v1",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Cyber Threat Intelligence",
      "subdomain": "Underground Forum Analysis",
      "specific_problem": "Graph-based stratified sampling to select posts for supervised classification of criminal activities in underground forums",
      "attack_types": [
        "Access to system",
        "Bots & Malware",
        "eWhoring",
        "Currency Exchange",
        "DDoS",
        "Identity Theft",
        "Spam",
        "Trading credentials",
        "VPN"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Other",
        "specific": "Graph-based stratified sampling using centrality metrics (post degree, thread degree, eigenvector)",
        "novel_contribution": "Methodology to generate stratified training samples based on forum graph centrality distributions and evaluate classifier performance on the population"
      },
      {
        "type": "baseline",
        "category": "Classical ML",
        "specific": null,
        "novel_contribution": "Supervised text classification (model unspecified); paper focuses on sampling, not model tuning"
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "CrimeBB (Underground forums corpus)",
        "type": "proprietary",
        "domain": "forum_posts",
        "link": "https://www.cambridgecybercrime.uk/process.html",
        "is_new_contribution": false,
        "availability": "available_on_request"
      },
      {
        "name": "Hack Forums (HF) subset of CrimeBB",
        "type": "proprietary",
        "domain": "forum_posts",
        "link": "https://www.cambridgecybercrime.uk/process.html",
        "is_new_contribution": false,
        "availability": "available_on_request"
      },
      {
        "name": "Anonymized forum graph structure (Zenodo, derived from HF)",
        "type": "public",
        "domain": "forum_graph",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Proportional stratified sampling (respect population centrality distribution)",
        "paper_reference": null,
        "metric": "Precision, Recall",
        "their_result": "“maintain the same level of precision but significantly increase the recall (+30%)” with uniform post-degree sampling",
        "baseline_result": "Same precision, lower recall (−30% compared to uniform post-degree sampling)"
      },
      {
        "method_name": "Simple random sampling (SRS)",
        "paper_reference": "Cited as prevailing practice in prior work (Table I)",
        "metric": "Not directly benchmarked head-to-head; referenced as common practice",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Precision",
      "Recall",
      "Inter-annotator agreement (Cohen’s/Fleiss’s kappa)",
      "Agresti Coull confidence interval (for disagreement estimates)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "RQ1: What are the changes in performance for a ML classifier using different centrality metrics to generate stratified training samples?",
        "RQ2: What are the changes in performance using a different proportion compared to the population for the stratified training sample?"
      ],
      "gaps_identified": [
        "Current supervised ML approaches predominantly use random sampling and ignore the social network structure of the forum population.",
        "Illicit activity classes in underground forums are typically imbalanced, making representative sampling hard.",
        "Off-the-shelf NLP struggles with domain-specific jargon; human annotation is costly and requires expertise."
      ],
      "limitations": [
        "The paper explicitly does not focus on tuning classifiers for best possible performance.",
        "Content data cannot be publicly released due to ethics; only anonymized graph structure is released.",
        "Details of the specific text classification model are not emphasized; results focus on sampling effects.",
        "Evidence is from one major forum (HF); cross-forum generalization is not evaluated in this work."
      ],
      "future_work": [
        "Explore additional centrality/graph metrics and multi-layer network features for sampling.",
        "Validate the methodology across multiple forums and languages to assess generalization.",
        "Combine graph-aware stratified sampling with active learning strategies.",
        "Study temporal dynamics (e.g., evolving centrality) to design time-aware sampling schemes."
      ],
      "motivation": "Reduce costly human annotation by selecting informative and representative samples that account for the forum’s graph structure, improving supervised classification of criminal activities.",
      "potential_research_ideas": [
        "Integrate graph-aware stratified sampling with pool-based active learning to adaptively annotate contentious strata.",
        "Use graph neural networks or label propagation to estimate pseudo-labels and guide stratified sampling toward high-uncertainty regions.",
        "Design Bayesian optimal experimental design criteria over graph strata to maximize expected information gain.",
        "Develop temporal stratification based on user lifecycle and thread aging to capture emerging criminal activity.",
        "Evaluate transferability: derive stratification that is robust across forums (domain shift aware sampling).",
        "Investigate fairness-like properties: ensure sampling does not overfit to highly central actors at the expense of peripheral yet relevant activity.",
        "Quantify cost-benefit tradeoffs: optimize sample size per stratum under annotation budget constraints."
      ],
      "architectural_improvement_recommendations": [
        "Adopt domain-adaptive pretraining of transformer LMs on HF-like corpora before supervised fine-tuning to improve per-stratum recall.",
        "Apply class-imbalance strategies (focal loss, reweighting) and calibration per stratum.",
        "Use semi-supervised learning (self-training, consistency regularization) leveraging unlabeled posts within strata.",
        "Incorporate graph-regularization or GNN-based features (member/thread embeddings) in the classifier to complement text.",
        "Automate bin size selection via data-driven optimization (e.g., minimum expected samples per bin with uncertainty thresholds).",
        "Add active disagreement sampling: prioritize annotation where stratified-trained models disagree most."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "Neo4j"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Offline research analysis of large-scale underground forum data (Hack Forums) using a graph database",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "High annotation cost and need for domain expertise",
        "Ethical and data sharing constraints limit open data access",
        "Imbalanced classes and rare-event detection",
        "Potential lack of cross-forum generalization",
        "Model disagreement on deployment to full population (up to 33%)"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "A graph-based stratified sampling methodology using centrality metrics for selecting training samples in underground forum analysis.",
      "Release of an anonymized forum graph structure on Zenodo to facilitate research (content available via data sharing agreement).",
      "Empirical finding: using a uniform distribution over post-degree centrality maintains precision while increasing recall by +30% versus proportional sampling.",
      "Observation that classifiers trained with similar samples can disagree on the classification of criminal activities up to 33% of the time when run over the full forum population.",
      "A step-by-step methodology including population projection, distribution extraction, stratified sample generation, and population-level validation (using Agresti Coull CI)."
    ]
  },
  {
    "arxiv_id": "2308.15237v1",
    "title": "Assessing Cyclostationary Malware Detection via Feature Selection and Classification",
    "authors": "Mike Nkongolo",
    "abstract": "Cyclostationarity involves periodic statistical variations in signals and processes, commonly used in signal analysis and network security. In the context of attacks, cyclostationarity helps detect malicious behaviors within network traffic, such as traffic patterns in Distributed Denial of Service (DDoS) attacks or hidden communication channels in malware. This approach enhances security by identifying abnormal patterns and informing Network Intrusion Detection Systems (NIDSs) to recognize potential attacks, enhancing protection against both known and novel threats. This research focuses on identifying cyclostationary malware behavior and its detection. The main goal is to pinpoint essential cyclostationary features used in NIDSs. These features are extracted using algorithms such as Boruta and Principal Component Analysis (PCA), and then categorized to find the most significant cyclostationary patterns. The aim of this article is to reveal periodically changing malware behaviors through cyclostationarity. The study highlights the importance of spotting cyclostationary malware in NIDSs by using established datasets like KDD99, NSL-KDD, and the UGRansome dataset. The UGRansome dataset is designed for anomaly detection research and includes both normal and abnormal network threat categories of zero-day attacks. A comparison is made using the Random Forest (RF) and Support Vector Machine (SVM) algorithms, while also evaluating the effectiveness of Boruta and PCA. The findings show that PCA is more promising than using Boruta alone for extracting cyclostationary network feature patterns. Additionally, the analysis identifies the internet protocol as the most noticeable cyclostationary feature pattern used by malware. Notably, the UGRansome dataset outperforms the KDD99 and NSL-KDD, achieving 99% accuracy in signature malware detection using the RF algorithm and 98% with the SVM.",
    "published_date": "2023-08-29",
    "pdf_link": "https://arxiv.org/pdf/2308.15237v1",
    "paper_types": [
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection (NIDS)",
      "specific_problem": "Detection and classification of cyclostationary malware behaviors in network traffic (including zero-day threats) via feature selection/extraction and supervised learning",
      "attack_types": [
        "Zero-day threats",
        "Ransomware",
        "DDoS/DoS",
        "Probe/Port Scanning",
        "User-to-Root (U2R)",
        "Remote-to-Local (R2L)",
        "Advanced Persistent Threats (APT)",
        "Botnet"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Feature Selection (Wrapper/Tree-based)",
        "specific": "Boruta",
        "novel_contribution": "Applied to extract cyclostationary-relevant features for NIDS; compared against PCA for effectiveness"
      },
      {
        "type": "primary",
        "category": "Dimensionality Reduction / Feature Extraction",
        "specific": "PCA",
        "novel_contribution": "Used as feature extractor to reveal cyclostationary components; found more promising than Boruta alone"
      },
      {
        "type": "primary",
        "category": "Ensemble (Random Forest)",
        "specific": "Random Forest",
        "novel_contribution": "Used as main classifier for cyclostationary malware detection; recommended by the study for best performance"
      },
      {
        "type": "primary",
        "category": "Kernel Method",
        "specific": "Support Vector Machine (SVM)",
        "novel_contribution": "Used as comparative classifier baseline to RF for cyclostationary malware detection"
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "KDD Cup 99 (KDD99)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "NSL-KDD",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "UGRansome",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Random Forest vs SVM on UGRansome (signature malware class)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "\"achieving 99% accuracy in signature malware detection using the RF algorithm and 98% with the SVM\"",
        "baseline_result": "RF: 99%; SVM: 98% (UGRansome, signature class)"
      },
      {
        "method_name": "PCA vs Boruta (feature extraction)",
        "paper_reference": null,
        "metric": null,
        "their_result": "\"The findings show that PCA is more promising than using Boruta alone for extracting cyclostationary network feature patterns.\"",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "Precision",
      "Recall",
      "F1-Score",
      "Confusion Matrix"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can cyclostationary malware behaviors be identified and detected using feature selection/extraction and supervised classification?",
        "Which network features are most significant for capturing cyclostationary patterns in NIDS?",
        "Is PCA or Boruta more effective for extracting cyclostationary network feature patterns?",
        "Which datasets (KDD99, NSL-KDD, UGRansome) better support cyclostationary malware detection?",
        "How do RF and SVM compare for classifying cyclostationary malware behaviors?"
      ],
      "gaps_identified": [
        "Most current NIDS solutions overlook the use of cyclostationary techniques for pattern detection.",
        "The NIDL lacks analysis of stochastic, cyclostationary traffic, queuing of network flow, intrusion modelisation, and zero-day threats taxonomy.",
        "There is a dearth of cyclostationary datasets for NIDP to comprehend long-term evolving malware like zero-day threats.",
        "Threshold-based approaches for distinguishing cyclostationary traffic have evolving thresholds that remain a challenge.",
        "KDD99 is outdated and imbalanced, limiting suitability for cyclostationarity analysis."
      ],
      "limitations": [
        "Certain methodologies to assess the cyclostationarity of malware evolution may require specialized skills.",
        "Recognizing periodicity relies heavily on time and often necessitates rare or transient process analysis.",
        "KDD99 dataset is outdated and highly imbalanced, potentially biasing classifiers."
      ],
      "future_work": [],
      "motivation": "Enhance NIDS by leveraging cyclostationarity to detect long-term evolving malware (including zero-day threats) that conventional methods may miss; identify essential cyclostationary features and effective classifiers.",
      "potential_research_ideas": [
        "Construct a purpose-built, temporally labeled cyclostationary network dataset capturing periodic behaviors across modern protocols and diverse attack campaigns.",
        "Develop end-to-end time-series models (e.g., temporal Transformers) that learn cyclostationary patterns directly from raw or flow-level sequences, comparing spectral vs. temporal representations.",
        "Design semi-supervised or self-supervised pretraining on large unlabeled traffic to learn periodic structure, followed by supervised fine-tuning for attack classes.",
        "Integrate cyclostationary signal-processing features (e.g., cyclic autocorrelation, spectral correlation density) with ML classifiers to improve robustness and interpretability.",
        "Online/streaming cyclostationary anomaly detection with concept drift handling and adaptive periodicity estimation.",
        "Evaluate adversarial robustness for cyclostationary detectors (evasion via jitter/randomization) and propose defenses (randomized smoothing over periods).",
        "Multi-modal fusion: combine flow-level features with DNS/host telemetry to strengthen periodicity cues and reduce false positives.",
        "Quantify and visualize periodic feature importance over time (e.g., SHAP over cycles) to aid analyst triage and feedback loops."
      ],
      "architectural_improvement_recommendations": [
        "Augment feature space with explicit cyclostationary descriptors (cyclic autocorrelation, spectral coherence) before classification.",
        "Adopt imbalance-aware training (class weights, focal loss, SMOTE/ADASYN) especially on KDD/NSL splits; report per-class metrics.",
        "Replace/augment RF/SVM with gradient boosting (XGBoost/LightGBM) and temporal models (1D CNNs, TCNs, Transformers) to capture periodic structures.",
        "Use nested cross-validation with stratification by time windows to avoid temporal leakage; perform hyperparameter tuning for SVM kernels and RF depth/trees.",
        "Hybrid feature selection: combine Boruta with mutual information and recursive feature elimination; compare to embedded methods (L1/Lasso, tree-based gain).",
        "Calibrate classifiers (Platt/Isotonic) and report calibrated precision-recall, especially for anomaly vs signature classes.",
        "Implement explainability (SHAP/TreeSHAP) to validate that Internet Protocol and related headers drive decisions; provide cycle-aware explanations.",
        "Deploy a streaming pipeline (e.g., Kafka + Flink/Spark) with windowed feature extraction and periodicity estimation for near-real-time operation."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Not specified; dataset split described as 80% train / 20% test; evaluation with Accuracy, Precision, Recall, F1-Score, Confusion Matrix."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Specialized skills may be required to assess malware cyclostationarity.",
        "Periodicity detection relies on temporal windows and may be sensitive to time granularity and drift.",
        "Dataset imbalance and obsolescence (e.g., KDD99) can bias models and hinder generalization.",
        "Threshold-based methods for cyclostationarity can be unstable due to evolving thresholds."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces a supervised learning framework applying Boruta and PCA for cyclostationary feature extraction and RF/SVM for classification in NIDS.",
      "Empirically shows PCA is more promising than Boruta alone for extracting cyclostationary network feature patterns.",
      "Identifies the Internet Protocol as the most noticeable cyclostationary feature pattern used by malware.",
      "Demonstrates UGRansome outperforms KDD99 and NSL-KDD for this task, with RF achieving 99% and SVM 98% accuracy in signature malware detection.",
      "Highlights gaps in NIDS research regarding cyclostationary analysis and motivates using cyclostationarity for zero-day detection."
    ]
  },
  {
    "arxiv_id": "2309.04798v1",
    "title": "Low-Quality Training Data Only? A Robust Framework for Detecting Encrypted Malicious Network Traffic",
    "authors": "Yuqi Qing; Qilei Yin; Xinhao Deng; Yihao Chen; Zhuotao Liu; Kun Sun; Ke Xu; Jia Zhang; Qi Li",
    "abstract": "Machine learning (ML) is promising in accurately detecting malicious flows in encrypted network traffic; however, it is challenging to collect a training dataset that contains a sufficient amount of encrypted malicious data with correct labels. When ML models are trained with low-quality training data, they suffer degraded performance. In this paper, we aim at addressing a real-world low-quality training dataset problem, namely, detecting encrypted malicious traffic generated by continuously evolving malware. We develop RAPIER that fully utilizes different distributions of normal and malicious traffic data in the feature space, where normal data is tightly distributed in a certain area and the malicious data is scattered over the entire feature space to augment training data for model training. RAPIER includes two pre-processing modules to convert traffic into feature vectors and correct label noises. We evaluate our system on two public datasets and one combined dataset. With 1000 samples and 45% noises from each dataset, our system achieves the F1 scores of 0.770, 0.776, and 0.855, respectively, achieving average improvements of 352.6%, 284.3%, and 214.9% over the existing methods, respectively. Furthermore, We evaluate RAPIER with a real-world dataset obtained from a security enterprise. RAPIER effectively achieves encrypted malicious traffic detection with the best F1 score of 0.773 and improves the F1 score of existing methods by an average of 272.5%.",
    "published_date": "2023-09-09",
    "pdf_link": "https://arxiv.org/pdf/2309.04798v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Detection of encrypted malicious network traffic under low-quality training data (limited samples with label noise)",
      "attack_types": [
        "malware-generated encrypted traffic",
        "encrypted C2/communication over SSL/TLS/DoH (general)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Autoencoder (RNN/GRU)",
        "specific": "Bi-directional GRU Auto-Encoder for packet-length sequences",
        "novel_contribution": "Unsupervised feature extractor for encrypted traffic that avoids contamination from noisy labels and captures fine-grained flow behaviors"
      },
      {
        "type": "primary",
        "category": "Autoregressive Generative Model",
        "specific": "MADE (Masked Autoencoder for Distribution Estimation)",
        "novel_contribution": "Estimates data distribution of limited high-dimensional traffic to identify densest/sparsest regions for distribution-aware label noise correction"
      },
      {
        "type": "primary",
        "category": "GAN",
        "specific": "Improved GAN with multiple generators and a discriminator",
        "novel_contribution": "Targets possible distribution regions of new malicious data to synthesize training samples while also generating normal samples to preserve decision boundaries"
      },
      {
        "type": "primary",
        "category": "Ensemble Learning",
        "specific": null,
        "novel_contribution": "Infers labels of remaining samples after confident relabeling from distribution extremes"
      },
      {
        "type": "primary",
        "category": "MLP",
        "specific": "Multilayer Perceptron classifier",
        "novel_contribution": "Final detector trained on label-corrected and GAN-augmented data"
      },
      {
        "type": "primary",
        "category": "Noise-robust training",
        "specific": "Co-teaching",
        "novel_contribution": "Mitigates residual label noise during supervised training of the final detector"
      }
    ],
    "learning_paradigm": [
      "Unsupervised",
      "Supervised"
    ],
    "datasets": [
      {
        "name": "DoHBrw2020",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "IDS2018 (CICIDS2018)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "AAGM2017",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Combined dataset (from public sources)",
        "type": "synthetic",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Real-world enterprise dataset",
        "type": "proprietary",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "F1 score"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How to detect encrypted malicious network traffic when the available training data is limited and contains non-negligible label noise?",
        "Can differences in distribution between benign (dense) and malicious (sparse) traffic be leveraged to correct labels and augment data without relying on large additional datasets?"
      ],
      "gaps_identified": [
        "Time-sensitive malware data is hard to collect; captured samples often lack time sensitivity.",
        "Labels from malware detection services (e.g., VirusTotal) are unreliable and may change over time; manual labeling is costly.",
        "Encryption (e.g., SSL/TLS) prevents manual correction of label noise.",
        "Conventional data augmentation confuses class distributions under label noise and can create more noisy samples.",
        "Noise-robust ML methods rely on strong assumptions/prior knowledge (e.g., known noise rates or transition matrices) or require large training sets, which are not satisfied here.",
        "Transfer learning methods that use large-scale unlabeled data are expensive to collect/preprocess and raise privacy leakage risks."
      ],
      "limitations": [
        "Does not consider the scenario where malicious traffic exhibits identical distributions as benign traffic over time (an extreme concept drift case).",
        "Assumes benign traffic tends to have denser distribution than malicious, which may not hold under sophisticated mimicry in the long term."
      ],
      "future_work": [
        "Address the extreme distributional convergence (identical benign/malicious distributions) by introducing more fine-grained features and recollecting the entire training set."
      ],
      "motivation": "Enable accurate detection of encrypted malicious traffic in realistic settings where training data is scarce and labels are noisy, without relying on large additional unlabeled datasets or strong noise assumptions.",
      "potential_research_ideas": [
        "Online concept drift detection and adaptive retraining that monitors distribution shifts and triggers targeted augmentation or feature refinement.",
        "Self-supervised pretraining on packet-length sequences (e.g., masked sequence modeling or contrastive learning) to improve feature quality under limited labels.",
        "Diffusion-based data augmentation targeted to sparse malicious regions, with controllable sampling to preserve class boundaries.",
        "Uncertainty-aware label correction combining density estimates with predictive uncertainty/calibration to reduce relabeling errors.",
        "Federated or privacy-preserving training for enterprise deployments to mitigate privacy leakage while aggregating knowledge across sites.",
        "Multi-modal flow representation (e.g., timing, directionality, TLS handshake metadata when available) for improved separability under mimicry.",
        "Active learning to selectively query high-value labels from analysts, guided by distributional novelty and uncertainty.",
        "Theoretical analysis of density-separation assumptions and conditions under which benign density > malicious density holds in encrypted traffic."
      ],
      "architectural_improvement_recommendations": [
        "Replace bi-GRU AE with Transformer-based sequence autoencoders or temporal CNNs to better capture long-range dependencies in packet-length sequences.",
        "Use normalizing flows or energy-based models instead of (or in addition to) MADE for more expressive density estimation in high dimensions.",
        "Adopt diffusion models or class-conditional generators for augmentation with explicit control over target regions and diversity.",
        "Incorporate domain-adversarial training to enhance generalization across networks and time periods.",
        "Employ calibrated, noise-robust losses (e.g., generalized cross entropy) and confidence-based sample reweighting alongside Co-teaching.",
        "Introduce curriculum learning that gradually expands from high-confidence relabeled samples to harder ones.",
        "Leverage ensemble diversity explicitly (heterogeneous architectures) and stacking to improve relabeling robustness."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Gateway of an intranet (e.g., campus or enterprise network) monitoring outgoing traffic",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Collecting time-sensitive malware traffic and reliable labels is difficult in practice.",
        "Label noise cannot be easily corrected due to encryption preventing payload inspection.",
        "Privacy risks and cost associated with collecting large-scale unlabeled traffic for transfer learning."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes RAPIER, a system for encrypted malicious traffic detection robust to low-quality training data (both scarcity and label noise) and claimed as the first to address both challenges simultaneously.",
      "Unsupervised feature extraction via an improved bi-GRU Auto-Encoder on packet-length sequences to avoid label-noise contamination and capture fine-grained behaviors.",
      "Distribution-aware label noise correction using an autoregressive generative model (MADE) to estimate data distributions, relabel confident extremes, and ensemble learning to infer remaining labels.",
      "Targeted data augmentation using an improved GAN to synthesize malicious samples in likely new malicious regions and normal samples to maintain decision boundaries.",
      "With 1000 samples and 45% noise per dataset, achieves F1 scores of 0.770, 0.776, and 0.855 on two public datasets and one combined dataset, with average improvements of 352.6%, 284.3%, and 214.9% over existing methods.",
      "On a real-world enterprise dataset, achieves best F1 score of 0.773 and improves F1 of existing methods by an average of 272.5%."
    ]
  },
  {
    "arxiv_id": "2308.02152v1",
    "title": "ExploitFlow, cyber security exploitation routes for Game Theory and AI research in robotics",
    "authors": "Víctor Mayoral-Vilches; Gelei Deng; Yi Liu; Martin Pinzger; Stefan Rass",
    "abstract": "This paper addresses the prevalent lack of tools to facilitate and empower Game Theory and Artificial Intelligence (AI) research in cybersecurity. The primary contribution is the introduction of ExploitFlow (EF), an AI and Game Theory-driven modular library designed for cyber security exploitation. EF aims to automate attacks, combining exploits from various sources, and capturing system states post-action to reason about them and understand potential attack trees. The motivation behind EF is to bolster Game Theory and AI research in cybersecurity, with robotics as the initial focus. Results indicate that EF is effective for exploring machine learning in robot cybersecurity. An artificial agent powered by EF, using Reinforcement Learning, outperformed both brute-force and human expert approaches, laying the path for using ExploitFlow for further research. Nonetheless, we identified several limitations in EF-driven agents, including a propensity to overfit, the scarcity and production cost of datasets for generalization, and challenges in interpreting networking states across varied security settings. To leverage the strengths of ExploitFlow while addressing identified shortcomings, we present Malism, our vision for a comprehensive automated penetration testing framework with ExploitFlow at its core.",
    "published_date": "2023-08-04",
    "pdf_link": "https://arxiv.org/pdf/2308.02152v1",
    "paper_types": [
      "position",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Robotics Security",
      "subdomain": "Penetration Testing Automation",
      "specific_problem": "Automating security exploitation routes and learning attack trees against robotic systems using a modular library (ExploitFlow) and RL agents",
      "attack_types": [
        "reconnaissance/footprinting",
        "network scanning/fingerprinting",
        "credential abuse (hard-coded public credentials; RVD#672 on UR3)",
        "exploit chaining/attack path discovery"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Reinforcement Learning",
        "specific": "Tabular Q-Learning (value-based RL)",
        "novel_contribution": "Use of ExploitFlow to structure state/action space and capture post-action system state for learning attack trees in robotic pentesting; EF-powered Q-Learning agent demonstrated to outperform brute-force and a human expert in a UR3 scenario"
      }
    ],
    "learning_paradigm": [
      "Reinforcement Learning"
    ],
    "datasets": [
      {
        "name": "Synthetic networking data from OS-virtualized CTF-like robotic environments",
        "type": "synthetic",
        "domain": "network_traffic",
        "link": "https://github.com/vmayoral/ExploitFlow",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Brute-force actor",
        "paper_reference": null,
        "metric": "Cumulative reward",
        "their_result": "100 (Q-Learning agent)",
        "baseline_result": "-2680"
      },
      {
        "method_name": "Human expert (manually programmed exploitation route with ExploitFlow)",
        "paper_reference": null,
        "metric": "Cumulative reward",
        "their_result": "100 (Q-Learning agent)",
        "baseline_result": "8"
      }
    ],
    "performance_metrics_used": [
      "cumulative_reward",
      "attacks_attempted (implicit via reward penalties)",
      "network_traffic_penalty (reconnaissance negative rewards)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Lack of consistent, publicly available datasets for robot cybersecurity and ML research",
        "Inconsistency in state-space representations across cybersecurity ML studies",
        "Capturing networking data at scale is non-trivial; cybersecurity environments are highly unstructured and interactive",
        "EF-driven agents showed a propensity to overfit, impeding generalization across new environments",
        "Challenges in interpreting networking states across varied security settings",
        "Naive one-hot state encoding becomes computationally infeasible, requiring heavy abstraction (e.g., reduced IP/port space)"
      ],
      "limitations": [
        "Overfitting behavior of the RL agent (e.g., learning to trigger a favorable exploit and then idling to maximize reward)",
        "Scarcity and production cost of datasets hinder generalization",
        "Simplified and heavily abstracted state representation (reduced IP and port counts) to maintain computational tractability",
        "Evaluation limited to OS-virtualized simulated robotic targets (no real-world deployment results)",
        "Tabular Q-Learning only; no comparison with stronger RL baselines (e.g., DQN, PPO) or planning approaches",
        "State encoding discarded explicit exploit success/failure to model uncertainty, potentially losing informative signal"
      ],
      "future_work": [
        "Malism: a comprehensive automated penetration testing framework centered on ExploitFlow",
        "PentestGPT: LLM-guided testing heuristics per discrete state",
        "PentestPerf: a comprehensive penetration testing benchmark for fair and robust comparisons",
        "Address overfitting and improve generalization across varied environments",
        "Scale to broader targets and more realistic networking scopes"
      ],
      "motivation": "To facilitate and empower Game Theory and AI research in cybersecurity by providing a modular library (ExploitFlow) to compose exploitation routes, capture post-action state, and enable learning of attack trees, with robotics as the initial focus amid a scarcity of tools and datasets.",
      "potential_research_ideas": [
        "Develop a standardized, extensible state representation for penetration testing that balances fidelity and tractability and can generalize across targets and environments",
        "Integrate model-based or planning-augmented RL (e.g., POMDP solvers, MCTS) to reason under uncertainty and sparse rewards",
        "Leverage graph learning on dynamically built attack graphs (e.g., GNNs over hosts/services/exploits) for policy learning and transfer",
        "Apply domain randomization and curriculum learning across synthetic network/robot scenarios to combat overfitting",
        "Create a reusable synthetic data generation toolkit for robotics pentesting with configurable protocols and robot-specific services",
        "Combine LLM reasoning (tool use) with RL for decision-time planning and action validation (closed-loop PentestGPT+RL)",
        "Formalize safety constraints and safe exploration in offensive RL to avoid destabilizing target systems",
        "Investigate few-shot transfer and meta-RL to quickly adapt policies to new robotic platforms and network topologies"
      ],
      "architectural_improvement_recommendations": [
        "Augment EF’s state to include probabilistic beliefs and explicit exploit success/failure flags; consider a POMDP formulation",
        "Expose an OpenAI Gymnasium-style interface for ExploitFlow to standardize interaction with RL libraries and benchmarking",
        "Add pluggable connectors for common tools (Nmap, Nessus, OpenVAS) and richer protocol analyzers for robotics (ROS, ROS 2, industrial fieldbuses)",
        "Implement an attack-graph builder within EF with export to standard graph formats and features for GNN-based policies",
        "Introduce asynchronous and batched execution with caching to accelerate data collection and improve sample efficiency",
        "Provide dataset export utilities (schemas, versioning) to share reproducible traces for training and evaluation",
        "Incorporate reward shaping and hierarchical options (HRL) to reduce sparsity and improve exploration",
        "Add explainability hooks (e.g., action-state saliency, policy visualization) aligned with learned attack graphs"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/vmayoral/ExploitFlow",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Runs on CPU with tabular Q-Learning; no GPU required after reducing state space. Original naive encoding was computationally infeasible; experiments used reduced IP/port scope."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "OS-virtualized CTF-like environments with robotic targets (simulated networks)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Dataset scarcity and cost to produce realistic robot cybersecurity data",
        "Overfitting and poor generalization across different networks/security settings",
        "Complexity of modeling robotic protocols/services alongside standard IT network behaviors",
        "Interpreting diverse networking states and integrating multiple exploitation tools",
        "Computational constraints when scaling state representations"
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces ExploitFlow, a modular library to compose cybersecurity exploitation routes and capture post-action system states for reasoning and attack-tree learning",
      "Demonstrates an EF-powered RL agent (tabular Q-Learning) that outperforms brute-force and a human expert in a simulated UR3 exploitation scenario",
      "Provides an EF representation compatible with penetration testing tools (e.g., Metasploit) and programmatic flow construction",
      "Releases code for ExploitFlow and ML experiments",
      "Proposes the Malism framework vision, including PentestGPT (LLM-guided testing) and PentestPerf (benchmark) to advance automated pentesting"
    ]
  },
  {
    "arxiv_id": "2309.02637v2",
    "title": "Killing Two Birds with One Stone: Malicious Package Detection in NPM and PyPI using a Single Model of Malicious Behavior Sequence",
    "authors": "Junan Zhang; Kaifeng Huang; Yiheng Huang; Bihuan Chen; Ruisi Wang; Chong Wang; Xin Peng",
    "abstract": "Open-source software (OSS) supply chain enlarges the attack surface, which makes package registries attractive targets for attacks. Recently, package registries NPM and PyPI have been flooded with malicious packages. The effectiveness of existing malicious NPM and PyPI package detection approaches is hindered by two challenges. The first challenge is how to leverage the knowledge of malicious packages from different ecosystems in a unified way such that multi-lingual malicious package detection can be feasible. The second challenge is how to model malicious behavior in a sequential way such that maliciousness can be precisely captured. To address the two challenges, we propose and implement Cerebro to detect malicious packages in NPM and PyPI. We curate a feature set based on a high-level abstraction of malicious behavior to enable multi-lingual knowledge fusing. We organize extracted features into a behavior sequence to model sequential malicious behavior. We fine-tune the BERT model to understand the semantics of malicious behavior. Extensive evaluation has demonstrated the effectiveness of Cerebro over the state-of-the-art as well as the practically acceptable efficiency. Cerebro has successfully detected 306 and 196 new malicious packages in PyPI and NPM, and received 385 thank letters from the official PyPI and NPM teams.",
    "published_date": "2023-09-06",
    "pdf_link": "https://arxiv.org/pdf/2309.02637v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software Supply Chain Security",
      "subdomain": "Malicious Package Detection in Package Registries",
      "specific_problem": "Cross-ecosystem detection of malicious packages in NPM and PyPI using a single sequential behavior model",
      "attack_types": [
        "Typosquatting",
        "Dependency confusion",
        "Malicious install-time scripts",
        "Import-time code execution",
        "Run-time payload execution",
        "SEO poisoning/phishing for package promotion",
        "Data exfiltration"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "BERT",
        "novel_contribution": "Fine-tunes BERT on textualized malicious behavior sequences derived from static, language-agnostic features to enable a single cross-ecosystem (NPM/PyPI) detector."
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Transfer Learning (fine-tuning)"
    ],
    "datasets": [
      {
        "name": "Cerebro training/evaluation dataset (NPM + PyPI)",
        "type": "proprietary",
        "domain": "package_registry_source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Newly published packages snapshot for deployment evaluation (923,638 package versions)",
        "type": "proprietary",
        "domain": "package_registry_source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Amalfi",
        "paper_reference": "Sejfia et al. [70]",
        "metric": "Precision/Recall (mono-lingual and bi-lingual scenarios)",
        "their_result": "\"outperforms the state-of-the-art by an average of 10.0% in precision and 7.4% in recall in the mono-lingual scenario\"; \"by 9.9% in precision and 8.9% in recall in the bi-lingual scenario\"",
        "baseline_result": null
      },
      {
        "method_name": "MalOSS",
        "paper_reference": "Duan et al. [12]",
        "metric": "Precision/Recall (mono-lingual and bi-lingual scenarios)",
        "their_result": "\"outperforms the state-of-the-art by an average of 10.0% in precision and 7.4% in recall in the mono-lingual scenario\"; \"by 9.9% in precision and 8.9% in recall in the bi-lingual scenario\"",
        "baseline_result": null
      },
      {
        "method_name": "PPD",
        "paper_reference": "Liang et al. [47]",
        "metric": "Precision/Recall (mono-lingual and bi-lingual scenarios)",
        "their_result": "\"outperforms the state-of-the-art by an average of 10.0% in precision and 7.4% in recall in the mono-lingual scenario\"; \"by 9.9% in precision and 8.9% in recall in the bi-lingual scenario\"",
        "baseline_result": null
      },
      {
        "method_name": "Garrett et al.",
        "paper_reference": "[21]",
        "metric": "Precision/Recall (mono-lingual and bi-lingual scenarios)",
        "their_result": "\"outperforms the state-of-the-art by an average of 10.0% in precision and 7.4% in recall in the mono-lingual scenario\"; \"by 9.9% in precision and 8.9% in recall in the bi-lingual scenario\"",
        "baseline_result": null
      },
      {
        "method_name": "Ohm et al. (hybrid/unsupervised variants)",
        "paper_reference": "[57]",
        "metric": "Precision/Recall (mono-lingual and bi-lingual scenarios)",
        "their_result": "\"outperforms the state-of-the-art by an average of 10.0% in precision and 7.4% in recall in the mono-lingual scenario\"; \"by 9.9% in precision and 8.9% in recall in the bi-lingual scenario\"",
        "baseline_result": null
      },
      {
        "method_name": "Fang et al.",
        "paper_reference": "[14]",
        "metric": "Precision/Recall (mono-lingual and bi-lingual scenarios)",
        "their_result": "\"outperforms the state-of-the-art by an average of 10.0% in precision and 7.4% in recall in the mono-lingual scenario\"; \"by 9.9% in precision and 8.9% in recall in the bi-lingual scenario\"",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Precision",
      "Recall",
      "Average analysis time per package"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "How to leverage the knowledge of malicious packages from different ecosystems in a unified way such that multi-lingual malicious package detection can be feasible?",
        "How to model malicious behavior in a sequential way such that maliciousness can be precisely captured?"
      ],
      "gaps_identified": [
        "Cross-ecosystem knowledge is not sufficiently leveraged; detectors are usually ecosystem-specific.",
        "Existing rule-based and learning-based approaches fail to model sequential malicious behavior, leading to false positives/negatives.",
        "Public datasets of malicious NPM and PyPI packages are small compared with the actual volume; registries do not release complete malicious lists.",
        "Rule-based systems often incur high false positives and lack practicality."
      ],
      "limitations": [
        "Focused solely on static features; does not consider metadata or dynamic features (an explicit design choice).",
        "Demonstrated on two ecosystems (NPM and PyPI); generalization to other ecosystems not evaluated in provided text.",
        "Potential blind spots for attacks detectable only via dynamic/runtime behaviors or metadata anomalies."
      ],
      "future_work": [],
      "motivation": "Prevalence of malicious packages in NPM and PyPI and two limitations in prior work: lack of unified cross-ecosystem knowledge use and absence of sequential modeling for malicious behavior.",
      "potential_research_ideas": [
        "Extend the single-model behavior-sequence approach to additional ecosystems (e.g., RubyGems, Maven, NuGet) and assess zero-/few-shot transfer.",
        "Incorporate lightweight dynamic signals (e.g., safe sandboxed install/import tracing) to complement static features while controlling cost.",
        "Apply code-aware pre-trained models (e.g., CodeBERT/GraphCodeBERT/CodeT5) and compare to natural-language BERT on behavior sequences.",
        "Contrastive learning to align behavior sequences across ecosystems, improving cross-lingual robustness and transfer.",
        "Adversarial robustness study against obfuscation and evasion (e.g., string/AST obfuscation, staged payload delivery) with defenses (augmentation, adversarial training).",
        "Online/continual learning pipeline to adapt to evolving attack patterns and concept drift in registries.",
        "Explainability layer (e.g., attention attribution over behavior tokens) for analyst triage and actionable alerts."
      ],
      "architectural_improvement_recommendations": [
        "Use hierarchical transformer encodings with segment embeddings for install-time, import-time, and run-time phases to better model execution likelihood.",
        "Augment the sequence with call-graph-aware positional encodings or integrate a GNN over the call graph fused with the transformer encoder.",
        "Leverage code-specific PLMs (CodeBERT/GraphCodeBERT) or multi-modal fusion (AST + behavior tokens) to capture code semantics beyond the curated features.",
        "Add a CRF or sequence tagging head to locate malicious subsequences and improve interpretability.",
        "Employ self-supervised pretraining on large unlabeled package corpora to learn behavior token representations before fine-tuning."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Monitoring newly published NPM and PyPI packages (registry vetting/triage support; offline scanning).",
      "scalability_discussed": true,
      "inference_time": "\"Cerebro takes an average of 10.5 seconds to analyze a package\"",
      "deployment_challenges": [
        "Limited publicly available labeled malicious package data from registries.",
        "High volume of new packages vs. limited human triage capacity at registries.",
        "Attackers may transpose techniques across ecosystems and evolve to evade static detection."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes Cerebro, a single-model detector using malicious behavior sequences to detect malicious packages in NPM and PyPI.",
      "Curates a language-agnostic static feature set (16 features) and organizes them into sequential behavior representations (install/import/run-time).",
      "Fine-tunes a pre-trained language model (BERT) on textualized behavior sequences for binary maliciousness classification.",
      "Extensive evaluation on 2,675 malicious and 7,391 benign packages; reports average +10.0% precision and +7.4% recall improvements (mono-lingual) and +9.9%/+8.9% (bi-lingual) over SOTA.",
      "Practical deployment: scanned 923,638 new versions; detected and reported 683 (PyPI) and 799 (NPM) malicious package versions; all removed by registries; received 707 thank letters."
    ]
  },
  {
    "arxiv_id": "2308.13645v1",
    "title": "Active learning for fast and slow modeling attacks on Arbiter PUFs",
    "authors": "Vincent Dumoulin; Wenjing Rao; Natasha Devroye",
    "abstract": "Modeling attacks, in which an adversary uses machine learning techniques to model a hardware-based Physically Unclonable Function (PUF) pose a great threat to the viability of these hardware security primitives. In most modeling attacks, a random subset of challenge-response-pairs (CRPs) are used as the labeled data for the machine learning algorithm. Here, for the arbiter-PUF, a delay based PUF which may be viewed as a linear threshold function with random weights (due to manufacturing imperfections), we investigate the role of active learning in Support Vector Machine (SVM) learning. We focus on challenge selection to help SVM algorithm learn ``fast'' and learn ``slow''. Our methods construct challenges rather than relying on a sample pool of challenges as in prior work. Using active learning to learn ``fast'' (less CRPs revealed, higher accuracies) may help manufacturers learn the manufactured PUFs more efficiently, or may form a more powerful attack when the attacker may query the PUF for CRPs at will. Using active learning to select challenges from which learning is ``slow'' (low accuracy despite a large number of revealed CRPs) may provide a basis for slowing down attackers who are limited to overhearing CRPs.",
    "published_date": "2023-08-25",
    "pdf_link": "https://arxiv.org/pdf/2308.13645v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Hardware Security",
      "subdomain": "Physically Unclonable Functions (PUFs)",
      "specific_problem": "Active-learning-based modeling attacks on Arbiter PUFs (fast modeling via challenge construction; slow/adversarial challenge selection to hinder eavesdroppers)",
      "attack_types": [
        "Modeling attack (machine learning using CRPs)",
        "Active query attack (adaptive CRP querying)",
        "Eavesdropping/overheard CRPs (passive adversary slowed by challenge selection)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Active Learning",
        "specific": "Constructive, margin-based challenge selection without a sample pool",
        "novel_contribution": "Algorithm that constructs new challenges at a desired approximate distance to the current SVM hyperplane while enforcing low correlation with prior challenges; tunable parameter to learn fast (near-margin) or slow (not too near, not too far)."
      },
      {
        "type": "primary",
        "category": "SVM",
        "specific": "Linear SVM (hard/soft margin aligned with linear threshold APUF model, b=0)",
        "novel_contribution": "Use of SVM as the core learner for APUFs with challenge-construction active learning; matches the physical linear threshold model of APUFs."
      },
      {
        "type": "primary",
        "category": "Design of Experiments / Coding-theoretic selection",
        "specific": "Hadamard-set challenge selection",
        "novel_contribution": "PUF-realization-independent small challenge set: use rows of a Sylvester Hadamard matrix (with appended 1 column) to generate near-orthogonal, uncorrelated, maximum-entropy CRPs for highly data-efficient initial learning."
      },
      {
        "type": "baseline",
        "category": "Linear Model",
        "specific": "Logistic Regression",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Active Learning"
    ],
    "datasets": [
      {
        "name": "Simulated Arbiter PUF CRPs (n=128 stages)",
        "type": "synthetic",
        "domain": "puf_challenge_response_pairs",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Random challenge selection + SVM (Rand SVM)",
        "paper_reference": null,
        "metric": "Prediction accuracy on random test challenges",
        "their_result": "Hadamard-based SVM outperforms random SVM in the very small-data regime (< n CRPs); after ~additional 100 random CRPs beyond n=128, performances become similar.",
        "baseline_result": null
      },
      {
        "method_name": "Random challenge selection + Logistic Regression (Rand LR)",
        "paper_reference": null,
        "metric": "Prediction accuracy on random test challenges",
        "their_result": "At extremely low #CRPs, Hadamard-based SVM is best; around n=128, LR with Hadamard set catches up to SVM.",
        "baseline_result": null
      },
      {
        "method_name": "Pool-based active learning (uncertainty sampling / margin/committee) on APUFs",
        "paper_reference": "[15] (fast active learning on APUFs with a pool); also uncertainty sampling, query-by-committee variants",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "AdaBoost-based challenge ordering for slow/adversarial learning (pool-based)",
        "paper_reference": "[5]",
        "metric": null,
        "their_result": "Paper states: \"10,000 challenges can be overheard yet still lead to an accuracy of around 70%\" using their slow-learning construction (no pool).",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "prediction accuracy",
      "generalization error",
      "internal recognition accuracy",
      "external recognition accuracy"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can we construct challenges (without a sample pool) that make SVM-based modeling of Arbiter PUFs learn faster in the small-CRP regime?",
        "Can we actively select/construct challenges to slow an attacker’s learning when they only overhear CRPs (low accuracy despite many CRPs)?",
        "Does a PUF-realization-independent Hadamard set of challenges maximize response entropy and improve generalization with very few CRPs?"
      ],
      "gaps_identified": [
        "Prior active-learning work on APUFs assumes access to a large random sample pool of unlabeled challenges, which is impractical when the pool size equals the entire challenge space (2^n).",
        "Existing slow-learning approaches (e.g., AdaBoost-based ordering) are experimentally heavy and pool-based, with high computational cost.",
        "Arbiter PUF modeling typically uses randomly sampled CRPs, missing highly informative, low-correlation challenge sets for extreme small-data regimes."
      ],
      "limitations": [
        "Focuses on (linear) Arbiter PUFs; does not evaluate XOR/Feed-Forward or other non-linear PUF variants.",
        "Experiments are on simulated APUFs (e.g., n=128); no real-silicon evaluation or robustness to environmental noise/instability reported.",
        "Authors note: \"We cannot yet explain why LR with the Hadamard set suddenly catches up\" indicating incomplete theoretical understanding of LR behavior with Hadamard challenges.",
        "No public code or dataset; hyperparameters and implementation details are limited in the provided text."
      ],
      "future_work": [],
      "motivation": "Modeling attacks threaten strong PUF viability; active learning can accelerate manufacturer modeling or strengthen attackers with query access, and conversely enable challenge selection that slows eavesdroppers limited to overheard CRPs.",
      "potential_research_ideas": [
        "Provide a formal generalization/theory analysis proving optimality (or bounds) of Hadamard-set challenge selection for APUF generalization in the small-CRP regime.",
        "Extend the constructive active learning to XOR/Feed-Forward/Interpose PUFs and quantify gains under non-linear/obfuscated PUF architectures.",
        "Incorporate label noise/reliability into the active selection (e.g., reliability-aware margins or Bayesian SVM/logistic models) and study robustness across PVT variations.",
        "Hybrid pool+constructive active learning: combine determinantal point processes (diversity) with margin targeting to ensure both informativeness and de-correlation at scale.",
        "Defense design: optimize server-side challenge scheduling to minimize information leakage (slow learning) while preserving authentication reliability and latency.",
        "Use constrained optimization or SAT/ILP formulations to exactly generate near-hyperplane challenges with bounded pairwise correlations.",
        "Hardware-in-the-loop evaluation on real APUF arrays to validate speed-ups/slow-downs and measure impact of metastability/noise.",
        "Adaptive tuning of the distance-to-hyperplane parameter using bandit optimization to maximize sample efficiency online."
      ],
      "architectural_improvement_recommendations": [
        "Adopt efficient primal SVM solvers (e.g., Pegasos) or online large-margin methods to scale active learning to large n without heavy retraining cost per query.",
        "Add explicit diversity regularization (e.g., cosine similarity penalty or DPP selection) to avoid redundant challenges while staying near the margin.",
        "Generalize the Hadamard design to other high-distance codes (e.g., BCH, Reed–Muller) to control pairwise correlations for k < n or n not a power of two.",
        "Model reliability via soft labels or probabilistic outputs (e.g., Platt scaling, Bayesian logistic regression) and select challenges maximizing expected information gain under noise.",
        "Implement a two-phase strategy: Hadamard-seeded bootstrapping (< n CRPs) followed by constructive margin queries; switch criterion based on validation uncertainty."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "PUF manufacturing/enrollment (fast modeling), authentication server challenge scheduling (slow learning to hinder eavesdroppers)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Requires ability to adaptively query the PUF for fast learning; some deployments restrict queries.",
        "Challenge selection to slow learning must not degrade authentication success or reliability for legitimate devices.",
        "CRP space is exponential (2^n); methods relying on full pools are impractical—necessitating constructive approaches.",
        "Real silicon introduces noise/reliability drift; margin-based selection must account for error rates and metastability."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "PUF-realization-independent small challenge set (Hadamard set) that yields high test accuracy in the very small-data regime by maximizing response entropy and pairwise uncorrelation.",
      "Constructive active learning algorithm (no sample pool) for fast modeling: generate new challenges at controllable distance to the current SVM hyperplane while enforcing low correlation with previous challenges—claimed as the fastest APUF modeling attack.",
      "Constructive active learning for slow/adversarial learning: parameter choice to avoid near-margin and far-margin regions to yield informative-yet-correlated-limited challenges, achieving low external generalization (e.g., \"10,000 challenges can be overheard yet still lead to an accuracy of around 70%\")."
    ]
  },
  {
    "arxiv_id": "2308.04898v1",
    "title": "An Empirical Study on Using Large Language Models to Analyze Software Supply Chain Security Failures",
    "authors": "Tanmay Singla; Dharun Anandayuvaraj; Kelechi G. Kalu; Taylor R. Schorlemmer; James C. Davis",
    "abstract": "As we increasingly depend on software systems, the consequences of breaches in the software supply chain become more severe. High-profile cyber attacks like those on SolarWinds and ShadowHammer have resulted in significant financial and data losses, underlining the need for stronger cybersecurity. One way to prevent future breaches is by studying past failures. However, traditional methods of analyzing these failures require manually reading and summarizing reports about them. Automated support could reduce costs and allow analysis of more failures. Natural Language Processing (NLP) techniques such as Large Language Models (LLMs) could be leveraged to assist the analysis of failures. In this study, we assessed the ability of Large Language Models (LLMs) to analyze historical software supply chain breaches. We used LLMs to replicate the manual analysis of 69 software supply chain security failures performed by members of the Cloud Native Computing Foundation (CNCF). We developed prompts for LLMs to categorize these by four dimensions: type of compromise, intent, nature, and impact. GPT 3.5s categorizations had an average accuracy of 68% and Bard had an accuracy of 58% over these dimensions. We report that LLMs effectively characterize software supply chain failures when the source articles are detailed enough for consensus among manual analysts, but cannot yet replace human analysts. Future work can improve LLM performance in this context, and study a broader range of articles and failures.",
    "published_date": "2023-08-09",
    "pdf_link": "https://arxiv.org/pdf/2308.04898v1",
    "paper_types": [
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Software and Application Security",
      "subdomain": "Software Supply Chain Security",
      "specific_problem": "Automated characterization of software supply chain failures from open-source intelligence using LLMs",
      "attack_types": [
        "Dev Tooling",
        "Negligence",
        "Publishing Infrastructure",
        "Source Code",
        "Trust and Signing",
        "Malicious Maintainer",
        "Attack Chaining"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer LLM",
        "specific": "GPT-3.5-turbo (ChatGPT)",
        "novel_contribution": "Prompt-engineered zero/few-shot prompting to extract structured labels (type of compromise, intent, nature, impact) and open-ended lessons learned from failure reports"
      },
      {
        "type": "baseline",
        "category": "Transformer LLM",
        "specific": "Google Bard (LaMDA-based)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Prompting",
        "specific": "Prompt engineering (iterative refinement)",
        "novel_contribution": "Systematic prompt engineering across dimensions with a 20% development split and final prompts (Table 9) used for evaluation"
      }
    ],
    "learning_paradigm": [
      "Zero-shot",
      "In-context learning",
      "Prompt-based inference"
    ],
    "datasets": [
      {
        "name": "CNCF Catalog of Supply Chain Compromises",
        "type": "public",
        "domain": "news_articles_and_blogs (OSINT on software supply chain incidents)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Google Bard (LaMDA-based)",
        "paper_reference": null,
        "metric": "Average accuracy across four dimensions (type of compromise, intent, nature, impact)",
        "their_result": "GPT-3.5 average accuracy: 68%",
        "baseline_result": "Bard average accuracy: 58%"
      },
      {
        "method_name": "Manual catalog (CNCF/Geer et al.)",
        "paper_reference": null,
        "metric": "Agreement used as ground truth for type of compromise; authors’ labels for intent, nature, impact",
        "their_result": "GPT-3.5 ranged 52–88% accuracy across predefined dimensions; helpfulness on lessons learned 3.83/5",
        "baseline_result": "Human raters’ inter-rater agreement: Intent κ=0.87; Nature κ=0.58; Impacts κ=0.34"
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "Cohen's kappa (inter-rater agreement)",
      "Likert-scale helpfulness (5-point)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "RQ1: How effective are LLMs in replicating manual analysis of software supply chain failures?",
        "RQ2: Do LLMs suggest viable mitigation strategies for preventing future failures?"
      ],
      "gaps_identified": [
        "Manual analysis of software supply chain failures is costly and does not scale.",
        "Lack of prior use of NLP/LLMs to analyze software supply chain failures from open-source intelligence.",
        "Ground truth is difficult to establish due to ambiguity and variation in source article detail.",
        "High disagreement among human raters for some dimensions (e.g., impacts) indicates inherent uncertainty."
      ],
      "limitations": [
        "Evaluation limited to two proprietary LLMs (GPT-3.5 and Bard) available at the time.",
        "Performance depends strongly on the level of detail in the source articles.",
        "Small dataset size (69 failures) and reliance on a single catalog (CNCF).",
        "Prompt engineering performed on 20% of most recent articles introduces potential bias.",
        "Ground-truth uncertainty; sometimes authors preferred GPT’s rating over the catalog.",
        "Non-determinism and version opacity of closed LLMs may affect reproducibility."
      ],
      "future_work": [
        "Improve LLM performance in this context.",
        "Study a broader range of articles and failures.",
        "Further evaluate and refine prompts and methods for extracting lessons learned."
      ],
      "motivation": "Reduce the costs and increase the scalability of analyzing software supply chain failures by leveraging LLMs to extract structured insights from open-source reports.",
      "potential_research_ideas": [
        "Create and release a larger, standardized, multi-source benchmark dataset of annotated supply chain failures (with multi-label consensus) to enable robust training and evaluation.",
        "Investigate retrieval-augmented generation (RAG) over curated OSINT corpora to ground LLM outputs and improve accuracy/consistency.",
        "Develop uncertainty-aware LLM classification (e.g., calibrated confidence, abstention) for ambiguous incident reports.",
        "Use self-consistency, chain-of-thought, and critique-and-revise prompting to improve label quality and reduce hallucinations.",
        "Build a multi-document, multi-source fusion pipeline to reconcile conflicting reports and update incident characterizations over time.",
        "Explore lightweight fine-tuning or instruction-tuning on annotated incident corpora for domain adaptation.",
        "Construct a knowledge graph of supply chain entities, events, and relationships; integrate with LLMs for constrained reasoning.",
        "Design evaluation protocols that measure robustness to sparse, noisy, or conflicting report evidence."
      ],
      "architectural_improvement_recommendations": [
        "Add a retrieval layer (RAG) with source citation and evidence highlighting to support grounded labeling and lessons learned.",
        "Incorporate self-consistency decoding and majority voting across diverse prompts and seeds to stabilize outputs.",
        "Introduce an uncertainty/abstention mechanism and triage uncertain cases to humans.",
        "Use few-shot exemplars derived from high-consensus incidents; auto-select exemplars via similarity search.",
        "Implement multi-agent review (generator, fact-checker, and policy-enforcer agents) to cross-verify classifications.",
        "Adopt schema-guided extraction (structured output constraints) to reduce ambiguity and increase reproducibility.",
        "Track model/version metadata and temperature settings; log prompts and responses to improve auditability.",
        "Experiment with domain-adaptive fine-tuning or LoRA on labeled incidents if licensing allows."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Output quality depends on detail and reliability of source articles.",
        "Ambiguity and lack of consensus in labeling certain dimensions (e.g., impacts).",
        "Prompt sensitivity and model non-determinism.",
        "Ground-truth uncertainty and potential catalog inconsistencies.",
        "Use of closed-source LLMs with version drift and opaque training data.",
        "Human oversight still required; LLMs cannot yet replace human analysts."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Extended analysis of a catalog of software supply chain failures (adding intent, nature, and impact dimensions).",
      "Evaluation of LLMs (GPT-3.5 and Bard) at replicating manual characterization of software supply chain failures.",
      "Evaluation of LLMs at extracting lessons learned from software supply chain failures; GPT helpfulness scored 3.83/5.",
      "Empirical finding: GPT-3.5 average accuracy 68% vs Bard 58%; GPT ranged 52–88% across predefined dimensions; performance improves with more detailed source articles."
    ]
  },
  {
    "arxiv_id": "2308.16391v2",
    "title": "Improving the Accuracy of Transaction-Based Ponzi Detection on Ethereum",
    "authors": "Phuong Duy Huynh; Son Hoang Dau; Xiaodong Li; Phuc Luong; Emanuele Viterbo",
    "abstract": "The Ponzi scheme, an old-fashioned fraud, is now popular on the Ethereum blockchain, causing considerable financial losses to many crypto investors. A few Ponzi detection methods have been proposed in the literature, most of which detect a Ponzi scheme based on its smart contract source code. This contract-code-based approach, while achieving very high accuracy, is not robust because a Ponzi developer can fool a detection model by obfuscating the opcode or inventing a new profit distribution logic that cannot be detected. On the contrary, a transaction-based approach could improve the robustness of detection because transactions, unlike smart contracts, are harder to be manipulated. However, the current transaction-based detection models achieve fairly low accuracy. In this paper, we aim to improve the accuracy of the transaction-based models by employing time-series features, which turn out to be crucial in capturing the life-time behaviour a Ponzi application but were completely overlooked in previous works. We propose a new set of 85 features (22 known account-based and 63 new time-series features), which allows off-the-shelf machine learning algorithms to achieve up to 30% higher F1-scores compared to existing works.",
    "published_date": "2023-08-31",
    "pdf_link": "https://arxiv.org/pdf/2308.16391v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Blockchain Security",
      "subdomain": "Smart Contract and DApp Fraud Detection",
      "specific_problem": "Transaction-based detection of Ponzi schemes on Ethereum smart contracts",
      "attack_types": [
        "Ponzi scheme",
        "financial scam"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Feature Engineering",
        "specific": null,
        "novel_contribution": "Introduces 63 new time-series features (with 12/24/48-hour aggregation and statistical summarization) combined with 22 known account-based features; trims from 545 engineered features to an 85-feature set via LGBM feature importance"
      },
      {
        "type": "primary",
        "category": "Gradient Boosted Trees",
        "specific": "LightGBM (LGBM)",
        "novel_contribution": "Identified as best-performing classifier on the proposed feature set; used to derive feature importance for feature set trimming"
      },
      {
        "type": "baseline",
        "category": "Random Forest",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Gradient Boosted Trees",
        "specific": "XGBoost",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Dimensionality Reduction",
        "specific": null,
        "novel_contribution": "Applies a dimensionality-reduction technique over 43 constructed time series using a finite set of 12 statistical measures to compress 3-D time-series data into 2-D features"
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "SADPonzi labeled Ponzi and non-Ponzi addresses (from Chen et al. [14])",
        "type": "public",
        "domain": "smart_contract_labels",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "XBlock-ETH Ethereum transactions",
        "type": "public",
        "domain": "blockchain_transactions",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Refined dataset derived by authors (filtered SADPonzi labels + XBlock-ETH transactions)",
        "type": "proprietary",
        "domain": "blockchain_transactions",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Transaction-based model from Jung et al. [32] (Random Forest)",
        "paper_reference": "[32]",
        "metric": "F1-score",
        "their_result": "11% higher F1-score compared to [32]",
        "baseline_result": null
      },
      {
        "method_name": "Transaction-based model from Chen et al. [12] (XGBoost)",
        "paper_reference": "[12]",
        "metric": "F1-score",
        "their_result": "30% higher F1-score compared to [12]",
        "baseline_result": null
      },
      {
        "method_name": "Account-features-only variant (internal baseline)",
        "paper_reference": null,
        "metric": "F1-score",
        "their_result": "Up to 5.7% higher F1-score when adding time-series features vs using only account features",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "Precision",
      "Recall",
      "F1-score",
      "running time"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Contract-code-based detection lacks robustness due to opcode obfuscation and novel profit distribution logic",
        "Most prior transaction-based models overlook the time dimension of application behavior",
        "Existing transaction-based models report low F1-scores (e.g., around 44%–69%)"
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Improve robustness and accuracy of Ethereum Ponzi detection by relying on harder-to-manipulate transactions and capturing temporal behavior via time-series features.",
      "potential_research_ideas": [
        "Temporal graph learning over investor–contract interaction graphs with time-aware GNNs to capture evolving participation patterns",
        "Self-supervised or semi-supervised representation learning from raw transaction sequences to mitigate limited labeled Ponzi data",
        "Online/streaming detection for early-stage Ponzi identification with concept drift handling",
        "Cross-chain Ponzi detection by transferring temporal representations between Ethereum and other EVM-compatible chains",
        "Robustness evaluation against adversarial transaction manipulation (e.g., cost-aware fake transaction injection) and development of defenses",
        "Explainable detection by linking temporal features to human-interpretable lifecycle phases (launch, growth, payout cliffs, collapse)",
        "Integration of event logs (e.g., ERC-20 transfers, emitted events) with ETH flows for richer temporal signals"
      ],
      "architectural_improvement_recommendations": [
        "Replace hand-engineered time-series statistics with a temporal Transformer or TCN that ingests intervalized transaction sequences directly",
        "Model heterogeneous interactions (investments vs payments) with dual-channel encoders and attention over temporal segments",
        "Leverage gradient-boosted trees with calibrated probabilities and cost-sensitive learning to handle class imbalance",
        "Adopt automated feature selection (e.g., BorutaSHAP) alongside LGBM feature importance to validate the 85-feature subset",
        "Fuse account-, time-series-, and graph-structure features using late fusion ensembles"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes a new transaction-based feature set for Ethereum Ponzi detection comprising 85 features (22 known account-based + 63 new time-series features).",
      "Demonstrates that incorporating time-series features improves F1-score by up to 5.7% over account-features-only models.",
      "Shows substantial gains over prior transaction-based baselines: 11% higher F1 vs [32] (Random Forest) and 30% higher F1 vs [12] (XGBoost).",
      "Analyzes temporal behaviors of Ponzi vs non-Ponzi applications (transaction volume concentration near creation, investment/payment sequencing, and balance cliffs).",
      "Uses LGBM feature importance to trim 545 engineered features to an 85-feature subset that improves accuracy, precision, recall, F1-score, and runtime across five off-the-shelf algorithms.",
      "Demonstrates detection of new types of Ponzi schemes not present in the training dataset with high accuracy.",
      "Constructs 43 time-series per application over 12/24/48-hour intervals and applies a dimensionality-reduction technique using 12 statistical measures to create compact temporal features.",
      "Curates a refined labeled dataset (1182 non-Ponzi and 79 Ponzi applications) by combining SADPonzi labels with XBlock-ETH transactions and filtering for quality."
    ]
  },
  {
    "arxiv_id": "2309.00700v1",
    "title": "Cross-temporal Detection of Novel Ransomware Campaigns: A Multi-Modal Alert Approach",
    "authors": "Sathvik Murli; Dhruv Nandakumar; Prabhat Kumar Kushwaha; Cheng Wang; Christopher Redino; Abdul Rahman; Shalini Israni; Tarun Singh; Edward Bowen",
    "abstract": "We present a novel approach to identify ransomware campaigns derived from attack timelines representations within victim networks. Malicious activity profiles developed from multiple alert sources support the construction of alert graphs. This approach enables an effective and scalable representation of the attack timelines where individual nodes represent malicious activity detections with connections describing the potential attack paths. This work demonstrates adaptability to different attack patterns through implementing a novel method for parsing and classifying alert graphs while maintaining efficacy despite potentially low-dimension node features.",
    "published_date": "2023-09-01",
    "pdf_link": "https://arxiv.org/pdf/2309.00700v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Malware Analysis",
      "subdomain": "Ransomware Detection",
      "specific_problem": "Cross-temporal detection of novel ransomware campaigns by modeling multi-source alerts as evolving alert graphs",
      "attack_types": [
        "ransomware",
        "data exfiltration",
        "command-and-control (C2)",
        "lateral movement",
        "DDoS (as diversion)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "GNN",
        "specific": "Graph Isomorphism Network (GIN)",
        "novel_contribution": "GIN-based node embedding over alert graphs combined with multi-stage training and downstream graph classification for ransomware campaign detection"
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "Transformer encoder (on most recent 120 nodes)",
        "novel_contribution": "Applies a transformer encoder to the latest 120 alert nodes to encode temporal/positional context within alert graphs"
      },
      {
        "type": "primary",
        "category": "Pooling/Readout",
        "specific": "Attention-based pooling and summation READOUT",
        "novel_contribution": "Uses attention-based pooling in embedding stage and summation aggregator for final graph-level representation used by the classifier"
      },
      {
        "type": "primary",
        "category": "Contrastive Learning",
        "specific": "GraphCL with NT-Xent loss",
        "novel_contribution": "Self-supervised pretraining with edge perturbation augmentations to improve node/graph embeddings under low-dimensional node features"
      },
      {
        "type": "primary",
        "category": "Metric Learning",
        "specific": "Siamese network with Sub-center ArcFace Loss",
        "novel_contribution": "Campaign-discriminative projection head trained with Sub-center ArcFace Loss to learn separable campaign representations"
      },
      {
        "type": "primary",
        "category": "Multi-task Learning",
        "specific": "Cross-stitch units",
        "novel_contribution": "Cross-stitch units share information between the GraphCL and Siamese projection heads to boost both tasks"
      },
      {
        "type": "primary",
        "category": "CNN",
        "specific": "2D Convolution + MaxPooling",
        "novel_contribution": "Two 2D CNN projection heads operate on concatenated multi-view GIN outputs (treated as an image) for GraphCL and metric-learning tasks"
      },
      {
        "type": "baseline",
        "category": "GNN",
        "specific": "Graph Convolutional Network (GCN) + BCE Loss",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Self-supervised",
      "Metric Learning"
    ],
    "datasets": [
      {
        "name": "Simulated Multi-Modal Alert Graph Dataset (this paper): ransomware and other malware campaign simulations including Yanluowang, Wannacry, Maze, MuddyWater, K12, DeepBlueMagic, Cuba, Conti, Clop; and non-ransomware (APT29, APT39, FireEye assessment tool, AppleJeus, Badcall, Pulse Secure, Hidden Cobra, AA22-216A, Qsnatch)",
        "type": "synthetic",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Graph Convolutional Network (GCN) with BCE Loss",
        "paper_reference": null,
        "metric": "AUC/Precision/Recall",
        "their_result": "Average AUC ≈ 0.9 on unseen campaign combinations",
        "baseline_result": null
      },
      {
        "method_name": "Standard classifier with BCE Loss (non-contrastive, non-metric learning)",
        "paper_reference": null,
        "metric": "AUC/Precision/Recall",
        "their_result": "Average AUC ≈ 0.9 (ours)",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "AUC",
      "precision",
      "recall"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can correlating disparate alerts over extended time windows into alert graphs enable early and robust detection of ransomware campaigns?",
        "Can a model remain effective with low-dimensional node features while adapting to diverse attack paths and patterns?"
      ],
      "gaps_identified": [
        "Prior ransomware detection focuses on static/dynamic binary analysis, overlooking multi-stage campaign timelines and cross-host activity.",
        "Lack of datasets capturing full ransomware campaign timelines across multiple alert sources.",
        "Existing tools generate noisy, general-purpose alerts not tuned for ransomware; need approaches robust to noise and heterogeneity."
      ],
      "limitations": [
        "Performance degrades when trained exclusively on small campaigns but evaluated on larger campaigns; requires a mix of large and small campaigns for robustness.",
        "Evaluation conducted on simulated campaigns; real-world generalization not demonstrated.",
        "Relies on multiple alert sources with heterogeneous formats; integration quality may affect performance."
      ],
      "future_work": [],
      "motivation": "Detect ransomware earlier in the campaign by modeling multi-source alerts as evolving graphs to capture cross-host, cross-stage attack timelines, despite noisy alerts and low-dimensional features.",
      "potential_research_ideas": [
        "Pretrain alert-graph encoders on large unlabeled enterprise alert streams via self-supervised temporal/dynamic-graph objectives to improve generalization.",
        "Develop domain adaptation or federated learning to transfer models across organizations with differing tool stacks and alert distributions.",
        "Incorporate dynamic graph neural networks with continuous-time temporal attention to better model evolving campaigns beyond the latest 120 nodes.",
        "Integrate causal/sequence-of-attacks reasoning (e.g., ATT&CK-informed constraints) to improve early-stage campaign inference.",
        "Design early-warning scoring and time-to-detection optimization, explicitly training for early-stage recall with survival analysis or time-aware losses.",
        "Evaluate and harden against adversarial alert poisoning/edge perturbations; add certified or adversarial training for graph robustness.",
        "Add uncertainty estimation and selective prediction for SOC triage (e.g., conformal prediction on graphs).",
        "Augment node/edge features with richer context (process lineage, auth logs, asset criticality) via heterogeneous graphs."
      ],
      "architectural_improvement_recommendations": [
        "Replace or complement GIN with Graph Transformers or GATv2 to capture long-range cross-host dependencies without restricting to last 120 nodes.",
        "Adopt end-to-end temporal graph encoders (e.g., TGAT/TGN) and positional encodings tailored for irregular timestamps instead of static 120-node windowing.",
        "Use multi-scale readouts (hierarchical pooling over hosts and time) with learnable temporal pooling to emphasize early-stage signals.",
        "Leverage more diverse graph augmentations (feature masking, subgraph sampling, temporal jitter) for GraphCL to improve robustness.",
        "Introduce class-imbalance handling and focal/ASF losses for the classifier to stabilize training when ransomware:malware ratios drift.",
        "Incorporate heterogeneous graph modeling (typed nodes/edges) to encode source tool, host role, and alert category explicitly.",
        "Add calibration layers and temperature scaling for reliable probability outputs used in SOC playbooks."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Enterprise network (simulated multi-host environment with EDR, rules-based network detection, and network flow MSR model)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Model performance depends on training exposure to campaigns of comparable or larger size; exclusively small-campaign training hurts generalization.",
        "Integration across heterogeneous alert sources and formats; quality and coverage of tools may vary and introduce noise/false positives.",
        "Dataset not public; domain shift to real enterprise environments untested."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduce an effective and scalable alert-graph representation built from multiple alert sources, linking detections into potential attack paths across hosts and time.",
      "Propose a novel parsing and classification pipeline for alert graphs combining GIN, transformer encoding on recent nodes, self-supervised GraphCL, Siamese metric learning with Sub-center ArcFace, and cross-stitch multi-task learning.",
      "Demonstrate strong performance on unseen campaigns with an average AUC ≈ 0.9 and efficacy on early campaign stages (first 20–60% of timelines).",
      "Show robustness to noisy, low-dimensional node features and adaptability to different attack patterns."
    ]
  },
  {
    "arxiv_id": "2308.06782v2",
    "title": "PentestGPT: An LLM-empowered Automatic Penetration Testing Tool",
    "authors": "Gelei Deng; Yi Liu; Víctor Mayoral-Vilches; Peng Liu; Yuekang Li; Yuan Xu; Tianwei Zhang; Yang Liu; Martin Pinzger; Stefan Rass",
    "abstract": "Penetration testing, a crucial industrial practice for ensuring system security, has traditionally resisted automation due to the extensive expertise required by human professionals. Large Language Models (LLMs) have shown significant advancements in various domains, and their emergent abilities suggest their potential to revolutionize industries. In this research, we evaluate the performance of LLMs on real-world penetration testing tasks using a robust benchmark created from test machines with platforms. Our findings reveal that while LLMs demonstrate proficiency in specific sub-tasks within the penetration testing process, such as using testing tools, interpreting outputs, and proposing subsequent actions, they also encounter difficulties maintaining an integrated understanding of the overall testing scenario.   In response to these insights, we introduce PentestGPT, an LLM-empowered automatic penetration testing tool that leverages the abundant domain knowledge inherent in LLMs. PentestGPT is meticulously designed with three self-interacting modules, each addressing individual sub-tasks of penetration testing, to mitigate the challenges related to context loss. Our evaluation shows that PentestGPT not only outperforms LLMs with a task-completion increase of 228.6\\% compared to the \\gptthree model among the benchmark targets but also proves effective in tackling real-world penetration testing challenges. Having been open-sourced on GitHub, PentestGPT has garnered over 4,700 stars and fostered active community engagement, attesting to its value and impact in both the academic and industrial spheres.",
    "published_date": "2023-08-13",
    "pdf_link": "https://arxiv.org/pdf/2308.06782v2",
    "paper_types": [
      "benchmark",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Offensive Security",
      "subdomain": "Penetration Testing Automation",
      "specific_problem": "Automating multi-step penetration testing with LLMs via a modular agentic framework that maintains context and plans actions across sub-tasks",
      "attack_types": [
        "OWASP Top 10 vulnerabilities (e.g., injection, broken access control, security misconfiguration, etc.)",
        "Privilege escalation",
        "General network/service exploitation across CTF-like targets"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "LLM Agent / Orchestration",
        "specific": "PentestGPT with Reasoning, Generation, and Parsing modules using GPT-family LLMs",
        "novel_contribution": "Tripartite self-interacting modules and a Pentesting Task Tree (PTT) representation to preserve global context and guide next actions"
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "GPT-4 (OpenAI API) as backbone LLM for the system",
        "novel_contribution": "Used within a structured agentic workflow with PTT and parsing for tool output interpretation"
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "GPT-3.5",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "GPT-4 (direct prompting, no orchestration)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "Bard",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Prompt-based (in-context)",
      "Zero-shot",
      "Tool-augmented agent"
    ],
    "datasets": [
      {
        "name": "Pentest benchmark from HackTheBox and VulnHub (13 targets, 182 sub-tasks covering OWASP Top 10 and 18 CWE items)",
        "type": "public",
        "domain": "penetration_test_targets",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "HackTheBox machines",
        "type": "public",
        "domain": "penetration_test_targets",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "VulnHub machines",
        "type": "public",
        "domain": "penetration_test_targets",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "picoMini CTF challenges (picoCTF)",
        "type": "public",
        "domain": "ctf_challenges",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "GPT-3.5 (direct prompting)",
        "paper_reference": null,
        "metric": "Sub-task completion rate",
        "their_result": "“increases in sub-task completion rates of 228.6% … compared to the GPT-3.5 model among the benchmark targets”",
        "baseline_result": null
      },
      {
        "method_name": "GPT-4 (direct prompting)",
        "paper_reference": null,
        "metric": "Sub-task completion rate",
        "their_result": "“increases in sub-task completion rates of … 58.6% [vs GPT‑4]”",
        "baseline_result": null
      },
      {
        "method_name": "Bard (direct prompting)",
        "paper_reference": null,
        "metric": "Exploratory qualitative/quantitative capability assessment on benchmark tasks",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Human experts / official walkthroughs",
        "paper_reference": null,
        "metric": "Comparative analysis of strategies and progress across decomposed sub-tasks",
        "their_result": "Used for comparison and validation; PentestGPT solved 4/10 active HTB targets and scored 1500/4200 in picoMini (24/248).",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Sub-task completion rate",
      "Task completion rate",
      "CTF score and ranking (picoMini: 1500/4200, rank 24/248)",
      "API cost (USD $131.5 for OpenAI usage)",
      "Progress tracking across NIST 800-115 phases"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "RQ1 (Capability): To what extent can LLMs perform penetration testing tasks?",
        "RQ2 (Comparative Analysis): How do the problem-solving strategies of human penetration testers and LLMs differ?"
      ],
      "gaps_identified": [
        "Absence of a systematic, quantitative assessment of LLMs for penetration testing",
        "Existing benchmarks focus narrowly (e.g., lack privilege escalation) and only on final exploitation success, not progressive accomplishments",
        "LLMs struggle to maintain a coherent grasp of the overarching testing scenario and overemphasize recent context",
        "Automation is impeded by the need to integrate multiple specialized tools and maintain strategic plans"
      ],
      "limitations": [
        "Benchmark does not include benign targets to assess false positives",
        "LLMs may lose sight of earlier discoveries and fail to maintain whole-task context, affecting end-goal completion",
        "Human-in-the-loop executor required for executing commands; full autonomy not achieved",
        "Dependence on proprietary LLM APIs with non-trivial cost",
        "Reported results include only certain real-world tasks (e.g., 4/10 HTB active machines) and CTF evaluation; generalization remains to be further validated"
      ],
      "future_work": [
        "Develop PENTEST PERF, a comprehensive penetration testing benchmark for broad evaluations",
        "Advance MALISM framework to fully automated penetration testing (cybersecurity cognitive engines)",
        "EXPLOIT FLOW to represent exploitation routes and states, enabling learning attack trees and integration with tools like Metasploit"
      ],
      "motivation": "Determine how far LLMs can automate penetration testing and address the lack of robust benchmarks and systematic evaluations in this domain.",
      "potential_research_ideas": [
        "Learn a neural-symbolic planner over Pentesting Task Trees (PTT) to improve long-horizon planning and recovery from dead-ends",
        "Self-play or RL from environment interaction on reproducible exploit labs to learn tool-using policies and reduce reliance on human executors",
        "Memory-augmented agents with retrieval over prior steps, tool outputs, and artifacts to combat recency bias and context loss",
        "Graph-based state tracking with formal attack graphs integrated into the agent loop for consistent goal-directed reasoning",
        "Safety- and attack-aware prompting that detects and mitigates prompt injection and tool-output confusion during web exploitation",
        "Active learning from failed attempts by automatically generating counterfactual playbooks and augmenting prompt libraries",
        "Hybrid local+cloud LLM architecture to reduce costs and latency while preserving capabilities via tool specialization and function-calling"
      ],
      "architectural_improvement_recommendations": [
        "Introduce a persistent, machine-readable PTT/attack-graph store (e.g., graph database) with retrieval and constraint-based planning",
        "Add execution monitoring and verification loops (e.g., tool-grounded checkers) before proceeding to next PTT node",
        "Implement function-calling/tool wrappers for standard pentest tools (nmap, gobuster, sqlmap, metasploit) with schema-validated arguments",
        "Use retrieval-augmented generation over curated exploit knowledge bases (CVE, CWE, vendor advisories) tied to detected banners and versions",
        "Incorporate a long-term memory module (vector DB) to recall earlier discoveries and artifacts across long sessions",
        "Add multi-agent debate (planner vs. critic) to validate chosen paths and reduce hallucinated actions",
        "Optional fine-tuning on exploit flows and historical pentest transcripts to specialize reasoning for common patterns"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/GreyDGL/PentestGPT",
      "frameworks": [
        "Python",
        "OpenAI API"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "No GPU required; relies on API calls to GPT models. Reported OpenAI API cost: $131.5 to solve 4/10 HTB active challenges; human-in-the-loop execution."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Penetration testing workstation executing LLM-suggested commands against HTB/VulnHub targets and CTF infrastructure",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Maintaining global context over long sessions",
        "Integration with heterogeneous pentest tools and parsing diverse outputs",
        "Cost and rate limits of proprietary LLM APIs",
        "Legal/ethical constraints and need for controlled target environments",
        "Human-in-the-loop execution required for safety and correctness"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Development of a comprehensive penetration testing benchmark (13 targets, 182 sub-tasks) covering OWASP Top 10 and 18 CWE items with progress tracking",
      "Systematic exploratory evaluation of GPT-3.5, GPT-4, and Bard on real-world pentesting tasks with human-in-the-loop execution",
      "Design and open-source release of PentestGPT: an LLM-powered automated penetration testing system with Reasoning, Generation, and Parsing modules and a Pentesting Task Tree (PTT) representation",
      "Demonstrated practical effectiveness on HTB active machines (4/10 solved, $131.5 API cost) and picoMini CTF (1500/4200 points, rank 24/248)",
      "Reported substantial gains over direct LLM prompting: “task-completion increase of 228.6% compared to the GPT-3.5 model …” and “58.6% [vs GPT‑4]”"
    ]
  },
  {
    "arxiv_id": "2309.07841v1",
    "title": "Two Timin': Repairing Smart Contracts With A Two-Layered Approach",
    "authors": "Abhinav Jain; Ehan Masud; Michelle Han; Rohan Dhillon; Sumukh Rao; Arya Joshi; Salar Cheema; Saurav Kumar",
    "abstract": "Due to the modern relevance of blockchain technology, smart contracts present both substantial risks and benefits. Vulnerabilities within them can trigger a cascade of consequences, resulting in significant losses. Many current papers primarily focus on classifying smart contracts for malicious intent, often relying on limited contract characteristics, such as bytecode or opcode. This paper proposes a novel, two-layered framework: 1) classifying and 2) directly repairing malicious contracts. Slither's vulnerability report is combined with source code and passed through a pre-trained RandomForestClassifier (RFC) and Large Language Models (LLMs), classifying and repairing each suggested vulnerability. Experiments demonstrate the effectiveness of fine-tuned and prompt-engineered LLMs. The smart contract repair models, built from pre-trained GPT-3.5-Turbo and fine-tuned Llama-2-7B models, reduced the overall vulnerability count by 97.5% and 96.7% respectively. A manual inspection of repaired contracts shows that all retain functionality, indicating that the proposed method is appropriate for automatic batch classification and repair of vulnerabilities in smart contracts.",
    "published_date": "2023-09-14",
    "pdf_link": "https://arxiv.org/pdf/2309.07841v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Blockchain Security",
      "subdomain": "Smart Contract Security",
      "specific_problem": "Vulnerability detection and automated source-level repair of Solidity smart contracts",
      "attack_types": [
        "Solidity smart contract vulnerabilities (generic)",
        "DeadCode",
        "BadPRNG",
        "Assembly",
        "ArrayLengthAssignment",
        "IncorrectSolidityVersionPragma"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Ensemble Tree",
        "specific": "RandomForestClassifier",
        "novel_contribution": "Two-layer pipeline using Slither-derived features (impact) and bag-of-words source code tokens to classify malicious contracts with low false positives"
      },
      {
        "type": "primary",
        "category": "Transformer (LLM)",
        "specific": "Llama-2-7B",
        "novel_contribution": "Fine-tuned via PEFT/QLoRA on paired malicious→repaired contracts to perform automated code repair"
      },
      {
        "type": "primary",
        "category": "Transformer (LLM)",
        "specific": "GPT-3.5-Turbo",
        "novel_contribution": "Prompt-engineered with Chain-of-Thought and Slither vulnerability list to generate repaired source code"
      },
      {
        "type": "primary",
        "category": "Prompt Engineering",
        "specific": "Chain-of-Thought (CoT)",
        "novel_contribution": "Used to elicit step-by-step explanation of vulnerabilities and repairs, improving patch quality"
      },
      {
        "type": "primary",
        "category": "Feature Extraction",
        "specific": "Bag-of-Words (CountVectorizer)",
        "novel_contribution": "Tokenization of Solidity source code as features for RFC"
      },
      {
        "type": "primary",
        "category": "Parameter-Efficient Fine-Tuning",
        "specific": "PEFT/QLoRA (4-bit)",
        "novel_contribution": "Quantized fine-tuning of Llama-2-7B on Colab T4 within 16GB VRAM"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Prompt-based (Chain-of-Thought)",
      "Fine-tuning (supervised)"
    ],
    "datasets": [
      {
        "name": "Etherscan-scraped Ethereum contracts (custom compilation)",
        "type": "proprietary",
        "domain": "smart_contract_source",
        "link": "https://etherscan.io",
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Synthetic malicious Solidity contracts generated via GPT-3.5",
        "type": "synthetic",
        "domain": "smart_contract_source",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "Paired malicious→repaired contracts for Llama-2 fine-tuning (50 pairs)",
        "type": "proprietary",
        "domain": "smart_contract_source",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Slither-only (static analyzer) false positive rate",
        "paper_reference": null,
        "metric": "False positive rate",
        "their_result": "3.8% (RFC on their test set of 800)",
        "baseline_result": "10.9% (Slither, as reported in their Table II benchmark)"
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "F1-score",
      "Confusion matrix",
      "False positive rate",
      "Vulnerability reduction percentage",
      "Count of remaining vulnerabilities",
      "Impact stratification (high/medium/low)"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can a two-layered pipeline combining Slither with a RandomForestClassifier reduce false positives while accurately classifying vulnerable smart contracts?",
        "Can LLMs (prompt-engineered GPT-3.5-Turbo and fine-tuned Llama-2-7B) automatically repair vulnerabilities in Solidity source code while retaining functionality?",
        "Is the combined approach suitable for automatic batch classification and repair of smart contract vulnerabilities?"
      ],
      "gaps_identified": [
        "Lack of a broad approach to smart contract vulnerability detection",
        "Little detail on specific errors provided by prior ML classifiers",
        "High false positive evaluations in existing methods",
        "Lack of a direct repair framework in prior work"
      ],
      "limitations": [
        "Source code often unavailable; required web scraping from Etherscan; dataset reduced from 200,000 to 2,500, then to 2,000 usable contracts",
        "Slither failed on a subset (about 25%) due to compiler/version issues; only ~75% yielded vulnerability results",
        "Difficulty decompiling bytecode to source; reliance on verified source code only",
        "Small fine-tuning set for Llama-2-7B (50 pairs)",
        "Repair evaluation relies on Slither’s reported vulnerabilities rather than ground-truth exploits or formal verification",
        "LLM safety/ethics constraints required prompt adjustments to avoid the term 'malicious'",
        "Repair evaluation done on small samples (20 and 30 contracts) and manual inspection of a subset only"
      ],
      "future_work": [],
      "motivation": "Strengthen smart contracts by enabling accurate detection with low false positives and, crucially, direct automated source-level repair to prevent costly exploits.",
      "potential_research_ideas": [
        "Benchmark the repair pipeline against standard smart contract vulnerability datasets with ground-truth labels and exploits (e.g., curated DeFi incident datasets)",
        "Integrate multiple analyzers (e.g., Slither, Mythril, Echidna fuzzing) for triage, cross-validation, and verifier-in-the-loop repair",
        "Formal verification or symbolic execution as post-repair validators to ensure semantic preservation and security properties",
        "Multi-agent or toolformer-style systems where specialized agents handle specific vulnerability classes (reentrancy, access control, arithmetic, etc.)",
        "Retrieval-augmented repair using a library of historical patches and vulnerability patterns",
        "AST-level or graph-based edit models (program repair via tree/graph transformers) rather than free-form generation",
        "Reinforcement learning from verifier feedback: optimize repairs to minimize analyzer findings and maximize test coverage",
        "Robustness evaluation against adversarially constructed contracts and prompt attacks",
        "Cross-language generalization (e.g., Vyper) and multi-compiler-version compatibility handling",
        "Public release of a large paired malicious→repaired dataset with tests to facilitate reproducible research"
      ],
      "architectural_improvement_recommendations": [
        "Adopt an iterative repair loop with multiple analyzers (Slither + Mythril + Echidna fuzz tests) and self-consistency across LLM samples",
        "Use AST/IR (SlithIR) constrained decoding to ensure syntactic correctness and minimize semantic drift",
        "Introduce retrieval-augmented generation from a patch bank with vector search over vulnerability embeddings",
        "Train larger code-specialized LLMs (e.g., CodeLlama family) with LoRA/QLoRA on more diverse repair pairs",
        "Add unit/integration tests via Foundry/Hardhat; enforce compile-and-test gates for every proposed patch",
        "Calibrate the classifier with better features (e.g., graph-based code representations) and class-imbalance handling",
        "Automate Solidity version normalization and multi-compiler pipelines to reduce analysis failures",
        "Employ self-reflection/critic prompts and majority-vote or reranking on multiple repair candidates"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "scikit-learn",
        "Slither",
        "Hugging Face Transformers",
        "PEFT/QLoRA",
        "OpenAI API",
        "CountVectorizer"
      ],
      "reproducibility_score": "low",
      "computational_requirements": "Llama-2-7B fine-tuned on Google Colab T4 (16GB VRAM) using 4-bit QLoRA; ~100 training steps. RFC trained on tokenized source code and Slither-derived features. GPT-3.5-Turbo via API."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Availability of verified source code; scraping from Etherscan required",
        "Solidity version compatibility and compilation failures impacting analysis",
        "LLM safety constraints when handling 'malicious' terminology",
        "GPU/VRAM constraints for fine-tuning LLMs; required PEFT/QLoRA quantization",
        "Reliance on a single static analyzer (Slither) for evaluation"
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": true,
      "fairness_bias": false
    },
    "contributions": [
      "Two-layered framework combining Slither + RandomForestClassifier for classification and LLMs for automated repair of smart contracts",
      "Prompt-engineered GPT-3.5-Turbo with Chain-of-Thought to repair vulnerabilities while explaining changes",
      "Fine-tuned Llama-2-7B via QLoRA on paired malicious→repaired contracts for source-level patching",
      "Reported classification performance: 89.6% accuracy, F1 = 0.76, with 3.8% false positive rate on test set of 800",
      "Reported repair effectiveness: “reduced the overall vulnerability count by 97.5% and 96.7% respectively,” eliminating all high-impact vulnerabilities in tested samples while retaining functionality in manually reviewed contracts"
    ]
  },
  {
    "arxiv_id": "2308.16769v2",
    "title": "Towards Low-Barrier Cybersecurity Research and Education for Industrial Control Systems",
    "authors": "Colman McGuan; Chansu Yu; Qin Lin",
    "abstract": "The protection of Industrial Control Systems (ICS) that are employed in public critical infrastructures is of utmost importance due to catastrophic physical damages cyberattacks may cause. The research community requires testbeds for validation and comparing various intrusion detection algorithms to protect ICS. However, there exist high barriers to entry for research and education in the ICS cybersecurity domain due to expensive hardware, software, and inherent dangers of manipulating real-world systems. To close the gap, built upon recently developed 3D high-fidelity simulators, we further showcase our integrated framework to automatically launch cyberattacks, collect data, train machine learning models, and evaluate for practical chemical and manufacturing processes. On our testbed, we validate our proposed intrusion detection model called Minimal Threshold and Window SVM (MinTWin SVM) that utilizes unsupervised machine learning via a one-class SVM in combination with a sliding window and classification threshold. Results show that MinTWin SVM minimizes false positives and is responsive to physical process anomalies. Furthermore, we incorporate our framework with ICS cybersecurity education by using our dataset in an undergraduate machine learning course where students gain hands-on experience in practicing machine learning theory with a practical ICS dataset. All of our implementations have been open-sourced.",
    "published_date": "2023-08-31",
    "pdf_link": "https://arxiv.org/pdf/2308.16769v2",
    "paper_types": [
      "benchmark",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Industrial Control Systems Security",
      "subdomain": "Intrusion/Anomaly Detection",
      "specific_problem": "Physical-process anomaly detection for ICS using unsupervised one-class SVM with sliding window and threshold; evaluation on simulated chemical and manufacturing processes with automated attack launching and data collection",
      "attack_types": [
        "Man-in-the-middle (MITM) on Modbus TCP/IP",
        "Sensor value spoofing (single/multiple)",
        "Actuator command spoofing (single/multiple)",
        "Complex attacks (simultaneous sensor and actuator manipulation)",
        "Stealthy attacks (small deviations mimicking normal operation)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "SVM",
        "specific": "One-Class SVM (RBF kernel)",
        "novel_contribution": "MinTWin SVM: one-class SVM combined with a sliding window and a tunable classification threshold to minimize false positives; window size chosen relative to ICS control-cycle length; minimal feature set (sensors, actuators, deltas, PLC outputs) for scalable deployment"
      },
      {
        "type": "baseline",
        "category": "Ensemble Tree",
        "specific": "Isolation Forest",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Density-based",
        "specific": "Local Outlier Factor (LOF)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "GRFICSv2 ICS process dataset (chemical plant) — attacks and benign",
        "type": "synthetic",
        "domain": "log_files",
        "link": "https://github.com/csmcguan/mintwin-svm-grfics",
        "is_new_contribution": true,
        "availability": "available_on_request"
      },
      {
        "name": "Factory I/O production line dataset — attacks and benign",
        "type": "synthetic",
        "domain": "log_files",
        "link": "https://github.com/csmcguan/mintwin-svm-factoryio",
        "is_new_contribution": true,
        "availability": "available_on_request"
      }
    ],
    "baselines": [
      {
        "method_name": "Isolation Forest",
        "paper_reference": "[12]",
        "metric": "TPR/FPR (GRFICSv2)",
        "their_result": "88.9% / 0.0%",
        "baseline_result": "18.4% / 0.0%"
      },
      {
        "method_name": "Local Outlier Factor (LOF)",
        "paper_reference": "[13]",
        "metric": "TPR/FPR (GRFICSv2)",
        "their_result": "88.9% / 0.0%",
        "baseline_result": "100.0% / 100.0%"
      },
      {
        "method_name": "Isolation Forest",
        "paper_reference": "[12]",
        "metric": "TPR/FPR (Factory I/O)",
        "their_result": "100.0% / 0.0%",
        "baseline_result": "85.7% / 0.0%"
      },
      {
        "method_name": "Local Outlier Factor (LOF)",
        "paper_reference": "[13]",
        "metric": "TPR/FPR (Factory I/O)",
        "their_result": "100.0% / 0.0%",
        "baseline_result": "100.0% / 0.0%"
      }
    ],
    "performance_metrics_used": [
      "True Positive Rate (TPR)",
      "False Positive Rate (FPR)",
      "True Positives (TP)",
      "False Positives (FP)",
      "True Negatives (TN)",
      "False Negatives (FN)",
      "Median attack detection time (seconds)",
      "Category-wise identification rates"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can a low-barrier, realistic simulated testbed be built to automate cyberattacks, data collection, training, and evaluation for ICS security research and education?",
        "Can an unsupervised anomaly detector minimize false positives while remaining responsive to physical process anomalies in ICS?",
        "How do sliding window size and classification threshold affect TPR/FPR, and how should they relate to ICS control-cycle length?",
        "Is the proposed detector adaptable across ICS with continuous vs. discrete sensors (chemical plant vs. manufacturing line)?"
      ],
      "gaps_identified": [
        "High barriers to entry for ICS cybersecurity research due to expensive hardware/software and safety risks in real systems",
        "Need for testbeds to validate and compare ICS intrusion detection algorithms",
        "Labeling difficulties in ICS security; benign data is abundant while malicious data is rare",
        "Existing simulated tools lacked integrated automation for attack launching and data collection suitable for benchmarking"
      ],
      "limitations": [
        "TPR on GRFICSv2 is 88.9% with six false negatives; all missed cases are stealthy single-sensor attacks",
        "Detection performance and configuration depend on control-cycle length; sliding window must be tuned per ICS",
        "Trade-off between detecting more stealthy attacks and sharply increased false positives (e.g., 5s window at 60% threshold yields 93.8% stealthy TPR but 82.0% FPR)",
        "Packet-manipulation tooling constraints (Ettercap byte-wise limitation required a custom bitwise tool for Factory I/O)"
      ],
      "future_work": [],
      "motivation": "Lower the barrier for ICS cybersecurity research/education and provide an unsupervised detector that minimizes false positives while being responsive to process anomalies.",
      "potential_research_ideas": [
        "Adaptive/multi-scale windowing and dynamic thresholds based on estimated control-cycle and process state",
        "Hybrid models combining one-class SVM with change-point detection or statistical residual checks to improve stealthy attack detection without raising FPR",
        "Physics-informed anomaly detection incorporating process invariants and PLC logic constraints",
        "Feature selection/representation learning (e.g., autoencoders) to capture subtle process drifts while controlling FPR",
        "Cross-domain transfer learning to port models across different ICS with minimal re-tuning",
        "Incorporate network-layer features (Modbus traffic timing/content) with process data for multi-modal detection",
        "Comprehensive robustness tests against crafted adversarial/stealthy strategies (e.g., mimicry, causally consistent sensor-actuator manipulations)",
        "Evaluation on hardware-in-the-loop or limited real-world ICS pilots to assess deployment hurdles"
      ],
      "architectural_improvement_recommendations": [
        "Implement adaptive sliding windows (e.g., exponential windows) with threshold calibration via conformal prediction to maintain low FPR",
        "Ensemble unsupervised detectors (OC-SVM + Isolation Forest + LOF/autoencoder) with veto logic to preserve low FP while boosting recall on stealthy attacks",
        "Incorporate per-feature anomaly scores and temporal derivatives/aggregates (e.g., EWMA) to enhance sensitivity to subtle changes",
        "Online model updating with drift detection to maintain performance over long operations",
        "Constraint checking layer using PLC/state-machine invariants to filter false alarms and flag causally inconsistent manipulations"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/csmcguan/mintwin-svm-grfics; https://github.com/csmcguan/mintwin-svm-factoryio",
      "frameworks": [
        "PyModbus",
        "Scapy",
        "NetfilterQueue",
        "Ettercap",
        "VirtualBox/VBoxManage"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Simulated ICS: GRFICSv2 chemical plant (Tennessee Eastman process); Factory I/O production line",
      "scalability_discussed": true,
      "inference_time": "Median detection time: 17.7s (GRFICSv2), 8.9s (Factory I/O)",
      "deployment_challenges": [
        "Minimizing false positives is critical in safety-critical ICS",
        "Requires tuning window size to ICS control-cycle length for transfer",
        "Different sensor/actuator types (continuous vs. discrete) affect packet manipulation; required custom bitwise MITM tool",
        "Need to balance stealthy attack recall against FPR for operational viability"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Open-source, realistic, comprehensive simulated framework (GRFICSv2 and Factory I/O) enabling automated attack launching, dataset generation, and evaluation for benchmarking anomaly detectors",
      "Design of MinTWin SVM (one-class SVM with sliding window and threshold) emphasizing minimal false positives and adaptability across ICS; evidence of suitability for continuous monitoring",
      "Integration with ICS cybersecurity education: dataset used in an undergraduate ML course for hands-on experience with practical ICS data; demos provided (https://youtu.be/ckglvMokx6M, https://youtu.be/h-0m85NjdCc)"
    ]
  },
  {
    "arxiv_id": "2310.06841v1",
    "title": "Malware Classification using Deep Neural Networks: Performance Evaluation and Applications in Edge Devices",
    "authors": "Akhil M R; Adithya Krishna V Sharma; Harivardhan Swamy; Pavan A; Ashray Shetty; Anirudh B Sathyanarayana",
    "abstract": "With the increasing extent of malware attacks in the present day along with the difficulty in detecting modern malware, it is necessary to evaluate the effectiveness and performance of Deep Neural Networks (DNNs) for malware classification. Multiple DNN architectures can be designed and trained to detect and classify malware binaries. Results demonstrate the potential of DNNs in accurately classifying malware with high accuracy rates observed across different malware types. Additionally, the feasibility of deploying these DNN models on edge devices to enable real-time classification, particularly in resource-constrained scenarios proves to be integral to large IoT systems. By optimizing model architectures and leveraging edge computing capabilities, the proposed methodologies achieve efficient performance even with limited resources. This study contributes to advancing malware detection techniques and emphasizes the significance of integrating cybersecurity measures for the early detection of malware and further preventing the adverse effects caused by such attacks. Optimal considerations regarding the distribution of security tasks to edge devices are addressed to ensure that the integrity and availability of large scale IoT systems are not compromised due to malware attacks, advocating for a more resilient and secure digital ecosystem.",
    "published_date": "2023-08-21",
    "pdf_link": "https://arxiv.org/pdf/2310.06841v1",
    "paper_types": [
      "empirical_analysis",
      "survey"
    ],
    "security_domain": {
      "primary": "Malware Analysis",
      "subdomain": "Malware Classification",
      "specific_problem": "Multiclass malware family classification from static malware images with edge-device feasibility assessment",
      "attack_types": [
        "Adware",
        "Trojan",
        "Worm",
        "Backdoor",
        "Virus"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "CNN",
        "specific": "ResNetV2 (transfer learning from ImageNet)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "CNN",
        "specific": "DenseNet201 (transfer learning from ImageNet)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "CNN",
        "specific": "InceptionV3 (transfer learning from ImageNet)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "CNN",
        "specific": "Xception (transfer learning from ImageNet)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "CNN",
        "specific": "MobileNet Small (depthwise separable convolutions; transfer learning)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "CNN",
        "specific": "MobileNet Large (depthwise separable convolutions; transfer learning)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Transfer Learning"
    ],
    "datasets": [
      {
        "name": "MaleVis",
        "type": "public",
        "domain": "malware_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "accuracy",
      "recall",
      "F1-score",
      "compute latency (seconds for 550 images)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How effective are Deep Neural Networks (DNNs) for malware classification from static malware images?",
        "What is the latency–accuracy trade-off of different DNN architectures for malware classification?",
        "Is it feasible to deploy DNN-based malware classifiers on edge devices in resource-constrained scenarios?",
        "What considerations guide the distribution of security tasks to edge devices without compromising large-scale IoT systems?"
      ],
      "gaps_identified": [
        "Traditional signature-based methods are ineffective against rapidly evolving and zero-day malware.",
        "Dynamic analysis is complex, time-consuming, and can be evaded by anti-analysis techniques.",
        "Static analysis struggles with packing and obfuscation.",
        "Shallow ML approaches require manual feature engineering and face scalability issues."
      ],
      "limitations": [
        "Observed trade-off between computational latency and accuracy across evaluated architectures.",
        "Ethical and privacy considerations are acknowledged but not empirically addressed."
      ],
      "future_work": [],
      "motivation": "Evaluate DNNs for accurate malware classification and assess their feasibility on edge devices for real-time detection in resource-constrained IoT environments.",
      "potential_research_ideas": [
        "Evaluate cross-dataset generalization and domain adaptation (e.g., train on MaleVis, test on Malimg/Microsoft BIG 2015) to assess robustness.",
        "Develop multimodal malware classifiers that combine image-based features with raw bytes, PE headers, or opcode sequences.",
        "Investigate adversarial robustness of malware image classifiers against evasion (packing, obfuscation, adversarial perturbations).",
        "Federated or split learning for edge/cloud co-training to preserve data privacy while updating models.",
        "Integrate OOD detection and uncertainty estimation to flag unseen/novel malware families.",
        "Automate latency–accuracy co-design via neural architecture search (NAS) constrained for edge hardware.",
        "Use explainability (e.g., Grad-CAM) to interpret salient regions in malware images and support analyst triage."
      ],
      "architectural_improvement_recommendations": [
        "Apply quantization-aware training and structured pruning for MobileNet/ResNet to further reduce edge inference latency.",
        "Use knowledge distillation from high-accuracy DenseNet201/Xception teacher to compact MobileNet student.",
        "Adopt mixed-precision inference and operator fusion tailored to target edge accelerators (e.g., ARM NN, TensorRT, NNAPI).",
        "Introduce data augmentations specific to malware images (e.g., block-wise shuffling, compression artifacts) to improve generalization.",
        "Implement early-exit/branchy networks to dynamically trade accuracy for latency on-device.",
        "Benchmark per-image throughput and memory footprint on real edge devices (e.g., Raspberry Pi, Jetson Nano) and optimize input resolution."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Training on a single NVIDIA RTX 3050 GPU for 10 epochs; transfer learning with ImageNet weights; initial LR=1e-4 with adaptive scheduling (min LR=1e-7); image sizes 224x224 and 300x300; train set 1,400 images (10 classes), test/validation 550 images."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": "Compute latency measured as 3.51–10.87 s for 550 images (≈6.4–19.8 ms/image) depending on model.",
      "deployment_challenges": [
        "Latency–accuracy trade-off on resource-constrained edge devices.",
        "Handling obfuscation and packing in static analysis pipelines.",
        "Privacy and ethical considerations for data collection and deployment.",
        "Model size and memory footprint constraints on IoT hardware."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Empirical evaluation of multiple DNN architectures (ResNetV2, DenseNet201, InceptionV3, Xception, MobileNet Small/Large) for malware image classification using the MaleVis dataset.",
      "Latency–accuracy analysis for 550-image batch inference to gauge edge-device feasibility: \"The compute latency depicts the time taken in seconds to classify 550 test images.\"",
      "Reported quantitative results across models; e.g., \"DenseNet201 showed the best performance among the models evaluated with an accuracy of 94.5\" and MobileNet-Small achieved 3.51 s latency with 85.63% accuracy.",
      "Guidance for selecting models for resource-constrained deployments based on observed trade-offs."
    ]
  },
  {
    "arxiv_id": "2308.10180v1",
    "title": "An IoT Architecture Leveraging Digital Twins: Compromised Node Detection Scenario",
    "authors": "Khaled Alanezi; Shivakant Mishra",
    "abstract": "Modern IoT (Internet of Things) environments with thousands of low-end and diverse IoT nodes with complex interactions among them and often deployed in remote and/or wild locations present some unique challenges that make traditional node compromise detection services less effective. This paper presents the design, implementation and evaluation of a fog-based architecture that utilizes the concept of a digital-twin to detect compromised IoT nodes exhibiting malicious behaviors by either producing erroneous data and/or being used to launch network intrusion attacks to hijack other nodes eventually causing service disruption. By defining a digital twin of an IoT infrastructure at a fog server, the architecture is focused on monitoring relevant information to save energy and storage space. The paper presents a prototype implementation for the architecture utilizing malicious behavior datasets to perform misbehaving node classification. An extensive accuracy and system performance evaluation was conducted based on this prototype. Results show good accuracy and negligible overhead especially when employing deep learning techniques such as MLP (multilayer perceptron).",
    "published_date": "2023-08-20",
    "pdf_link": "https://arxiv.org/pdf/2308.10180v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "IoT Security",
      "subdomain": "Compromised Node Detection",
      "specific_problem": "Detecting compromised IoT nodes via a fog-based Digital Twin architecture combining data anomaly detection and network intrusion detection",
      "attack_types": [
        "False data injection/data anomalies",
        "Probing",
        "Denial of Service (DoS)",
        "Malicious control",
        "Malicious operation",
        "Scan",
        "Spying",
        "Wrong setup/misconfiguration",
        "Mirai botnet traffic",
        "Man-in-the-Middle (MITM) ARP spoofing"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Feedforward Neural Network",
        "specific": "Multilayer Perceptron (MLP)",
        "novel_contribution": "Deployed within a fog-based Digital Twin architecture for online classification; reported good accuracy with negligible overhead in this setting"
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "AnoML-IoT",
        "type": "public",
        "domain": "sensor_data",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "DS2OS",
        "type": "public",
        "domain": "sensor_data",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "IoTID20",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "accuracy",
      "system overhead"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Lack of a systems implementation demonstrating how to utilize Digital Twins in IoT applications and how component interplay affects performance for security use cases"
      ],
      "limitations": [
        "Only the first two DT evolution steps (mirroring and monitoring) are covered; simulation, federation, and autonomous stages are not implemented",
        "Online prototype implementation covers only the AnoML-IoT scenario for system performance measurements",
        "No detailed comparative evaluation against alternative IDS/anomaly detection architectures is provided"
      ],
      "future_work": [
        "Extend DT evolution to simulation, federation, and autonomous stages",
        "Broaden online implementation beyond AnoML-IoT to include additional datasets and attack scenarios",
        "Periodic retraining and model update pipeline is outlined; deeper exploration of adaptation to changing norms is implied"
      ],
      "motivation": "Provide a concrete fog-based Digital Twin architecture for IoT compromised node detection, addressing the gap of missing system-level implementations showing DT utilization and performance impacts in IoT security.",
      "potential_research_ideas": [
        "Implement the remaining DT lifecycle (simulation, federation, autonomous) to enable what-if analysis and coordinated response across many IoT nodes",
        "Develop concept-drift-aware online learning for both data anomaly and network intrusion models within the DT framework",
        "Design multi-modal fusion that jointly reasons over sensor anomalies and network behaviors to reduce false negatives/positives",
        "Investigate adversarial robustness (evasion/poisoning) of DT-driven detectors and design defenses",
        "Incorporate privacy-preserving training/updates (e.g., federated learning) across fog nodes and sites",
        "Add explainability modules for operator trust and faster incident response",
        "Evaluate on diverse real-world IoT deployments (industrial, smart home, healthcare) and under resource constraints",
        "Explore graph-based models (e.g., GNNs) over DT representations to capture inter-device relationships and coordinated attacks",
        "Integrate energy-aware and compressed models for edge/fog efficiency, including on-microcontroller inference where feasible"
      ],
      "architectural_improvement_recommendations": [
        "Introduce streaming feature extraction and message bus integration (e.g., Kafka) between gateway and DT to scale ingestion",
        "Add ensemble of lightweight classical ML with MLP to balance accuracy/latency across device classes",
        "Implement automated concept drift detection and scheduled/triggered retraining with A/B performance monitoring",
        "Adopt model compression/distillation and hardware-aware quantization for low-latency fog inference",
        "Use cross-DT correlation and graph analytics to detect coordinated multi-node attacks",
        "Harden the DT infrastructure (authN/Z, integrity of DT updates) and secure model update channel from cloud to fog",
        "Provide an explainability layer (e.g., feature attribution) in the DT UI for flagged anomalies"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://osf.io/mh6es/?view_only=f7dce520b1b64ce198ab039563c29e5f",
      "frameworks": [
        "Docker",
        "Eclipse Ditto",
        "Python"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Prototype ran with a MacBook Air as fog server and Arduino Uno R3 + Ethernet Shield as IoT node; models deployed as Dockerized Python services loading pickled (*.pkl) classifiers; no specific GPU or training-time requirements reported."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Fog/edge environment: IoT devices via gateway to a fog server (MacBook Air) running Eclipse Ditto and Dockerized model services on a LAN",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Heterogeneity and large scale of IoT devices increase attack surface and monitoring complexity",
        "Resource constraints on devices; need to minimize energy, storage, and network usage by mirroring only relevant features",
        "Remote/wild deployments with limited monitoring and delayed security updates",
        "Need for periodic retraining and reliable parameter updates from cloud to fog",
        "Reliance on administrators for behavior monitoring and malware analysis to establish ground truth"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Detailed design of a fog-based Digital Twin architecture for compromised IoT node detection",
      "Demonstration of how the architecture employs data anomaly and network intrusion models with reported accuracy and system performance",
      "Prototype implementation using open-source components (Eclipse Ditto, Docker) and evaluation showing good accuracy and negligible overhead, especially with MLP"
    ]
  },
  {
    "arxiv_id": "2308.05362v1",
    "title": "FINER: Enhancing State-of-the-art Classifiers with Feature Attribution to Facilitate Security Analysis",
    "authors": "Yiling He; Jian Lou; Zhan Qin; Kui Ren",
    "abstract": "Deep learning classifiers achieve state-of-the-art performance in various risk detection applications. They explore rich semantic representations and are supposed to automatically discover risk behaviors. However, due to the lack of transparency, the behavioral semantics cannot be conveyed to downstream security experts to reduce their heavy workload in security analysis. Although feature attribution (FA) methods can be used to explain deep learning, the underlying classifier is still blind to what behavior is suspicious, and the generated explanation cannot adapt to downstream tasks, incurring poor explanation fidelity and intelligibility. In this paper, we propose FINER, the first framework for risk detection classifiers to generate high-fidelity and high-intelligibility explanations. The high-level idea is to gather explanation efforts from model developer, FA designer, and security experts. To improve fidelity, we fine-tune the classifier with an explanation-guided multi-task learning strategy. To improve intelligibility, we engage task knowledge to adjust and ensemble FA methods. Extensive evaluations show that FINER improves explanation quality for risk detection. Moreover, we demonstrate that FINER outperforms a state-of-the-art tool in facilitating malware analysis.",
    "published_date": "2023-08-10",
    "pdf_link": "https://arxiv.org/pdf/2308.05362v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Malware Analysis",
      "subdomain": "Malware/Vulnerability Detection",
      "specific_problem": "Improving fidelity and intelligibility of feature-attribution explanations for risk detection classifiers to facilitate security analysis",
      "attack_types": [
        "Android malware",
        "Windows PE malware",
        "Software vulnerabilities"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Multi-task Learning",
        "specific": null,
        "novel_contribution": "Explanation-guided data augmentation for risk samples and multi-task fine-tuning of the classifier to improve explanation fidelity without accuracy trade-off"
      },
      {
        "type": "primary",
        "category": "Ensemble / Feature Attribution",
        "specific": null,
        "novel_contribution": "Task-aware adjustment and IC (Intelligible Component)-level ensemble of multiple FA methods with an adapted fidelity metric to improve intelligibility and fidelity"
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "DAMD (Android malware detection), DR-VGG (PE malware detection)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "RNN",
        "specific": "VulDeePecker (vulnerability detection)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "MLP",
        "specific": "Mimicus+, Drebin+ (discussed as pattern-driven baselines in related work)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Feature Attribution",
        "specific": "Gradients",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Feature Attribution",
        "specific": "Integrated Gradients (IG)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Feature Attribution",
        "specific": "DeepLIFT",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Feature Attribution",
        "specific": "LIME",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Feature Attribution",
        "specific": "LEMNA",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Feature Attribution",
        "specific": "Shapley",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Multi-task"
    ],
    "datasets": [
      {
        "name": "Android apps dataset (14K apps)",
        "type": "public",
        "domain": "mobile_apps_android_malware",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "Windows PE binaries dataset (48K binaries)",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "Vulnerability gadgets dataset (32K gadgets)",
        "type": "public",
        "domain": "code_gadgets",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Gradients",
        "paper_reference": "Simonyan et al., 2014",
        "metric": "Descriptive Accuracy (DA), explanation fidelity",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Integrated Gradients",
        "paper_reference": "Sundararajan et al., 2017",
        "metric": "Descriptive Accuracy (DA), explanation fidelity",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "DeepLIFT",
        "paper_reference": "Shrikumar et al., 2017",
        "metric": "Descriptive Accuracy (DA), explanation fidelity",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "LIME",
        "paper_reference": "Ribeiro et al., 2016",
        "metric": "Descriptive Accuracy (DA), explanation fidelity",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "LEMNA",
        "paper_reference": "Guo et al., 2018",
        "metric": "Descriptive Accuracy (DA), explanation fidelity",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Shapley",
        "paper_reference": "Lundberg & Lee, 2017 (SHAP)",
        "metric": "Descriptive Accuracy (DA), explanation fidelity",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "State-of-the-art tool for malicious functionality localization",
        "paper_reference": null,
        "metric": "Malicious functionality localization performance",
        "their_result": "FINER outperforms a state-of-the-art tool in facilitating malware analysis",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Model accuracy",
      "Descriptive Accuracy (DA) for explanation fidelity",
      "Explanation fidelity (drop-in-output after nullifying top-k features)",
      "Intelligibility via IC-level explanations",
      "Malicious functionality localization performance"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "How can we improve the fidelity of FA-based explanations for risk detection classifiers across diverse data representations and model architectures?",
        "How can we improve the intelligibility of explanations so that they align with actionable security analysis needs?",
        "Can explanation-guided multi-task fine-tuning enhance classifier interpretability without sacrificing accuracy?",
        "Can task-aware adjustment and ensemble of FA methods produce higher-fidelity explanations than any single FA method?",
        "Does improving explanation quality translate to measurable gains in downstream malware analysis tasks (e.g., functionality localization)?"
      ],
      "gaps_identified": [
        "Existing FA explanations for risk detection have poor fidelity and cannot adapt to downstream tasks",
        "Security-customized FA methods are task-specific, incur high computational cost, and offer limited generalization",
        "Semantic gap between model features (e.g., opcodes/bytecode) and actionable understanding for analysts limits intelligibility",
        "Lack of labels and general evaluation metrics for local explanations makes assessment non-trivial",
        "Performance of domain-general FA methods is unstable across instances and sensitive to data/model assumptions"
      ],
      "limitations": [
        "Focus on data-driven classifiers; pattern-driven classifiers are discussed separately and may have lower semantic capacity for FA explanations",
        "Local post-hoc explanations only for risky samples; explanations of normality are not the main focus",
        "Requires task knowledge to define IC-level explanations, which may need expert input",
        "Computational cost can be high for perturbation-based explainers on large feature spaces",
        "Some methods require white-box access (gradients, activations), limiting applicability to black-box settings"
      ],
      "future_work": [
        "Extend explanation to normal-class behavior and broader tasks (noted as worthy of investigation)",
        "Apply FINER to additional risk domains and data modalities beyond malware/vulnerability detection",
        "Refine IC definitions and task knowledge interfaces for other analysis tasks",
        "Investigate further automation of task-aware baseline selection and FA method selection"
      ],
      "motivation": "Reduce the heavy workload of security experts by providing high-fidelity and high-intelligibility explanations from risk detection classifiers, overcoming the opacity of deep learning and limitations of existing FA methods.",
      "potential_research_ideas": [
        "Learned concept/IC supervision via weak labels or distant supervision to reduce expert effort in defining intelligible components",
        "Causal attribution for code/malware: integrate causal interventions or counterfactual generation to improve faithfulness beyond DA",
        "Human-in-the-loop explanation refinement with active learning for ICs to iteratively improve intelligibility",
        "Program analysis–augmented FA where static/dynamic analysis guides or constrains attributions for higher semantic alignment",
        "Cross-model consistency training: enforce consistent IC-level attributions across different architectures for robustness",
        "Adversarially robust explanation training to ensure attributions are stable under perturbations or evasion tactics"
      ],
      "architectural_improvement_recommendations": [
        "Introduce concept bottleneck or prototype layers aligned to ICs to natively generate intelligible representations",
        "Use hierarchical attention over code units (tokens → basic blocks → functions) to better map to analyst-relevant ICs",
        "Incorporate GNNs over control/data-flow graphs for code-based tasks and align FA to graph-level concepts",
        "Jointly optimize prediction and an attribution-alignment loss that penalizes reliance on spurious artifacts",
        "Leverage lightweight parameter-efficient fine-tuning (e.g., adapters) for explanation-guided updates across multiple classifiers",
        "Calibrate and normalize attributions across FA methods before ensembling to reduce method-specific biases"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Offline malware/vulnerability analysis workflows to assist security analysts",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Need for task knowledge and IC definitions tailored to each application",
        "High computational cost for perturbation-based FA methods on large feature spaces",
        "Diversity of classifier data representations complicates unified deployment",
        "Potential limited access to model internals in black-box environments"
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Formalization of explainable risk detection system (ERDS) and explanation desiderata; addressing fidelity and intelligibility via explanation-guided model updating and IC-based explanation ensemble",
      "FINER framework implementation with explanation-guided data augmentation, multi-task fine-tuning, task-aware explanation generation, and IC-level ensemble with adapted fidelity metric",
      "Extensive evaluation on Android malware, Windows PE malware, and vulnerability detection using six FA methods; improved interpretability (reported improvements from 21.28% to 82.05% depending on classifier) without accuracy trade-off",
      "Ensemble module achieves higher explanation fidelity than the baseline (reported gains from 10.12% to 17.00% depending on scenario)",
      "FINER outperforms a state-of-the-art tool in malicious functionality localization task for malware analysis",
      "Open-sourced dataset and code"
    ]
  },
  {
    "arxiv_id": "2308.06932v1",
    "title": "DIVAS: An LLM-based End-to-End Framework for SoC Security Analysis and Policy-based Protection",
    "authors": "Sudipta Paria; Aritra Dasgupta; Swarup Bhunia",
    "abstract": "Securing critical assets in a bus-based System-On-Chip (SoC) is imperative to mitigate potential vulnerabilities and prevent unauthorized access, ensuring the integrity, availability, and confidentiality of the system. Ensuring security throughout the SoC design process is a formidable task owing to the inherent intricacies in SoC designs and the dispersion of assets across diverse IPs. Large Language Models (LLMs), exemplified by ChatGPT (OpenAI) and BARD (Google), have showcased remarkable proficiency across various domains, including security vulnerability detection and prevention in SoC designs. In this work, we propose DIVAS, a novel framework that leverages the knowledge base of LLMs to identify security vulnerabilities from user-defined SoC specifications, map them to the relevant Common Weakness Enumerations (CWEs), followed by the generation of equivalent assertions, and employ security measures through enforcement of security policies. The proposed framework is implemented using multiple ChatGPT and BARD models, and their performance was analyzed while generating relevant CWEs from the SoC specifications provided. The experimental results obtained from open-source SoC benchmarks demonstrate the efficacy of our proposed framework.",
    "published_date": "2023-08-14",
    "pdf_link": "https://arxiv.org/pdf/2308.06932v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Hardware Security",
      "subdomain": "SoC Security",
      "specific_problem": "End-to-end automated identification of security weaknesses from SoC specifications, mapping to MITRE CWEs, generating/verifying SystemVerilog Assertions, and enforcing policy-based protections in bus-based SoCs",
      "attack_types": [
        "bus-based attacks",
        "access control violations",
        "side-channel attacks",
        "timing attacks"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer LLM",
        "specific": "ChatGPT",
        "novel_contribution": "Used within a pipeline to map SoC specs to relevant CWEs and generate SVAs"
      },
      {
        "type": "primary",
        "category": "Transformer LLM",
        "specific": "Google Bard (PaLM 2)",
        "novel_contribution": "Used within a pipeline to map SoC specs to relevant CWEs and generate SVAs"
      },
      {
        "type": "primary",
        "category": "LLM Orchestration / Prompting",
        "specific": null,
        "novel_contribution": "LLM-based filtering technique to retain only CWEs relevant to the given SoC context; automated prompt/query generation from structured SoC specs"
      }
    ],
    "learning_paradigm": [
      "Prompt-based inference",
      "Zero-shot"
    ],
    "datasets": [
      {
        "name": "MITRE CWE List",
        "type": "public",
        "domain": "security_taxonomy",
        "link": "https://cwe.mitre.org",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Open-source SoC benchmarks (unspecified)",
        "type": "public",
        "domain": "hardware_designs (SoC RTL/specifications)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Lack of automation frameworks to identify security vulnerabilities and enforce security requirements in complex SoC designs; current processes are predominantly manual",
        "Existing methodologies limited to hardware designs covering specific vulnerabilities, not addressing security requirements for generic bus-based SoCs",
        "Static HDL tools focus on functional/structural checks and miss security weaknesses",
        "Existing techniques are mostly restricted to specific SoC benchmarks and do not fully automate CWE identification and fixes",
        "Need for a standardized way to derive relevant security concerns (CWEs) from SoC design specifications"
      ],
      "limitations": [
        "LLM-generated outputs vary with prompts/time; responses may lack factual accuracy and have limited knowledge base",
        "CWE lists produced by LLMs vary with user specifications and assumptions; necessitates filtering",
        "LLM-generated SVAs may require analysis and correction before use",
        "Simulation-based validation/formal verification alone have limited scope and scalability for complex SoCs (motivational constraint)"
      ],
      "future_work": [],
      "motivation": "SoC security is critical and current practices rely on manual expertise; the complexity and heterogeneity of bus-based SoCs make timely identification and enforcement of security requirements difficult. Leveraging LLM knowledge to automate CWE mapping, assertion generation, and policy enforcement can accelerate and standardize SoC security.",
      "potential_research_ideas": [
        "Domain-adaptive fine-tuning or instruction-tuning of LLMs on hardware security corpora (CWE narratives, AXI/AMBA specs, SVA corpora) to improve factuality and consistency",
        "Retrieval-augmented generation that grounds CWE mapping on an up-to-date CWE knowledge base and SoC documentation to reduce hallucinations",
        "Constrained or grammar-based decoding for synthesizing syntactically correct SystemVerilog Assertions and policy code",
        "Closed-loop co-verification: integrate formal engines to automatically counterexample-check and iteratively refine LLM-generated SVAs/policies",
        "Multi-agent LLM setup where separate agents propose, critique, and consolidate CWE mappings and SVAs to improve precision/recall",
        "Quantitative benchmarking suite for SoC CWE mapping with labeled ground truth to standardize evaluation metrics",
        "Adversarial and robustness evaluation of LLM prompts to ensure stability under prompt perturbations",
        "Extend enforcement beyond bus/IP level to microarchitectural and firmware layers (cross-layer security policies)"
      ],
      "architectural_improvement_recommendations": [
        "Add retrieval augmentation over CWE database and SoC/bus specifications; ground LLM answers via citations to CWE IDs and sections",
        "Use self-consistency and majority voting across multiple LLM samples and across different models (ChatGPT/Bard) before final CWE/SVA selection",
        "Adopt a structured intermediate representation (IR) for security requirements and assets to improve determinism in query generation",
        "Implement grammar-constrained decoding or AST-guided generation for SVAs to ensure syntactic/semantic correctness",
        "Integrate formal verification and fuzzing in-the-loop to validate and prune LLM outputs automatically",
        "Develop a validation dataset and scoring rubric (precision/recall/F1 on CWE mapping; syntax/semantic pass rates for SVAs) for continuous evaluation",
        "Consider lightweight, on-prem LLMs for confidentiality and repeatability in EDA flows"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "OpenAI ChatGPT",
        "Google Bard",
        "SystemVerilog",
        "DiSPEL"
      ],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "SoC design/verification flow with centralized security module on bus interconnect or IP-level wrapper for policy enforcement",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Variability and inconsistency of LLM outputs across prompts/time requiring filtering and human-in-the-loop checks",
        "Potential inaccuracies in LLM-generated SVAs that need correction/verification",
        "Integration into existing RTL design and verification flows without disrupting functionality/timing",
        "Ensuring enforced security policies do not introduce performance regressions or functional bugs",
        "Dependence on third-party LLM services and their model updates affecting repeatability"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "An automated framework that generates queries from user-given SoC specifications/security requirements to identify CWEs via LLMs",
      "Curated CWE lists and classifications for bus-level and IP-level vulnerabilities with an LLM-based filtering methodology to retain relevant CWEs",
      "Analysis and correction process for LLM-generated SVAs enabling simulation-based validation and formal verification",
      "Conversion of SVAs to 3-tuple security policies and enforcement via DiSPEL through centralized bus-level module or IP-level wrapper",
      "End-to-end pipeline from SoC specs to CWE mapping, SVA generation/verification, and policy-based protection using RTL enforcement"
    ]
  },
  {
    "arxiv_id": "2308.15804v3",
    "title": "Collaborative Learning Framework to Detect Attacks in Transactions and Smart Contracts",
    "authors": "Tran Viet Khoa; Do Hai Son; Chi-Hieu Nguyen; Dinh Thai Hoang; Diep N. Nguyen; Tran Thi Thuy Quynh; Trong-Minh Hoang; Nguyen Viet Ha; Eryk Dutkiewicz; Abu Alsheikh; Nguyen Linh Trung",
    "abstract": "With the escalating prevalence of malicious activities exploiting vulnerabilities in blockchain systems, there is an urgent requirement for robust attack detection mechanisms. To address this challenge, this paper presents a novel collaborative learning framework designed to detect attacks in blockchain transactions and smart contracts by analyzing transaction features. Our framework exhibits the capability to classify various types of blockchain attacks, including intricate attacks at the machine code level (e.g., injecting malicious codes to withdraw coins from users unlawfully), which typically necessitate significant time and security expertise to detect. To achieve that, the proposed framework incorporates a unique tool that transforms transaction features into visual representations, facilitating efficient analysis and classification of low-level machine codes. Furthermore, we propose an advanced collaborative learning model to enable real-time detection of diverse attack types at distributed mining nodes. Our model can efficiently detect attacks in smart contracts and transactions for blockchain systems without the need to gather all data from mining nodes into a centralized server. In order to evaluate the performance of our proposed framework, we deploy a pilot system based on a private Ethereum network and conduct multiple attack scenarios to generate a novel dataset. To the best of our knowledge, our dataset is the most comprehensive and diverse collection of transactions and smart contracts synthesized in a laboratory for cyberattack detection in blockchain systems. Our framework achieves a detection accuracy of approximately 94% through extensive simulations and 91% in real-time experiments with a throughput of over 2,150 transactions per second.",
    "published_date": "2023-08-30",
    "pdf_link": "https://arxiv.org/pdf/2308.15804v3",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Blockchain Security",
      "subdomain": "Smart Contract and Transaction Security",
      "specific_problem": "Real-time detection of attacks in blockchain transactions and smart contracts from bytecode/transaction features without centralizing data",
      "attack_types": [
        "Re-entrancy",
        "Delegatecall misuse",
        "Integer overflow",
        "Integer underflow",
        "Denial-of-Service via block gas limit",
        "Flooding of transactions",
        "Function default visibility (FDV)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "CNN",
        "specific": "Deep Convolutional Neural Network",
        "novel_contribution": "CNN trained on grayscale images generated from transaction/bytecode features to classify normal vs. multiple attack types"
      },
      {
        "type": "primary",
        "category": "Federated Learning",
        "specific": "Decentralized collaborative model aggregation (peer-to-peer)",
        "novel_contribution": "Collaborative learning across mining nodes by exchanging trained models and aggregating into a global model at each node without a centralized server"
      },
      {
        "type": "primary",
        "category": "Representation Learning",
        "specific": "Feature-to-image transformation",
        "novel_contribution": "Blockchain Code Extraction and Conversion Tool (BCEC) converts transaction features (e.g., bytecode/opcode/value) into grayscale images for ML"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Federated Learning (decentralized collaborative)"
    ],
    "datasets": [
      {
        "name": "Blockchain Transaction-based Attacks Dataset (BTAT)",
        "type": "synthetic",
        "domain": "blockchain_transactions_and_smart_contracts",
        "link": "https://avitech-vnu.github.io/BTAT",
        "is_new_contribution": true,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Accuracy",
      "Throughput (transactions per second)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How to detect diverse attacks in blockchain transactions and smart contracts in real time without relying on source code (only bytecode/transaction features available at mining nodes)?",
        "How to transform transaction/bytecode content into a representation amenable to efficient ML classification?",
        "How to train effective detection models across distributed mining nodes without centralizing data, preserving privacy and reducing network load?"
      ],
      "gaps_identified": [
        "Lack of a laboratory-synthesized, diverse dataset of normal and attacked transactions/smart contracts for blockchain systems.",
        "Only ~1% of smart contract source code is publicly available; analyzing bytecode without source code is time-consuming and unreliable.",
        "Most existing detection models are centralized and require aggregating data at a server, which is impractical and raises privacy concerns in decentralized blockchain networks."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Escalating malicious activities exploiting blockchain vulnerabilities and the need for robust, real-time, decentralized attack detection without relying on source code or centralized data collection.",
      "potential_research_ideas": [
        "Adversarially robust detection: study robustness of image-based bytecode models against evasion (e.g., opcode padding/reordering) and develop adversarial training or certified defenses.",
        "Multimodal fusion: combine bytecode-image features with transaction graph features (e.g., call graphs, account interaction graphs) via GNNs for improved detection of complex multi-tx attacks.",
        "Cross-chain generalization: extend to other platforms (e.g., Solana, BNB Chain, EVM-compatible chains) and study domain adaptation/transfer learning across chains.",
        "Zero-day/anomaly detection: add unsupervised or self-supervised anomaly detectors to flag novel attack types beyond supervised classes.",
        "Explainability for auditors: generate opcode/bytecode-region attributions to aid incident response and smart contract auditing.",
        "Privacy enhancement: integrate secure aggregation and/or differential privacy into collaborative learning for stronger privacy guarantees.",
        "On-chain/near-chain deployment: explore validators integrating lightweight inference with safe fallback policies (e.g., quarantining suspicious txs).",
        "Data-efficient learning: pretrain on large unlabeled bytecode corpora with contrastive/self-supervised objectives, then fine-tune on labeled attacks."
      ],
      "architectural_improvement_recommendations": [
        "Replace/augment CNN with Vision Transformers or ConvNeXt to capture long-range opcode dependencies in bytecode images.",
        "Hybrid encoder: CNN for local bytecode patterns + Transformer over opcode sequences for global control-flow semantics.",
        "Graph-based branch: build EVM control-flow graphs from bytecode and add a GNN branch fused with image features.",
        "Class imbalance handling with focal loss, mixup/cutmix on images, and hard negative mining.",
        "Federated optimization improvements: use FedAvg with adaptive optimizers (e.g., FedAdam), periodic partial aggregation, and secure aggregation.",
        "Edge efficiency: quantization and pruning to maintain TPS targets; evaluate INT8 inference on validator hardware.",
        "Streaming pipeline: micro-batching with GPU pipelining to guarantee latency SLAs while maintaining 2k+ TPS."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Reported real-time throughput over 2,150 transactions per second on a private Ethereum network; specific hardware/GPU not specified."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Private Ethereum 2.0 network (Proof-of-Stake) with distributed mining/validator nodes in a lab setting",
      "scalability_discussed": true,
      "inference_time": "Throughput >2,150 transactions per second (TPS) reported in real-time experiments",
      "deployment_challenges": [
        "Decentralized data makes centralized collection impractical.",
        "Need to analyze bytecode without source code (only ~1% source code available).",
        "Preserving privacy by avoiding raw data sharing across nodes.",
        "Real-time processing on pending transactions before block validation."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": true,
      "fairness_bias": false
    },
    "contributions": [
      "Novel laboratory-synthesized dataset (BTAT) of normal and attacked transactions/smart contracts in a blockchain system.",
      "Blockchain Code Extraction and Conversion Tool (BCEC) to extract transaction features and convert them into images in real time.",
      "Real-time attack detection framework deployable at mining nodes capable of analyzing ~2,150 transactions per second.",
      "Collaborative learning model enabling distributed miners to exchange trained models and aggregate a global model without a centralized server, preserving data privacy.",
      "Extensive simulations (≈94% accuracy) and real-time experiments (≈91% accuracy) demonstrating effectiveness across diverse attack types."
    ]
  },
  {
    "arxiv_id": "2308.09171v1",
    "title": "Forensic Data Analytics for Anomaly Detection in Evolving Networks",
    "authors": "Li Yang; Abdallah Moubayed; Abdallah Shami; Amine Boukhtouta; Parisa Heidari; Stere Preda; Richard Brunner; Daniel Migault; Adel Larabi",
    "abstract": "In the prevailing convergence of traditional infrastructure-based deployment (i.e., Telco and industry operational networks) towards evolving deployments enabled by 5G and virtualization, there is a keen interest in elaborating effective security controls to protect these deployments in-depth. By considering key enabling technologies like 5G and virtualization, evolving networks are democratized, facilitating the establishment of point presences integrating different business models ranging from media, dynamic web content, gaming, and a plethora of IoT use cases. Despite the increasing services provided by evolving networks, many cybercrimes and attacks have been launched in evolving networks to perform malicious activities. Due to the limitations of traditional security artifacts (e.g., firewalls and intrusion detection systems), the research on digital forensic data analytics has attracted more attention. Digital forensic analytics enables people to derive detailed information and comprehensive conclusions from different perspectives of cybercrimes to assist in convicting criminals and preventing future crimes. This chapter presents a digital analytics framework for network anomaly detection, including multi-perspective feature engineering, unsupervised anomaly detection, and comprehensive result correction procedures. Experiments on real-world evolving network data show the effectiveness of the proposed forensic data analytics solution.",
    "published_date": "2023-08-17",
    "pdf_link": "https://arxiv.org/pdf/2308.09171v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion/Anomaly Detection",
      "specific_problem": "Unsupervised forensic analytics for detecting service-targeting attacks and identifying affected entities (malicious client IPs, abnormal contents, compromised nodes) in evolving networks",
      "attack_types": [
        "Denial of Service (DoS)",
        "Cache Pollution Attacks (CPA)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Unsupervised Anomaly Detection",
        "specific": null,
        "novel_contribution": "Comprehensive framework combining multi-perspective time-based feature engineering with unsupervised detection and result-correction procedures for forensic analytics in evolving networks"
      },
      {
        "type": "primary",
        "category": "Feature Engineering",
        "specific": "Multi-perspective time-based feature engineering",
        "novel_contribution": "Extracts time-based/evolving features to retain time-series correlations across multiple perspectives (clients, contents, nodes) in evolving networks"
      },
      {
        "type": "primary",
        "category": "Post-processing",
        "specific": "Comprehensive result correction procedures",
        "novel_contribution": "Forensic analysis-driven correction/validation of ML identification outputs to reduce false positives and derive actionable digital evidence"
      }
    ],
    "learning_paradigm": [
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "Real-world service data logs from dynamic evolving networks (452M+ unlabeled log lines)",
        "type": "proprietary",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Traditional ADS solutions focus only on ML identification and not the broader digital forensic analytics process",
        "Most existing forensic techniques assume labeled data, which is difficult to obtain in real-world networks",
        "Static network analysis ignores evolving/time-series correlations critical in modern networks",
        "ML-based identification approaches often suffer from high false positive rates and require further analysis/correction",
        "Integration challenges in data collection due to heterogeneous sources, speeds, and formats in evolving networks",
        "Raw network data issues (missing/errors, imbalance, feature ranges) degrade analytics performance without careful preprocessing"
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Address limitations of traditional security artifacts and provide a forensic data analytics framework that detects service-targeting attacks in evolving networks while fingerprinting attackers, victims, and compromised entities with time-aware features and unsupervised methods.",
      "potential_research_ideas": [
        "Develop a semi-supervised/active-learning extension to iteratively label a minimal set of anomalies to reduce false positives and improve precision",
        "Incorporate self-supervised representation learning on raw logs to improve anomaly separability without labels",
        "Model cross-entity relations with a temporal graph-based approach (e.g., entity interaction graphs over time) for multi-entity anomaly attribution",
        "Perform causal inference/temporal root-cause analysis to link anomalous entities and events to attack types and triggering factors",
        "Online/streaming version of the framework for near real-time detection on evolving networks with concept drift handling",
        "Evaluate and harden against adversarial manipulation of log features (evasion/poisoning) typical in stealthy attackers",
        "Add explainability modules to generate human-understandable rationales for flagged anomalies to aid presentation and investigation",
        "Domain adaptation across different network environments (Telco, CDN, IoT) to generalize the framework without retraining from scratch",
        "Integrate external threat intelligence (e.g., known bad IPs, ASNs, content signatures) as priors in unsupervised detection and post-hoc correction"
      ],
      "architectural_improvement_recommendations": [
        "Augment the unsupervised detector with a small supervised head trained on confirmed anomalies for hybrid detection (semi-supervised)",
        "Introduce temporal graph neural networks over client–content–node interactions to capture evolving multi-entity dependencies",
        "Use drift detection and adaptive thresholding to maintain performance under changing traffic patterns",
        "Employ self-supervised pretraining (contrastive learning) on sequences of log events prior to anomaly scoring",
        "Implement rule- and knowledge-based validators in the result-correction stage to encode domain constraints (e.g., service capacity, cache behavior)",
        "Adopt streaming data infrastructure (e.g., Kafka + Flink/Spark Structured Streaming) for scalable online inference and backfill reprocessing"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Evolving networks in Telco/industry contexts with 5G/virtualization; services such as CDN, IoT",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Heterogeneous data collection and integration from multiple sources at different speeds",
        "Large-scale data volumes (hundreds of millions of log lines) requiring scalable processing",
        "High false positive rates typical of unsupervised detection necessitating result correction",
        "Noisy/missing and imbalanced data impacting model performance",
        "Need to present technical results in human-understandable form for investigators"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "A comprehensive digital forensic analytics framework for network anomaly detection aligned with the CEAIP process",
      "Multi-perspective time-based feature engineering to retain time-series correlations in evolving networks",
      "Unsupervised anomaly detection for service-targeting attacks with entity fingerprinting (malicious IPs, abnormal contents, compromised nodes)",
      "Comprehensive result correction procedures to validate and refine ML identification outputs",
      "Empirical evaluation on real-world evolving network service logs (>452M unlabeled records), demonstrating effectiveness"
    ]
  },
  {
    "arxiv_id": "2309.07388v1",
    "title": "On Autonomous Agents in a Cyber Defence Environment",
    "authors": "Mitchell Kiely; David Bowman; Maxwell Standen; Christopher Moir",
    "abstract": "Autonomous Cyber Defence is required to respond to high-tempo cyber-attacks. To facilitate the research in this challenging area, we explore the utility of the autonomous cyber operation environments presented as part of the Cyber Autonomy Gym for Experimentation (CAGE) Challenges, with a specific focus on CAGE Challenge 2. CAGE Challenge 2 required a defensive Blue agent to defend a network from an attacking Red agent. We provide a detailed description of the this challenge and describe the approaches taken by challenge participants. From the submitted agents, we identify four classes of algorithms, namely, Single- Agent Deep Reinforcement Learning (DRL), Hierarchical DRL, Ensembles, and Non-DRL approaches. Of these classes, we found that the hierarchical DRL approach was the most capable of learning an effective cyber defensive strategy. Our analysis of the agent policies identified that different algorithms within the same class produced diverse strategies and that the strategy used by the defensive Blue agent varied depending on the strategy used by the offensive Red agent. We conclude that DRL algorithms are a suitable candidate for autonomous cyber defence applications.",
    "published_date": "2023-09-14",
    "pdf_link": "https://arxiv.org/pdf/2309.07388v1",
    "paper_types": [
      "benchmark",
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection and Response (Autonomous Cyber Defence)",
      "specific_problem": "Learning a Blue agent policy to defend a small enterprise network against a Red attacker performing post-exploitation lateral movement and impact on an operational server within a partially observable environment",
      "attack_types": [
        "Reconnaissance (host and service discovery)",
        "Exploitation",
        "Privilege escalation",
        "Lateral movement",
        "Service disruption/Impact"
      ]
    },
    "ml_techniques": [
      {
        "type": "baseline",
        "category": "Policy Gradient (Actor-Critic) DRL",
        "specific": "PPO",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Hierarchical DRL",
        "specific": "Hierarchical PPO (selector + multiple PPO defenders)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Ensemble RL",
        "specific": "Ensemble of PPO policies",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Ensemble RL",
        "specific": "Ensemble of ensembles (stacked ensembles)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Ensemble RL",
        "specific": "Belief Ensemble PPO",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Value-based Deep RL",
        "specific": "DDDQN (ensemble)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Actor-Critic Deep RL",
        "specific": "IMPALA (ACME)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Exploration Bonus for DRL",
        "specific": "RE3 + PPO",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transfer Learning in DRL",
        "specific": "PPO with Transfer Learning",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Constraint/Action-Masked DRL",
        "specific": "PPO + Action Masking",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Planning/Search",
        "specific": "Monte Carlo Tree Search (MCTS)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Reinforcement Learning",
      "Deep Reinforcement Learning",
      "Planning/Search",
      "Transfer Learning"
    ],
    "datasets": [
      {
        "name": "CAGE Challenge 2 (Cyber Autonomy Gym for Experimentation)",
        "type": "public",
        "domain": "enterprise_network_simulation",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CybORG (Cyber Operations Research Gym)",
        "type": "public",
        "domain": "cyber_operations_simulation",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CAGE Challenges (set of environments)",
        "type": "public",
        "domain": "autonomous_cyber_operations_environments",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Agent 1: Hierarchical PPO + Heuristic",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) over 1000 episodes at 50 timesteps vs B-Line and Meander",
        "their_result": "B-Line: [-6.56, -6.25]; Meander: [-8.82, -8.56]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 1: Hierarchical PPO + Heuristic",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) over 1000 episodes at 100 timesteps vs B-Line and Meander",
        "their_result": "B-Line: [-14.01, -13.51]; Meander: [-16.84, -16.35]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 2: Ensemble of Ensembles PPO",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 50 timesteps",
        "their_result": "B-Line: [-6.05, -5.65]; Meander: [-10.51, -10.16]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 2: Ensemble of Ensembles PPO",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 100 timesteps",
        "their_result": "B-Line: [-11.78, -11.01]; Meander: [-19.71, -19.05]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 3: Hierarchical PPO",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 50 timesteps",
        "their_result": "B-Line: [-6.41, -6.00]; Meander: [-10.27, -9.99]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 3: Hierarchical PPO",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 100 timesteps",
        "their_result": "B-Line: [-13.17, -12.39]; Meander: [-17.94, -17.36]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 4: Hierarchical PPO + Heuristic",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 50 timesteps",
        "their_result": "B-Line: [-6.47, -6.02]; Meander: [-10.23, -9.94]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 4: Hierarchical PPO + Heuristic",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 100 timesteps",
        "their_result": "B-Line: [-13.26, -12.46]; Meander: [-17.89, -17.31]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 5: Hierarchical PPO",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 50 timesteps",
        "their_result": "B-Line: [-7.14, -6.17]; Meander: [-10.11, -9.84]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 5: Hierarchical PPO",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 100 timesteps",
        "their_result": "B-Line: [-13.26, -12.46]; Meander: [-17.63, -17.14]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 6: Ensemble PPO",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 50 timesteps",
        "their_result": "B-Line: [-6.30, -5.91]; Meander: [-10.50, -10.15]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 6: Ensemble PPO",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 100 timesteps",
        "their_result": "B-Line: [-12.28, -11.53]; Meander: [-20.53, -19.77]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 7: Belief Ensemble PPO",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 50 timesteps",
        "their_result": "B-Line: [-6.90, -6.57]; Meander: [-12.28, -11.74]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 7: Belief Ensemble PPO",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 100 timesteps",
        "their_result": "B-Line: [-14.95, -14.30]; Meander: [-26.83, -23.35]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 8: PPO with Transfer Learning",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 50 timesteps",
        "their_result": "B-Line: [-10.03, -9.45]; Meander: [-10.37, -10.04]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 8: PPO with Transfer Learning",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 100 timesteps",
        "their_result": "B-Line: [-20.00, -19.14]; Meander: [-19.05, -18.26]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 9: PPO + Action Masking",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 50 timesteps",
        "their_result": "B-Line: [-8.32, -7.51]; Meander: [-11.46, -10.99]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 9: PPO + Action Masking",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 100 timesteps",
        "their_result": "B-Line: [-18.95, -16.54]; Meander: [-25.14, -23.30]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 10: PPO",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 50 timesteps",
        "their_result": "B-Line: [-8.42, -7.83]; Meander: [-13.19, -12.63]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 10: PPO",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 100 timesteps",
        "their_result": "B-Line: [-17.84, -16.55]; Meander: [-23.45, -22.44]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 11: Hierarchical PPO",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 50 timesteps",
        "their_result": "B-Line: [-7.83, -7.56]; Meander: [-12.10, -11.45]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 11: Hierarchical PPO",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 100 timesteps",
        "their_result": "B-Line: [-15.90, -15.47]; Meander: [-30.74, -28.31]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 12: Ensemble PPO",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 50 timesteps",
        "their_result": "B-Line: [-10.47, -9.92]; Meander: [-11.98, -11.5]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 12: Ensemble PPO",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 100 timesteps",
        "their_result": "B-Line: [-22.63, -21.54]; Meander: [-26.05, -25.04]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 13: PPO",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 50 timesteps",
        "their_result": "B-Line: [-15.37, -14.46]; Meander: [-12.30, -11.78]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 13: PPO",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 100 timesteps",
        "their_result": "B-Line: [-31.26, -29.12]; Meander: [-22.79, -21.63]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 14: Hierarchical PPO",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 50 timesteps",
        "their_result": "B-Line: [-8.69, -8.21]; Meander: [-22.65, -21.95]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 14: Hierarchical PPO",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 100 timesteps",
        "their_result": "B-Line: [-17.75, -17.12]; Meander: [-59.25, -56.19]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 15: ACME IMPALA",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 50 timesteps",
        "their_result": "B-Line: [-10.69, -9.81]; Meander: [-22.48, -22.06]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 15: ACME IMPALA",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 100 timesteps",
        "their_result": "B-Line: [-20.53, -18.90]; Meander: [-55.28, -54.29]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 16: PPO + RE3 (exploration bonus)",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 50 timesteps",
        "their_result": "B-Line: [-24.76, -21.89]; Meander: [-12.94, -11.58]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 16: PPO + RE3 (exploration bonus)",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 100 timesteps",
        "their_result": "B-Line: [-56.88, -51.14]; Meander: [-30.62, -26.75]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 17: Ensemble DDDQN",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 50 timesteps",
        "their_result": "B-Line: [-11.26, -10.37]; Meander: [-26.76, -25.82]",
        "baseline_result": null
      },
      {
        "method_name": "Agent 17: Ensemble DDDQN",
        "paper_reference": null,
        "metric": "Cumulative reward (95% CI) at 100 timesteps",
        "their_result": "B-Line: [-25.08, -22.84]; Meander: [-69.31, -66.47]",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Cumulative reward per episode",
      "95% confidence intervals",
      "Evaluation horizons: 30, 50, 100 timesteps",
      "1000 episodes per Red-agent/horizon pairing"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Which DRL techniques produce effective defensive protocols in autonomous cyber defence, and why?",
        "How do different algorithmic classes (Single-Agent DRL, Hierarchical DRL, Ensembles, Non-DRL) compare under a common evaluation protocol in CAGE Challenge 2?",
        "How do Blue agent strategies vary across algorithms and depending on the Red agent's strategy?"
      ],
      "gaps_identified": [
        "No openly-available systemic classification and comparison of DRL and non-DRL algorithm classes for autonomous cyber operations environments.",
        "Absence of analysis of strengths and weaknesses of each algorithmic class in the cyber security context under a unified protocol."
      ],
      "limitations": [
        "Only sixteen of eighteen submitted agents could be evaluated.",
        "The Non-DRL MCTS approach could not complete the full evaluation due to time constraints; only two episodes per agent-duration pair were run.",
        "Evaluation focused on rules-based Red agents (B-line, Meander, Sleep) within a single enterprise network environment (CAGE Challenge 2)."
      ],
      "future_work": [],
      "motivation": "Facilitate research into autonomous cyber defence by leveraging the CAGE challenges to compare DRL approaches under a unified protocol, develop a taxonomy of solutions, analyze learned policies, and foster a research community.",
      "potential_research_ideas": [
        "Train and evaluate multi-agent Blue defenders that coordinate actions in multi-agent CAGE scenarios.",
        "Develop adaptive selector mechanisms for hierarchical DRL that detect Red strategies online under partial observability.",
        "Use curriculum learning or domain randomization over Red behaviors and network conditions to improve robustness.",
        "Integrate model-based RL or planning components with DRL to better handle the large action space and POMDP structure.",
        "Employ belief-state estimation via recurrent policies (LSTM/GRU) or explicit Bayesian filters to address partial observability.",
        "Evaluate against learning/adaptive Red agents rather than fixed rules-based Red agents.",
        "Investigate transfer and meta-RL across different network topologies and action spaces.",
        "Incorporate cost-aware or constrained RL to manage the Restore action penalty versus defensive effectiveness."
      ],
      "architectural_improvement_recommendations": [
        "Adopt recurrent DRL (e.g., PPO/IMPALA with LSTM/GRU) to handle partial observability.",
        "Use an options framework for hierarchical DRL with learned subpolicies and a robust attacker-type classifier as selector.",
        "Apply action masking and constraint satisfaction to prune ineffective decoy actions per host configuration.",
        "Promote ensemble diversity (e.g., different training seeds/red behaviors) and use weighted or stacking strategies rather than simple majority vote.",
        "Add auxiliary prediction tasks (e.g., Red presence, host compromise state) to improve representations and policy learning.",
        "Leverage offline RL from logged episodes and behavior cloning warm-starts to stabilize training.",
        "Incorporate uncertainty estimates and risk-aware decision-making for safer Restore/Remove choices."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Simulated small enterprise network (CAGE Challenge 2 via CybORG)",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Partially observable stochastic environment complicates state estimation.",
        "Large discrete action space (145 actions) increases exploration and learning difficulty.",
        "Need to maintain network functionality while countering Red activity and managing Restore costs.",
        "Varying Red strategies (B-line, Meander, Sleep) require adaptable policies."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Detailed description of the CAGE Challenge 2 autonomous cyber defence environment and evaluation protocol.",
      "Taxonomy of solution approaches categorized into Single-Agent DRL, Hierarchical DRL, Ensembles, and Non-DRL.",
      "Empirical evaluation of 16 submitted Blue agents across three horizons (30/50/100 timesteps) against multiple rule-based Red agents.",
      "Finding: Hierarchical DRL approaches were the most capable of learning effective cyber defensive strategies under the given protocol.",
      "Analysis of learned policies showing diverse strategies within classes and dependence on the Red agent’s behavior.",
      "Conclusion that DRL algorithms are suitable candidates for autonomous cyber defence applications."
    ]
  },
  {
    "arxiv_id": "2308.04964v4",
    "title": "ModSec-AdvLearn: Countering Adversarial SQL Injections with Robust Machine Learning",
    "authors": "Giuseppe Floris; Christian Scano; Biagio Montaruli; Luca Demetrio; Andrea Valenza; Luca Compagna; Davide Ariu; Luca Piras; Davide Balzarotti; Battista Biggio",
    "abstract": "Many Web Application Firewalls (WAFs) leverage the OWASP CRS to block incoming malicious requests. The CRS consists of different sets of rules designed by domain experts to detect well-known web attack patterns. Both the set of rules and the weights used to combine them are manually defined, yielding four different default configurations of the CRS. In this work, we focus on the detection of SQLi attacks, and show that the manual configurations of the CRS typically yield a suboptimal trade-off between detection and false alarm rates. Furthermore, we show that these configurations are not robust to adversarial SQLi attacks, i.e., carefully-crafted attacks that iteratively refine the malicious SQLi payload by querying the target WAF to bypass detection. To overcome these limitations, we propose (i) using machine learning to automate the selection of the set of rules to be combined along with their weights, i.e., customizing the CRS configuration based on the monitored web services; and (ii) leveraging adversarial training to significantly improve its robustness to adversarial SQLi manipulations. Our experiments, conducted using the well-known open-source ModSecurity WAF equipped with the CRS rules, show that our approach, named ModSec-AdvLearn, can (i) increase the detection rate up to 30%, while retaining negligible false alarm rates and discarding up to 50% of the CRS rules; and (ii) improve robustness against adversarial SQLi attacks up to 85%, marking a significant stride toward designing more effective and robust WAFs. We release our open-source code at https://github.com/pralab/modsec-advlearn.",
    "published_date": "2023-08-09",
    "pdf_link": "https://arxiv.org/pdf/2308.04964v4",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Web Application Security",
      "subdomain": "Web Application Firewalls (WAF)",
      "specific_problem": "Detection of SQL Injection (SQLi) and robustness against adversarial SQLi evasion",
      "attack_types": [
        "SQL Injection (SQLi)",
        "Adversarial evasion (black-box, mutational fuzzing)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "SVM",
        "specific": "Linear SVM with ℓ∞-norm regularization (SecSVM)",
        "novel_contribution": "Shows feature-space adversarial training against sparse (ℓ1) perturbations is equivalent to training a linear SVM with ℓ∞ regularization via weight bounding; implemented as a linear program (SecSVM) to harden CRS-rule-weighted detection"
      },
      {
        "type": "primary",
        "category": "Adversarial Training",
        "specific": "Problem-space AT with WAF-A-MoLE",
        "novel_contribution": "First demonstration of adversarial training in WAF domain for SQLi using realizable, functionality-preserving input-space manipulations generated by a mutational fuzzer"
      },
      {
        "type": "primary",
        "category": "Linear Model",
        "specific": "Logistic Regression (ℓ1/ℓ2)",
        "novel_contribution": "Learns optimal CRS rule weights and enables sparsity for automatic rule selection"
      },
      {
        "type": "baseline",
        "category": "Random Forest",
        "specific": "RF on CRS rule-activation features",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Rule-based system",
        "specific": "OWASP CRS default configurations (PL1–PL4) in ModSecurity",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Adversarial training",
      "Robust optimization"
    ],
    "datasets": [
      {
        "name": "Public SQLi/WAF dataset from prior work [10]",
        "type": "public",
        "domain": "web_requests",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Public SQLi/WAF dataset from prior work [11]",
        "type": "public",
        "domain": "web_requests",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "ModSecurity + OWASP CRS PL1 (default)",
        "paper_reference": null,
        "metric": "Detection rate / False alarm rate",
        "their_result": "“increase the detection rate up to 30%, while retaining negligible false alarm rates”",
        "baseline_result": "Manual CRS configuration yields suboptimal trade-off; lower detection at comparable low FPR"
      },
      {
        "method_name": "ModSecurity + OWASP CRS PL2–PL4 (default)",
        "paper_reference": null,
        "metric": "Detection rate / False alarm rate",
        "their_result": "Automatically learned weights discard up to 50% of rules while improving detection",
        "baseline_result": "Higher PL increases detection but also false alarms; manual severity weights"
      },
      {
        "method_name": "CRS (any PL) under adversarial SQLi (WAF-A-MoLE)",
        "paper_reference": null,
        "metric": "Robustness to adversarial SQLi (attack success/evasion rate)",
        "their_result": "“improve robustness against adversarial SQLi attacks up to 85%”",
        "baseline_result": "Default CRS configurations are not robust; high evasion rate under adversarial SQLi"
      },
      {
        "method_name": "ModSec-Learn without adversarial training (linear LR/SVM on CRS features)",
        "paper_reference": null,
        "metric": "Robustness to adversarial SQLi",
        "their_result": "Adversarial training (problem-space or SecSVM) outperforms non-AT models in robustness",
        "baseline_result": "Non-AT models achieve improved clean detection but lower robustness"
      }
    ],
    "performance_metrics_used": [
      "Detection rate (true positive rate)",
      "False alarm rate (false positive rate)",
      "Robustness against adversarial SQLi (reduction in attack evasion / increase in robust detection)",
      "Rule count/selection (model sparsity)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can machine learning automatically select and weight CRS rules to improve the detection vs. false alarm trade-off for SQLi in WAFs?",
        "Can adversarial training significantly improve robustness of WAFs against adversarial SQLi manipulations generated by state-of-the-art fuzzers?",
        "Is ℓ∞-regularized linear SVM an effective and efficient surrogate for feature-space adversarial training under sparse perturbations?"
      ],
      "gaps_identified": [
        "Manual CRS configurations yield suboptimal detection/false-alarm trade-offs.",
        "Default CRS configurations lack robustness to adversarial SQLi attacks generated via black-box mutational fuzzing.",
        "CRS severity weights are manually assigned and not data-driven.",
        "Feature-space AT assumptions (independent rule toggling) may not reflect realizable SQLi manipulations."
      ],
      "limitations": [
        "Focus limited to SQLi (one attack class within CRS); generalization to other attack classes not demonstrated in provided text.",
        "Integration of non-linear models within CRS may pose complexity, scalability, and interpretability issues (acknowledged by authors).",
        "Experimental details beyond two publicly-available datasets are not provided in the excerpt."
      ],
      "future_work": [],
      "motivation": "Improve WAF effectiveness by data-driven customization of OWASP CRS rule selection and weighting, and enhance robustness against adversarial SQLi evasion.",
      "potential_research_ideas": [
        "Extend ModSec-AdvLearn to other CRS attack classes (XSS, RCE) and evaluate cross-attack transfer of robustness.",
        "Develop certified robustness guarantees for regex-based WAFs against bounded sets of SQLi manipulations.",
        "Online/continual learning to adapt rule weights to evolving traffic and attack patterns while controlling false alarms.",
        "Neuro-symbolic rule induction: learn new regex-like rules or augment existing CRS rules using program synthesis guided by adversarial examples.",
        "Multi-objective optimization to jointly optimize detection, robustness, and rule sparsity with explicit constraints.",
        "Transfer learning/meta-learning of rule weights across services with small labeled data.",
        "Combine rule-activation features with token-level models (e.g., lightweight transformers) for hybrid detection while preserving interpretability via linear heads.",
        "Adversary-aware evaluation frameworks that vary fuzzer budgets, query limits, and black-box feedback to map robustness under realistic constraints."
      ],
      "architectural_improvement_recommendations": [
        "Use group-sparse regularization (group lasso) over CRS rule families to encourage compact, semantically coherent rule subsets.",
        "Ensemble a SecSVM with the default CRS scoring to preserve fail-safe behavior while gaining robustness (oracle/AND-OR fusion).",
        "Calibrate thresholds per endpoint/parameter with conformal prediction to maintain negligible false alarms.",
        "Incorporate monotonic linear models to ensure weight signs align with expert knowledge and reduce unintended rule inversions.",
        "Leverage differentiable approximations of regex matching to allow gradient-based joint training on raw inputs plus rule features, then distill back to linear weights for deployment.",
        "Mixed-integer optimization to select a minimal rule subset under constraints on FPR and computational cost."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/pralab/modsec-advlearn",
      "frameworks": [
        "SciPy (linear programming solver)",
        "scikit-learn",
        "ModSecurity (runtime WAF engine; integration target)"
      ],
      "reproducibility_score": "high",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "ModSecurity WAF with OWASP CRS protecting enterprise web applications",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Per-service customization and re-training to align with monitored web traffic.",
        "Model drift and need for periodic re-calibration to maintain low false alarms.",
        "Potential complexity and interpretability issues if using non-linear models (authors favor linear for deployment).",
        "Adversary adaptation—robustness must be maintained against evolving fuzzing/manipulation strategies.",
        "Integration of learned weights into CRS pipelines and ensuring compatibility with existing rule processing."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": true,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Automated selection and weighting of OWASP CRS rules via supervised ML on rule-activation features (ModSec-Learn).",
      "Novel robust training scheme (ModSec-AdvLearn) combining problem-space adversarial training with WAF-A-MoLE and feature-space robustness via ℓ∞-regularized linear SVM (SecSVM).",
      "Theoretical and practical linkage between feature-space AT under sparse manipulations and ℓ∞ regularization for linear SVMs, enabling efficient robust training without attack optimization.",
      "Empirical results on two public datasets showing up to +30% detection rate at negligible false alarms while discarding up to 50% of CRS rules.",
      "Robustness improvement against adversarial SQLi up to +85% compared to default CRS configurations.",
      "Open-sourced codebase for reproducibility and adoption."
    ]
  },
  {
    "arxiv_id": "2308.04177v2",
    "title": "How Generalizable are Deepfake Image Detectors? An Empirical Study",
    "authors": "Boquan Li; Jun Sun; Christopher M. Poskitt; Xingmei Wang",
    "abstract": "Deepfakes are becoming increasingly credible, posing a significant threat given their potential to facilitate fraud or bypass access control systems. This has motivated the development of deepfake detection methods, in which deep learning models are trained to distinguish between real and synthesized footage. Unfortunately, existing detectors struggle to generalize to deepfakes from datasets they were not trained on, but little work has been done to examine why or how this limitation can be addressed. Especially, those single-modality deepfake images reveal little available forgery evidence, posing greater challenges than detecting deepfake videos. In this work, we present the first empirical study on the generalizability of deepfake detectors, an essential goal for detectors to stay one step ahead of attackers. Our study utilizes six deepfake datasets, five deepfake image detection methods, and two model augmentation approaches, confirming that detectors do not generalize in zero-shot settings. Additionally, we find that detectors are learning unwanted properties specific to synthesis methods and struggling to extract discriminative features, limiting their ability to generalize. Finally, we find that there are neurons universally contributing to detection across seen and unseen datasets, suggesting a possible path towards zero-shot generalizability.",
    "published_date": "2023-08-08",
    "pdf_link": "https://arxiv.org/pdf/2308.04177v2",
    "paper_types": [
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Multimedia Security",
      "subdomain": "Deepfake Detection",
      "specific_problem": "Generalizability of deepfake image detectors (especially zero-shot across unseen datasets)",
      "attack_types": [
        "face swapping",
        "face reenactment",
        "GAN-based deepfakes"
      ]
    },
    "ml_techniques": [
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "MesoNet",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "MesoInception",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "ShallowNet (V3 referenced)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "XceptionNet",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "EfficientNet",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Model augmentation (general)",
        "specific": null,
        "novel_contribution": "Two augmentation approaches are used in experiments (not specified in provided excerpt)."
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "FaceForensics++ (FF++) C23 (FS, DF, F2F, NT sub-datasets)",
        "type": "public",
        "domain": "face_images",
        "link": "https://github.com/ondyari/FaceForensics",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "DeepFakeDetection (DFD)",
        "type": "public",
        "domain": "face_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CELEB-DF-V2",
        "type": "public",
        "domain": "face_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CELEB-DF-V1 (mentioned, not used)",
        "type": "public",
        "domain": "face_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "accuracy"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "RQ1: Do existing detectors generalize to deepfakes across different datasets?",
        "RQ2: Do detectors learn deepfake properties that are unwanted for generalization?",
        "RQ3: Do detectors extract meaningful features that are discriminative enough for generalization?",
        "RQ4: Do detection models have neurons that are universally contributing across seen and unseen datasets?"
      ],
      "gaps_identified": [
        "Existing deepfake image detectors are not generalizable in zero-shot settings.",
        "Lack of a systematic, comparative evaluation of detectors and augmentation approaches for generalizability.",
        "Detectors learn unwanted properties specific to synthesis methods, harming cross-dataset generalization.",
        "Detectors struggle to extract discriminative features that support generalization."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Generalizable deepfake image detectors are essential to stay ahead of attackers; single-modality images leave limited forgery evidence, making generalization challenging and underexplored.",
      "potential_research_ideas": [
        "Develop zero-shot deepfake detectors that explicitly learn synthesis-invariant representations guided by analysis of universal neurons.",
        "Design training objectives that penalize reliance on synthesis/dataset-specific artifacts (e.g., invariance or domain-contrastive losses).",
        "Neuron-level selection/pruning or causal intervention to amplify universally contributing neurons across datasets.",
        "Self-supervised or foundation-model pretraining on large-scale real face image corpora followed by artifact-focused fine-tuning for zero-shot transfer.",
        "Robust domain generalization pipelines (e.g., style augmentation, feature whitening) tailored to deepfake artifacts rather than content.",
        "Unified cross-dataset benchmark protocol for zero-shot image deepfake detection with standardized splits and reporting."
      ],
      "architectural_improvement_recommendations": [
        "Incorporate domain-invariant representation learning (e.g., IRM-style penalties or feature whitening) to reduce synthesis-method shortcuts.",
        "Add supervised contrastive or metric-learning heads that cluster real vs. fake across multiple synthesis methods while pushing apart method-specific cues.",
        "Introduce neuron-importance regularization using causal attribution to upweight universal neurons and downweight dataset-specific ones.",
        "Apply feature disentanglement to separate identity/content from forgery cues; adversarially remove synthesis-style information.",
        "Use multi-source training across diverse synthesis pipelines with distributionally robust optimization to handle unseen targets."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/boutiquelee/DeepfakeEmpiricalStudy",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Domain shift across synthesis methods and datasets causing poor zero-shot generalization.",
        "Models latch onto dataset/synthesis-specific artifacts rather than generalizable cues.",
        "Limited discriminative evidence in single-modality images vs. video-based methods."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "First empirical study on the generalizability of deepfake image detectors (images).",
      "Comprehensive evaluation across six datasets, five detectors, and two model augmentation approaches (study confirms zero-shot non-generalizability; some mitigation in few-shot).",
      "Evidence that detectors learn synthesis-method-specific unwanted properties, limiting generalization.",
      "Interpretability analysis showing detectors struggle to extract discriminative features for generalization.",
      "Identification of neurons that universally contribute to detection across seen and unseen datasets, suggesting a path to zero-shot generalizability.",
      "Released repository of datasets, trained detectors, and code for the community."
    ]
  },
  {
    "arxiv_id": "2308.12175v1",
    "title": "Unsupervised anomalies detection in IIoT edge devices networks using federated learning",
    "authors": "Niyomukiza Thamar; Hossam Samy Elsaid Sharara",
    "abstract": "In a connection of many IoT devices that each collect data, normally training a machine learning model would involve transmitting the data to a central server which requires strict privacy rules. However, some owners are reluctant of availing their data out of the company due to data security concerns. Federated learning(FL) as a distributed machine learning approach performs training of a machine learning model on the device that gathered the data itself. In this scenario, data is not share over the network for training purpose. Fedavg as one of FL algorithms permits a model to be copied to participating devices during a training session. The devices could be chosen at random, and a device can be aborted. The resulting models are sent to the coordinating server and then average models from the devices that finished training. The process is repeated until a desired model accuracy is achieved. By doing this, FL approach solves the privacy problem for IoT/ IIoT devices that held sensitive data for the owners. In this paper, we leverage the benefits of FL and implemented Fedavg algorithm on a recent dataset that represent the modern IoT/ IIoT device networks. The results were almost the same as the centralized machine learning approach. We also evaluated some shortcomings of Fedavg such as unfairness that happens during the training when struggling devices do not participate for every stage of training. This inefficient training of local or global model could lead in a high number of false alarms in intrusion detection systems for IoT/IIoT gadgets developed using Fedavg. Hence, after evaluating the FedAv deep auto encoder with centralized deep auto encoder ML, we further proposed and designed a Fair Fedavg algorithm that will be evaluated in the future work.",
    "published_date": "2023-08-23",
    "pdf_link": "https://arxiv.org/pdf/2308.12175v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "IoT/IIoT Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Unsupervised anomaly detection for IIoT edge device network traffic using federated learning with attention to fairness under stragglers",
      "attack_types": [
        "DDoS",
        "Botnets (e.g., Mirai)",
        "Man-in-the-Middle (MITM)",
        "Malware"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Federated Learning",
        "specific": "FedAvg (synchronous)",
        "novel_contribution": "Implements unsupervised deep autoencoder under FedAvg on a modern IIoT dataset; analyzes fairness issues; proposes a Fair FedAvg variant to penalize large client deviations (design only, not evaluated)."
      },
      {
        "type": "primary",
        "category": "Neural Network",
        "specific": "Deep Autoencoder (unsupervised)",
        "novel_contribution": "Used as the anomaly detector both in centralized and federated settings."
      },
      {
        "type": "baseline",
        "category": "Neural Network",
        "specific": "Centralized Deep Autoencoder",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Unsupervised",
      "Federated Learning"
    ],
    "datasets": [
      {
        "name": "Edge-IIoTset 2022",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "NSL-KDD",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Mirai-infected IoT traffic (Hafeez et al., IOT-KEEPER experiments)",
        "type": "proprietary",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Centralized Deep Autoencoder",
        "paper_reference": null,
        "metric": "accuracy",
        "their_result": "“The results were almost the same as the centralized machine learning approach.”",
        "baseline_result": "Comparable to federated approach (no exact numbers reported)."
      }
    ],
    "performance_metrics_used": [
      "accuracy"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How to detect anomalies in modern IoT/IIoT networks using privacy-preserving federated learning with unlabeled, distributed data?",
        "Can an unsupervised deep autoencoder trained with FedAvg match centralized performance on a modern IIoT dataset?",
        "How do stragglers and device heterogeneity impact fairness and false alarms in synchronous FedAvg for IIoT IDS?"
      ],
      "gaps_identified": [
        "Many prior works train/evaluate on datasets that do not represent modern IoT/IIoT traffic and attacks.",
        "IIoT data at the edge is often unlabeled, necessitating unsupervised methods.",
        "Vanilla synchronous FedAvg assumes homogeneous clients; stragglers and variable compute/network conditions cause unfair participation and biased models.",
        "Communication constraints and device heterogeneity hinder convergence and can increase false alarms."
      ],
      "limitations": [
        "Fair FedAvg is only proposed/designed; not experimentally evaluated.",
        "Experiments consider IID client data (“considering the presence of Independent and Identically Distributed data across clients”), limiting realism for non-IID IIoT scenarios.",
        "No detailed quantitative metrics (e.g., precision/recall/F1/AUC) or numerical results are reported in the provided text.",
        "No robustness evaluation against adversarial/poisoning attacks.",
        "No real-world deployment or system-level evaluation.",
        "Only FedAvg is studied among FL optimizers; no comparison to alternatives (FedProx, SCAFFOLD, FedNova, etc.)."
      ],
      "future_work": [
        "Evaluate the proposed Fair FedAvg algorithm that penalizes large local deviations to improve fairness and communication efficiency.",
        "Address non-IID data distributions and client heterogeneity in IIoT.",
        "Mitigate increased false alarms due to straggler-related unfairness.",
        "Improve convergence under communication limitations."
      ],
      "motivation": "Preserve privacy and comply with regulations (e.g., GDPR) while enabling anomaly-based IDS for IoT/IIoT using modern datasets; address heterogeneity and fairness issues in federated learning to avoid high false alarms.",
      "potential_research_ideas": [
        "Develop and evaluate a concrete Fair FedAvg implementation with adaptive regularization that penalizes client drift (e.g., proximal or control variate terms) tailored to unsupervised anomaly detection.",
        "Incorporate asynchronous or semi-synchronous aggregation with resource-aware client selection to reduce straggler impact while preserving fairness.",
        "Personalized FL for IIoT IDS (e.g., clustered FL, meta-learning) to handle non-IID device behaviors and reduce false alarms.",
        "Robust aggregation against poisoning/backdoor attacks (e.g., Krum, Trimmed Mean, FLTrust) specifically evaluated for autoencoder-based anomaly detection.",
        "Integrate differential privacy and secure aggregation to strengthen privacy without degrading anomaly detection (tune noise using anomaly thresholds).",
        "Self-supervised or contrastive pretraining on network flows prior to federated autoencoder fine-tuning to improve representation quality.",
        "Drift detection and adaptive thresholding for autoencoder reconstruction errors in federated settings.",
        "Multi-modal IIoT IDS combining flow features with device telemetry or logs in a hybrid federated architecture."
      ],
      "architectural_improvement_recommendations": [
        "Replace/augment FedAvg with FedProx or SCAFFOLD to mitigate client drift; evaluate against Fair FedAvg design.",
        "Adopt robust aggregators (Median, Trimmed Mean, Krum, FLTrust) to handle outliers and potential poisoning.",
        "Use variational autoencoders or sequence models (GRU/LSTM/Temporal CNN) to capture temporal patterns in IIoT flows.",
        "Implement asynchronous FL with staleness-aware weighting and resource-aware client selection to reduce waiting on stragglers.",
        "Introduce adaptive anomaly thresholds using federated calibration or percentile-based methods per client to reduce false alarms.",
        "Add differential privacy (DP-SGD) and secure aggregation for stronger privacy guarantees.",
        "Evaluate on non-IID partitioning schemes of Edge-IIoTset to stress-test fairness and performance."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Conceptual IIoT edge network with central FL server and participating edge devices",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Data privacy and regulatory compliance",
        "Client heterogeneity (compute, battery, network)",
        "Stragglers and communication delays in synchronous FL",
        "Unlabeled data requiring unsupervised learning",
        "Potential increase in false alarms due to unfair participation"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": true,
      "fairness_bias": true
    },
    "contributions": [
      "Implements unsupervised federated learning (FedAvg) with a deep autoencoder on a modern IIoT dataset (Edge-IIoTset 2022).",
      "Empirically compares federated versus centralized deep autoencoder, reporting similar performance (“The results were almost the same as the centralized machine learning approach.”).",
      "Analyzes shortcomings of synchronous FedAvg (unfairness due to stragglers/heterogeneity) and the risk of increased false alarms for IIoT IDS.",
      "Proposes and designs a Fair FedAvg algorithm that penalizes large local deviations to improve fairness and communication efficiency (to be evaluated in future work)."
    ]
  },
  {
    "arxiv_id": "2308.02031v1",
    "title": "Knowledge-enhanced Neuro-Symbolic AI for Cybersecurity and Privacy",
    "authors": "Aritran Piplai; Anantaa Kotal; Seyedreza Mohseni; Manas Gaur; Sudip Mittal; Anupam Joshi",
    "abstract": "Neuro-Symbolic Artificial Intelligence (AI) is an emerging and quickly advancing field that combines the subsymbolic strengths of (deep) neural networks and explicit, symbolic knowledge contained in knowledge graphs to enhance explainability and safety in AI systems. This approach addresses a key criticism of current generation systems, namely their inability to generate human-understandable explanations for their outcomes and ensure safe behaviors, especially in scenarios with \\textit{unknown unknowns} (e.g. cybersecurity, privacy). The integration of neural networks, which excel at exploring complex data spaces, and symbolic knowledge graphs, which represent domain knowledge, allows AI systems to reason, learn, and generalize in a manner understandable to experts. This article describes how applications in cybersecurity and privacy, two most demanding domains in terms of the need for AI to be explainable while being highly accurate in complex environments, can benefit from Neuro-Symbolic AI.",
    "published_date": "2023-07-25",
    "pdf_link": "https://arxiv.org/pdf/2308.02031v1",
    "paper_types": [
      "position",
      "empirical_analysis",
      "survey"
    ],
    "security_domain": {
      "primary": "Cyber Threat Intelligence",
      "subdomain": "Knowledge-graph–guided Detection and Response",
      "specific_problem": "Explainable rule generation and knowledge-guided reinforcement learning for malware/intrusion detection and privacy-preserving data sharing",
      "attack_types": [
        "malware",
        "network intrusion",
        "adversarial behavior (unknown unknowns)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Knowledge Graphs / Neuro-Symbolic",
        "specific": "Cybersecurity Knowledge Graph (CKG) aligned with STIX/TAXII; Unified Cyber Ontology (UCO)",
        "novel_contribution": "Use of CKG to generate hypotheses/rules and to guide exploration and rewards in RL; integration of KG-derived knowledge into rule generation and data synthesis workflows"
      },
      {
        "type": "primary",
        "category": "Reinforcement Learning",
        "specific": "Knowledge-guided RL; Offline RL; Two-player (attacker/defender) RL",
        "novel_contribution": "Shaping rewards and exploration using prior knowledge from KGs and rules; demonstrated faster convergence, improved detection, and higher network availability"
      },
      {
        "type": "primary",
        "category": "Transformer / LLM",
        "specific": "GPT-family models used as Reasoning Engine",
        "novel_contribution": "Combining KG-extracted hypotheses and observations as prompts/inputs to LLMs to generate prioritized cybersecurity rules with explainability"
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "BERT for relation extraction / semantic triple extraction",
        "novel_contribution": "Building CKG from 474 technical reports and posts to encode domain knowledge for downstream explainable inference"
      },
      {
        "type": "primary",
        "category": "GAN",
        "specific": "Conditional GAN (CGAN) with t-closeness",
        "novel_contribution": "Privacy-preserving tabular data synthesis augmented with domain KG (UCO) to enrich discrete values and improve utility while preserving privacy"
      },
      {
        "type": "baseline",
        "category": "Rule-based IDS",
        "specific": "SNORT",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Reinforcement Learning",
        "specific": "RL without knowledge guidance (standard exploration/reward)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "GAN",
        "specific": "Standard CGAN without KG augmentation",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Reinforcement Learning",
      "Offline RL",
      "Supervised (relation extraction)",
      "Generative Adversarial (GAN)"
    ],
    "datasets": [
      {
        "name": "Cybersecurity Knowledge Graph constructed from 474 technical reports and technical posts",
        "type": "proprietary",
        "domain": "knowledge_graph",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Threat intelligence feeds via TAXII (STIX format) integrated into CKG",
        "type": "public",
        "domain": "threat_intel",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Packet capture (pcap) files (3–4 GB) used in two-player RL study",
        "type": "private",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Unified Cyber Ontology (UCO) used to guide data synthesis",
        "type": "public",
        "domain": "knowledge_graph",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "RL without knowledge guidance",
        "paper_reference": null,
        "metric": "Network availability (two-player RL)",
        "their_result": "78% network availability preserved with knowledge-guided RL",
        "baseline_result": "25% network availability without knowledge guidance"
      },
      {
        "method_name": "RL without prior knowledge (offline RL)",
        "paper_reference": "A. Piplai et al., AAAI Spring Symposium 2023",
        "metric": "Detection rate",
        "their_result": "+4% detection in 3 of 4 malware families with prior knowledge",
        "baseline_result": null
      },
      {
        "method_name": "Standard RL (no knowledge shaping)",
        "paper_reference": "A. Piplai et al., IEEE Big Data 2020",
        "metric": "Average episode time",
        "their_result": "8% reduction in average episode time with prior knowledge",
        "baseline_result": "0% reduction (reference condition)"
      },
      {
        "method_name": "SNORT (traditional rule-based IDS)",
        "paper_reference": null,
        "metric": "Qualitative comparison (accuracy/Explainability)",
        "their_result": "Claimed improved accuracy, dependability, and explainability with Neuro-Symbolic rule base framework",
        "baseline_result": null
      },
      {
        "method_name": "Standard CGAN (no KG augmentation)",
        "paper_reference": "Kotal et al., IWSPA 2022 (Privetab)",
        "metric": "Modeling conditionally continuous variables; discrete coverage",
        "their_result": "KG-augmented CGAN addresses limitations and enhances privacy-preserving synthesis",
        "baseline_result": "Standard CGAN struggles with conditionally continuous variables and repeats only seen discrete values"
      }
    ],
    "performance_metrics_used": [
      "average episode time",
      "detection rate",
      "network availability",
      "response time (qualitative)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How can Neuro-Symbolic AI (neural + knowledge graphs) improve explainability and safety for cybersecurity and privacy?",
        "Can knowledge from cybersecurity KGs guide RL to achieve faster convergence and better detection/defense?",
        "Can KGs enhance privacy-preserving data synthesis for tabular cybersecurity data?"
      ],
      "gaps_identified": [
        "Lack of high-quality/shareable cybersecurity data; sensitivity/PII concerns",
        "High dynamism/data drift in cyber domains; learned patterns become stale",
        "Current ML models lack explainability and safety guarantees, especially under unknown unknowns",
        "Limited availability of reliable privacy-preserving training datasets"
      ],
      "limitations": [
        "Additional research needed to develop optimal reasoning engines focused on analyst users",
        "Need to identify which knowledge (paths/rules) is most beneficial to the ML/RL model",
        "Current results are summarized from prior studies; no end-to-end real-world deployment demonstrated in this article"
      ],
      "future_work": [
        "Develop reasoning engines tailored for cybersecurity/privacy decision-making",
        "Use transformers to select informative KG paths based on real data",
        "Augment training with graph embeddings of state spaces",
        "Extend knowledge-infusion methods to biomedicine/healthcare privacy scenarios"
      ],
      "motivation": "Bridge neural models with symbolic knowledge to deliver explainable, safe AI for cybersecurity and privacy where decisions must be accurate, auditable, and robust to unknown unknowns.",
      "potential_research_ideas": [
        "Design standardized benchmarks for knowledge-guided RL and neuro-symbolic rule generation in cybersecurity",
        "Investigate adversarial robustness of KG-guided RL to knowledge poisoning or deceptive observations",
        "Combine retrieval-augmented generation from CKG with program synthesis to produce verifiable security rules",
        "Develop continual learning for KGs from streaming threat intel with drift detection and automatic rule updates",
        "Integrate formal verification/constraints into RL policies derived from security specifications",
        "Apply knowledge-guided RL to ICS/IoT settings with scarce labels and strict safety constraints",
        "Quantify privacy-utility trade-offs of KG-augmented CGANs with formal DP guarantees"
      ],
      "architectural_improvement_recommendations": [
        "Incorporate graph neural networks over CKG to learn path/context embeddings for both RL shaping and LLM prompting",
        "Use constrained/safe RL with KG-derived temporal logic constraints during training and inference",
        "Adopt retrieval-augmented generation (RAG) where LLM prompts include verified KG triples and provenance for rule synthesis",
        "Add causal graph discovery from logs to complement KG facts and improve generalization to unseen attacks",
        "Introduce rule verification loops (symbolic reasoners) that test LLM-generated rules against KG and simulated traffic before deployment"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Security Operations Center (conceptual architecture)",
      "scalability_discussed": true,
      "inference_time": "\"8% reduction in the average episode time\" reported for knowledge-guided RL",
      "deployment_challenges": [
        "Data sensitivity and PII limit dataset sharing",
        "Rapidly evolving threats cause data drift and model staleness",
        "Need to ensure policy/regulatory compliance during detection and response",
        "Integration of heterogeneous threat intel sources into a unified, high-quality KG"
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": true,
      "fairness_bias": false
    },
    "contributions": [
      "Articulates a neuro-symbolic framework for cybersecurity: (a) rule base framework using KGs + LLM reasoning, and (b) knowledge-guided RL",
      "Builds/uses a Cybersecurity Knowledge Graph from 474 technical reports and open-source posts; integrates STIX/TAXII intel",
      "Demonstrates knowledge-guided RL benefits: 8% reduction in average episode time; +4% detection in 3/4 families; 78% vs 25% network availability in two-player RL",
      "Proposes privacy-preserving tabular data synthesis combining CGAN with t-closeness and KG (UCO) augmentation to improve utility under privacy constraints"
    ]
  },
  {
    "arxiv_id": "2308.05978v4",
    "title": "CyberForce: A Federated Reinforcement Learning Framework for Malware Mitigation",
    "authors": "Chao Feng; Alberto Huertas Celdran; Pedro Miguel Sanchez Sanchez; Jan Kreischer; Jan von der Assen; Gerome Bovet; Gregorio Martinez Perez; Burkhard Stiller",
    "abstract": "Recent research has shown that the integration of Reinforcement Learning (RL) with Moving Target Defense (MTD) can enhance cybersecurity in Internet-of-Things (IoT) devices. Nevertheless, the practicality of existing work is hindered by data privacy concerns associated with centralized data processing in RL, and the unsatisfactory time needed to learn right MTD techniques that are effective against a rising number of heterogeneous zero-day attacks. Thus, this work presents CyberForce, a framework that combines Federated and Reinforcement Learning (FRL) to collaboratively and privately learn suitable MTD techniques for mitigating zero-day attacks. CyberForce integrates device fingerprinting and anomaly detection to reward or penalize MTD mechanisms chosen by an FRL-based agent. The framework has been deployed and evaluated in a scenario consisting of ten physical devices of a real IoT platform affected by heterogeneous malware samples. A pool of experiments has demonstrated that CyberForce learns the MTD technique mitigating each attack faster than existing RL-based centralized approaches. In addition, when various devices are exposed to different attacks, CyberForce benefits from knowledge transfer, leading to enhanced performance and reduced learning time in comparison to recent works. Finally, different aggregation algorithms used during the agent learning process provide CyberForce with notable robustness to malicious attacks.",
    "published_date": "2023-08-11",
    "pdf_link": "https://arxiv.org/pdf/2308.05978v4",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "IoT Security",
      "subdomain": "Malware Mitigation and Response",
      "specific_problem": "Learning and selecting effective Moving Target Defense (MTD) mechanisms to mitigate zero-day malware attacks on distributed IoT devices while preserving data privacy",
      "attack_types": [
        "Ransomware",
        "Command-and-Control (C&C)",
        "Rootkit",
        "Model poisoning (against FL aggregation)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Reinforcement Learning",
        "specific": "Deep Q-Network (DQN)",
        "novel_contribution": "Federated RL agent that uses DQN to learn optimal MTD actions across multiple IoT clients with privacy-preserving parameter sharing"
      },
      {
        "type": "primary",
        "category": "Federated Learning",
        "specific": "Parameter aggregation (FedAvg, Krum)",
        "novel_contribution": "Integration of FL aggregation into the RL policy/value network training to enable collaborative, privacy-preserving learning and robustness to poisoning"
      },
      {
        "type": "primary",
        "category": "Anomaly Detection",
        "specific": null,
        "novel_contribution": "Behavioral fingerprinting and anomaly detection used as the reward signal to evaluate the effect of chosen MTD actions"
      },
      {
        "type": "baseline",
        "category": "Reinforcement Learning",
        "specific": "Centralized RL (non-federated DQN-based agent)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Reinforcement Learning",
      "Federated Learning",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "Electrosense IoT platform malware-mitigation traces (Raspberry Pi 4 spectrum sensors)",
        "type": "private",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Heterogeneous malware samples executed on IoT devices (ransomware, C&C, rootkit)",
        "type": "private",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Centralized RL-based MTD selection (non-federated)",
        "paper_reference": null,
        "metric": "Learning time; MTD selection accuracy/performance",
        "their_result": "CyberForce learns the MTD technique mitigating each attack faster and with enhanced performance",
        "baseline_result": null
      },
      {
        "method_name": "FedAvg aggregation (within FRL)",
        "paper_reference": null,
        "metric": "Robustness to poisoning; performance under secure non-IID",
        "their_result": "FedAvg optimized performance in a secure environment with non-IID data",
        "baseline_result": null
      },
      {
        "method_name": "Krum aggregation (within FRL)",
        "paper_reference": null,
        "metric": "Robustness to model poisoning; performance trade-off",
        "their_result": "Krum improved model robustness against poisoning in IID environments",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Learning time (training time to identify effective MTD)",
      "MTD selection accuracy/effectiveness",
      "Knowledge transfer benefit across clients (performance improvement under heterogeneous attacks)",
      "Robustness to poisoning attacks (under different aggregation algorithms)",
      "Scalability across number of clients (10 to 20)",
      "Convergence behavior"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can federated reinforcement learning collaboratively and privately learn effective MTD techniques to mitigate zero-day attacks on IoT devices?",
        "Does knowledge transfer in an FRL setup reduce learning time and improve performance compared to centralized RL?",
        "How do different FL aggregation algorithms (e.g., FedAvg, Krum) impact robustness to poisoning and performance under IID and non-IID settings?",
        "How does the approach scale with number of clients and heterogeneous attack distributions?"
      ],
      "gaps_identified": [
        "Centralized RL brings data privacy concerns due to pooling data on a server",
        "Existing RL-based MTD approaches require unsatisfactory learning time and do not scale to many devices",
        "Lack of effective integration of FL and RL for MTD against zero-day attacks",
        "Limited robustness analyses against poisoning attacks in prior works"
      ],
      "limitations": [
        "Robustness-performance trade-off between aggregation methods (e.g., Krum increases robustness under IID but may reduce performance; FedAvg performs better in secure non-IID settings)",
        "Evaluation scenarios limited to Electrosense Raspberry Pi 4 sensors and three malware families (ransomware, C&C, rootkit)",
        "Experiments conducted with 10–20 clients; larger-scale federations not evaluated in the presented text"
      ],
      "future_work": [],
      "motivation": "Address privacy, scalability, and slow learning time of RL-based MTD for zero-day attacks on resource-constrained IoT devices by combining RL with FL and leveraging behavioral fingerprinting for reward signals.",
      "potential_research_ideas": [
        "Explore on-policy or actor-critic FRL algorithms (e.g., PPO, A3C, SAC) and compare convergence, stability, and sample efficiency to DQN in federated settings",
        "Personalized FRL for non-IID clients (e.g., FedPer, FedProx, pFedMe) to balance global knowledge with device-specific adaptation",
        "Meta-RL or continual FRL for fast adaptation to new zero-day attacks with minimal exploration time",
        "Robust federated aggregation beyond Krum (e.g., Multi-Krum, Bulyan, coordinate-wise median, trimmed mean) and Byzantine-resilient RL analyses",
        "Integrate secure aggregation and differential privacy to strengthen confidentiality without sacrificing learning efficacy",
        "Hierarchical or multi-agent RL to coordinate MTD actions across networked devices considering system-wide objectives and costs",
        "Temporal modeling (e.g., LSTM/Transformer-based Q-networks) for richer behavioral fingerprinting signals and delayed effects of MTD actions",
        "Formal optimization of MTD action cost vs. security benefit with constrained RL or reward shaping",
        "Evaluate generalization across broader device types and larger federations; create a reproducible benchmark for FRL-based MTD"
      ],
      "architectural_improvement_recommendations": [
        "Replace DQN with distributional or dueling DQN and prioritized replay to improve learning stability and sample efficiency",
        "Adopt actor-critic methods (e.g., PPO/SAC) with asynchronous federated updates to reduce variance and improve convergence in non-IID settings",
        "Introduce client-side personalization layers or adapters to capture device-specific behavior while sharing a global backbone",
        "Use robust aggregation (e.g., Bulyan, median, trimmed mean) with anomaly detection on client updates to resist poisoning",
        "Incorporate temporal encoders (LSTM/Transformer) in the policy/value network to leverage sequential device fingerprints",
        "Apply secure aggregation and optional differential privacy on model updates to enhance privacy guarantees",
        "Implement cost-aware reward shaping that accounts for MTD overhead, usability impact, and security gain"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Evaluated on 10 to 20 Raspberry Pi 4 clients in a real IoT platform; federated server requirements not specified"
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Real IoT platform (Electrosense) with Raspberry Pi 4 spectrum sensors",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Data privacy constraints for collaborative learning",
        "Heterogeneous devices and non-IID data distributions",
        "Potential poisoning attacks on federated aggregation",
        "Trade-off between robustness and model performance depending on aggregation choice",
        "Resource constraints on IoT devices"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": true,
      "privacy_preserving": true,
      "fairness_bias": false
    },
    "contributions": [
      "Design and implementation of CyberForce, a federated reinforcement learning framework that privately and collaboratively learns optimal MTD techniques for zero-day attack mitigation",
      "Integration of behavioral fingerprinting and ML-based anomaly detection to reward/penalize MTD actions chosen by an FRL-based DQN agent",
      "Real-world deployment on Raspberry Pi 4 Electrosense sensors with six malware attacks across ransomware, C&C, and rootkit families and four MTD mechanisms",
      "Experimental evaluation across IID and non-IID scenarios with 10–20 clients demonstrating faster learning and enhanced performance vs. centralized RL via knowledge transfer",
      "Evaluation of robustness to poisoning attacks using different aggregation algorithms; demonstrated trade-offs where Krum improves robustness in IID settings and FedAvg optimizes performance in secure non-IID environments"
    ]
  },
  {
    "arxiv_id": "2309.01070v1",
    "title": "Multidomain transformer-based deep learning for early detection of network intrusion",
    "authors": "Jinxin Liu; Murat Simsek; Michele Nogueira; Burak Kantarci",
    "abstract": "Timely response of Network Intrusion Detection Systems (NIDS) is constrained by the flow generation process which requires accumulation of network packets. This paper introduces Multivariate Time Series (MTS) early detection into NIDS to identify malicious flows prior to their arrival at target systems. With this in mind, we first propose a novel feature extractor, Time Series Network Flow Meter (TS-NFM), that represents network flow as MTS with explainable features, and a new benchmark dataset is created using TS-NFM and the meta-data of CICIDS2017, called SCVIC-TS-2022. Additionally, a new deep learning-based early detection model called Multi-Domain Transformer (MDT) is proposed, which incorporates the frequency domain into Transformer. This work further proposes a Multi-Domain Multi-Head Attention (MD-MHA) mechanism to improve the ability of MDT to extract better features. Based on the experimental results, the proposed methodology improves the earliness of the conventional NIDS (i.e., percentage of packets that are used for classification) by 5x10^4 times and duration-based earliness (i.e., percentage of duration of the classified packets of a flow) by a factor of 60, resulting in a 84.1% macro F1 score (31% higher than Transformer) on SCVIC-TS-2022. Additionally, the proposed MDT outperforms the state-of-the-art early detection methods by 5% and 6% on ECG and Wafer datasets, respectively.",
    "published_date": "2023-09-03",
    "pdf_link": "https://arxiv.org/pdf/2309.01070v1",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "benchmark",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Early detection of network intrusions using multivariate time series to classify flows from only the first packets and short durations",
      "attack_types": [
        "Advanced Persistent Threat (APT)",
        "penetration tests",
        "meterpreter payloads",
        "shell payloads",
        "PowerShell payloads"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "Multi-Domain Transformer (MDT)",
        "novel_contribution": "Incorporates 2D IFFT-based frequency-domain augmentation into a Transformer encoder for multivariate time series early detection"
      },
      {
        "type": "primary",
        "category": "Attention",
        "specific": "Multi-Domain Multi-Head Attention (MD-MHA)",
        "novel_contribution": "Computes attention in both time and frequency domains (FFT(Q,K,V)) and concatenates heads without increasing parameters to reduce overfitting"
      },
      {
        "type": "primary",
        "category": "Feature Fusion",
        "specific": "2D IFFT concatenation with original MTS",
        "novel_contribution": "Uses 2D IFFT to increase dimensionality and preserve amplitude information before Transformer layer normalization"
      },
      {
        "type": "primary",
        "category": "Hybrid DL+ML",
        "specific": null,
        "novel_contribution": "Uses MDT as a feature extractor with a downstream ML classifier for imbalanced classes; mentions use of ML models such as XGBoost"
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "Vanilla Transformer encoder",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "kNN",
        "specific": "1NN-full",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Shapelet-based",
        "specific": "MSD (Multivariate Shapelets Detection)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN-LSTM Hybrid",
        "specific": "MDDNN (Multi-Domain DNN)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Attention-based TSC",
        "specific": "ETSCM (Explainable Time Series Classification Model)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Ensemble SVM",
        "specific": "Distributed DBN+SVM (Marir et al.)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Genetic Algorithm optimized ML",
        "specific": "GA-based adaptive method (Resende et al.)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Semi-supervised Autoencoder",
        "specific": "SU-IDS AutoEncoder",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Few-shot metric learning",
        "specific": "FC-Net",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Anomaly Detection",
        "specific": "IFSE-AD",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "SCVIC-TS-2022",
        "type": "public",
        "domain": "network_traffic",
        "link": "https://dx.doi.org/10.21227/qm9h-8c05",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "CICIDS2017 (meta-data used to create SCVIC-TS-2022)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "ECG (heartbeat MTS)",
        "type": "public",
        "domain": "ecg_signals",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Wafer (semiconductor process MTS)",
        "type": "public",
        "domain": "industrial_sensor",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Transformer (vanilla)",
        "paper_reference": null,
        "metric": "SCVIC-TS-2022: Macro F1 at E=2e-5, DE=0.016",
        "their_result": "0.84 (84.1%)",
        "baseline_result": "0.53 (53%)"
      },
      {
        "method_name": "GA-based Adaptive Method [6]",
        "paper_reference": "[6]",
        "metric": "CICIDS2017-equivalent flows: ACC, DR, Earliness",
        "their_result": "ACC 99.7%, DR 99.7%, E=2e-5, DE=1.6e-2",
        "baseline_result": "DR 92.85%, E=1, DE=1"
      },
      {
        "method_name": "Ensemble SVM (DBN+SVM) [7]",
        "paper_reference": "[7]",
        "metric": "CICIDS2017-equivalent flows: DR, Earliness",
        "their_result": "DR 99.7%, E=2e-5, DE=1.6e-2",
        "baseline_result": "DR 94.94%, E=1, DE=1"
      },
      {
        "method_name": "IFSE-AD [22]",
        "paper_reference": "[22]",
        "metric": "CICIDS2017-equivalent flows: ACC, Earliness",
        "their_result": "ACC 99.7%, E=2e-5, DE=1.6e-2",
        "baseline_result": "ACC 97.3%, E=1, DE=1"
      },
      {
        "method_name": "SU-IDS [8]",
        "paper_reference": "[8]",
        "metric": "CICIDS2017-equivalent flows: ACC, Earliness",
        "their_result": "ACC 99.7%, E=2e-5, DE=1.6e-2",
        "baseline_result": "ACC 71.02%, E=1, DE=1"
      },
      {
        "method_name": "FC-Net [9]",
        "paper_reference": "[9]",
        "metric": "CICIDS2017-equivalent flows: ACC, DR, Earliness",
        "their_result": "ACC 99.7%, DR 99.7%, E=2e-5, DE=1.6e-2",
        "baseline_result": "ACC 94.64%, DR 99.62%, E=1, DE=1"
      },
      {
        "method_name": "MSD",
        "paper_reference": "[14]",
        "metric": "ECG: F1 at E=0.08",
        "their_result": "0.84 (at E=0.06); best 0.94 (at E=0.4)",
        "baseline_result": "0.59"
      },
      {
        "method_name": "REACT",
        "paper_reference": "[15]",
        "metric": "ECG: F1 at E=0.06; Wafer: F1 at E=0.23",
        "their_result": "ECG 0.84 (E=0.06); Wafer 0.98 (E=0.23)",
        "baseline_result": "ECG 0.77; Wafer 0.92"
      },
      {
        "method_name": "1NN-full",
        "paper_reference": "[13]",
        "metric": "ECG: F1 at E=1; Wafer: F1 at E=1",
        "their_result": "ECG 0.84 (E=0.06); Wafer 0.98 (E=0.23)",
        "baseline_result": "ECG 0.79; Wafer 0.87"
      },
      {
        "method_name": "MDDNN",
        "paper_reference": "[16]",
        "metric": "ECG: F1 at E=0.06; Wafer: F1 at E=0.23",
        "their_result": "ECG 0.84; Wafer 0.98",
        "baseline_result": "ECG 0.81; Wafer 0.91"
      },
      {
        "method_name": "ETSCM",
        "paper_reference": "[17]",
        "metric": "ECG: F1 (best) at E=0.13; Wafer: F1 at E=0.5",
        "their_result": "ECG 0.94 (best at E=0.4); Wafer 0.98 (E=0.23)",
        "baseline_result": "ECG 0.89; Wafer 0.93"
      },
      {
        "method_name": "Transformer (vanilla)",
        "paper_reference": null,
        "metric": "ECG: F1 at E=0.06; Wafer: F1 at E=0.23",
        "their_result": "ECG 0.84; Wafer 0.98",
        "baseline_result": "ECG 0.59; Wafer 0.94"
      }
    ],
    "performance_metrics_used": [
      "macro F1 score",
      "accuracy",
      "detection rate",
      "earliness (E)",
      "duration-based earliness (DE)"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "How to represent network flows as multivariate time series suitable for early intrusion detection?",
        "How to accurately detect intrusions using only the early subsequence of packets (few packets/short duration)?",
        "Can incorporating frequency-domain information improve early detection performance in NIDS?"
      ],
      "gaps_identified": [
        "Conventional NIDS require accumulating packets to form flows, limiting earliness of detection.",
        "Early detection methods for time series had not been integrated into NIDS.",
        "Transformer struggles on very short multivariate sequences and with highly imbalanced classes.",
        "Layer normalization in low-dimensional MTS can lose amplitude information, hurting discrimination.",
        "Oversampling can cause overfitting on imbalanced NIDS datasets."
      ],
      "limitations": [
        "Imbalanced classes in network intrusion datasets challenge deep learning classifiers; ML tabular classifiers may perform better without careful handling.",
        "Baseline Transformer fails with small number of packets or low-dimensional MTS without frequency augmentation.",
        "Extremely long potential flow lengths (L up to 511,681) imply very small earliness fractions, complicating evaluation.",
        "Experiments are offline; no live deployment or on-the-wire evaluation reported."
      ],
      "future_work": [],
      "motivation": "Enable NIDS to detect intrusions before they reach target systems by introducing multivariate time series early detection with a frequency-enhanced Transformer and a new dataset that supports early packet-level analysis.",
      "potential_research_ideas": [
        "Learned spectral mixing (e.g., learnable Fourier/wavelet layers or time-frequency transformers) to replace fixed FFT/IFFT.",
        "Adaptive early-exit policies that decide when to stop observing packets based on uncertainty and cost-aware earliness objectives.",
        "Self-supervised pretraining on large unlabeled PCAPs to improve representation quality for rare attack classes.",
        "Online/streaming MDT for line-rate packet processing with sliding-window updates and bounded latency.",
        "Robust training against adversarial evasion/poisoning specific to packet timing/flag manipulations.",
        "Explainable early detection with attribution over packets/time-frequency regions to aid analyst triage.",
        "Protocol-aware embeddings that incorporate TCP state and application-layer semantics when available.",
        "Memory- and compute-efficient attention (e.g., Performer/FlashAttention/linear attention) for extremely long sequences.",
        "Hybrid anomaly+classification pipeline using MST/MDT features for zero-day detection.",
        "Confidence calibration and selective classification for safe early blocking decisions."
      ],
      "architectural_improvement_recommendations": [
        "Replace fixed 2D IFFT with learnable time-frequency front-ends (STFT, wavelet scattering, S4/Hyena operators).",
        "Use multi-scale hierarchical encoders with local convolutions + global attention to capture short/long-range packet patterns.",
        "Incorporate focal loss, class-balanced loss, or cost-sensitive training; complement with mixup/augmentations for imbalance.",
        "Add uncertainty estimation and early-exit branches to trade off earliness vs accuracy dynamically.",
        "Adopt linear/efficient attention (Performer, Nyström, FlashAttention-2) for scalability to very long flows.",
        "Evaluate and integrate gradient-boosted trees (XGBoost/LightGBM) downstream with better feature pooling and calibration.",
        "Design streaming inference with stateful encoders and byte/flag embeddings to reduce recomputation."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Highly imbalanced classes typical in NIDS traffic.",
        "Trade-off between earliness and accuracy; determining minimal packets/duration for safe decisions.",
        "Very long potential flow lengths create memory/latency challenges for sequence models.",
        "Transformer sensitivity to low-dimensional MTS without appropriate preprocessing.",
        "Risk of overfitting with oversampling on minority attack classes."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces multivariate time series early detection to NIDS to identify malicious flows before reaching targets.",
      "Proposes Time Series Network Flow Meter (TS-NFM) to represent flows as MTS with packet-level explainable features.",
      "Creates a new benchmark dataset SCVIC-TS-2022 from CICIDS2017 meta-data using TS-NFM (publicly available).",
      "Proposes Multi-Domain Transformer (MDT) that integrates frequency domain (2D IFFT) with a Transformer encoder for early detection.",
      "Introduces Multi-Domain Multi-Head Attention (MD-MHA) combining time and frequency domain attention without increasing parameters.",
      "Defines duration-based earliness (DE) and Flow Earliness FE=(E, DE) as evaluation metrics for NIDS early detection.",
      "Demonstrates large earliness gains (5×10^4 in E and 60× in DE) with strong performance: 84.1% macro F1 on SCVIC-TS-2022; ACC/DR 99.7%.",
      "Shows MDT outperforms state-of-the-art early detection methods on ECG (+5%) and Wafer (+6%) datasets."
    ]
  },
  {
    "arxiv_id": "2308.09239v3",
    "title": "SHAPFUZZ: Efficient Fuzzing via Shapley-Guided Byte Selection",
    "authors": "Kunpeng Zhang; Xiaogang Zhu; Xi Xiao; Minhui Xue; Chao Zhang; Sheng Wen",
    "abstract": "Mutation-based fuzzing is popular and effective in discovering unseen code and exposing bugs. However, only a few studies have concentrated on quantifying the importance of input bytes, which refers to the degree to which a byte contributes to the discovery of new code. They often focus on obtaining the relationship between input bytes and path constraints, ignoring the fact that not all constraint-related bytes can discover new code. In this paper, we conduct Shapely analysis to understand the effect of byte positions on fuzzing performance, and find that some byte positions contribute more than others and this property often holds across seeds. Based on this observation, we propose a novel fuzzing solution, ShapFuzz, to guide byte selection and mutation. Specifically, ShapFuzz updates Shapley values (importance) of bytes when each input is tested during fuzzing with a low overhead, and utilizes contextual multi-armed bandit to trade off between mutating high Shapley value bytes and low-frequently chosen bytes. We implement a prototype of this solution based on AFL++, i.e., ShapFuzz. We evaluate ShapFuzz against ten state-of-the-art fuzzers, including five byte schedule-reinforced fuzzers and five commonly used fuzzers. Compared with byte schedule-reinforced fuzzers, ShapFuzz discovers more edges and exposes more bugs than the best baseline on three different sets of initial seeds. Compared with commonly used fuzzers, ShapFuzz exposes 20 more bugs than the best comparison fuzzer, and discovers 6 more CVEs than the best baseline on MAGMA. Furthermore, ShapFuzz discovers 11 new bugs on the latest versions of programs, and 3 of them are confirmed by vendors.",
    "published_date": "2023-08-18",
    "pdf_link": "https://arxiv.org/pdf/2308.09239v3",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Fuzzing",
      "specific_problem": "Coverage-guided fuzzing with efficient byte selection and mutation scheduling via Shapley-guided importance and contextual bandits",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Game-theoretic attribution",
        "specific": "Shapley value",
        "novel_contribution": "Temporary, incrementally updated Shapley values that quantify byte importance for code discovery; shared across same-length seed families; gain defined as self-new edges to avoid statefulness"
      },
      {
        "type": "primary",
        "category": "Bandit/Online Learning",
        "specific": "Contextual Multi-Armed Bandit (CMAB)",
        "novel_contribution": "CMAB balances exploiting high-Shapley bytes with exploring low-frequently chosen bytes during fuzzing"
      }
    ],
    "learning_paradigm": [
      "Reinforcement Learning (bandit)",
      "Online learning"
    ],
    "datasets": [
      {
        "name": "UNIFUZZ benchmark",
        "type": "public",
        "domain": "program_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "MAGMA bug suite",
        "type": "public",
        "domain": "program_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "18 real-world programs for Shapley analysis (e.g., nm, tiff2bw, flvmeta, imginfo, infotocap, lame, mp42aac, mp3gain, ...)",
        "type": "private",
        "domain": "program_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "GreyOne",
        "paper_reference": "[12]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "ProFuzzer",
        "paper_reference": "[37]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Angora",
        "paper_reference": "[6]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "PreFuzz",
        "paper_reference": "[36]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "NEUZZ",
        "paper_reference": "[32]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "AFL++",
        "paper_reference": "[10]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "MOPT",
        "paper_reference": "[25]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "AFLFast",
        "paper_reference": "[3]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "FairFuzz",
        "paper_reference": "[18]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "AFL",
        "paper_reference": "[40]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Best byte-scheduling baseline (among GreyOne, ProFuzzer, Angora, PreFuzz, NEUZZ)",
        "paper_reference": null,
        "metric": "edges discovered; bugs exposed",
        "their_result": "ShapFuzz discovers 4,170 more edges and exposes 19 more bugs when all initial seeds are given",
        "baseline_result": null
      },
      {
        "method_name": "Best commonly used fuzzer baseline (among AFL++, MOPT, AFLFast, FairFuzz, AFL)",
        "paper_reference": null,
        "metric": "bugs exposed; CVEs discovered (MAGMA)",
        "their_result": "ShapFuzz exposes 20 more bugs than the best commonly used fuzzer and discovers 6 more CVEs than the best baseline on MAGMA",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "edge coverage (edges discovered)",
      "bugs exposed (unique bugs)",
      "CVEs discovered",
      "self-new edges (internal gain definition for Shapley analysis)"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Do certain byte positions consistently contribute more than others to discovering new code during fuzzing, and does this hold across seeds?",
        "Can Shapley analysis efficiently quantify byte importance during fuzzing without a costly extra analysis stage?",
        "Can a contextual multi-armed bandit effectively trade off exploiting high-importance bytes and exploring less-chosen bytes to improve fuzzing performance?"
      ],
      "gaps_identified": [
        "Few studies quantify the importance of input bytes for discovering new code.",
        "Prior work focuses on constraint-related bytes, but not all constraint-related bytes help discover new coverage.",
        "Extra analysis (e.g., taint or constraint solving) is time-consuming and often byte-by-byte.",
        "Deep learning-based fuzzers may fail for large inputs due to out-of-memory during model building."
      ],
      "limitations": [
        "Generalizability: authors note the need for additional validation on a broader range of programs.",
        "Seed family sharing of Shapley values assumes no length changes; length-changing mutations complicate sharing.",
        "For analysis runs, length-changing mutators were disabled to keep byte positions stable.",
        "Temporary Shapley values may be less useful once all constraints relevant to a byte are explored."
      ],
      "future_work": [
        "Additional validation on a broader range of programs to further establish generality of the findings."
      ],
      "motivation": "Improve mutation-based fuzzing by quantitatively identifying and prioritizing input bytes that most contribute to discovering new code, avoiding expensive pre-analyses and ineffective focus on all constraint-related bytes.",
      "potential_research_ideas": [
        "Support variable-length mutations by introducing token/field-level importance and alignment to maintain Shapley consistency across insertions/deletions.",
        "Combine lightweight taint or CMP instrumentation to initialize priors for Shapley values and accelerate early-stage learning.",
        "Transfer or meta-learn Shapley priors across programs within the same file format to reduce cold-start.",
        "Integrate grammar/structure-aware fuzzing and compute Shapley at field-level for structured inputs (e.g., parsers).",
        "Evaluate alternative bandit algorithms (e.g., Thompson Sampling, LinUCB) with uncertainty-aware exploration for byte selection.",
        "Augment reward beyond edges (e.g., distance-to-uncovered CMPs, path novelty scores) to better guide mutations.",
        "Parallelize/incremental Shapley updates and CMAB decisions for multi-core fuzzing with shared memory.",
        "Study robustness against anti-fuzzing and path nondeterminism; add mechanisms to de-noise rewards."
      ],
      "architectural_improvement_recommendations": [
        "Maintain confidence intervals over Shapley estimates and use UCB-style exploration proportional to uncertainty.",
        "Introduce hierarchical bandits: top-level selects seed families, lower-level selects bytes within a seed.",
        "Compute Shapley over semantic fields (via lightweight parsing) instead of raw bytes to improve generalization.",
        "Use CMP feedback (e.g., frequency, comparison distance) as context features for CMAB.",
        "Employ reservoir sampling or capped history to bound per-family state and reduce memory overhead.",
        "Blend Shapley-guided scheduling with directed fuzzing (e.g., distance-to-target metrics) for goal-oriented scenarios."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/ShapFuzz/ShapFuzz",
      "frameworks": [
        "AFL++"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "CPU-only; similar to AFL++ fuzzing. Long-running fuzzing campaigns (hours to days) used in experiments; no GPU required."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Linux user-space fuzzing of real-world programs (benchmarks UNIFUZZ and MAGMA)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Managing overhead of online Shapley updates across many seeds and long runs",
        "Handling variable-length mutations that break seed-family sharing of Shapley values",
        "Dependence on seed quality and initial coverage",
        "Balancing exploration vs. exploitation in diverse program contexts",
        "Storing and updating per-family byte-importance state efficiently"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Empirical Shapley analysis showing a small subset of byte positions account for most new coverage and this property often holds across seeds.",
      "Formalization of byte selection as a Shapley analysis problem; definition of self-new edges as a stable gain.",
      "Incremental, low-overhead computation of temporary Shapley values shared within same-length seed families.",
      "Contextual multi-armed bandit to trade off mutating high-importance bytes and exploring low-frequently chosen bytes.",
      "Prototype implementation on AFL++ (ShapFuzz) and open-sourced.",
      "Extensive evaluation on UNIFUZZ and MAGMA: discovers 4,170 more edges and 19 more bugs than best byte-scheduling baseline; +20 bugs and +6 CVEs over best commonly used fuzzer on MAGMA; 11 new bugs on latest program versions with 3 confirmed by vendors."
    ]
  },
  {
    "arxiv_id": "2308.10821v2",
    "title": "Optimized Deep Learning Models for Malware Detection under Concept Drift",
    "authors": "William Maillet; Benjamin Marais",
    "abstract": "Despite the promising results of machine learning models in malicious files detection, they face the problem of concept drift due to their constant evolution. This leads to declining performance over time, as the data distribution of the new files differs from the training one, requiring frequent model update. In this work, we propose a model-agnostic protocol to improve a baseline neural network against drift. We show the importance of feature reduction and training with the most recent validation set possible, and propose a loss function named Drift-Resilient Binary Cross-Entropy, an improvement to the classical Binary Cross-Entropy more effective against drift. We train our model on the EMBER dataset, published in2018, and evaluate it on a dataset of recent malicious files, collected between 2020 and 2023. Our improved model shows promising results, detecting 15.2% more malware than a baseline model.",
    "published_date": "2023-08-21",
    "pdf_link": "https://arxiv.org/pdf/2308.10821v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Malware Analysis",
      "subdomain": "Static Malware Detection",
      "specific_problem": "Drift-resilient malware detection under concept drift (covariate shift) for Windows PE files",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Loss Function",
        "specific": "Drift-Resilient Binary Cross-Entropy (DRBCE)",
        "novel_contribution": "Combines spectral decoupling regularization with explicit false negative/false positive penalization and class weighting to improve robustness against concept drift"
      },
      {
        "type": "primary",
        "category": "MLP/Residual Network",
        "specific": "Residual feed-forward neural network (two residual blocks, ReLU, dropout)",
        "novel_contribution": "Used as a model-agnostic backbone to evaluate the proposed training protocol"
      },
      {
        "type": "primary",
        "category": "Feature Selection",
        "specific": "Permutation Feature Importance (PFI)",
        "novel_contribution": "Applied to select a stable subset of EMBER features to improve generalization and reduce drift impact"
      },
      {
        "type": "baseline",
        "category": "Loss Function",
        "specific": "Binary Cross-Entropy (BCE)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "EMBER (2017–2018)",
        "type": "public",
        "domain": "malware_binaries",
        "link": "https://github.com/elastic/ember",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "BODMAS (2019–2020, monthly splits)",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "MalwareBazaar-derived recent malware (2020–2023)",
        "type": "private",
        "domain": "malware_binaries",
        "link": "https://bazaar.abuse.ch/",
        "is_new_contribution": true,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "BCE loss baseline (BODMAS)",
        "paper_reference": null,
        "metric": "ACC/F1/FNR",
        "their_result": "DRBCE: 0.9079 / 0.8995 / 0.0348",
        "baseline_result": "BCE: 0.9247 / 0.9127 / 0.0769"
      },
      {
        "method_name": "BCE loss baseline (MalwareBazaar)",
        "paper_reference": null,
        "metric": "ACC/F1/FNR",
        "their_result": "DRBCE: 0.4877 / 0.6552 / 0.5123",
        "baseline_result": "BCE: 0.3640 / 0.5333 / 0.6360"
      },
      {
        "method_name": "Validation strategy baseline for DRBCE (random vs recent validation set) on BODMAS",
        "paper_reference": null,
        "metric": "ACC/F1/FNR",
        "their_result": "Recent validation: 0.9128 / 0.9056 / 0.0193",
        "baseline_result": "Random validation: 0.9079 / 0.8995 / 0.0348"
      },
      {
        "method_name": "Validation strategy baseline for DRBCE (random vs recent validation set) on MalwareBazaar",
        "paper_reference": null,
        "metric": "ACC/F1/FNR",
        "their_result": "Recent validation: 0.4971 / 0.6634 / 0.5029",
        "baseline_result": "Random validation: 0.4877 / 0.6552 / 0.5123"
      },
      {
        "method_name": "Penalty coefficients (PFN,PFP) tuning (MalwareBazaar)",
        "paper_reference": null,
        "metric": "ACC/F1/FNR",
        "their_result": "(5,1): 0.4877 / 0.6957 / 0.5123",
        "baseline_result": "(1,1): 0.3715 / 0.5410 / 0.6285"
      },
      {
        "method_name": "Penalty coefficients (PFN,PFP) tuning (BODMAS)",
        "paper_reference": null,
        "metric": "ACC/F1/FNR/FPR",
        "their_result": "(5,1): 0.9079 / 0.8995 / 0.0348 / 0.1346",
        "baseline_result": "(1,1): 0.9370 / 0.9274 / 0.0550 / 0.0690"
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "F1-Score",
      "False Positive Rate (FPR)",
      "False Negative Rate (FNR)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can a model-agnostic training protocol reduce performance degradation from concept drift in static malware detection?",
        "Does validating on the most recent available samples improve robustness to drift?",
        "Does the proposed Drift-Resilient Binary Cross-Entropy (DRBCE) loss improve long-term malware detection performance compared to standard BCE?",
        "Does feature reduction via Permutation Feature Importance improve generalization under drift?"
      ],
      "gaps_identified": [
        "Machine learning-based malware detectors suffer from concept drift, leading to declining performance over time and frequent retraining needs.",
        "Classical BCE optimization focuses on accuracy and may not align with drift-resilient detection objectives (FN vs FP trade-offs).",
        "Validation on randomly sampled, older data can encourage overfitting to outdated distributions.",
        "Difficulty measuring drift in real settings where ground-truth labels for newly seen samples are not immediately available."
      ],
      "limitations": [
        "MalwareBazaar contains no benign files and is collected from a collaborative, unmoderated database, making it semantically distant from EMBER/BODMAS.",
        "Evaluation is restricted to static features extracted with the EMBER feature extractor (dynamic behavior not considered).",
        "Results shown only with a single neural architecture; no head-to-head comparisons with strong non-neural baselines (e.g., LightGBM) in experiments."
      ],
      "future_work": [],
      "motivation": "Reduce the impact of concept drift on malware detection by proposing a model-agnostic training protocol (recent validation selection, drift-resilient loss, feature selection) to improve detection of recent malware.",
      "potential_research_ideas": [
        "Integrate DRBCE into a continual/online learning pipeline with drift detectors to trigger updates and evaluate long-term stability.",
        "Extend to multi-modal models that combine static PE features with dynamic behavioral features to mitigate covariate shift.",
        "Domain adaptation or time-aware representation learning between EMBER (2017–2018) and post-2020 corpora (e.g., adversarial domain adaptation or CORAL).",
        "Self-supervised pretraining on raw PE bytes/sections (e.g., byte-level transformers) followed by fine-tuning with DRBCE.",
        "Benchmark DRBCE against cost-sensitive baselines (focal loss, class-balanced loss) and calibrate thresholds for different operating points.",
        "Curate and release a standardized, labeled, post-2020 static malware benchmark with benigns to systematically study drift.",
        "Evaluate calibration quality over time and propose calibration-aware DRBCE variants.",
        "Assess robustness to adversarial malware obfuscations and packers; augment training with deobfuscation-invariant features."
      ],
      "architectural_improvement_recommendations": [
        "Replace/compare the residual MLP with stronger tabular models (FT-Transformer, TabTransformer, TabNet, CatBoost/LightGBM) under the same protocol.",
        "Incorporate feature-grouped regularization or sparsity-inducing penalties to stabilize feature reliance over time.",
        "Use time-decayed sampling or curriculum learning that emphasizes more recent data during training and validation.",
        "Combine DRBCE with focal loss-style modulating factors or with dynamic penalty schedules for PFN/PFP over time.",
        "Add an explicit drift detection module (e.g., Kolmogorov–Smirnov tests on features or error-rate monitoring) to adapt thresholds or trigger retraining.",
        "Explore ensembling (e.g., snapshot ensembles across time) to hedge against temporal shifts."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Concept drift necessitates frequent retraining and monitoring.",
        "Trade-off management between FNR (missed malware) and FPR (alert fatigue) depending on deployment context.",
        "Label scarcity/delay for newly observed samples complicates drift measurement and supervised updates."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Model-agnostic protocol to improve drift robustness via (i) recent validation set selection, (ii) a new loss function (DRBCE), and (iii) feature reduction with PFI.",
      "Introduces Drift-Resilient Binary Cross-Entropy (DRBCE), which augments BCE with spectral decoupling and FN/FP penalization.",
      "Empirical evaluation with training on EMBER (2017–2018) and testing on BODMAS (2019–2020) and a 2020–2023 MalwareBazaar-derived dataset to emulate temporal drift.",
      "Demonstrates that using a recent validation set improves performance, especially when combined with DRBCE.",
      "Reports improved long-term detection: “detecting 15.2% more malware than a baseline model.”"
    ]
  },
  {
    "arxiv_id": "2308.12287v2",
    "title": "Devising and Detecting Phishing: Large Language Models vs. Smaller Human Models",
    "authors": "Fredrik Heiding; Bruce Schneier; Arun Vishwanath; Jeremy Bernstein; Peter S. Park",
    "abstract": "AI programs, built using large language models, make it possible to automatically create phishing emails based on a few data points about a user. They stand in contrast to traditional phishing emails that hackers manually design using general rules gleaned from experience. The V-Triad is an advanced set of rules for manually designing phishing emails to exploit our cognitive heuristics and biases. In this study, we compare the performance of phishing emails created automatically by GPT-4 and manually using the V-Triad. We also combine GPT-4 with the V-Triad to assess their combined potential. A fourth group, exposed to generic phishing emails, was our control group. We utilized a factorial approach, sending emails to 112 randomly selected participants recruited for the study. The control group emails received a click-through rate between 19-28%, the GPT-generated emails 30-44%, emails generated by the V-Triad 69-79%, and emails generated by GPT and the V-Triad 43-81%. Each participant was asked to explain why they pressed or did not press a link in the email. These answers often contradict each other, highlighting the need for personalized content. The cues that make one person avoid phishing emails make another person fall for them. Next, we used four popular large language models (GPT, Claude, PaLM, and LLaMA) to detect the intention of phishing emails and compare the results to human detection. The language models demonstrated a strong ability to detect malicious intent, even in non-obvious phishing emails. They sometimes surpassed human detection, although often being slightly less accurate than humans. Finally, we make an analysis of the economic aspects of AI-enabled phishing attacks, showing how large language models can increase the incentives of phishing and spear phishing by reducing their costs.",
    "published_date": "2023-08-23",
    "pdf_link": "https://arxiv.org/pdf/2308.12287v2",
    "paper_types": [
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Human Factors and Social Engineering",
      "subdomain": "Phishing and Email Security",
      "specific_problem": "Effectiveness of LLM-generated vs. human-crafted phishing emails and LLM-based phishing intent detection",
      "attack_types": [
        "phishing",
        "spear phishing",
        "social engineering"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer LLM",
        "specific": "GPT-4 (ChatGPT)",
        "novel_contribution": "Used to automatically generate targeted phishing emails and to detect malicious intent via prompt-based zero-shot inference"
      },
      {
        "type": "primary",
        "category": "Transformer LLM",
        "specific": "Claude",
        "novel_contribution": "Evaluated for phishing intent detection; best-performing in the study including when primed for suspicion"
      },
      {
        "type": "primary",
        "category": "Transformer LLM",
        "specific": "PaLM (Bard)",
        "novel_contribution": "Evaluated for phishing intent detection"
      },
      {
        "type": "primary",
        "category": "Transformer LLM",
        "specific": "LLaMA",
        "novel_contribution": "Evaluated for phishing intent detection"
      },
      {
        "type": "baseline",
        "category": "Rule-based/Heuristics (Human-engineered)",
        "specific": "V-Triad framework",
        "novel_contribution": "Manual social engineering framework used to craft phishing emails (Credibility, Compatibility, Customizability)"
      }
    ],
    "learning_paradigm": [
      "Zero-shot prompting",
      "Prompt-based inference"
    ],
    "datasets": [
      {
        "name": "Participant phishing email interaction dataset (112-person study)",
        "type": "private",
        "domain": "phishing_emails_and_user_responses",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "Control phishing email: Starbucks gift from a friend (KnowBe4 example)",
        "type": "public",
        "domain": "phishing_emails",
        "link": "https://blog.knowbe4.com/bid/383111/scam-of-the-week-starbucks-gift-from-a-friend-phishing-emails",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Online databases of real-world phishing emails (unspecified sources)",
        "type": "public",
        "domain": "phishing_emails",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Additional emails from authors' private inboxes for detection set",
        "type": "private",
        "domain": "emails",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "V-Triad (manual phishing crafting) vs GPT-4-generated emails",
        "paper_reference": null,
        "metric": "Click-through rate (CTR)",
        "their_result": "V-Triad: 69–79% CTR",
        "baseline_result": "GPT-4: 30–44% CTR"
      },
      {
        "method_name": "GPT-4-generated emails vs Control (generic phishing emails)",
        "paper_reference": null,
        "metric": "Click-through rate (CTR)",
        "their_result": "GPT-4: 30–44% CTR",
        "baseline_result": "Control: 19–28% CTR"
      },
      {
        "method_name": "GPT-4 + V-Triad (hybrid) vs V-Triad",
        "paper_reference": null,
        "metric": "Click-through rate (CTR)",
        "their_result": "GPT-4 + V-Triad: 43–81% CTR",
        "baseline_result": "V-Triad: 69–79% CTR"
      },
      {
        "method_name": "Claude (unprimed) phishing intent detection on control vs GPT-generated vs GPT+V-Triad emails",
        "paper_reference": null,
        "metric": "Detection accuracy (correctly detecting malicious intention)",
        "their_result": "Claude: 75% (control), 25% (GPT-generated), 25% (GPT+V-Triad)",
        "baseline_result": null
      },
      {
        "method_name": "Claude (primed for suspicion) phishing intent detection across categories",
        "paper_reference": null,
        "metric": "Detection accuracy (with suspicion priming prompt)",
        "their_result": "Claude: 75% (control), 100% (GPT-generated), 100% (V-Triad), 100% (GPT+V-Triad)",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Click-through rate (CTR)",
      "Detection accuracy (percent of emails where malicious intent correctly identified)",
      "Self-reported rationale analysis (qualitative)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How do phishing emails created automatically by GPT-4 compare to those manually created using the V-Triad in terms of click-through rate?",
        "Can combining GPT-4 with the V-Triad (hybrid approach) improve phishing effectiveness while reducing effort?",
        "How well can popular LLMs (GPT, Claude, PaLM, LLaMA) detect the intention of phishing emails relative to humans?",
        "What are the economic implications of AI-enabled phishing for attacker incentives and costs?"
      ],
      "gaps_identified": [
        "Prior works on LLM-generated phishing rarely validated emails by sending them in real-world contexts.",
        "Existing studies did not investigate LLMs for detecting the intention of phishing emails.",
        "User cues for detecting phishing are individualized and sometimes contradictory, indicating a need for personalization.",
        "LLM safety filters can be bypassed through prompt reformulation, underscoring challenges in preventing malicious use."
      ],
      "limitations": [
        "“The quantitative detection results should be seen as an indication. A larger data-sample is required to draw more decisive conclusions.”",
        "Only GPT-4 was used for phishing generation; other LLMs were not used for generation due to scope and participants.",
        "Small detection set (20 emails) limits generalizability of LLM detector results.",
        "Human-subject study confined to a university population; external validity may be limited.",
        "Dependence on self-reported reasons for clicks/non-clicks introduces subjectivity."
      ],
      "future_work": [
        "Create phishing emails using multiple LLMs beyond GPT-4 and test with larger participant pools.",
        "Build larger, standardized datasets for LLM-based phishing intent detection.",
        "Investigate personalization strategies at scale and their effect on phishing susceptibility.",
        "Explore more robust LLM safeguards that do not unduly restrict legitimate marketing-like generation.",
        "Extend economic analysis with real-world cost and yield data from observed campaigns."
      ],
      "motivation": "Assess the dual-use impact of large language models on phishing: their ability to generate convincing phishing at scale and their potential to detect and mitigate such attacks, with implications for attacker incentives and defender strategies.",
      "potential_research_ideas": [
        "Develop a personalized phishing-defense assistant that models individual users’ susceptibility cues and provides tailored just-in-time warnings.",
        "Construct a benchmark and shared dataset for phishing ‘intent detection’ spanning benign, generic phishing, targeted spear-phishing, and LLM-generated variants.",
        "Evaluate co-evolutionary red-team/blue-team frameworks where generators (LLMs) and detectors (LLMs/ML) are jointly improved via adversarial training.",
        "Study calibration and uncertainty estimation for LLM suspicion scores to reduce over/under-flagging in email clients.",
        "Measure long-term training effects: do LLM-guided recommendations reduce future susceptibility in controlled longitudinal studies?"
      ],
      "architectural_improvement_recommendations": [
        "Use an ensemble detector combining an LLM with lexical/URL heuristics, SPF/DKIM/DMARC authentication signals, and sender reputation.",
        "Employ retrieval-augmented prompting to ground detection in threat intel feeds and known phishing templates.",
        "Fine-tune small/efficient transformer classifiers on labeled intent-detection data to reduce cost and latency versus general-purpose LLMs.",
        "Incorporate rationale generation with structured checklists (credibility/compatibility/customizability) to improve explainability and user trust.",
        "Implement calibration techniques (temperature scaling, conformal prediction) to produce well-calibrated suspicion probabilities."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": "No model training; API-based inference with GPT-4, Claude, PaLM, LLaMA. Minimal compute (client-side prompting and email sending)."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Emails delivered to university participants’ real inboxes; detection tested via LLM chat interfaces",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Ethical review and consent management for phishing studies (IRB approval required).",
        "LLM safety filters may block explicit phishing generation, leading to prompt reformulations.",
        "High variability and personalization needs; cues that deter one user may entice another.",
        "Bot/automated signups contamination during recruitment; need for robust screening.",
        "Limited generalization from small detection sets; necessity of larger, diverse corpora."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Human-subjects evaluation (n=112) comparing click-through rates of GPT-4, V-Triad, GPT-4+V-Triad, and control phishing emails.",
      "Demonstration that V-Triad and hybrid GPT-4+V-Triad can outperform pure GPT-4 phishing in effectiveness.",
      "Evaluation of four LLMs (GPT, Claude, PaLM, LLaMA) for phishing intent detection, including a suspicion-priming condition where Claude achieved up to 100% on multiple categories.",
      "Qualitative analysis of participant rationales revealing contradictory cues and the need for personalization.",
      "Economic analysis of AI-enabled phishing showing how LLMs reduce costs and increase attacker incentives."
    ]
  },
  {
    "arxiv_id": "2308.03417v2",
    "title": "PURL: Safe and Effective Sanitization of Link Decoration",
    "authors": "Shaoor Munir; Patrick Lee; Umar Iqbal; Zubair Shafiq; Sandra Siby",
    "abstract": "While privacy-focused browsers have taken steps to block third-party cookies and mitigate browser fingerprinting, novel tracking techniques that can bypass existing countermeasures continue to emerge. Since trackers need to share information from the client-side to the server-side through link decoration regardless of the tracking technique they employ, a promising orthogonal approach is to detect and sanitize tracking information in decorated links. To this end, we present PURL (pronounced purel-l), a machine-learning approach that leverages a cross-layer graph representation of webpage execution to safely and effectively sanitize link decoration. Our evaluation shows that PURL significantly outperforms existing countermeasures in terms of accuracy and reducing website breakage while being robust to common evasion techniques. PURL's deployment on a sample of top-million websites shows that link decoration is abused for tracking on nearly three-quarters of the websites, often to share cookies, email addresses, and fingerprinting information.",
    "published_date": "2023-08-07",
    "pdf_link": "https://arxiv.org/pdf/2308.03417v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Web Security",
      "subdomain": "Online Tracking and Privacy",
      "specific_problem": "Detecting and sanitizing tracking link decorations in URLs without breaking site functionality",
      "attack_types": [
        "Link decoration-based tracking",
        "Identifier sharing via URL parameters/path/fragments",
        "Cookie syncing/sharing via URL",
        "PII sharing (e.g., email addresses) via URL",
        "Fingerprinting information sharing via URL"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Supervised classification with cross-layer graph-derived features",
        "specific": null,
        "novel_contribution": "Cross-layer graph representation of webpage execution (DOM, JavaScript behavior, browser storage, network requests) used to extract features and train a classifier to distinguish tracking vs functional link decorations"
      },
      {
        "type": "baseline",
        "category": "Heuristic",
        "specific": "CrumbCruncher",
        "novel_contribution": "Parallel and consecutive crawls to flag persistent query parameters (from Randall et al. [95]); reported to suffer from high false positives"
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "20K-site crawl of Tranco top-million websites (March–April 2023) with interactive browsing",
        "type": "private",
        "domain": "web_urls",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "PURL-generated filter list of tracking link decorations",
        "type": "public",
        "domain": "filter_list",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "EasyList",
        "type": "public",
        "domain": "filter_list",
        "link": "https://easylist.to/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "EasyPrivacy",
        "type": "public",
        "domain": "filter_list",
        "link": "https://easylist.to/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Tranco top-million list",
        "type": "public",
        "domain": "website_ranking",
        "link": "https://tranco-list.eu",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Filter-list parameter stripping (Brave)",
        "paper_reference": null,
        "metric": "Precision, Recall, Accuracy; Website breakage rate",
        "their_result": "“Overall PURL achieves 98.74% accuracy, significantly outperforming existing countermeasures by at least 7.71% in terms of precision, 4.83% in terms of recall, and 6.43% overall accuracy while reducing website breakage by more than 8×.”",
        "baseline_result": null
      },
      {
        "method_name": "Filter-list parameter stripping (Firefox ETP Strict)",
        "paper_reference": null,
        "metric": "Precision, Recall, Accuracy; Website breakage rate",
        "their_result": "“PURL can effectively (98.87% recall) and safely (98.62% precision) sanitize link decoration.”",
        "baseline_result": null
      },
      {
        "method_name": "Filter-list parameter stripping (Safari 17 private browsing)",
        "paper_reference": null,
        "metric": "Precision, Recall, Accuracy",
        "their_result": "“PURL significantly outperforms existing countermeasures in terms of accuracy and reducing website breakage…”",
        "baseline_result": null
      },
      {
        "method_name": "AdGuard removeparam filter rules",
        "paper_reference": null,
        "metric": "Precision, Recall, Accuracy; Website breakage rate",
        "their_result": "As above; PURL outperforms filter-list approaches and reduces breakage by >8×.",
        "baseline_result": null
      },
      {
        "method_name": "uBlock Origin removeparam/queryprune",
        "paper_reference": null,
        "metric": "Precision, Recall, Accuracy; Website breakage rate",
        "their_result": "As above; PURL outperforms filter-list approaches.",
        "baseline_result": null
      },
      {
        "method_name": "EasyList + EasyPrivacy (request blocking)",
        "paper_reference": null,
        "metric": "False positives/negatives; breakage",
        "their_result": "Paper states blocking whole URLs is impractical when tracking and functional decorations are mixed and leads to non-trivial false positives and false negatives.",
        "baseline_result": null
      },
      {
        "method_name": "CrumbCruncher (heuristic)",
        "paper_reference": "Randall et al. [95]",
        "metric": "False positive rate (manual review)",
        "their_result": "PURL: precision 98.62%, recall 98.87%, accuracy 98.74%.",
        "baseline_result": "“Manual review by the authors showed that CrumbCruncher suffers from a 36% false positive rate.”"
      }
    ],
    "performance_metrics_used": [
      "Precision",
      "Recall",
      "Accuracy",
      "Website breakage rate",
      "Robustness to evasion (parameter renaming; value splitting/combining)",
      "Prevalence measurement (percentage of sites with tracking link decorations; average number per site)"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can a machine-learning approach using a cross-layer graph of webpage execution safely and effectively distinguish tracking vs functional link decorations?",
        "How prevalent is the abuse of link decoration for tracking on the web, and what kinds of identifiers are shared?",
        "Can an automated approach outperform existing filter-list and heuristic-based countermeasures while minimizing website breakage?"
      ],
      "gaps_identified": [
        "Manual filter lists are small, slow to update, and error-prone; they cannot keep pace with the growing and changing use of tracking query parameters.",
        "Existing request-blocking approaches operate at too coarse a granularity, risking breakage of legitimate functionality or allowing tracking through decorated URLs.",
        "Heuristic methods like CrumbCruncher incur high false positives and miss non-persistent identifiers, leading to both false positives and false negatives.",
        "Prior work focused on query parameters; link decoration can also occur in resource paths and fragments."
      ],
      "limitations": [
        "Threat model excludes first-party requests, assuming first parties use identifiers for legitimate functionality.",
        "Prevalence labeling of ATS vs Non-ATS relies on EasyList/EasyPrivacy, which themselves may have coverage limitations.",
        "Stateless crawling (clearing state before each site) may miss behaviors dependent on persistent login or multi-visit state.",
        "Evaluation of evasion focuses on specific strategies (renaming parameters, splitting/combining values); other evasions may exist."
      ],
      "future_work": [],
      "motivation": "Trackers must ultimately convey identifiers from client to server via URL decoration regardless of tracking technique; a fine-grained, generalizable sanitization approach can block tracking without breaking functionality and can keep pace with evolving techniques.",
      "potential_research_ideas": [
        "Develop a graph neural network operating directly on the cross-layer execution graph to learn richer structural patterns of tracking behavior beyond hand-crafted features.",
        "Combine supervised and self-supervised pretraining (e.g., contrastive learning on graphs of tracking vs non-tracking flows) to improve generalization to unseen trackers.",
        "Online/active learning in the browser to continuously refine the model and auto-generate new filter rules with human-in-the-loop verification.",
        "Extend detection to first-party requests to identify privacy-invasive uses while minimizing functional impact via contextual policies.",
        "Joint multi-task learning to classify decoration type (tracking/functional) and infer the category of identifier (cookie, email, fingerprint) to support targeted sanitization.",
        "Robustness-focused training (adversarial training/data augmentation) against obfuscation strategies like encoding, chunking, or nesting identifiers in paths/fragments.",
        "Static-dynamic JS analysis integration to attribute decorations to specific scripts and enable script-level remediation or policy enforcement."
      ],
      "architectural_improvement_recommendations": [
        "Replace or augment feature-based classifier with a GNN (e.g., GraphSAGE/GAT) over the cross-layer graph to capture higher-order dependencies.",
        "Introduce attention-based feature fusion across DOM, JS execution, storage, and network subgraphs to weight salient signals.",
        "Leverage sequence modeling (transformers) on request/parameter temporal sequences across page loads to capture persistence and correlation.",
        "Implement uncertainty estimation and abstention to avoid sanitization when confidence is low, reducing breakage risk.",
        "Use explainability techniques (e.g., SHAP on features or GNNExplainer) to interpret sanitization decisions and aid in rule generation.",
        "Add a lightweight in-browser inference pipeline (e.g., WebAssembly) with caching of per-domain decisions to cut latency."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Web browser client (in-browser sanitization; deployment over a subset of top-million sites and generation of a filter list used by browsers/extensions)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Avoiding website breakage when sanitizing functional parameters mixed with tracking parameters",
        "Evasion by trackers (renaming parameters, splitting/combining values, using paths/fragments)",
        "Keeping pace with evolving tracking techniques and parameter vocabularies"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": true,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposed PURL, a machine-learning approach using a cross-layer graph of webpage execution to detect and sanitize tracking link decorations.",
      "Large-scale measurement on a sample of the top-million websites quantifying prevalence: “PURL detects that 73.02% of the sites abuse link decorations for tracking, with an average site using 10.75 tracking link decorations.”",
      "Generated a filter list from PURL that is already used by privacy-focused browsers and extensions."
    ]
  },
  {
    "arxiv_id": "2307.16149v3",
    "title": "A Novel DDPM-based Ensemble Approach for Energy Theft Detection in Smart Grids",
    "authors": "Xun Yuan; Yang Yang; Asif Iqbal; Prosanta Gope; Biplab Sikdar",
    "abstract": "Energy theft, characterized by manipulating energy consumption readings to reduce payments, poses a dual threat-causing financial losses for grid operators and undermining the performance of smart grids. Effective Energy Theft Detection (ETD) methods become crucial in mitigating these risks by identifying such fraudulent activities in their early stages. However, the majority of current ETD methods rely on supervised learning, which is hindered by the difficulty of labelling data and the risk of overfitting known attacks. To address these challenges, several unsupervised ETD methods have been proposed, focusing on learning the normal patterns from honest users, specifically the reconstruction of input. However, our investigation reveals a limitation in current unsupervised ETD methods, as they can only detect anomalous behaviours in users exhibiting regular patterns. Users with high-variance behaviours pose a challenge to these methods. In response, this paper introduces a Denoising Diffusion Probabilistic Model (DDPM)-based ETD approach. This innovative approach demonstrates impressive ETD performance on high-variance smart grid data by incorporating additional attributes correlated with energy consumption. The proposed methods improve the average ETD performance on high-variance smart grid data from below 0.5 to over 0.9 w.r.t. AUC. On the other hand, our experimental findings indicate that while the state-of-the-art ETD methods based on reconstruction error can identify ETD attacks for the majority of users, they prove ineffective in detecting attacks for certain users. To address this, we propose a novel ensemble approach that considers both reconstruction error and forecasting error, enhancing the robustness of the ETD methodology. The proposed ensemble method improves the average ETD performance on the stealthiest attacks from nearly 0 to 0.5 w.r.t. 5%-TPR.",
    "published_date": "2023-07-30",
    "pdf_link": "https://arxiv.org/pdf/2307.16149v3",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Cyber-Physical Systems Security",
      "subdomain": "Smart Grid Security",
      "specific_problem": "Energy Theft Detection from smart meter time-series",
      "attack_types": [
        "fixed reduction",
        "partial reduction",
        "random partial reduction",
        "random average consumption",
        "average consumption",
        "reverse",
        "selective bypass"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Diffusion Model",
        "specific": "DDPM (Denoising Diffusion Probabilistic Model)",
        "novel_contribution": "First application of DDPM to energy theft detection; unified objective to jointly minimize reconstruction and forecasting errors; conditional generation using features correlated with energy consumption; strong performance on high-variance users"
      },
      {
        "type": "primary",
        "category": "RNN",
        "specific": "LSTM (as conditioning feature extractor)",
        "novel_contribution": "Used to extract temporal/context features to condition DDPM for both reconstruction and forecasting"
      },
      {
        "type": "primary",
        "category": "Ensemble",
        "specific": "REM + FEM (reconstruction-error + forecasting-error)",
        "novel_contribution": "Novel unsupervised ensemble combining reconstruction and forecasting anomaly scores for ETD; improves detection on users where single-objective methods fail"
      },
      {
        "type": "baseline",
        "category": "Fully Connected Network",
        "specific": "FC reconstruction (REM) [11]",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "VAE",
        "specific": "LSTM-VAE (REM) [12]",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "RNN",
        "specific": "LSTM-based multi-sensor anomaly detection adapted as REM [19]",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "RNN",
        "specific": "LSTM forecasting adapted as FEM [20]",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "",
        "type": "public",
        "domain": "smart_meter_readings",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      },
      {
        "name": "Synthetic smart grid dataset",
        "type": "synthetic",
        "domain": "smart_meter_readings",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "FC network REM (Alromih et al. [11])",
        "paper_reference": "[11]",
        "metric": "AUC (high-variance users)",
        "their_result": "“improve the average ETD performance on high-variance smart grid data from below 0.5 to over 0.9 w.r.t. AUC.”",
        "baseline_result": "“below 0.5 w.r.t. AUC”"
      },
      {
        "method_name": "LSTM-VAE REM (Takiddin et al. [12])",
        "paper_reference": "[12]",
        "metric": "AUC (high-variance users)",
        "their_result": "“over 0.9 w.r.t. AUC”",
        "baseline_result": "“below 0.5 w.r.t. AUC”"
      },
      {
        "method_name": "Ensemble (REM+FEM) vs SOTA REMs",
        "paper_reference": null,
        "metric": "5%-TPR (stealthiest attacks)",
        "their_result": "“improves the average ETD performance on the stealthiest attacks from nearly 0 to 0.5 w.r.t. 5%-TPR.”",
        "baseline_result": "“nearly 0 w.r.t. 5%-TPR”"
      },
      {
        "method_name": "LSTM-based REM adapted from [19]",
        "paper_reference": "[19]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "LSTM-based FEM adapted from [20]",
        "paper_reference": "[20]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "AUC (ROC-AUC)",
      "TPR at 5% (5%-TPR)",
      "Reconstruction error (MAE) as anomaly score",
      "Forecasting error (MAE) as anomaly score"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can DDPM be effectively applied to energy theft detection, especially for high-variance smart grid users?",
        "Does combining reconstruction-error and forecasting-error (REM + FEM) improve ETD robustness over single-objective unsupervised methods?",
        "Can a unified objective jointly optimizing reconstruction and forecasting improve ETD performance?"
      ],
      "gaps_identified": [
        "Supervised ETD requires labeled data, is prone to imbalance and overfitting known attacks, and generalizes poorly to unseen attacks",
        "Limited research on unsupervised ETD; most rely solely on reconstruction error (REM)",
        "REMs fail for certain users and for high-variance consumption patterns",
        "Forecasting error (FEM) had not been explored for ETD or combined with REM",
        "DDPM had not been studied for ETD despite success in image anomaly detection"
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Address shortcomings of supervised ETD and REM-only unsupervised methods, enable robust unsupervised ETD for high-variance users, and investigate DDPM for time-series anomaly detection in smart grids.",
      "potential_research_ideas": [
        "Extend to population-level modeling with graph neural networks linking users by feeder/transformer to share context and detect coordinated theft",
        "Incorporate probabilistic calibration and uncertainty-aware thresholds (e.g., conformal prediction) for reliable alerting under distribution shift",
        "Develop online/continual diffusion-based ETD to handle concept drift and seasonal changes without full retraining",
        "Design privacy-preserving or federated DDPM training across utilities to leverage larger datasets without sharing raw meter data",
        "Study adversarial robustness where attackers craft manipulations targeting the diffusion/conditioning pipeline; propose robust training",
        "Integrate exogenous variables (weather, holidays, tariffs) via cross-attention conditioning to improve forecasting-based detection",
        "Create realistic, physics-informed theft simulators to benchmark ETD under stealthy strategies and evaluate generalization",
        "Explainable ETD: map anomaly contributions to time-of-day and attributes; use attribution over conditioning features",
        "Lightweight diffusion distillation for edge deployment on substation/AMI gateways"
      ],
      "architectural_improvement_recommendations": [
        "Replace/augment LSTM with Temporal Convolutional Networks or Transformer encoders for richer conditioning and long-range dependencies",
        "Use score-based SDE diffusion for continuous-time forecasting and multi-horizon uncertainty quantification",
        "Add cross-attention between noise-level embeddings and conditioning context for stronger guidance in DDPM sampling",
        "Multi-scale objectives: reconstruct/forecast at 15-min, hourly, and daily resolutions to capture patterns across scales",
        "Dynamic thresholding via conformal calibration using recent honest windows per user to control false alarm rate",
        "Federated training with secure aggregation to scale across utilities while preserving privacy",
        "Adversarial/consistency training to resist covariate manipulation (e.g., corrupted auxiliary attributes)",
        "Uncertainty-aware ensemble (e.g., deep ensembles of diffusion samplers) to stabilize anomaly scores"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Smart grid server ingesting smart meter readings from users over the Internet",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "High-variance user behavior degrades traditional methods; requires robust handling",
        "Selection and calibration of anomaly thresholds (per-user) for REM and FEM",
        "Availability and quality of auxiliary attributes correlated with consumption",
        "Distribution shift/seasonality and concept drift in consumption patterns"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Propose ETDddpm, a DDPM-based unsupervised ensemble approach combining reconstruction and forecasting errors for ETD",
      "First to apply DDPM to the ETD problem",
      "Show ensemble (REM+FEM) improves robustness where single REM or FEM fails; first to combine REM and FEM for ETD",
      "Achieve strong ETD on high-variance smart grid data where current methods fail (AUC > 0.9)",
      "Introduce a unified learning objective to jointly optimize reconstruction and forecasting"
    ]
  },
  {
    "arxiv_id": "2308.06973v1",
    "title": "Routing Recovery for UAV Networks with Deliberate Attacks: A Reinforcement Learning based Approach",
    "authors": "Sijie He; Ziye Jia; Chao Dong; Wei Wang; Yilu Cao; Yang Yang; Qihui Wu",
    "abstract": "The unmanned aerial vehicle (UAV) network is popular these years due to its various applications. In the UAV network, routing is significantly affected by the distributed network topology, leading to the issue that UAVs are vulnerable to deliberate damage. Hence, this paper focuses on the routing plan and recovery for UAV networks with attacks. In detail, a deliberate attack model based on the importance of nodes is designed to represent enemy attacks. Then, a node importance ranking mechanism is presented, considering the degree of nodes and link importance. However, it is intractable to handle the routing problem by traditional methods for UAV networks, since link connections change with the UAV availability. Hence, an intelligent algorithm based on reinforcement learning is proposed to recover the routing path when UAVs are attacked. Simulations are conducted and numerical results verify the proposed mechanism performs better than other referred methods.",
    "published_date": "2023-08-14",
    "pdf_link": "https://arxiv.org/pdf/2308.06973v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Secure/Resilient Routing",
      "specific_problem": "Attack-resilient routing recovery in UAV ad hoc networks under deliberate node-targeting attacks, minimizing end-to-end delay",
      "attack_types": [
        "deliberate targeted attacks based on node importance",
        "targeted node removal",
        "denial-of-service (modeled as node becoming unavailable)",
        "topology disruption"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Reinforcement Learning (tabular TD)",
        "specific": "Sarsa(λ) with eligibility traces",
        "novel_contribution": "Application of Sarsa(λ) to UAV routing recovery under a deliberate attack model; integrates node-importance-based attack modeling and routing state (distance, queue length) to minimize end-to-end delay"
      },
      {
        "type": "baseline",
        "category": "Reinforcement Learning (tabular TD, on-policy)",
        "specific": "Sarsa",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Reinforcement Learning (tabular TD, off-policy)",
        "specific": "Q-learning (ε-greedy)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Reinforcement Learning",
      "Model-free",
      "On-policy (for Sarsa/Sarsa(λ))",
      "Temporal-Difference Learning"
    ],
    "datasets": [
      {
        "name": "Synthetic MATLAB UAV network simulation",
        "type": "synthetic",
        "domain": "uav_network_simulation",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Sarsa",
        "paper_reference": null,
        "metric": "reward (convergence), steps per episode, end-to-end delay (ms), path distance, hop count",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Q-learning",
        "paper_reference": null,
        "metric": "reward (convergence), steps per episode, end-to-end delay (ms), path distance, hop count",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "end-to-end delay (ms)",
      "reward (negative of delay proxy)",
      "steps per episode",
      "path distance",
      "hop count",
      "convergence across episodes"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How to recover routing paths in UAV networks under deliberate attacks that disable nodes while minimizing end-to-end delay?",
        "Can a model-free RL method adapt routing decisions to dynamic topology changes and attacked nodes?"
      ],
      "gaps_identified": [
        "Traditional shortest-path routing (e.g., Dijkstra, Bellman-Ford) is unsuitable for highly dynamic UAV networks and cannot ensure robustness/security.",
        "Prior RL-based UAV routing works did not explicitly consider deliberate attacks that damage nodes and destroy original routes."
      ],
      "limitations": [
        "Evaluation is simulation-only (MATLAB) with no real-world deployment or emulation evidence.",
        "Attack model abstracts denial-of-service as node removal via a node-importance ranking; other attack modalities (e.g., jamming, spoofing) are not modeled.",
        "Communication and channel model simplified to free-space path loss; interference and fading not considered.",
        "Tabular Sarsa(λ) may not scale to large state/action spaces or continuous state features without function approximation.",
        "Limited scalability analysis (node count up to ~40) and no complexity or runtime/inference-time reporting.",
        "Reward design provides negative delay and zero at termination; potential sparse/weak learning signal at episode end not explored.",
        "No comparison to graph-based or deep RL routing methods; baselines limited to tabular Sarsa and Q-learning."
      ],
      "future_work": [],
      "motivation": "UAV networks are vulnerable to deliberate attacks that disrupt routing; dynamic, resilient routing recovery is needed to maintain low end-to-end delay under node failures/attacks.",
      "potential_research_ideas": [
        "Formulate routing recovery as a multi-agent RL problem with decentralized policies and centralized training to better capture distributed UAV decision-making.",
        "Incorporate graph neural networks to encode dynamic topology and learn scalable policies (e.g., GNN-based actor-critic for routing).",
        "Model the interaction between attacker and defender as a stochastic game; apply robust/adversarial RL or minimax training for worst-case resilience.",
        "Extend the attack model to include jamming, spoofing, and link-layer attacks; evaluate robustness under combined faults and attacks.",
        "Adopt POMDP formulations with recurrent policies (e.g., DRQN) to handle partial observability and delayed/uncertain state information.",
        "Optimize multi-objective trade-offs (delay, energy, reliability, link utilization) using constrained RL or reward shaping.",
        "Transfer learning/domain randomization across environments and topologies to improve generalization and reduce sample complexity.",
        "Integrate real-time constraints and safety (e.g., shielded RL, safe exploration) to avoid routing loops and blackholes during learning.",
        "Benchmark against more advanced baselines (DQN, Double/Dueling DQN, PPO, A3C, GNN-RL routing) on standardized UAVNet simulators.",
        "Hybrid approach combining fast heuristic shortest-path re-routing with RL meta-controller for attack-aware adjustments."
      ],
      "architectural_improvement_recommendations": [
        "Replace tabular Sarsa(λ) with function approximation (e.g., DQN/Double DQN/Dueling DQN) to scale with continuous state features (distance, queue length, link quality).",
        "Use graph neural network encoders over local k-hop neighborhoods to produce action-values for neighbor selection.",
        "Adopt actor-critic methods (e.g., PPO, A2C/A3C) with eligibility traces or GAE to stabilize training under non-stationary topology.",
        "Introduce prioritized experience replay and n-step returns to accelerate convergence.",
        "Enhance reward shaping to include per-hop penalties, loop penalties, delivery success bonuses, and trust/attack-avoidance terms.",
        "Incorporate robust RL regularization (e.g., entropy bonuses, adversarial perturbations of topology) to improve resilience.",
        "Use centralized training with decentralized execution (CTDE) to share global context during training while executing locally.",
        "Add recurrent layers (LSTM/GRU) for partial observability and to model time-varying traffic queues and mobility.",
        "Model action masking to restrict actions to feasible neighbor sets and enforce communication range constraints efficiently.",
        "Calibrate channel model realism (shadowing, fading, interference) and include link quality as inputs to the policy."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "MATLAB"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Not specified; MATLAB-based simulation; example hyperparameters provided (γ=0.9, α=0.01, λ=0.9, ε=0.001); training episodes in plots up to ~600."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "UAV ad hoc network (simulated)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Dynamic topology and mobility require rapid policy decisions under limited neighbor information.",
        "Convergence time and sample efficiency of RL in non-stationary environments.",
        "Reliance on accurate state estimation (positions, queues) and communication overhead for neighbor discovery.",
        "Robustness to varied attack types beyond node removal (e.g., jamming, spoofing).",
        "Scalability of tabular RL to larger UAV swarms."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes a node importance ranking mechanism (NIRM) combining node degree and link importance to model deliberate attacks on UAV networks.",
      "Reformulates routing recovery under attacks as an MDP and designs a Sarsa(λ)-based intelligent routing algorithm.",
      "Simulation study demonstrating faster convergence and lower routing delay versus Sarsa and Q-learning baselines under attack and non-attack scenarios."
    ]
  },
  {
    "arxiv_id": "2309.02247v2",
    "title": "Towards Autonomous Cyber Operation Agents: Exploring the Red Case",
    "authors": "Li Li; Jean-Pierre S. El Rami; Ryan Kerr; Adrian Taylor; Grant Vandenberghe",
    "abstract": "Recently, reinforcement and deep reinforcement learning (RL/DRL) have been applied to develop autonomous agents for cyber network operations(CyOps), where the agents are trained in a representative environment using RL and particularly DRL algorithms. The training environment must simulate CyOps with high fidelity, which the agent aims to learn and accomplish. A good simulator is hard to achieve due to the extreme complexity of the cyber environment. The trained agent must also be generalizable to network variations because operational cyber networks change constantly. The red agent case is taken to discuss these two issues in this work. We elaborate on their essential requirements and potential solution options, illustrated by some preliminary experimentations in a Cyber Gym for Intelligent Learning (CyGIL) testbed.",
    "published_date": "2023-09-05",
    "pdf_link": "https://arxiv.org/pdf/2309.02247v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Offensive Security / Red Teaming",
      "specific_problem": "Training and sim-to-real deployment of an autonomous red (penetration testing) agent that plans an attack course-of-action to compromise an enterprise Active Directory Domain Controller, with generalization across network updates",
      "attack_types": [
        "Initial access (phishing)",
        "Lateral movement",
        "Privilege escalation",
        "Credential theft/abuse",
        "Domain compromise (AD/DC takeover)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Value-based Deep RL",
        "specific": "C51 Rainbow (Categorical DQN with Double Q)",
        "novel_contribution": "Selected as the final policy model due to faster convergence with equivalent optimal policy; integrated into a unified emulator-simulator training loop to reduce training latency."
      },
      {
        "type": "baseline",
        "category": "Value-based Deep RL",
        "specific": "DQN",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Policy Gradient (Actor-Critic)",
        "specific": "PPO (Proximal Policy Optimization)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Representation/Observation Design",
        "specific": "ACTNeT (full information), ACT (action-only), TACT (action-target with CEEC) embeddings",
        "novel_contribution": "Proposes action-centric observation embeddings to improve agent generalization across network changes; introduces CEEC (command executable encodings) concept and investigates invariance strategies (order/attention)."
      }
    ],
    "learning_paradigm": [
      "Reinforcement Learning",
      "Deep Reinforcement Learning"
    ],
    "datasets": [
      {
        "name": "CyGIL-E action traces (agent action and tool output logs)",
        "type": "private",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "CyGIL-S simulated environment data (FSM generated from CyGIL-E traces)",
        "type": "private",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "DQN",
        "paper_reference": null,
        "metric": "Training time (full emulator)",
        "their_result": "~15 hours with unified CyGIL-E/S when using CDQN as final model (Table 1)",
        "baseline_result": "> 96 hours when trained entirely in CyGIL-E (Table 1)"
      },
      {
        "method_name": "PPO",
        "paper_reference": null,
        "metric": "Training time (full emulator)",
        "their_result": "~15 hours with unified CyGIL-E/S when using CDQN as final model (Table 1)",
        "baseline_result": "> 96 hours when trained entirely in CyGIL-E (Table 1)"
      },
      {
        "method_name": "DQN",
        "paper_reference": null,
        "metric": "Sample efficiency (training steps)",
        "their_result": "\"C51 (CDQN) converges much faster than the DQN agent\" (Fig. 5)",
        "baseline_result": "Slower convergence to optimal policy (Fig. 5)"
      },
      {
        "method_name": "PPO",
        "paper_reference": null,
        "metric": "Convergence vs optimal policy",
        "their_result": "\"CDQN is selected for its significantly faster convergence and equivalent optimal policy capability\" (Fig. 6)",
        "baseline_result": "Effectively converges to optimal policy but slower than CDQN (Fig. 6)"
      },
      {
        "method_name": "DQN/PPO (E) + CDQN (S)",
        "paper_reference": null,
        "metric": "End-to-end training time (unified E+S)",
        "their_result": "~15 hours when CDQN and PPO in E then final CDQN in S (Table 1)",
        "baseline_result": "~38 hours when DQN and PPO in E then final CDQN in S (Table 1)"
      }
    ],
    "performance_metrics_used": [
      "training time (hours)",
      "accumulated return (episode return)",
      "training steps to convergence",
      "episode length (actions to goal)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How to build a high-fidelity, efficient sim-to-real training environment for autonomous CyOps agents (red case) that supports direct deployment?",
        "How can a trained red agent generalize across network configuration changes without extensive retraining?"
      ],
      "gaps_identified": [
        "High-fidelity cyber simulators are hard to build due to extreme environment complexity; existing detailed state-transition models require meticulous abstraction and are difficult to implement at scale.",
        "Trained agents generally cannot operate real CyOps tools or handle network topology changes.",
        "Generalizability of trained agents across network updates remains a critical problem.",
        "Data collection in real/emulated networks is time-consuming; need to minimize data while covering necessary state distributions for optimal policy.",
        "Prior CyGIL work had only limited test results reported."
      ],
      "limitations": [
        "Results are preliminary; generalization results are discussed conceptually with limited quantitative evaluation on unseen network variations.",
        "TACT/CEEC design trade-offs (field selection, condition bits) and invariance strategies are still under investigation.",
        "The 'Retry' condition for switching data collection/generation is empirically chosen; not yet systematically optimized.",
        "Agent_s is retrained from scratch in each new CyGIL-S (though simulator training latency is low).",
        "ACT embedding increases simulator training time by about 70% vs ACTNeT (still under 1 hour), indicating a performance trade-off.",
        "Generalization claims focus on robustness to certain variations (e.g., IP addresses), while broader topology/host-order variations can still break policies with ACTNeT."
      ],
      "future_work": [
        "Refine TACT/CEEC design, including order/number invariance (via tool-managed hand order or attention-based position invariance).",
        "Systematically select CEEC fields to balance precision vs generalizability; evaluate condition-bit granularity.",
        "Formalize and optimize the 'Retry' condition and data collection schedule for environment generation.",
        "Extend experiments to more diverse networks, objectives, and dynamic topology changes; quantify generalization performance.",
        "Explore algorithm scheduling across training phases (sample-efficient early, policy-accurate later)."
      ],
      "motivation": "Address resource scalability and effectiveness of red team operations for network hardening and blue training by creating deployable autonomous red agents that can be trained efficiently in realistic environments and generalize to operational network changes.",
      "potential_research_ideas": [
        "Graph-based observation modeling (e.g., GNNs) to achieve permutation- and size-invariant representations of hosts/services, improving generalization across topology changes.",
        "Attention-based Transformer policies over variable-length CEEC sets to enforce order/number invariance and selective focus on actionable opportunities.",
        "Domain randomization and meta-RL across procedurally varied network topologies/services to learn broadly generalizable policies without exhaustive retraining.",
        "Hierarchical RL with options (tactics→techniques→commands) to reduce action-space branching and enable reusable subpolicies across networks.",
        "Model-based RL or learned simulators to better capture action preconditions/success probabilities and perform planning under uncertainty.",
        "POMDP formulations with belief-state tracking over unknown network conditions to reduce brittleness to partial observability and stale tool outputs.",
        "Multi-agent/self-play (red vs blue) co-training to produce robust red policies that adapt to active defenses and deception.",
        "Offline RL from historical red/blue exercise logs to bootstrap policies and reduce emulator data collection costs."
      ],
      "architectural_improvement_recommendations": [
        "Replace hand-ordered CEEC lists with set-encoding architectures (Deep Sets) or attention pooling to ensure permutation invariance.",
        "Adopt graph neural networks over host-service graphs with action masking to only allow executable actions, reducing invalid exploration.",
        "Incorporate uncertainty estimates (e.g., ensembles) and risk-aware reward shaping to handle low success rates and unknown network conditions.",
        "Curriculum learning that gradually increases network size/complexity and introduces topology perturbations for robustness.",
        "Use action-parameter decoupling and parameter-sharing heads to scale to hundreds of action templates without explosion in outputs.",
        "Leverage reward shaping/auxiliary objectives (e.g., state coverage, information gain) to guide exploration in sparse-reward red scenarios.",
        "Automate the 'Retry' scheduling via bandit/BO to decide when to switch E↔S and when to regenerate the simulator."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "Custom (CyGIL library)"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Emulated network on a Windows machine with Intel Core i9 and 64GB RAM; agent training on a DL laptop with similar configuration. Full emulator training took >96 hours for DQN/PPO; unified CyGIL-E/S reduced total training to ~38 hours or ~15 hours depending on algorithm schedule; simulator training typically <1 hour."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Emulated enterprise network (VMware vSphere or Mininet+ONOS SDN) with real red-team tools (MITRE CALDERA) and C2 integration via VMware REST API.",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Achieving high-fidelity yet efficient simulation for DRL training (sim-to-real gap).",
        "Generalization to frequent network updates (host order, subnets, service instances).",
        "Large, sparse, and parameterized action space with low success rates and unknown conditions.",
        "Limited visibility from red perspective; partial observability and stale/incorrect tool outputs.",
        "Selecting robust observation embeddings that avoid overfitting to irrelevant network details.",
        "Time-consuming data collection in emulator; choosing when/how much to collect."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Unified emulator–simulator training workflow (CyGIL-E/S) with representation learning and continual agent transfer to enable sim-to-real training for autonomous red agents.",
      "Action-centric observation embeddings (ACTNeT, ACT, TACT/CEEC) to improve policy generalization across network changes, with analysis of trade-offs and invariance options.",
      "Empirical comparison showing C51 (CDQN) converges faster than DQN and PPO with equivalent optimal policy, enabling substantial reduction in total training time.",
      "Demonstration on an emulated enterprise network (AD/DC compromise objective), including reward design and actionable red-team tool integration (MITRE CALDERA).",
      "Preliminary generalizability findings: action-only embedding increases simulator training time by ~70% vs full-information ACTNeT but remains <1 hour and offers better robustness to certain variations."
    ]
  },
  {
    "arxiv_id": "2308.00542v1",
    "title": "SF-IDS: An Imbalanced Semi-Supervised Learning Framework for Fine-grained Intrusion Detection",
    "authors": "Xinran Zheng; Shuo Yang; Xingjun Wang",
    "abstract": "Deep learning-based fine-grained network intrusion detection systems (NIDS) enable different attacks to be responded to in a fast and targeted manner with the help of large-scale labels. However, the cost of labeling causes insufficient labeled samples. Also, the real fine-grained traffic shows a long-tailed distribution with great class imbalance. These two problems often appear simultaneously, posing serious challenges to fine-grained NIDS. In this work, we propose a novel semi-supervised fine-grained intrusion detection framework, SF-IDS, to achieve attack classification in the label-limited and highly class imbalanced case. We design a self-training backbone model called RI-1DCNN to boost the feature extraction by reconstructing the input samples into a multichannel image format. The uncertainty of the generated pseudo-labels is evaluated and used as a reference for pseudo-label filtering in combination with the prediction probability. To mitigate the effects of fine-grained class imbalance, we propose a hybrid loss function combining supervised contrastive loss and multi-weighted classification loss to obtain more compact intra-class features and clearer inter-class intervals. Experiments show that the proposed SF-IDS achieves 3.01% and 2.71% Marco-F1 improvement on two classical datasets with 1% labeled, respectively.",
    "published_date": "2023-08-01",
    "pdf_link": "https://arxiv.org/pdf/2308.00542v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Fine-grained network intrusion detection under limited labels and severe class imbalance using semi-supervised self-training",
      "attack_types": [
        "neptune",
        "satan",
        "ipsweep",
        "smurf",
        "portsweep",
        "nmap",
        "back",
        "warezmaster",
        "teardrop",
        "attack (aggregated minority class in NSL-KDD split)",
        "DDoS",
        "Hulk",
        "PortScan",
        "GoldenEye",
        "FTP-Patator",
        "SSH-Patator",
        "Web Attack",
        "Bot",
        "slowloris",
        "Slowhttptest"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "CNN",
        "specific": "RI-1DCNN (Residual 1D CNN with projection and classification heads)",
        "novel_contribution": "Reconstructs tabular traffic samples into multichannel image format via an initial FC layer; residual 1D CNN backbone tailored for self-training in NIDS"
      },
      {
        "type": "primary",
        "category": "Self-Training",
        "specific": "Pseudo-labeling with uncertainty-based filtering",
        "novel_contribution": "Combines prediction probability threshold with MC-Dropout uncertainty threshold; resamples pseudo-labeled data using Borderline-SMOTE and class-imbalance control"
      },
      {
        "type": "primary",
        "category": "Contrastive Learning",
        "specific": "Supervised Contrastive Loss (SupCon)",
        "novel_contribution": "Applies SupCon to imbalanced NIDS; integrated into a hybrid loss with epoch-wise inverse weighting (β) to emphasize feature learning early and classification later"
      },
      {
        "type": "primary",
        "category": "Loss Function",
        "specific": "Multi-weighted Cross-Entropy",
        "novel_contribution": "Smooth class-imbalance weights wi using log(Nmin+n)/log(Ni+n) and probabilistic reset weights to penalize normal/attack confusions (controlled by α)"
      },
      {
        "type": "primary",
        "category": "Uncertainty Estimation",
        "specific": "MC-Dropout",
        "novel_contribution": "Uses standard deviation over T stochastic forward passes as pseudo-label uncertainty for filtering"
      },
      {
        "type": "primary",
        "category": "Data Resampling",
        "specific": "Borderline-SMOTE",
        "novel_contribution": "Applied on pseudo-labeled data near decision boundaries to control pseudo-label class imbalance"
      },
      {
        "type": "baseline",
        "category": "Ensemble/Tree",
        "specific": "Random Forest",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "SVM",
        "specific": "Support Vector Machine",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Linear Model",
        "specific": "Logistic Regression",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "VGG16",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "DNN",
        "specific": "Feedforward DNN",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "RNN",
        "specific": "LSTM",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Semi-supervised",
        "specific": "FixMatch",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Semi-supervised",
        "specific": "Semi-WTC",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Semi-supervised",
      "Supervised"
    ],
    "datasets": [
      {
        "name": "NSL-KDD",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CICIDS2017",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Random Forest",
        "paper_reference": null,
        "metric": "Macro-F1 (NSL-KDD, 1% labeled)",
        "their_result": "94.60",
        "baseline_result": "80.99"
      },
      {
        "method_name": "SVM",
        "paper_reference": null,
        "metric": "Macro-F1 (NSL-KDD, 1% labeled)",
        "their_result": "94.60",
        "baseline_result": "82.19"
      },
      {
        "method_name": "Logistic Regression",
        "paper_reference": null,
        "metric": "Macro-F1 (NSL-KDD, 1% labeled)",
        "their_result": "94.60",
        "baseline_result": "81.84"
      },
      {
        "method_name": "VGG16",
        "paper_reference": null,
        "metric": "Macro-F1 (NSL-KDD, 1% labeled)",
        "their_result": "94.60",
        "baseline_result": "86.20"
      },
      {
        "method_name": "DNN",
        "paper_reference": null,
        "metric": "Macro-F1 (NSL-KDD, 1% labeled)",
        "their_result": "94.60",
        "baseline_result": "80.64"
      },
      {
        "method_name": "LSTM",
        "paper_reference": null,
        "metric": "Macro-F1 (NSL-KDD, 1% labeled)",
        "their_result": "94.60",
        "baseline_result": "84.52"
      },
      {
        "method_name": "FixMatch",
        "paper_reference": "[21]",
        "metric": "Macro-F1 (NSL-KDD, 1% labeled)",
        "their_result": "94.60",
        "baseline_result": "90.36"
      },
      {
        "method_name": "Semi-WTC",
        "paper_reference": "[15]",
        "metric": "Macro-F1 (NSL-KDD, 1% labeled)",
        "their_result": "94.60",
        "baseline_result": "91.84"
      },
      {
        "method_name": "Random Forest",
        "paper_reference": null,
        "metric": "Macro-F1 (CICIDS2017, 1% labeled)",
        "their_result": "93.94",
        "baseline_result": "72.56"
      },
      {
        "method_name": "SVM",
        "paper_reference": null,
        "metric": "Macro-F1 (CICIDS2017, 1% labeled)",
        "their_result": "93.94",
        "baseline_result": "75.27"
      },
      {
        "method_name": "Logistic Regression",
        "paper_reference": null,
        "metric": "Macro-F1 (CICIDS2017, 1% labeled)",
        "their_result": "93.94",
        "baseline_result": "73.01"
      },
      {
        "method_name": "VGG16",
        "paper_reference": null,
        "metric": "Macro-F1 (CICIDS2017, 1% labeled)",
        "their_result": "93.94",
        "baseline_result": "83.61"
      },
      {
        "method_name": "DNN",
        "paper_reference": null,
        "metric": "Macro-F1 (CICIDS2017, 1% labeled)",
        "their_result": "93.94",
        "baseline_result": "77.86"
      },
      {
        "method_name": "LSTM",
        "paper_reference": null,
        "metric": "Macro-F1 (CICIDS2017, 1% labeled)",
        "their_result": "93.94",
        "baseline_result": "78.91"
      },
      {
        "method_name": "FixMatch",
        "paper_reference": "[21]",
        "metric": "Macro-F1 (CICIDS2017, 1% labeled)",
        "their_result": "93.94",
        "baseline_result": "90.26"
      },
      {
        "method_name": "Semi-WTC",
        "paper_reference": "[15]",
        "metric": "Macro-F1 (CICIDS2017, 1% labeled)",
        "their_result": "93.94",
        "baseline_result": "91.46"
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "Precision",
      "Recall",
      "Macro-F1"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How to achieve fine-grained attack classification when labeled samples are extremely limited and classes are highly imbalanced?",
        "Can self-training with uncertainty-aware pseudo-label filtering and a hybrid loss improve fine-grained NIDS performance under long-tailed distributions?"
      ],
      "gaps_identified": [
        "Most prior works address either label scarcity or class imbalance but rarely both simultaneously in fine-grained NIDS, especially under semi-supervised settings.",
        "Existing imbalance-handling methods rely on labels and may overfit when labels are scarce.",
        "Long-tailed fine-grained traffic distributions have not been deeply explored in semi-supervised NIDS."
      ],
      "limitations": [
        "In experimental setup, very sparse categories after train/test split are merged to keep 11 categories per dataset.",
        "Hyperparameters (e.g., τ=0.05, α=0.95) were selected via multiple full-data trainings; sensitivity to these choices is not fully explored.",
        "Uncertainty estimation via MC-Dropout requires multiple stochastic forward passes (T not specified), which can be computationally costly."
      ],
      "future_work": [],
      "motivation": "Address co-occurring challenges of limited labeled data and long-tailed class imbalance in fine-grained NIDS using a semi-supervised framework that leverages unlabeled data while mitigating pseudo-label noise and bias.",
      "potential_research_ideas": [
        "Incorporate deep ensembles or evidential deep learning for more calibrated pseudo-label uncertainty compared to MC-Dropout.",
        "Adopt per-class adaptive thresholds for pseudo-label acceptance based on class-wise calibration or expected calibration error.",
        "Pretrain the backbone with self-supervised objectives (e.g., SimCLR/Barlow Twins for tabular/time-series) before semi-supervised fine-tuning.",
        "Explore graph-based representations (e.g., GNNs over host-flow graphs) to capture relational patterns among flows or hosts.",
        "Domain adaptation and cross-dataset generalization from NSL-KDD/CICIDS2017 to other enterprise datasets via unsupervised domain adaptation.",
        "Streaming/online semi-supervised learning with concept drift detection to handle evolving attack distributions.",
        "Integrate consistency regularization (e.g., MixUp/CutMix for tabular) with the current self-training pipeline.",
        "Calibrate the classifier with temperature scaling or focal losses to improve minority class decision boundaries.",
        "Federated/semi-supervised learning across organizations with privacy-preserving techniques.",
        "Out-of-distribution and open-set detection for unseen attack types integrated into the pseudo-labeling pipeline."
      ],
      "architectural_improvement_recommendations": [
        "Replace or augment MC-Dropout with deep ensembles for stronger uncertainty estimates in pseudo-label filtering.",
        "Introduce a class-aware dynamic thresholding mechanism for pseudo-label acceptance and resampling intensity.",
        "Add attention mechanisms or temporal convolutions to the RI-1DCNN to better capture long-range dependencies in flows.",
        "Use the 'effective number of samples' class weighting instead of log-based weights, and evaluate focal loss variants.",
        "Adopt a curriculum pseudo-labeling schedule that increases class difficulty and decreases uncertainty thresholds over epochs.",
        "Incorporate strong data augmentations tailored to tabular network features (feature masking, jittering, permutation) with consistency losses.",
        "Evaluate hybrid uncertainty (aleatoric + epistemic) and confidence calibration before adding pseudo-labels.",
        "Implement multi-task learning (e.g., joint coarse anomaly detection + fine-grained classification) to aid minority classes."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "PyTorch"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Experiments run on a server with NVIDIA GeForce RTX 3090 Ti GPU and Intel Xeon Gold 6230R 2.10GHz CPUs; MC-Dropout requires multiple stochastic forward passes; dataset split 80/20 with 1% labeled in training."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes SF-IDS, a semi-supervised fine-grained NIDS framework using self-training to jointly address limited labels and long-tailed class imbalance.",
      "Designs RI-1DCNN backbone that reconstructs samples into multichannel images and uses uncertainty-based pseudo-label filtering (probability + MC-Dropout).",
      "Introduces a hybrid loss combining supervised contrastive loss and multi-weighted classification loss to obtain compact intra-class features and clear inter-class boundaries.",
      "Demonstrates effectiveness on NSL-KDD and CICIDS2017 with only 1% labeled data, reporting Macro-F1 improvements (e.g., “3.01% and 2.71% Marco-F1 improvement on two classical datasets with 1% labeled, respectively.”)."
    ]
  },
  {
    "arxiv_id": "2308.05353v1",
    "title": "Preemptive Detection of Fake Accounts on Social Networks via Multi-Class Preferential Attachment Classifiers",
    "authors": "Adam Breuer; Nazanin Khosravani; Michael Tingley; Bradford Cottel",
    "abstract": "In this paper, we describe a new algorithm called Preferential Attachment k-class Classifier (PreAttacK) for detecting fake accounts in a social network. Recently, several algorithms have obtained high accuracy on this problem. However, they have done so by relying on information about fake accounts' friendships or the content they share with others--the very things we seek to prevent. PreAttacK represents a significant departure from these approaches. We provide some of the first detailed distributional analyses of how new fake (and real) accounts first attempt to request friends after joining a major network (Facebook). We show that even before a new account has made friends or shared content, these initial friend request behaviors evoke a natural multi-class extension of the canonical Preferential Attachment model of social network growth. We use this model to derive a new algorithm, PreAttacK. We prove that in relevant problem instances, PreAttacK near-optimally approximates the posterior probability that a new account is fake under this multi-class Preferential Attachment model of new accounts' (not-yet-answered) friend requests. These are the first provable guarantees for fake account detection that apply to new users, and that do not require strong homophily assumptions. This principled approach also makes PreAttacK the only algorithm with provable guarantees that obtains state-of-the-art performance on new users on the global Facebook network, where it converges to AUC=0.9 after new users send + receive a total of just 20 not-yet-answered friend requests. For comparison, state-of-the-art benchmarks do not obtain this AUC even after observing additional data on new users' first 100 friend requests. Thus, unlike mainstream algorithms, PreAttacK converges before the median new fake account has made a single friendship (accepted friend request) with a human.",
    "published_date": "2023-08-10",
    "pdf_link": "https://arxiv.org/pdf/2308.05353v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Social Network Security",
      "subdomain": "Sybil/Fake Account Detection",
      "specific_problem": "Preemptive detection of fake accounts before any friendships/content using not-yet-answered friend request behavior",
      "attack_types": [
        "sybils",
        "fake accounts",
        "fake news bots (context)",
        "sockpuppets (context)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Probabilistic generative model",
        "specific": "Multi-class Directed Preferential Attachment (kCDPA) with Bayesian posterior classifier (PreAttacK)",
        "novel_contribution": "Derives a near-optimal posterior approximation for a new user being fake under a k-class preferential attachment model using sequences of not-yet-answered friend requests (both sent and received). Provides instance-specific bounds and a scalable algorithm."
      },
      {
        "type": "baseline",
        "category": "Graph label propagation / Random Walk",
        "specific": "Homophily-based Random Walk propagation",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Belief Propagation",
        "specific": "Homophily-based BP methods",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Graph/Temporal Embeddings",
        "specific": "DEC",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Temporal Graph Representation Learning",
        "specific": "Jodie",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Graph Embedding-based Classifier",
        "specific": "Ties",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Probabilistic generative modeling",
      "Semi-supervised"
    ],
    "datasets": [
      {
        "name": "Global Facebook friend request logs for new users (not-yet-answered requests)",
        "type": "proprietary",
        "domain": "social_network_friend_requests",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Network-structural algorithms (homophily-based Random Walk/BP)",
        "paper_reference": "[9,10,11,15,19–21,43–45,49,52,53] as cited in paper",
        "metric": "AUC",
        "their_result": "\"converges to informative classifications (AUC ≈ 0.9) after ... just 20 not-yet-answered friend requests\"",
        "baseline_result": "\"do not obtain this performance even after observing additional data on new users’ first 100 friend requests\"; on new users, \"perform poorly (AUC < 0.6)\" (Facebook evaluations)"
      },
      {
        "method_name": "DEC",
        "paper_reference": "[47] as cited",
        "metric": "AUC (on long-tenured users)",
        "their_result": "PreAttacK AUC ≈ 0.9 after 20 requests on new users",
        "baseline_result": "DEC is cited as high AUC (>0.98) on long-tenured users but not applicable/insufficient for early detection; no numeric for new users given"
      },
      {
        "method_name": "Jodie",
        "paper_reference": "[25] as cited",
        "metric": "AUC",
        "their_result": "PreAttacK AUC ≈ 0.9 after 20 requests on new users",
        "baseline_result": "Not reported numerically for early detection; relies on features evolving after friendships/content"
      },
      {
        "method_name": "Ties",
        "paper_reference": "[30] as cited",
        "metric": "AUC",
        "their_result": "PreAttacK AUC ≈ 0.9 after 20 requests on new users",
        "baseline_result": "Not reported numerically for early detection; relies on network/content features"
      }
    ],
    "performance_metrics_used": [
      "AUC (Area Under ROC)",
      "number of not-yet-answered friend requests needed to reach target AUC (convergence speed)"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can we leverage a small number of not-yet-answered friend requests to distinguish new fake accounts from new real users before any friendships/content?",
        "Can a multi-class preferential attachment model explain and enable early detection of fake accounts?",
        "Can we derive a classifier with provable guarantees for new user detection without strong homophily assumptions?"
      ],
      "gaps_identified": [
        "Early detection paradox: mainstream algorithms use friendships/content which we seek to prevent.",
        "Network-structural methods rely on homophily and perform poorly on new users; evaluations often exclude low-tenure accounts.",
        "Feature-based methods require many features that are unknown until after a user has made several friends/shared content.",
        "Lack of provable guarantees for fake account detection for new users."
      ],
      "limitations": [
        "Model assumes the count of requests each new user sends/receives is independent of their label.",
        "kCDPA models friend requests (not accepted friendships); accepted-request network is not modeled.",
        "Baseline performance numbers for early detection are referenced qualitatively; limited direct numeric head-to-heads shown in the provided text.",
        "Excludes requests among new accounts to prevent adversarial manipulation, which may omit some real behaviors.",
        "Requires a set of preexisting users with known fake/real labels to estimate class-conditional request propensities."
      ],
      "future_work": [
        "Extend to k>2 classes (multiple fake types) and other latent labels (e.g., political affiliation) to address cold-start problems.",
        "Apply and validate on other platforms (Twitter, Instagram, LinkedIn) and other directed interaction types (follows, connects).",
        "Incorporate observed homophily to accelerate convergence (outlined in Section 5).",
        "Explore integration with content- or behavior-based features once available for joint early-to-late lifecycle detection."
      ],
      "motivation": "Overcome the early detection paradox by detecting fake accounts before they make friends or share content, using a principled model of initial friend request behavior with provable guarantees and scalable performance.",
      "potential_research_ideas": [
        "Calibrate and adapt the prior and class-conditional request propensities over time to handle drift and adversarial adaptation (online Bayesian updating).",
        "Combine PreAttacK’s log-likelihood ratios with lightweight behavior/metadata signals in a stacked ensemble for improved early accuracy.",
        "Generalize kCDPA with temporal decay or time-inhomogeneous attachment to capture rapidly shifting targeting patterns.",
        "Develop adversarially robust variants that anticipate strategic re-targeting by fakes (game-theoretic or robust Bayesian modeling).",
        "Incorporate hierarchical/Bayesian smoothing across regions/languages to handle sparse locales with few labeled recipients.",
        "Extend to joint modeling of acceptance probabilities (survival/hazard models) to predict both fakery and time-to-first-friendship.",
        "Design differentially private estimators for class-conditional request rates to enable privacy-preserving deployment and sharing."
      ],
      "architectural_improvement_recommendations": [
        "Maintain streaming counters of per-recipient and per-sender class-conditional request rates with hierarchical smoothing for low-count nodes.",
        "Use temporal decay kernels to emphasize recent request behaviors and adapt to drift.",
        "Calibrate output posteriors via isotonic regression or Platt scaling using a small labeled validation stream.",
        "Augment with a simple logistic regression on PreAttacK features plus basic metadata (e.g., device/IP novelty) for incremental gains.",
        "Implement cache/shard strategies for high-degree nodes to keep lookups O(1) at billion-scale.",
        "Add a controlled component for new-to-new requests with robust constraints to reduce adversarial exploitation while gaining signal."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": "Designed to be computationally efficient on mature networks with billions of users; lower computational cost than Random Walk/BP or deep embedding approaches. No specific hardware/training time reported."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Global Facebook production environment (enterprise-scale social network)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Dependence on availability and quality of seed labels for existing users.",
        "Potential distribution shift as adversaries adapt targeting strategies.",
        "Handling sparse regions or new markets with few historical requests.",
        "Avoiding adversarial manipulation via coordinated new-to-new requests (addressed by model design)."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces the k-Class Directed Preferential Attachment (kCDPA) model for initial friend request dynamics of new users.",
      "Derives PreAttacK, a classifier that near-optimally approximates the posterior probability a new user is fake under kCDPA.",
      "Provides the first provable guarantees for fake account detection applicable to new users without strong homophily assumptions.",
      "Presents detailed distributional analyses of how new fake/real accounts target initial friend requests on Facebook.",
      "Demonstrates state-of-the-art early detection at scale on Facebook: \"AUC ≈ 0.9\" after 20 not-yet-answered requests, outperforming network-based baselines even after 100 requests.",
      "Shows general applicability to other platforms and to other latent labels beyond fake/real."
    ]
  },
  {
    "arxiv_id": "2308.14434v1",
    "title": "Using ChatGPT as a Static Application Security Testing Tool",
    "authors": "Atieh Bakhshandeh; Abdalsamad Keramatfar; Amir Norouzi; Mohammad Mahdi Chekidehkhoun",
    "abstract": "In recent years, artificial intelligence has had a conspicuous growth in almost every aspect of life. One of the most applicable areas is security code review, in which a lot of AI-based tools and approaches have been proposed. Recently, ChatGPT has caught a huge amount of attention with its remarkable performance in following instructions and providing a detailed response. Regarding the similarities between natural language and code, in this paper, we study the feasibility of using ChatGPT for vulnerability detection in Python source code. Toward this goal, we feed an appropriate prompt along with vulnerable data to ChatGPT and compare its results on two datasets with the results of three widely used Static Application Security Testing tools (Bandit, Semgrep and SonarQube). We implement different kinds of experiments with ChatGPT and the results indicate that ChatGPT reduces the false positive and false negative rates and has the potential to be used for Python source code vulnerability detection.",
    "published_date": "2023-08-28",
    "pdf_link": "https://arxiv.org/pdf/2308.14434v1",
    "paper_types": [
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Application Security",
      "subdomain": "Static Application Security Testing (SAST)",
      "specific_problem": "Vulnerability detection and CWE classification with line-level localization in Python source code using an LLM (ChatGPT) and comparison against rule-based SAST tools",
      "attack_types": [
        "CWE-78 OS Command Injection",
        "Multiple CWE classes (75 types in SecurityEval dataset)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "LLM/Transformer",
        "specific": "ChatGPT (GPT-3.5-turbo)",
        "novel_contribution": "Prompt-based use of GPT-3.5 as a SAST assistant and classifier for CWE labeling and line-level localization; four experimental prompting setups including per-file candidate CWE verification to assist SAST tools"
      }
    ],
    "learning_paradigm": [
      "Zero-shot",
      "Prompt-based"
    ],
    "datasets": [
      {
        "name": "SecurityEval (Python subset)",
        "type": "public",
        "domain": "source_code_python",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "PyT vulnerable Python files (26 files)",
        "type": "public",
        "domain": "source_code_python",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Semgrep",
        "paper_reference": null,
        "metric": "Experiment 1 (Binary classification) - F1",
        "their_result": "GPT-3.5: 0.1475",
        "baseline_result": "Semgrep: 0.2457"
      },
      {
        "method_name": "Bandit",
        "paper_reference": null,
        "metric": "Experiment 1 (Binary classification) - F1",
        "their_result": "GPT-3.5: 0.1475",
        "baseline_result": "Bandit: 0.2424"
      },
      {
        "method_name": "SonarQube",
        "paper_reference": null,
        "metric": "Experiment 1 (Binary classification) - F1",
        "their_result": "GPT-3.5: 0.1475",
        "baseline_result": "SonarQube: 0.2060"
      },
      {
        "method_name": "Semgrep",
        "paper_reference": null,
        "metric": "Experiment 2 (Selecting from a global list) - F1",
        "their_result": "GPT-3.5: 0.1044",
        "baseline_result": "Semgrep: 0.1812"
      },
      {
        "method_name": "Bandit",
        "paper_reference": null,
        "metric": "Experiment 2 (Selecting from a global list) - F1",
        "their_result": "GPT-3.5: 0.1044",
        "baseline_result": "Bandit: 0.1022"
      },
      {
        "method_name": "SonarQube",
        "paper_reference": null,
        "metric": "Experiment 2 (Selecting from a global list) - F1",
        "their_result": "GPT-3.5: 0.1044",
        "baseline_result": "SonarQube: 0.0743"
      },
      {
        "method_name": "Semgrep",
        "paper_reference": null,
        "metric": "Experiment 3 (SAST assistant, Case 1 no new labels) - F1",
        "their_result": "GPT-3.5: 0.4101",
        "baseline_result": "Semgrep: 0.1812"
      },
      {
        "method_name": "Bandit",
        "paper_reference": null,
        "metric": "Experiment 3 (SAST assistant, Case 1 no new labels) - F1",
        "their_result": "GPT-3.5: 0.4101",
        "baseline_result": "Bandit: 0.1022"
      },
      {
        "method_name": "SonarQube",
        "paper_reference": null,
        "metric": "Experiment 3 (SAST assistant, Case 1 no new labels) - F1",
        "their_result": "GPT-3.5: 0.4101",
        "baseline_result": "SonarQube: 0.0743"
      },
      {
        "method_name": "Semgrep",
        "paper_reference": null,
        "metric": "Experiment 4 (Free classification) - F1",
        "their_result": "GPT-3.5: 0.1808",
        "baseline_result": "Semgrep: 0.1812"
      },
      {
        "method_name": "Bandit",
        "paper_reference": null,
        "metric": "Experiment 4 (Free classification) - F1",
        "their_result": "GPT-3.5: 0.1808",
        "baseline_result": "Bandit: 0.1022"
      },
      {
        "method_name": "SonarQube",
        "paper_reference": null,
        "metric": "Experiment 4 (Free classification) - F1",
        "their_result": "GPT-3.5: 0.1808",
        "baseline_result": "SonarQube: 0.0743"
      },
      {
        "method_name": "Semgrep",
        "paper_reference": null,
        "metric": "Experiment 3 (SAST assistant, Case 1) - Precision/Recall",
        "their_result": "Precision 0.7807 / Recall 0.2781",
        "baseline_result": "Precision 0.4682 / Recall 0.1123"
      },
      {
        "method_name": "Bandit",
        "paper_reference": null,
        "metric": "Experiment 3 (SAST assistant, Case 1) - Precision/Recall",
        "their_result": "Precision 0.7807 / Recall 0.2781",
        "baseline_result": "Precision 0.3168 / Recall 0.0609"
      },
      {
        "method_name": "SonarQube",
        "paper_reference": null,
        "metric": "Experiment 3 (SAST assistant, Case 1) - Precision/Recall",
        "their_result": "Precision 0.7807 / Recall 0.2781",
        "baseline_result": "Precision 0.3283 / Recall 0.0419"
      }
    ],
    "performance_metrics_used": [
      "Precision",
      "Recall",
      "F1-score"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Is it feasible to use ChatGPT (GPT-3.5) for vulnerability detection in Python source code?",
        "Does ChatGPT outperform traditional SAST tools (Bandit, Semgrep, SonarQube) for Python vulnerability detection?",
        "Can ChatGPT effectively assist SAST tools by verifying or filtering their reported CWEs at the file level?"
      ],
      "gaps_identified": [
        "High false positive and false negative rates in rule-based SAST tools.",
        "Limited academic evaluation of ChatGPT for Python vulnerability detection and direct comparison to SAST tools (prior work mostly on other languages; one Java study reported no better than a dummy classifier).",
        "Existing datasets often lack precise line-level labels for vulnerabilities."
      ],
      "limitations": [
        "Strong sensitivity to prompt wording and even to the order of provided CWEs.",
        "Relatively small dataset (156 files) and potential dataset distribution biases across CWEs.",
        "Evaluation limited to three SAST tools and one LLM version (GPT-3.5-turbo).",
        "Potential influence of public availability of code samples on the Internet.",
        "Only Python language considered."
      ],
      "future_work": [
        "Evaluate newer and stronger LLMs (e.g., GPT-4) on the same tasks.",
        "Expand to additional SAST tools, programming languages, and larger, more balanced datasets.",
        "Refine prompt engineering strategies and structured outputs for better stability.",
        "Provide or curate datasets with precise ground-truth line-level labels across more CWE classes."
      ],
      "motivation": "Investigate the feasibility of using ChatGPT for Python source code vulnerability detection and compare its performance with widely used SAST tools, aiming to reduce false positives and false negatives.",
      "potential_research_ideas": [
        "Integrate an LLM verifier to post-process SAST findings using CWE-aware retrieval (descriptions, examples) to reduce false positives.",
        "Develop a multi-stage pipeline combining AST/data-flow slicing with LLM reasoning for fine-grained line-level CWE localization.",
        "Fine-tune or instruct-tune code-focused LLMs on curated vulnerability-labeled Python corpora to improve recall while controlling hallucinations.",
        "Introduce self-consistency or multi-agent cross-checking among prompts to stabilize predictions without revealing chain-of-thought.",
        "Build a comprehensive, balanced Python vulnerability benchmark with exact line-level annotations across many CWE classes.",
        "Explore hybrid symbolic-LLM approaches where taint analysis results are fed to the LLM for contextual CWE assignment."
      ],
      "architectural_improvement_recommendations": [
        "Augment prompts with retrieved CWE definitions and code examples (RAG) to ground CWE predictions.",
        "Use few-shot exemplars with structured JSON schemas and strict output validation to reduce format and label drift.",
        "Incorporate AST and simple static analysis (e.g., taint flows, call graphs) into the context window to guide the LLM.",
        "Adopt iterative verification: initial prediction, justification-free verification pass, and consistency check before final JSON output.",
        "Calibrate thresholds using confidence proxies (e.g., logit bias, self-consistency agreement) to trade precision/recall per CWE."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/abakhshandeh/ChatGPTasSAST.git",
      "frameworks": [
        "OpenAI GPT-3.5-turbo API",
        "Semgrep",
        "Bandit",
        "SonarQube"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "API-based inference only (temperature=0); no training; modest compute requirements."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Prompt sensitivity and instability across small wording changes or label order.",
        "Balancing precision and recall across diverse CWE types.",
        "Need for robust structured output validation when integrating with pipelines."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "First empirical study to compare ChatGPT (GPT-3.5) with traditional SAST tools for Python vulnerability detection.",
      "Design of four prompting experiments, including a SAST-assistant setup where per-file candidate CWEs are verified by ChatGPT.",
      "Evidence that ChatGPT as a SAST assistant (Experiment 3 Case 1) substantially improves F1 over Semgrep, Bandit, and SonarQube on the evaluated dataset.",
      "Release of prompts and code to reproduce experiments."
    ]
  },
  {
    "arxiv_id": "2310.10657v1",
    "title": "Managing Networked IoT Assets Using Practical and Scalable Traffic Inference",
    "authors": "Arman Pashamokhtari",
    "abstract": "The Internet has recently witnessed unprecedented growth of a class of connected assets called the Internet of Things (IoT). Due to relatively immature manufacturing processes and limited computing resources, IoTs have inadequate device-level security measures, exposing the Internet to various cyber risks. Prior research leveraged predictable patterns in IoT network traffic to develop inference models. However, they fall short of expectations in addressing practical challenges, preventing them from being deployed in production settings. This thesis identifies four practical challenges and develops techniques to address them which can help secure businesses and protect user privacy against growing cyber threats.   My first contribution balances prediction gains against computing costs of traffic features for IoT traffic classification and monitoring. My second contribution addresses the challenges of measurement costs and data quality. I develop an inference method that uses stochastic and deterministic modeling to predict IoT devices in home networks from opaque and coarse-grained IPFIX flow data. Evaluations show that false positive rates can be reduced by 75% compared to related work without significantly affecting true positives. My third contribution focuses on the challenge of concept drifts by analyzing over six million flow records collected from 12 real home networks. Finally, my fourth contribution studies the resilience of machine learning models against adversarial attacks with a specific focus on decision tree-based models.",
    "published_date": "2023-09-07",
    "pdf_link": "https://arxiv.org/pdf/2310.10657v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "IoT Security",
      "subdomain": "Asset Identification and Behavior Monitoring",
      "specific_problem": "Network traffic-based inference to identify IoT devices and progressively monitor their behavior at scale under practical constraints (opaque IPFIX flows behind NAT, measurement cost, data quality, concept drift) and to assess and improve adversarial robustness of decision tree-based models",
      "attack_types": [
        "SYN reflection (volumetric)",
        "SSDP reflection (volumetric)",
        "DDoS/DoS (volumetric)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Ensemble/Meta-learning",
        "specific": "Multi-view classification with specialized-view classifiers and optimal view selection",
        "novel_contribution": "Cost-aware selection of a minimal set of specialized models that preserves accuracy (~99%) while reducing computational cost by 6x; conflict resolution across views"
      },
      {
        "type": "primary",
        "category": "One-Class/Anomaly Detection",
        "specific": null,
        "novel_contribution": "Hierarchy of one-class models per asset class to progressively monitor IoT traffic at increasing granularities"
      },
      {
        "type": "primary",
        "category": "Geometric boundary modeling",
        "specific": "Refined Convex Hull",
        "novel_contribution": "Refined convex hull boundaries for progressive behavior monitoring outperforming spherical boundaries for benign/attack separation in feature space"
      },
      {
        "type": "primary",
        "category": "Decision Tree Ensemble",
        "specific": "Random Forest",
        "novel_contribution": "Used for stochastic inference of IoT device types from opaque IPFIX flow data with class-specific confidence thresholds and trust metrics"
      },
      {
        "type": "primary",
        "category": "Deterministic rule-based",
        "specific": "Cloud service fingerprinting",
        "novel_contribution": "Deterministic identification using fingerprints of IoT cloud services (specific and weighted fingerprint-set methods) and combination with stochastic model"
      },
      {
        "type": "primary",
        "category": "Adversarial ML (model auditing and patching)",
        "specific": "AdIoTack for DT ensembles",
        "novel_contribution": "Algorithms to quantify vulnerability of decision tree-based models and refine vulnerable trees; patching yields robustness against 92% of adversarial attacks"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "IPFIX records from residential ISP networks (26 IoT devices)",
        "type": "private",
        "domain": "network_flows",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Residential ISP IPFIX dataset from 12 real home networks (>6 million flow records)",
        "type": "private",
        "domain": "network_flows",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "TEST1 dataset (benign testing dataset and attack traces for progressive monitoring)",
        "type": "private",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Related work on IoT inference from IPFIX (opaque flows)",
        "paper_reference": null,
        "metric": "False Positive Rate (FPR)",
        "their_result": "\"false positive rates can be reduced by 75% compared to related work without significantly affecting true positives\"",
        "baseline_result": null
      },
      {
        "method_name": "All specialized views vs optimal view set (multi-view classification)",
        "paper_reference": null,
        "metric": "Accuracy and computational cost",
        "their_result": "\"find the best set of specialized models for multi-view classification that can reach an average accuracy of 99% ... but reducing the cost by a factor of 6\"",
        "baseline_result": "Using all models yields similar accuracy but 6x higher cost"
      },
      {
        "method_name": "Stochastic vs Deterministic vs Combined (IPFIX inference)",
        "paper_reference": null,
        "metric": "Accuracy/F1/TP/FP (daily trust/TP traces; Table 4.6 comparison)",
        "their_result": "Combined stochastic+deterministic outperforms either alone; improved TP and reduced FP (see Table 4.6); Figure 4.18 shows performance gains",
        "baseline_result": "Stochastic-only and deterministic-only models perform worse than combined"
      },
      {
        "method_name": "Static model vs Dynamic contextual model selection under concept drift",
        "paper_reference": null,
        "metric": "Daily accuracy/F1 across homes",
        "their_result": "Dynamic selection more often achieves higher accuracy than static models across testing days (Table 5.2); distance-based selection approximates the ideal model (Tables 5.4–5.5)",
        "baseline_result": "Static global or per-home fixed models degrade under temporal/spatial drift"
      },
      {
        "method_name": "Pre-patching vs Post-patching (AdIoTack resilience)",
        "paper_reference": null,
        "metric": "Adversarial attack success/detection rate",
        "their_result": "\"refine vulnerable decision trees, making them robust against 92% of adversarial attacks\"; higher detection for SYN/SSDP reflection after patching (Fig. 6.15–6.16)",
        "baseline_result": "Original models vulnerable to many generated adversarial recipes (Tables/Figs in Ch. 6)"
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "F1-score",
      "False Positive Rate (FPR)",
      "True Positive Rate (TPR)/Detection Rate",
      "Trust (custom daily trust metric)",
      "Hit-time",
      "Hit-flow",
      "Computational cost"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How to balance prediction gains against computing costs of traffic features for IoT traffic classification and monitoring?",
        "How to infer IoT devices from opaque, coarse-grained IPFIX flow data collected post-NAT with limited visibility and data quality?",
        "How do concept drifts (temporal and spatial) impact IoT traffic inference, and what scalable strategies can mitigate their effect when labeled data is unavailable at test time?",
        "How resilient are decision tree-based inference models to adversarial volumetric attacks, and how can their robustness be quantified and improved?"
      ],
      "gaps_identified": [
        "Existing IoT traffic inference solutions overlook practical deployment challenges such as feature computation cost and scalability.",
        "Measurement constraints in many networks yield opaque, coarse-grained IPFIX data (post-NAT), limiting visibility compared to packet/PCAP approaches used in prior work.",
        "Concept drifts across time and across homes lead to model performance decay; many works assume stationary behavior or availability of labeled test data.",
        "Resilience of commonly used decision tree-based models against adversarial attacks is underexplored in IoT network inference.",
        "Limited methods to confidently accept/reject predictions and maintain trust over time in production settings."
      ],
      "limitations": [
        "Focus on decision tree-based models for adversarial analysis; results may not generalize to other model families.",
        "Adversarial study targets volumetric reflection attacks (e.g., SYN, SSDP), not a broader range of attack tactics.",
        "IPFIX-based inference is evaluated on residential ISP data (12 homes); generalization to enterprise/industrial settings is not empirically demonstrated in this thesis.",
        "Some datasets (e.g., IPFIX traces) are private; external reproducibility may be limited."
      ],
      "future_work": [
        "Extend adversarial robustness analysis and patching beyond decision tree ensembles to other model families.",
        "Broaden deployment contexts beyond residential ISPs and validate in enterprise/industrial IoT environments.",
        "Automate and refine dynamic model selection under concept drift with online adaptation.",
        "Enhance deterministic fingerprints for cloud services and maintain them over time as services evolve."
      ],
      "motivation": "IoT devices are widespread yet under-protected at the device level; practical and scalable network-level inference is needed to discover and monitor IoT assets under real-world constraints (limited visibility, cost, drift, adversarial threats).",
      "potential_research_ideas": [
        "Develop online, label-efficient drift detection and adaptation with active learning to request minimal labels only when uncertainty/trust drops.",
        "Design federated or privacy-preserving collaborative learning across homes/ISPs to improve generalization without sharing raw traffic.",
        "Explore self-supervised pretraining on unlabeled flows to create robust embeddings transferable across contexts and ISPs.",
        "Construct a continuously updated IoT cloud service knowledge graph to improve deterministic fingerprinting and handle service evolution.",
        "Adversarial training and certified robustness methods adapted to decision tree ensembles for network traffic features.",
        "Create synthetic IPFIX flow generators (e.g., conditional generative models) to augment rare-device classes and stress-test drift/robustness.",
        "Meta-learning approaches to quickly adapt model parameters to unseen homes/devices with few-shot updates.",
        "Explainability-driven monitoring (e.g., SHAP for trees) to identify feature shifts and aid root-cause analysis during drift."
      ],
      "architectural_improvement_recommendations": [
        "Introduce cost-aware, streaming feature computation with early-exit cascades conditioned on confidence to further reduce inference cost.",
        "Calibrate ensemble probabilities (e.g., temperature/Platt calibration for RF) to improve confidence thresholding and trust metrics.",
        "Incorporate online drift detectors (e.g., ADWIN/EDDM) to trigger dynamic model selection and incremental retraining.",
        "Use monotonic constraints or bounded-feature regularization in tree learning to reduce adversarially exploitable regions before patching.",
        "Hybrid pipeline with joint stochastic-deterministic inference orchestrated by a learned gate that adapts weights per device/context.",
        "Maintain a versioned fingerprint database with automatic discovery of new services via DNS/TLS SNI and periodic verification."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Residential ISP networks (post-NAT IPFIX collection across home networks); SDN-based campus/lab for progressive monitoring prototype",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Opaque, coarse-grained IPFIX visibility post-NAT limiting feature richness",
        "Measurement and computation cost constraints for feature extraction and model execution",
        "Data quality variability across homes and time",
        "Concept drift (temporal and spatial) causing model decay",
        "Adversarial manipulation of traffic features against tree-based models"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": true,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Cost-aware multi-view classification achieving \"an average accuracy of 99%\" with cost reduced \"by a factor of 6\" via optimal view selection.",
      "Progressive monitoring using a hierarchy of one-class models per asset class and refined convex hull boundaries.",
      "Inference from opaque IPFIX flows combining stochastic (Random Forest with confidence/trust) and deterministic cloud service fingerprinting, reducing false positives by \"75% compared to related work\".",
      "Comprehensive concept drift study over \"> six million flow records collected from 12 real home networks\" and dynamic/contextual model selection without labeled test data.",
      "AdIoTack framework to quantify and improve resilience of decision tree-based models against adversarial volumetric attacks, with patching that makes models \"robust against 92% of adversarial attacks\"."
    ]
  },
  {
    "arxiv_id": "2308.00943v1",
    "title": "IIDS: Design of Intelligent Intrusion Detection System for Internet-of-Things Applications",
    "authors": "KG Raghavendra Narayan; Srijanee Mookherji; Vanga Odelu; Rajendra Prasath; Anish Chand Turlapaty; Ashok Kumar Das",
    "abstract": "With rapid technological growth, security attacks are drastically increasing. In many crucial Internet-of-Things (IoT) applications such as healthcare and defense, the early detection of security attacks plays a significant role in protecting huge resources. An intrusion detection system is used to address this problem. The signature-based approaches fail to detect zero-day attacks. So anomaly-based detection particularly AI tools, are becoming popular. In addition, the imbalanced dataset leads to biased results. In Machine Learning (ML) models, F1 score is an important metric to measure the accuracy of class-level correct predictions. The model may fail to detect the target samples if the F1 is considerably low. It will lead to unrecoverable consequences in sensitive applications such as healthcare and defense. So, any improvement in the F1 score has significant impact on the resource protection. In this paper, we present a framework for ML-based intrusion detection system for an imbalanced dataset. In this study, the most recent dataset, namely CICIoT2023 is considered. The random forest (RF) algorithm is used in the proposed framework. The proposed approach improves 3.72%, 3.75% and 4.69% in precision, recall and F1 score, respectively, with the existing method. Additionally, for unsaturated classes (i.e., classes with F1 score < 0.99), F1 score improved significantly by 7.9%. As a result, the proposed approach is more suitable for IoT security applications for efficient detection of intrusion and is useful in further studies.",
    "published_date": "2023-08-02",
    "pdf_link": "https://arxiv.org/pdf/2308.00943v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "IoT Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Machine-learning-based intrusion detection on highly imbalanced IoT network traffic; improving per-class F1 (unsaturated classes) on CICIoT2023",
      "attack_types": [
        "DoS",
        "DDoS",
        "Web-based",
        "Reconnaissance",
        "Spoofing",
        "Mirai",
        "Brute Force"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Ensemble",
        "specific": "Random Forest",
        "novel_contribution": "Used as the core classifier across all frameworks; best performance achieved when combined with CFS feature selection and BRFC class balancing"
      },
      {
        "type": "primary",
        "category": "Feature Selection",
        "specific": "CfsSubsetEval (CFS) via Weka",
        "novel_contribution": "Selects 6 features from 46; when combined with BRFC yields the best overall improvements"
      },
      {
        "type": "primary",
        "category": "Feature Selection",
        "specific": "Intersection of RF-RFE and mRMR",
        "novel_contribution": "Intersection of top-25 RF-RFE and top-25 mRMR features (11 features) evaluated as an alternative feature subset"
      },
      {
        "type": "primary",
        "category": "Resampling",
        "specific": "Random Oversampling (ROS)",
        "novel_contribution": "Applied on training set to address class imbalance; alone gives minor improvements"
      },
      {
        "type": "primary",
        "category": "Ensemble",
        "specific": "Balanced Random Forest Classifier (BRFC)",
        "novel_contribution": "Class-balanced bootstrapping within RF to improve minority-class performance; paired with CFS provided the best gains"
      },
      {
        "type": "baseline",
        "category": "Preprocessing",
        "specific": "Standardization (unit variance)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Imbalanced Learning"
    ],
    "datasets": [
      {
        "name": "CICIoT2023",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "BoT-IoT",
        "type": "",
        "domain": "",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      },
      {
        "name": "ToN-IoT",
        "type": "",
        "domain": "",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      },
      {
        "name": "IoT-23",
        "type": "",
        "domain": "",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      },
      {
        "name": "CICIoT2022",
        "type": "",
        "domain": "",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      },
      {
        "name": "DS2OS",
        "type": "",
        "domain": "",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      },
      {
        "name": "Contiki",
        "type": "",
        "domain": "",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      }
    ],
    "baselines": [
      {
        "method_name": "FW4: CFS + BRFC vs FW1: Base (34 classes) - F1score",
        "paper_reference": null,
        "metric": "F1score",
        "their_result": "0.7603",
        "baseline_result": "0.7134"
      },
      {
        "method_name": "FW4: CFS + BRFC vs FW1: Base (34 classes) - Precision",
        "paper_reference": null,
        "metric": "Precision",
        "their_result": "0.7423",
        "baseline_result": "0.7051"
      },
      {
        "method_name": "FW4: CFS + BRFC vs FW1: Base (34 classes) - Recall",
        "paper_reference": null,
        "metric": "Recall",
        "their_result": "0.8241",
        "baseline_result": "0.7866"
      },
      {
        "method_name": "Unsaturated class: DictionaryBruteForce (F1score)",
        "paper_reference": null,
        "metric": "F1score",
        "their_result": "0.4831 (FW4: CFS+BRFC)",
        "baseline_result": "0.056 (FW1: Base)"
      },
      {
        "method_name": "Unsaturated class: BrowserHijacking (F1score)",
        "paper_reference": null,
        "metric": "F1score",
        "their_result": "0.4006 (FW4: CFS+BRFC)",
        "baseline_result": "0.1362 (FW1: Base)"
      },
      {
        "method_name": "Unsaturated class: SqlInjection (F1score)",
        "paper_reference": null,
        "metric": "F1score",
        "their_result": "0.176 (FW4: CFS+BRFC)",
        "baseline_result": "0.003 (FW1: Base)"
      }
    ],
    "performance_metrics_used": [
      "Precision",
      "Recall",
      "F1score",
      "Accuracy",
      "Cohen Kappa score"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "IoT IDS datasets are highly imbalanced, leading to biased results and poor minority-class detection",
        "Many prior IoT datasets lack extensive real IoT device topology; CICIoT2023 includes 105 devices and more diverse attacks",
        "Signature-based IDS approaches fail to detect zero-day attacks; anomaly/ML-based approaches needed",
        "Class balancing alone may not provide significant improvement without feature selection"
      ],
      "limitations": [
        "Performance may not improve for saturated classes (F1score >= 0.99)",
        "Balancing methods alone (e.g., ROS/BRFC without feature selection) yielded only minor gains"
      ],
      "future_work": [
        "Extend study on feature selection and class balancing techniques and how they influence ML-based models for imbalanced datasets, particularly in intrusion detection systems for IoT applications"
      ],
      "motivation": "Improve early detection of IoT security attacks, especially zero-day, under severe class imbalance where low per-class F1 can cause critical failures in sensitive applications (healthcare, defense).",
      "potential_research_ideas": [
        "Evaluate the proposed pipeline across multiple IoT IDS datasets (e.g., IoT-23, BoT-IoT, ToN-IoT) to assess cross-dataset generalization and domain shift",
        "Investigate cost-sensitive learning and calibrated per-class decision thresholds to further raise F1 on extreme minority classes",
        "Explore advanced resampling (SMOTE variants, SMOTE-NC, ADASYN, hybrid over/under-sampling) and compare against BRFC",
        "Replace/augment RF with gradient boosting (XGBoost/LightGBM/CatBoost) and deep models (TabNet, MLP-Mixer) for tabular IoT traffic",
        "Hierarchical classification (34 -> 8 -> 2) with hierarchical losses to exploit label structure",
        "Online/streaming learning with concept drift detection for evolving IoT traffic",
        "Federated or privacy-preserving IDS across distributed IoT gateways",
        "Model explainability for SOC analysts (SHAP/TreeExplainer) to interpret alerts and feature importance stability",
        "Adversarial robustness evaluation against evasion/poisoning on tabular features",
        "Automated feature engineering and selection stability analysis across folds and time windows"
      ],
      "architectural_improvement_recommendations": [
        "Perform rigorous hyperparameter optimization for RF/BRFC (n_estimators, max_depth, class_weight) with nested cross-validation",
        "Adopt cost-sensitive RF or class_weighted losses to complement BRFC",
        "Introduce advanced resampling (Borderline-SMOTE, SMOTEENN, SMOTETomek) and compare combinations with CFS",
        "Calibrate probabilities (Platt/Isotonic) and set per-class thresholds optimized for F1 or F-beta",
        "Evaluate gradient boosting ensembles (XGBoost/LightGBM/CatBoost) as drop-in classifiers on CFS features",
        "Implement hierarchical classifier heads aligned to the 8-category grouping before 34-class fine-grained prediction",
        "Assess feature selection stability (bootstrap/jackknife) and enforce stability-aware selection",
        "Add model explanation layer (SHAP) with drift monitoring on top features for deployment readiness"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "Weka",
        "scikit-learn"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Severe class imbalance in multi-attack IoT traffic",
        "Need for high per-class F1, especially on minority/unsaturated classes",
        "Detecting zero-day and rare attacks without sacrificing majority-class performance"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces IIDS, a three-stage framework (feature selection, class balancing, classification) for imbalanced IoT IDS",
      "Empirical evaluation on CICIoT2023 with 34, 8, and 2 class setups using Random Forest",
      "Shows that combining CFS (6 features) with BRFC yields best overall gains: +3.72% precision, +3.75% recall, +4.69% F1 over base RF on 46 features",
      "Highlights significant F1 improvements on unsaturated classes (average +7.9%), including large gains for DictionaryBruteForce, BrowserHijacking, and SqlInjection",
      "Provides analysis indicating balancing alone is insufficient; feature selection plays a critical role in improving minority-class performance"
    ]
  },
  {
    "arxiv_id": "2308.04662v3",
    "title": "VulLibGen: Generating Names of Vulnerability-Affected Packages via a Large Language Model",
    "authors": "Tianyu Chen; Lin Li; Liuchuan Zhu; Zongyang Li; Xueqing Liu; Guangtai Liang; Qianxiang Wang; Tao Xie",
    "abstract": "Security practitioners maintain vulnerability reports (e.g., GitHub Advisory) to help developers mitigate security risks. An important task for these databases is automatically extracting structured information mentioned in the report, e.g., the affected software packages, to accelerate the defense of the vulnerability ecosystem.   However, it is challenging for existing work on affected package identification to achieve a high accuracy. One reason is that all existing work focuses on relatively smaller models, thus they cannot harness the knowledge and semantic capabilities of large language models.   To address this limitation, we propose VulLibGen, the first method to use LLM for affected package identification. In contrast to existing work, VulLibGen proposes the novel idea to directly generate the affected package. To improve the accuracy, VulLibGen employs supervised fine-tuning (SFT), retrieval augmented generation (RAG) and a local search algorithm. The local search algorithm is a novel postprocessing algorithm we introduce for reducing the hallucination of the generated packages. Our evaluation results show that VulLibGen has an average accuracy of 0.806 for identifying vulnerable packages in the four most popular ecosystems in GitHub Advisory (Java, JS, Python, Go) while the best average accuracy in previous work is 0.721. Additionally, VulLibGen has high value to security practice: we submitted 60 <vulnerability, affected package> pairs to GitHub Advisory (covers four ecosystems). 34 of them have been accepted and merged and 20 are pending approval. Our code and dataset can be found in the attachments.",
    "published_date": "2023-08-09",
    "pdf_link": "https://arxiv.org/pdf/2308.04662v3",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Vulnerability Management and Intelligence",
      "specific_problem": "Automatic identification (entity linking) of vulnerability-affected package names from vulnerability reports (e.g., GitHub Advisory/CVE descriptions)",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer/LLM",
        "specific": "Vicuna-13B, Vicuna-7B, LLaMA-7B, LLaMA-13B; ChatGPT (gpt-3.5-turbo), GPT-4 (gpt-4-1106-preview)",
        "novel_contribution": "First to directly generate affected package names with an LLM for this task; combines SFT, RAG, and a novel local search post-processing to reduce hallucinations"
      },
      {
        "type": "primary",
        "category": "Retrieval-Augmented Generation (RAG)",
        "specific": "TF-IDF initial ranker + BERT-base re-ranker over top-512",
        "novel_contribution": "RAG used to inject package metadata into LLM prompting for this domain"
      },
      {
        "type": "primary",
        "category": "Post-processing / Heuristic Search",
        "specific": "Edit-distance local search with suffix-first matching of package names",
        "novel_contribution": "Novel local search that matches generated suffix to closest existing suffix then restricts prefix to co-occurring prefixes to reduce hallucination"
      },
      {
        "type": "primary",
        "category": "Supervised Fine-Tuning",
        "specific": null,
        "novel_contribution": "Domain SFT of open-source LLMs on training splits of the GitHub Advisory dataset"
      },
      {
        "type": "primary",
        "category": "In-context Learning",
        "specific": "Few-shot (3 examples)",
        "novel_contribution": "Used when SFT not applied; complements RAG"
      },
      {
        "type": "baseline",
        "category": "Extreme Multi-label Text Classification",
        "specific": "FastXML",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Extreme Multi-label Text Classification (deep)",
        "specific": "LightXML",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Extreme Multi-label Text Classification",
        "specific": "Chronos",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "BERT-based Re-ranking",
        "specific": "VulLibMiner (TF-IDF + BERT re-ranker)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Few-shot In-context learning",
      "Retrieval-augmented generation"
    ],
    "datasets": [
      {
        "name": "VulLib (Java)",
        "type": "public",
        "domain": "vulnerability_reports_to_packages",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "GitHub Advisory Vulnerability-Package Dataset (Java, JS, Python, Go) – extension of VulLib",
        "type": "public",
        "domain": "vulnerability_reports_to_packages",
        "link": "https://github.com/anonymous4ACL24/submission1129",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "Maven package registry (metadata for retrieval)",
        "type": "public",
        "domain": "package_registry_metadata",
        "link": "https://mvnrepository.com",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "NPM package registry (metadata for retrieval)",
        "type": "public",
        "domain": "package_registry_metadata",
        "link": "https://www.npmjs.com",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "PyPI package registry (metadata for retrieval)",
        "type": "public",
        "domain": "package_registry_metadata",
        "link": "https://pypi.org",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Go package registry (metadata for retrieval)",
        "type": "public",
        "domain": "package_registry_metadata",
        "link": "https://pkg.go.dev",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "VulLibMiner",
        "paper_reference": "Chen et al., 2023b",
        "metric": "Average Accuracy@1 across Java/JS/Python/Go",
        "their_result": "0.806",
        "baseline_result": "0.721"
      },
      {
        "method_name": "Chronos",
        "paper_reference": "Lyu et al., 2023",
        "metric": "Average Accuracy@1",
        "their_result": "0.806",
        "baseline_result": "0.556"
      },
      {
        "method_name": "LightXML",
        "paper_reference": "Haryono et al., 2022",
        "metric": "Average Accuracy@1",
        "their_result": "0.806",
        "baseline_result": "0.405"
      },
      {
        "method_name": "FastXML",
        "paper_reference": "Chen et al., 2020",
        "metric": "Average Accuracy@1",
        "their_result": "0.806",
        "baseline_result": "0.285"
      },
      {
        "method_name": "VulLibMiner (per-language)",
        "paper_reference": "Chen et al., 2023b",
        "metric": "Accuracy@1",
        "their_result": "Java: 0.710; JS: 0.773; Python: 0.935; Go: 0.804",
        "baseline_result": "Java: 0.669; JS: 0.742; Python: 0.825; Go: 0.647"
      }
    ],
    "performance_metrics_used": [
      "Accuracy@1 (Prec@1)",
      "Recall@1",
      "F1@1"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "How does VulLibGen compare to existing work on identifying vulnerable packages?"
      ],
      "gaps_identified": [
        "Existing methods rely on smaller models due to retrieval scalability, limiting accuracy.",
        "Retrieval-based approaches have inference cost linear in registry size (e.g., 435k Java packages), preventing use of large LLMs.",
        "Vulnerability descriptions may omit key package name details, creating a knowledge gap for models.",
        "LLMs can hallucinate non-existent package names.",
        "Existing public dataset (VeraCode) is error-prone; VulLib originally covered only Java."
      ],
      "limitations": [
        "Not explicitly stated in the provided text."
      ],
      "future_work": [
        "Not mentioned in the provided text."
      ],
      "motivation": "Accelerate and improve accuracy of automatically extracting affected package names from vulnerability reports to support vulnerability lifecycle management and reduce manual curation costs; leverage LLM knowledge and semantics while addressing hallucinations.",
      "potential_research_ideas": [
        "Extend generation to jointly predict affected versions and multiple affected packages per CVE with structured outputs.",
        "Domain-adaptive pretraining of LLMs on package registries, commit messages, advisories, and security blogs to reduce knowledge gaps.",
        "Constrained decoding with a dynamic lexicon from the registry to eliminate non-existent outputs during generation.",
        "End-to-end retriever-generator training (e.g., fusion-in-decoder, RAG with gradient updates) for better grounding.",
        "Graph-based constraints using dependency graphs and co-occurrence statistics to disambiguate prefixes/suffixes.",
        "Active learning pipeline with human-in-the-loop curation from advisory maintainers.",
        "Robustness evaluation under adversarially perturbed descriptions and cross-registry drift.",
        "Cross-ecosystem normalization and canonicalization for packages with mirrors/renames."
      ],
      "architectural_improvement_recommendations": [
        "Replace post-hoc local search with constrained decoding over a trie built from valid package names (suffix-first lexicon).",
        "Integrate retrieval tokens directly into the decoder via FiD or retrieval-augmented encoder to improve grounding.",
        "Jointly train the BERT re-ranker with the generator via reinforcement learning or contrastive loss to prefer valid packages.",
        "Use a hierarchical decoder: first generate suffix, then restricted-prefix conditioned on suffix, mirroring the local search logic.",
        "Leverage pointer-generator or copy mechanisms to copy from retrieved package metadata.",
        "Multi-task learning with auxiliary tasks (package description summarization, ecosystem classification) to improve representations."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/anonymous4ACL24/submission1129",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Training/inference on 8x NVIDIA A100 40GB GPUs; ~200 GPU-days total (32 groups in RQ1 + 68 groups in RQ2; each group ~0.25 GPU-days across 8 GPUs). Ubuntu 18.04; Intel Xeon Gold 6248R (64 cores), 512GB RAM."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Curation of GitHub Advisory database (submission of <vulnerability, affected package> pairs)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "LLM hallucination of non-existent package names requires post-processing or constrained decoding.",
        "Keeping retrieval indices synchronized with rapidly evolving package registries.",
        "Access/cost considerations for large commercial LLMs; need for on-prem fine-tuned open-source LLMs.",
        "Disambiguation for long or nested package namespaces (e.g., Java/Go) across ecosystems."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces VulLibGen, the first LLM-based framework that directly generates affected package names from vulnerability descriptions.",
      "Combines supervised fine-tuning (SFT), retrieval-augmented generation (RAG), and a novel local search post-processing algorithm to reduce hallucinations.",
      "Achieves average Accuracy@1 of 0.806 across Java, JS, Python, and Go, surpassing the best prior work's 0.721. Quote: “VulLibGen has an average accuracy of 0.806 … while the best average accuracy in previous work is 0.721.”",
      "Creates and releases the first multi-ecosystem dataset for the task (Java, JS, Python, Go) with splits in chronological order; code and dataset available.",
      "Ablation shows SFT, RAG, and local search all improve accuracy, with SFT contributing the most; fine-tuned Vicuna-13B outperforms unfine-tuned ChatGPT/GPT-4.",
      "Demonstrates practical value: submitted 60 <vulnerability, affected package> pairs to GitHub Advisory; 34 accepted/merged and 20 pending approval."
    ]
  },
  {
    "arxiv_id": "2308.04607v1",
    "title": "Different Mechanisms of Machine Learning and Optimization Algorithms Utilized in Intrusion Detection Systems",
    "authors": "Mohammad Aziz; Ali Saeed Alfoudi",
    "abstract": "Malicious software is an integral part of cybercrime defense. Due to the growing number of malicious attacks and their target sources, detecting and preventing the attack becomes more challenging due to the assault's changing behavior. The bulk of classic malware detection systems is based on statistics, analytic techniques, or machine learning. Virus signature methods are widely used to identify malware. The bulk of anti-malware systems categorizes malware using regular expressions and patterns. While antivirus software is less likely to update its databases to identify and block malware, file features must be updated to detect and prevent newly generated malware. Creating attack signatures requires practically all of a human being's work. The purpose of this study is to undertake a review of the current research on intrusion detection models and the datasets that support them. In this article, we discuss the state-of-the-art, focusing on the strategy that was devised and executed, the dataset that was utilized, the findings, and the assessment that was undertaken. Additionally, the surveyed articles undergo critical analysis and statements in order to give a thorough comparative review. Machine learning and deep learning methods, as well as new classification and feature selection methodologies, are studied and researched. Thus far, each technique has proved the capability of constructing very accurate intrusion detection models. The survey findings reveal that Clearly, the MultiTree and adaptive voting algorithms surpassed all other models in terms of persistency and performance, averaging 99.98 percent accuracy on average.",
    "published_date": "2023-08-08",
    "pdf_link": "https://arxiv.org/pdf/2308.04607v1",
    "paper_types": [
      "survey"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Survey and comparative analysis of machine learning, deep learning, and optimization algorithms for intrusion detection systems and the datasets supporting them",
      "attack_types": [
        "Users-to-Root (U2R)",
        "Remote-to-Local (R2L)",
        "Distributed Denial-of-Service (DDoS)",
        "Zero-day (detection challenge)"
      ]
    },
    "ml_techniques": [
      {
        "type": "baseline",
        "category": "Clustering",
        "specific": "Subspace Clustering (SSC)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "SVM",
        "specific": "One-Class SVM (OCSVM)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "SVM",
        "specific": "SVM (as local rule models within ECGC)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Clustering",
        "specific": "DBSCAN",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Clustering",
        "specific": "K-means",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Ensemble",
        "specific": "Random Subspace + Discriminant classifiers (majority voting)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "k-NN",
        "specific": "KNN",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Decision Tree",
        "specific": "Decision Tree (DT)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Random Forest",
        "specific": "Random Forest (RF)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Boosting",
        "specific": "Gradient Boosting",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Boosting",
        "specific": "AdaBoost",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Linear Model",
        "specific": "Linear Discriminant Analysis (LDA)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Bayesian Network",
        "specific": "KDBN (expanded Bayesian network)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Gradient Boosted Trees",
        "specific": "XGBoost",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Optimization",
        "specific": "Genetic Algorithm (GA) for parameter optimization",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Optimization",
        "specific": "Particle Swarm Optimization (PSO) for XGBoost tuning",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Clustering",
        "specific": "Evolving possibilistic clustering (Cauchy/\"Couchiy\"-based)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Ensemble",
        "specific": "MultiTree",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Ensemble",
        "specific": "Adaptive voting algorithms",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Imbalanced Learning",
        "specific": "SMOTE (Synthetic Minority Oversampling)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "DARPA (DARPA intrusion detection evaluation datasets)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "KDD Cup 1999",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "NSL-KDD",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "UNSW-NB15",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Kyoto2016",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "ADFA-LD/WD",
        "type": "public",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CIC-IDS2017",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CSE-CIC-IDS2018",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "SSC-OCSVM (hybrid clustering + One-Class SVM) [Pu et al.]",
        "paper_reference": "[12]",
        "metric": "Qualitative (compared to K-means, DBSCAN); ROC, recall, FPR also computed",
        "their_result": "“The proposed method outperforms both the K-means and the DBSCAN algorithms.”",
        "baseline_result": null
      },
      {
        "method_name": "Ensemble discriminant classifiers with Random Subspace (majority voting) [Bhati et al.]",
        "paper_reference": "[13]",
        "metric": "Accuracy (KDDCup99)",
        "their_result": "“The model achieved a 98.9 percent accuracy.”",
        "baseline_result": null
      },
      {
        "method_name": "ECGC entropy-based clustering + SVM with GA optimization [Liu et al.]",
        "paper_reference": "[14]",
        "metric": "Accuracy, TP, FP, TN, FN (KDDCup99)",
        "their_result": "Evaluated on KDDCUP 99; complexity not reported.",
        "baseline_result": null
      },
      {
        "method_name": "Evolving possibilistic clustering (Cauchy-based) [Škrjanc et al.]",
        "paper_reference": "[15]",
        "metric": null,
        "their_result": "The work is theoretical; “contains no empirical data or assessment instruments.”",
        "baseline_result": null
      },
      {
        "method_name": "Hybrid anomaly/signature IDS; best classifier KNN [Meryem et al.]",
        "paper_reference": "[16]",
        "metric": "Accuracy, Precision, Recall, FPR (NSL-KDD)",
        "their_result": "“The best result is reached with KNN, which achieves an accuracy of 98.80 percent, a precision of 99.80 percent, a recall of 98.80 percent, and a false-positive rate of 0.9 percent.”",
        "baseline_result": null
      },
      {
        "method_name": "SMOTE + ML classifiers (DT, KNN, Gradient Boosting, RF, AdaBoost, LDA) [Karataş et al.]",
        "paper_reference": "[17]",
        "metric": "Precision, Recall, F1-Score, Error Rate, Time (KDD-CUP99, NSL-KDD, CIC-IDS2017, CSE-CIC-IDS2018)",
        "their_result": "“Precision of 99.69 percent, a recall of 99.34 percent, an F1-Score of 99.35 percent, an error rate of 0.65 percent, and a time of 274 seconds.”",
        "baseline_result": null
      },
      {
        "method_name": "KDBN (expanded Bayesian network) + MAP; with virtual augmentation [Yin et al.]",
        "paper_reference": "[18]",
        "metric": "Accuracy (KDDCup99)",
        "their_result": "“The accuracy rate was greater than 95%.”",
        "baseline_result": null
      },
      {
        "method_name": "PSO-XGBoost (PSO-optimized XGBoost) [Jiang et al.]",
        "paper_reference": "[19]",
        "metric": "Precision, Recall, F1-Score (NSL-KDD)",
        "their_result": "“The precision is 0.81, the recall is 0.75, and the F-Score is 0.71.”",
        "baseline_result": null
      },
      {
        "method_name": "Service-aware dataset partitioning + RF classifier [Uhm et al.]",
        "paper_reference": "[20]",
        "metric": "Classification performance (Kyoto2016 binary; CIC-IDS2017 multi-class)",
        "their_result": "Qualitative improvement reported; no specific numeric values provided.",
        "baseline_result": null
      },
      {
        "method_name": "MultiTree and adaptive voting algorithms (survey finding)",
        "paper_reference": "Paper’s survey conclusion",
        "metric": "Accuracy",
        "their_result": "“surpassed all other models in terms of persistency and performance, averaging 99.98 percent accuracy on average.”",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "Precision",
      "Recall",
      "False Positive Rate (FPR)",
      "ROC",
      "Confusion Matrix",
      "F1-Score",
      "Error Rate",
      "Time"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "What are the current machine learning, deep learning, and optimization algorithms utilized in intrusion detection systems?",
        "What datasets are used to evaluate IDS models, and what are their strengths and limitations?",
        "How do different IDS approaches compare in terms of accuracy, false positive rate, and other evaluation metrics?",
        "Which strategies (e.g., hybrid IDS, data balancing, optimization) improve detection of minority/rare attacks and zero-day threats?"
      ],
      "gaps_identified": [
        "Signature-based IDS cannot detect zero-day attacks; anomaly-based IDS often produce high false alarms.",
        "Many widely used datasets (e.g., KDD Cup 1999) are simulated and criticized for not reflecting real-world networks; data becomes outdated as threats evolve.",
        "Real NIDS datasets are often private due to confidentiality, limiting public benchmarking.",
        "Class imbalance significantly degrades performance on minority attack classes; some works do not address imbalance.",
        "Some studies omit computational complexity, processing time, or scalability analyses.",
        "High-dimensional data challenges consistency and performance for some methods.",
        "Encrypted traffic and insider threats are difficult for NIDS to handle; HIDS/NIDS each have coverage limitations.",
        "Few ML models tailored to IoT traffic patterns despite IoT-specific constraints."
      ],
      "limitations": [
        "Several reviewed models did not account for imbalanced data or report processing time.",
        "Some evaluations report only accuracy without comprehensive metrics.",
        "Certain proposed techniques lack empirical evaluation (purely theoretical).",
        "Optimized models (e.g., PSO-XGBoost) may incur heavy computation and risk suboptimal global solutions."
      ],
      "future_work": [],
      "motivation": "Provide a comprehensive review of state-of-the-art IDS models, associated datasets, strategies, findings, and evaluations to guide the development of accurate intrusion detection models.",
      "potential_research_ideas": [
        "Design a unified, modern benchmark with realistic, up-to-date traffic and attack behaviors (including IoT and encrypted traffic), with standardized train/test splits and protocols.",
        "Develop continual and self-supervised learning IDS capable of adapting to evolving threats and detecting zero-day attacks with minimal labels.",
        "Investigate explainable IDS models that provide human-understandable rationales while maintaining high detection on minority classes.",
        "Build privacy-preserving federated IDS training across organizations to unlock real-world data without sharing raw logs.",
        "Integrate adversarial robustness into IDS (e.g., adversarial training for tabular/network traffic) to resist evasion.",
        "Create lightweight edge-friendly IDS for IoT with budgeted feature extraction and on-device inference.",
        "Explore graph-based models that capture host/service communication patterns for improved detection of lateral movement and rare attacks.",
        "Automate cost-sensitive calibration and thresholding to minimize operational false positives while maintaining recall."
      ],
      "architectural_improvement_recommendations": [
        "Adopt hybrid architectures combining signature modules with deep anomaly detectors and calibrated ensembling (e.g., stacking RF/XGBoost with one-class models).",
        "Use class-imbalance strategies beyond SMOTE: focal loss, cost-sensitive learning, and dynamic reweighting during training.",
        "Employ service-aware or protocol-aware partitioning with hierarchical ensembles to reduce class space per partition and improve minority class detection.",
        "Leverage self-supervised pretraining on large unlabeled network logs (e.g., masked feature modeling) before supervised fine-tuning.",
        "Implement online learning with drift detection for streaming IDS to maintain performance as traffic changes.",
        "Incorporate feature selection/representation learning to handle high dimensionality (e.g., autoencoders or learned embeddings) with calibration layers to control FPR."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "High false positive rates in anomaly-based IDS",
        "Inability of signature-based IDS to detect zero-day attacks",
        "Encrypted traffic inspection challenges",
        "Insider attack detection limitations for NIDS",
        "Big Data scale and high-dimensionality",
        "Imbalanced attack classes",
        "Privacy and confidentiality constraints limiting dataset sharing",
        "IoT devices’ resource constraints and diverse protocols"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Comprehensive survey and comparative review of machine learning, deep learning, and optimization algorithms for IDS.",
      "Review of widely used IDS benchmark datasets and discussion of their limitations.",
      "Critical analysis of surveyed works, highlighting issues such as class imbalance, false positives, dataset realism, and computational reporting.",
      "Synthesis finding that “MultiTree and adaptive voting algorithms” achieved the highest reported accuracies (~99.98%) among surveyed models."
    ]
  },
  {
    "arxiv_id": "2308.04972v1",
    "title": "can-train-and-test: A Curated CAN Dataset for Automotive Intrusion Detection",
    "authors": "Brooke Lampe; Weizhi Meng",
    "abstract": "When it comes to in-vehicle networks (IVNs), the controller area network -- CAN -- bus dominates the market; automobiles manufactured and sold around the world depend on the CAN bus for safety-critical communications between various components of the vehicle (e.g., the engine, the transmission, the steering column). Unfortunately, the CAN bus is inherently insecure; in fact, it completely lacks controls such as authentication, authorization, and confidentiality (i.e., encryption). Therefore, researchers have travailed to develop automotive security enhancements. The automotive intrusion detection system (IDS) is especially popular in the literature -- due to its relatively low cost in terms of money, resource utilization, and implementation effort. That said, developing and evaluating an automotive IDS is often challenging; if researchers do not have access to a test vehicle, then they are forced to depend on publicly available CAN data -- which is not without limitations. Lack of access to adequate CAN data, then, becomes a barrier to entry into automotive security research.   We seek to lower that barrier to entry by introducing a new CAN dataset to facilitate the development and evaluation of automotive IDSs. Our dataset, dubbed can-train-and-test, provides CAN data from four different vehicles produced by two different manufacturers. The attack captures for each vehicle model are equivalent, enabling researchers to assess the ability of a given IDS to generalize to different vehicle models and even different vehicle manufacturers. Our dataset contains replayable .log files as well as labeled and unlabeled .csv files, thereby meeting a variety of development and evaluation needs. Furthermore, can-train-and-test offers nine unique attacks, ranging from denial of service (DoS) to gear spoofing to standstill...",
    "published_date": "2023-08-09",
    "pdf_link": "https://arxiv.org/pdf/2308.04972v1",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "benchmark"
    ],
    "security_domain": {
      "primary": "Automotive Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "CAN bus intrusion detection dataset curation and benchmarking across vehicles and attack types",
      "attack_types": [
        "Denial of Service (DoS)",
        "Spoofing (e.g., gear spoofing, standstill)",
        "Masquerade",
        "Replay",
        "Fuzzing",
        "Suppress"
      ]
    },
    "ml_techniques": [
      {
        "type": "baseline",
        "category": "Multiple (traditional ML and deep learning)",
        "specific": null,
        "novel_contribution": "Benchmarks seven model families (18 models total) spanning supervised and unsupervised paradigms on the proposed dataset"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "can-train-and-test",
        "type": "public",
        "domain": "in_vehicle_network",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How well can IDSs trained on one vehicle and set of attacks generalize to different vehicle models/manufacturers?",
        "How well can IDSs detect unknown (previously unseen) attacks when trained on other attack types?"
      ],
      "gaps_identified": [
        "Lack of sufficient attack-free data in existing datasets",
        "Lack of sufficient attack variation",
        "Lack of sufficient vehicle variation (manufacturer/model/year)",
        "Lack of fidelity (simulation/testbed or stationary captures)",
        "Lack of severity (few high-severity attacks with physical impacts)",
        "Lack of modernity (attacks adapted to evade enhanced security features in modern vehicles)",
        "Lack of labels"
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Lower the barrier to entry for automotive IDS research by providing a realistic, labeled, multi-vehicle CAN dataset with varied, high-severity attacks and curated train/test splits for generalization evaluation.",
      "potential_research_ideas": [
        "Develop and evaluate cross-vehicle domain adaptation methods for CAN IDS (e.g., transfer or domain-invariant representations) using the known/unknown vehicle splits.",
        "Open-set and unknown attack detection methods that explicitly handle unseen attack types in the test splits.",
        "Self-supervised or contrastive representation learning on large attack-free segments to improve anomaly detection with limited labels.",
        "Temporal sequence modeling of CAN traffic (e.g., sequence anomalies across IDs) to better capture dynamics present in on-the-road captures.",
        "Meta-learning approaches to quickly adapt IDSs to new vehicles with minimal calibration data.",
        "Robustness studies against adaptive attackers (e.g., low-and-slow spoofing) using the dataset’s high-fidelity attack traces.",
        "Physical-process-informed IDS that ties CAN anomalies to physical plausibility constraints observed during on-road experiments.",
        "Evaluation frameworks that standardize cross-manufacturer generalization metrics using the provided splits."
      ],
      "architectural_improvement_recommendations": [
        "Incorporate multi-granularity modeling: per-ID autoencoders combined with a global temporal model (e.g., sequence-level transformer) for context.",
        "Learn embeddings for arbitration IDs and signal bytes, enabling sequence models (RNN/Transformer) to capture dependencies across IDs.",
        "Use domain adaptation (e.g., DANN/CORAL) between train and test vehicle distributions to improve unknown-vehicle performance.",
        "Leverage contrastive pretraining on attack-free drives (positive = nearby windows, negative = distant windows) before supervised fine-tuning.",
        "Add OOD scoring layers (e.g., energy-based or Mahalanobis distance on latent features) to flag unseen attack types.",
        "Calibrate thresholds per ID/DLC to reduce false positives in high-volume attack-free segments."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Generalization across manufacturers due to proprietary CAN implementations",
        "Modern vehicles may eject or rate-limit spoofing nodes, affecting attack patterns",
        "Safety constraints during data collection and real-world operation"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "New open-access CAN dataset for automotive IDS research",
      "High volume of attack-free data",
      "Nine unique attacks, including high-severity attacks with known physical impacts",
      "Data from four different vehicles produced by two manufacturers",
      "On-the-road, live capture of both attack-free and attack data for higher fidelity",
      "Attacks adapted to evade modern vehicle security enhancements",
      "Provision of labeled and unlabeled CSVs and replayable log files",
      "Curated into four train/test sub-datasets (set01–set04) with defined train and four test subsets per set",
      "Evaluation protocol enabling known/unknown vehicle and known/unknown attack testing (test01–test04)",
      "Benchmark of seven model families (18 models) spanning supervised and unsupervised, traditional ML and deep learning on the dataset"
    ]
  },
  {
    "arxiv_id": "2307.14480v2",
    "title": "PSOFuzz: Fuzzing Processors with Particle Swarm Optimization",
    "authors": "Chen Chen; Vasudev Gohil; Rahul Kande; Ahmad-Reza Sadeghi; Jeyavijayan Rajendran",
    "abstract": "Hardware security vulnerabilities in computing systems compromise the security defenses of not only the hardware but also the software running on it. Recent research has shown that hardware fuzzing is a promising technique to efficiently detect such vulnerabilities in large-scale designs such as modern processors. However, the current fuzzing techniques do not adjust their strategies dynamically toward faster and higher design space exploration, resulting in slow vulnerability detection, evident through their low design coverage. To address this problem, we propose PSOFuzz, which uses particle swarm optimization (PSO) to schedule the mutation operators and to generate initial input programs dynamically with the objective of detecting vulnerabilities quickly. Unlike traditional PSO, which finds a single optimal solution, we use a modified PSO that dynamically computes the optimal solution for selecting mutation operators required to explore new design regions in hardware. We also address the challenge of inefficient initial seed generation by employing PSO-based seed generation. Including these optimizations, our final formulation outperforms fuzzers without PSO. Experiments show that PSOFuzz achieves up to 15.25$\\times$ speedup for vulnerability detection and up to 2.22$\\times$ speedup for coverage compared to the state-of-the-art simulation-based hardware fuzzer.",
    "published_date": "2023-07-26",
    "pdf_link": "https://arxiv.org/pdf/2307.14480v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Hardware Security",
      "subdomain": "Processor/SoC Security",
      "specific_problem": "Hardware processor fuzzing with dynamic mutation scheduling and seed generation to accelerate vulnerability detection and coverage",
      "attack_types": [
        "hardware design vulnerabilities",
        "processor logic bugs",
        "privilege escalation (processor)",
        "memory isolation violations",
        "undefined behavior in hardware"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Swarm/Evolutionary Optimization",
        "specific": "Particle Swarm Optimization (PSO)",
        "novel_contribution": "Modified PSO with reset strategy for dynamically scheduling mutation operators based on coverage feedback; PSO-based dynamic seed generation optimizing instruction-type distributions per position in seed programs"
      }
    ],
    "learning_paradigm": [
      "Evolutionary/Swarm Optimization"
    ],
    "datasets": [
      {
        "name": "CVA6 RISC-V Processor",
        "type": "public",
        "domain": "processor_rtl",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "TheHuzz (state-of-the-art simulation-based hardware fuzzer) without PSO",
        "paper_reference": null,
        "metric": "Speedup in vulnerability detection time",
        "their_result": "up to 15.25× speedup for vulnerability detection",
        "baseline_result": "1× (baseline)"
      },
      {
        "method_name": "TheHuzz (state-of-the-art simulation-based hardware fuzzer) without PSO",
        "paper_reference": null,
        "metric": "Coverage speedup",
        "their_result": "up to 2.22× speedup for coverage",
        "baseline_result": "1× (baseline)"
      }
    ],
    "performance_metrics_used": [
      "time-to-vulnerability (detection latency)",
      "speedup factor (coverage, vulnerability detection)",
      "coverage percentage (e.g., branch coverage, toggle coverage, FSM coverage)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can particle swarm optimization dynamically schedule mutation operators to accelerate hardware processor fuzzing and improve coverage?",
        "How can PSO be adapted for hardware fuzzing to avoid particle saturation and account for changing optimal mutation distributions over time?",
        "Can PSO improve initial seed generation quality by learning distributions over instruction types to increase coverage speed and bug-finding efficacy?"
      ],
      "gaps_identified": [
        "Existing hardware fuzzers use static mutation operator scheduling and static/random seed generation, lacking feedback-guided dynamic updates.",
        "Coverage stagnation occurs quickly in hardware fuzzing, leaving design regions unexplored and vulnerabilities undetected.",
        "Prior PSO use in software fuzzing assumes a static optimal mutation distribution and uses random seeds—assumptions that do not hold for hardware fuzzing.",
        "Hardware fuzzing performance is highly sensitive to seed quality, which is under-addressed by current approaches."
      ],
      "limitations": [
        "Reset threshold βM introduces a trade-off between runtime and exploitation of learned knowledge; improper tuning can resemble random exploration or incur runtime overhead.",
        "Evaluation is performed on simulated open-source RISC-V processors; real-silicon deployment and broader ISAs are not demonstrated in the provided text.",
        "Performance remains dependent on seed quality and coverage feedback quality; improvements hinge on effective coverage metrics and simulation fidelity."
      ],
      "future_work": [],
      "motivation": "Accelerate hardware vulnerability detection and increase design space coverage by dynamically adapting mutation operator scheduling and seed generation using PSO, addressing stagnation and inefficiency in existing hardware fuzzers.",
      "potential_research_ideas": [
        "Hybridize PSO with bandit algorithms or reinforcement learning to adapt mutation and seed strategies at multiple time scales (per-iteration and per-phase).",
        "Multi-objective optimization that explicitly balances coverage breadth, depth, and bug diversity (distinct bug classes) via Pareto-front PSO.",
        "Transfer-learning of learned mutation/seed distributions across processor designs or configurations (e.g., from CVA6 to other RISC-V cores).",
        "Leverage richer microarchitectural feedback (e.g., pipeline stall reasons, cache/TLB miss patterns, scoreboard states) as fitness signals.",
        "Incorporate structural coverage metrics (e.g., MC/DC, state-transition path coverage) into PSO objectives to guide deeper corner-case exploration.",
        "Use hierarchical/ensemble swarms where sub-swarms focus on specific modules (e.g., decode, MMU, CSR, pipeline hazards) and coordinate globally.",
        "Integrate concolic or symbolic guidance sporadically to seed PSO with constraint-satisfying programs targeting hard-to-reach states.",
        "Apply adaptive restart strategies using statistical change-point detection to trigger resets and parameter adjustments more robustly than fixed βM.",
        "Generalize seed-generation PSO to instruction sequences plus operand/value distributions, including memory layouts and CSR configurations."
      ],
      "architectural_improvement_recommendations": [
        "Adaptive βM scheduling using statistical tests on coverage improvements (e.g., sequential probability ratio tests) to trigger resets.",
        "Multi-swarm or island-model PSO to maintain diversity and reduce premature convergence; occasional migration of gbest across islands.",
        "Module-aware fitness shaping that weights coverage from under-explored microarchitectural blocks higher to correct exploration imbalance.",
        "Augment seed-generation PSO with operand/value models (e.g., learned distributions for immediates, register allocation patterns, memory addresses).",
        "Incorporate dynamic operator set expansion/pruning based on contribution analysis (e.g., Shapley-like credit assignment for mutation operators).",
        "Asynchronous PSO updates across threads to improve scalability and reduce synchronization bottlenecks; lock-free statistics aggregation.",
        "GPU-accelerated or distributed RTL simulation backends to increase throughput of PSO evaluation loops.",
        "Logging and replay of (position, seed, coverage) tuples to enable offline meta-optimization and reproducibility."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "RTL simulation environment for open-source RISC-V processors",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Coverage stagnation due to particle saturation without resets",
        "Ineffective seed generation with static/random instruction distributions",
        "Hyperparameter tuning for reset threshold βM and PSO parameters",
        "Dependence on simulation speed and coverage metric quality"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "First technique to use PSO to schedule mutation operators and generate seeds for hardware fuzzing.",
      "Reset strategy to prevent particle saturation and dynamically adapt mutation operator probabilities based on coverage feedback.",
      "PSO-based dynamic seed generation algorithm that optimizes instruction-type distributions per instruction position in seeds.",
      "Evaluation on three open-source RISC-V processors showing up to 15.25× speedup in vulnerability detection and up to 2.22× speedup in coverage versus a state-of-the-art simulation-based fuzzer."
    ]
  },
  {
    "arxiv_id": "2307.16541v1",
    "title": "AMOE: a Tool to Automatically Extract and Assess Organizational Evidence for Continuous Cloud Audit",
    "authors": "Franz Deimling; Michela Fazzolari",
    "abstract": "The recent spread of cloud services has enabled many companies to take advantage of them. Nevertheless, the main concern about the adoption of cloud services remains the lack of transparency perceived by customers regarding security and privacy. To overcome this issue, Cloud Service Certifications (CSCs) have emerged as an effective solution to increase the level of trust in cloud services, possibly based on continuous auditing to monitor and evaluate the security of cloud services on an ongoing basis. Continuous auditing can be easily implemented for technical aspects, while organizational aspects can be challenging due to their generic nature and varying policies between service providers. In this paper, we propose an approach to facilitate the automatic assessment of organizational evidence, such as that extracted from security policy documents. The evidence extraction process is based on Natural Language Processing (NLP) techniques, in particular on Question Answering (QA). The implemented prototype provides promising results on an annotated dataset, since it is capable to retrieve the correct answer for more than half of the tested metrics. This prototype can be helpful for Cloud Service Providers (CSPs) to automate the auditing of textual policy documents and to help in reducing the time required by auditors to check policy documents.",
    "published_date": "2023-07-31",
    "pdf_link": "https://arxiv.org/pdf/2307.16541v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Cloud Security",
      "subdomain": "Compliance and Auditing",
      "specific_problem": "Automatic extraction and assessment of organizational evidence from cloud security policy documents for continuous audit/certification",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "RoBERTa (deepset/roberta-base-squad2)",
        "novel_contribution": "Applies QA over policy documents for organizational evidence extraction, combined with document structure preservation and keyword-based paragraph filtering within AMOE."
      },
      {
        "type": "baseline",
        "category": "Similarity-based retrieval",
        "specific": null,
        "novel_contribution": "Cosine-similarity paragraph ranking used to reduce search space for QA and as an alternative retrieval pipeline."
      },
      {
        "type": "baseline",
        "category": "Rule-based filtering",
        "specific": "Keyword-based paragraph filtering",
        "novel_contribution": "Uses metric keywords (after stopword removal and lemmatization) to pre-filter relevant sections before QA."
      }
    ],
    "learning_paradigm": [
      "Supervised (pretrained QA model fine-tuned on SQuAD2, used off-the-shelf)",
      "Rule-based filtering",
      "Similarity-based retrieval"
    ],
    "datasets": [
      {
        "name": "MEDINA organisational metrics",
        "type": "private",
        "domain": "policy_metrics",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Industry partner cloud policy documents",
        "type": "proprietary",
        "domain": "policy_documents",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Annotated CSP policy documents (INCEpTION)",
        "type": "proprietary",
        "domain": "policy_documents",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "SQuAD2.0",
        "type": "public",
        "domain": "qa_wikipedia",
        "link": "https://rajpurkar.github.io/SQuAD-explorer/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "percentage of metrics for which the correct answer was retrieved (exact match with annotated span)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Continuous auditing is effective for technical aspects but challenging for organizational aspects due to generic, NL policies varying by provider.",
        "Lack of standardized approaches for continuous auditing integration in existing certification processes.",
        "Organizational evidence is mostly available as unstructured text, making automated assessment difficult."
      ],
      "limitations": [
        "“it is capable to retrieve the correct answer for more than half of the tested metrics.”",
        "“Some documents contain more than 50 pages and this affects the processing duration as well as extraction accuracy.”",
        "“AMOE tool does not know a priori what policies are included in a document, thus every metric needs to be checked, even if it could not be answered since it is not covered by the document.”",
        "“The whole process depends heavily on the quality of the PDF, which is reflected in the final HTML document.”"
      ],
      "future_work": [],
      "motivation": "Increase trust and transparency in cloud services by enabling continuous audit of organizational controls via automated evidence extraction from policy documents.",
      "potential_research_ideas": [
        "Create and release a domain-specific QA dataset of annotated cloud policy documents to fine-tune models for organizational evidence extraction.",
        "Investigate multilingual and cross-jurisdiction policy QA to support providers operating in multiple regions and languages.",
        "Incorporate layout-aware document models (e.g., LayoutLM/DocFormer) to better exploit headings, lists, and tables in policies.",
        "Explore retrieval-augmented generation (RAG) with large language models for answer extraction plus justification and confidence calibration.",
        "Develop active learning and weak supervision strategies to scale annotations of policy documents with minimal expert effort.",
        "Design a policy change-tracking module for continuous auditing across document versions to detect drift and reassess metrics automatically."
      ],
      "architectural_improvement_recommendations": [
        "Fine-tune the QA model on a corpus of annotated cloud security policy documents (domain adaptation beyond SQuAD2).",
        "Replace/augment keyword filtering with embedding-based dense retrieval (e.g., SBERT/contriever) and a trained paragraph ranker as in Lee et al.",
        "Adopt layout-aware encoders to leverage PDF/HTML structure, tables, and lists during retrieval and QA.",
        "Introduce uncertainty estimation and answerability detection to abstain when evidence is absent or ambiguous.",
        "Implement a multi-hop QA component to aggregate evidence across multiple paragraphs or documents.",
        "Add semantic normalization and unit extraction/conversion modules to support numerical policy metrics (e.g., password age, retention periods)."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "Hugging Face Transformers",
        "NLTK",
        "Poppler (pdftohtml)",
        "INCEpTION (for annotation)"
      ],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Policy documents vary widely in structure and content; not all metrics are covered in every document.",
        "Large documents (50+ pages) increase processing time and may reduce accuracy.",
        "Quality of PDF-to-HTML conversion impacts downstream extraction.",
        "Need to check all metrics without prior knowledge of document coverage increases compute and noise.",
        "Organizational requirements are generic and expressed in NL, complicating automation."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "AMOE: a tool and pipeline for automatic extraction and assessment of organizational evidence from cloud policy documents.",
      "Formulation of organizational metrics as precise QA-style questions with keywords to drive evidence retrieval.",
      "Pre-processing pipeline converting PDFs to HTML to preserve structure and enable section/paragraph-level filtering.",
      "Evaluation on annotated policy documents from two CSPs, showing the QA approach with keyword filtering retrieves correct answers for more than half of tested metrics.",
      "Comparison of multiple retrieval pipelines, including QA with keyword filtering and similarity-based paragraph ranking."
    ]
  },
  {
    "arxiv_id": "2308.16861v1",
    "title": "Facing Unknown: Open-World Encrypted Traffic Classification Based on Contrastive Pre-Training",
    "authors": "Xiang Li; Beibei Feng; Tianning Zang; Shuyuan Zhao; Jingrun Ma",
    "abstract": "Traditional Encrypted Traffic Classification (ETC) methods face a significant challenge in classifying large volumes of encrypted traffic in the open-world assumption, i.e., simultaneously classifying the known applications and detecting unknown applications. We propose a novel Open-World Contrastive Pre-training (OWCP) framework for this. OWCP performs contrastive pre-training to obtain a robust feature representation. Based on this, we determine the spherical mapping space to find the marginal flows for each known class, which are used to train GANs to synthesize new flows similar to the known parts but do not belong to any class. These synthetic flows are assigned to Softmax's unknown node to modify the classifier, effectively enhancing sensitivity towards known flows and significantly suppressing unknown ones. Extensive experiments on three datasets show that OWCP significantly outperforms existing ETC and generic open-world classification methods. Furthermore, we conduct comprehensive ablation studies and sensitivity analyses to validate each integral component of OWCP.",
    "published_date": "2023-08-31",
    "pdf_link": "https://arxiv.org/pdf/2308.16861v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Traffic Analysis",
      "specific_problem": "Open-world encrypted traffic classification (simultaneous known-class classification and unknown-class detection)",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "Multi-headed attention encoder (Transformer-style)",
        "novel_contribution": "Used with supervised contrastive pre-training to learn robust representations for encrypted traffic flows"
      },
      {
        "type": "primary",
        "category": "Contrastive Learning",
        "specific": "InfoNCE loss with positive/negative flow pairs",
        "novel_contribution": "Supervised contrastive pre-training on traffic sequences to separate classes before open-set handling"
      },
      {
        "type": "primary",
        "category": "Generative Adversarial Network",
        "specific": "GAN (generator and discriminator)",
        "novel_contribution": "Trained on class-marginal flows to synthesize ‘unknown-like’ flows assigned to an unknown Softmax node"
      },
      {
        "type": "primary",
        "category": "Prototype/Centroid-based",
        "specific": "Spherical decision boundary around class centroids",
        "novel_contribution": "Discovers non-homogeneous marginal flows per class in learned feature space"
      },
      {
        "type": "primary",
        "category": "Open-set classification",
        "specific": "Softmax with additional unknown node + thresholding",
        "novel_contribution": "Two-stage decision with an explicit unknown node trained using GAN-synthesized flows and calibrated threshold σ"
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "Deep Fingerprinting (DF)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Fingerprinting/Flow-template",
        "specific": "FlowPrint",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Deep Neural Network",
        "specific": "Fs-Net",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Pre-trained encoder",
        "specific": "PERT",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Open-set classification",
        "specific": "OpenMax (PT-OM)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Open-set classification",
        "specific": "Thresholded Softmax (PT-ST)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Adversarial"
    ],
    "datasets": [
      {
        "name": "CrossPlatform",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "ISCX17 (VPN/non-VPN app traffic)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "USTC-TFC",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Deep Fingerprinting (DF)",
        "paper_reference": "[1]",
        "metric": "F1 (closed-world) on CrossPlatform",
        "their_result": "96.21",
        "baseline_result": "39.67"
      },
      {
        "method_name": "Deep Fingerprinting (DF)",
        "paper_reference": "[1]",
        "metric": "F1 (closed-world) on ISCX17",
        "their_result": "79.02",
        "baseline_result": "57.99"
      },
      {
        "method_name": "Deep Fingerprinting (DF)",
        "paper_reference": "[1]",
        "metric": "F1 (closed-world) on USTC-TFC",
        "their_result": "99.56",
        "baseline_result": "80.67"
      },
      {
        "method_name": "FlowPrint",
        "paper_reference": "[7]",
        "metric": "F1 (closed-world) on CrossPlatform",
        "their_result": "96.21",
        "baseline_result": "88.73"
      },
      {
        "method_name": "FlowPrint",
        "paper_reference": "[7]",
        "metric": "F1 (closed-world) on ISCX17",
        "their_result": "79.02",
        "baseline_result": "69.24"
      },
      {
        "method_name": "FlowPrint",
        "paper_reference": "[7]",
        "metric": "F1 (closed-world) on USTC-TFC",
        "their_result": "99.56",
        "baseline_result": "75.26"
      },
      {
        "method_name": "Fs-Net",
        "paper_reference": "[4]",
        "metric": "F1 (closed-world) on CrossPlatform",
        "their_result": "96.21",
        "baseline_result": "48.26"
      },
      {
        "method_name": "Fs-Net",
        "paper_reference": "[4]",
        "metric": "F1 (closed-world) on ISCX17",
        "their_result": "79.02",
        "baseline_result": "52.48"
      },
      {
        "method_name": "Fs-Net",
        "paper_reference": "[4]",
        "metric": "F1 (closed-world) on USTC-TFC",
        "their_result": "99.56",
        "baseline_result": "89.86"
      },
      {
        "method_name": "PERT",
        "paper_reference": "[5]",
        "metric": "F1 (closed-world) on CrossPlatform",
        "their_result": "96.21",
        "baseline_result": "89.14"
      },
      {
        "method_name": "PERT",
        "paper_reference": "[5]",
        "metric": "F1 (closed-world) on ISCX17",
        "their_result": "79.02",
        "baseline_result": "71.23"
      },
      {
        "method_name": "PERT",
        "paper_reference": "[5]",
        "metric": "F1 (closed-world) on USTC-TFC",
        "their_result": "99.56",
        "baseline_result": "99.40"
      },
      {
        "method_name": "FlowPrint",
        "paper_reference": "[7]",
        "metric": "F1ow (open-world) on CrossPlatform",
        "their_result": "94.62",
        "baseline_result": "68.79"
      },
      {
        "method_name": "FlowPrint",
        "paper_reference": "[7]",
        "metric": "F1ow (open-world) on ISCX17",
        "their_result": "77.85",
        "baseline_result": "62.36"
      },
      {
        "method_name": "FlowPrint",
        "paper_reference": "[7]",
        "metric": "F1ow (open-world) on USTC-TFC",
        "their_result": "90.96",
        "baseline_result": "69.01"
      },
      {
        "method_name": "DF",
        "paper_reference": "[1]",
        "metric": "F1ow (open-world) on CrossPlatform",
        "their_result": "94.62",
        "baseline_result": "36.25"
      },
      {
        "method_name": "DF",
        "paper_reference": "[1]",
        "metric": "F1ow (open-world) on ISCX17",
        "their_result": "77.85",
        "baseline_result": "55.76"
      },
      {
        "method_name": "DF",
        "paper_reference": "[1]",
        "metric": "F1ow (open-world) on USTC-TFC",
        "their_result": "90.96",
        "baseline_result": "71.14"
      },
      {
        "method_name": "PT-OM (OpenMax)",
        "paper_reference": "[9]",
        "metric": "F1ow (open-world) on CrossPlatform",
        "their_result": "94.62",
        "baseline_result": "92.26"
      },
      {
        "method_name": "PT-OM (OpenMax)",
        "paper_reference": "[9]",
        "metric": "F1ow (open-world) on ISCX17",
        "their_result": "77.85",
        "baseline_result": "73.75"
      },
      {
        "method_name": "PT-OM (OpenMax)",
        "paper_reference": "[9]",
        "metric": "F1ow (open-world) on USTC-TFC",
        "their_result": "90.96",
        "baseline_result": "89.01"
      },
      {
        "method_name": "PT-ST (Thresholded Softmax)",
        "paper_reference": "[10]",
        "metric": "F1ow (open-world) on CrossPlatform",
        "their_result": "94.62",
        "baseline_result": "92.44"
      },
      {
        "method_name": "PT-ST (Thresholded Softmax)",
        "paper_reference": "[10]",
        "metric": "F1ow (open-world) on ISCX17",
        "their_result": "77.85",
        "baseline_result": "73.19"
      },
      {
        "method_name": "PT-ST (Thresholded Softmax)",
        "paper_reference": "[10]",
        "metric": "F1ow (open-world) on USTC-TFC",
        "their_result": "90.96",
        "baseline_result": "87.18"
      }
    ],
    "performance_metrics_used": [
      "Accuracy (AC)",
      "F1",
      "Open-world Accuracy (ACow)",
      "Open-world F1 (F1ow)",
      "Macro Average"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "How to simultaneously classify known applications and detect unknown applications under the open-world assumption for encrypted traffic?",
        "Can supervised contrastive pre-training yield robust traffic representations that separate classes effectively for open-set recognition?",
        "How to identify and leverage marginal, non-homogeneous flows per class to synthesize unknown-like samples?",
        "Does adding an explicit unknown node to Softmax improve open-world ETC when trained with synthetic unknown flows?"
      ],
      "gaps_identified": [
        "Most ETC methods assume a closed world and misclassify unseen apps as known.",
        "Enumerating all apps and collecting corresponding traffic is impractical given millions of apps.",
        "Third-Party Libraries cause homogeneous network behavior across apps, increasing false positives.",
        "Softmax-threshold methods from computer vision are ill-suited for ETC due to unreadability and homogeneity leading to overlapping class distributions."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Encrypted traffic is pervasive and includes unknown applications; closed-world ETC fails in realistic settings with unknowns and TPL-induced homogeneity. The paper aims to build an open-world ETC framework that robustly represents flows, detects unknowns, and maintains high known-class performance.",
      "potential_research_ideas": [
        "Replace GANs with diffusion models or VAEs for more stable and diverse synthesis of unknown-like flows.",
        "Incorporate Extreme Value Theory (EVT)-based calibration for the unknown threshold or OpenMax-style tail modeling on the learned feature space.",
        "Develop incremental learning to incorporate newly discovered classes while preserving unknown detection performance.",
        "Apply multi-view or modality-specific contrastive learning that separately encodes payload and length/time sequences with cross-view alignment.",
        "Investigate domain adaptation across datasets (e.g., CrossPlatform -> ISCX17) to reduce performance drop under distribution shifts.",
        "Explore energy-based open-set detection or out-of-distribution scoring layered on top of the pre-trained encoder.",
        "Leverage federated or privacy-preserving training for deployment across organizations without sharing raw traffic.",
        "Study robustness against adversarially crafted flows that mimic known-class margins to evade unknown detection."
      ],
      "architectural_improvement_recommendations": [
        "Use supervised contrastive loss with class prototypes (e.g., SupCon or prototypical networks) to strengthen class separation and prototype quality for spherical boundaries.",
        "Adopt a two-branch encoder for NP (payload) and PL (packet length) with cross-attention fusion, allowing modality-specific inductive biases.",
        "Replace GAN with conditional diffusion or WGAN-GP to improve training stability and coverage of unknown-like regions.",
        "Introduce an EVT/OpenMax head or energy-based scoring alongside the unknown Softmax node for complementary detection signals.",
        "Employ adaptive, class-conditional thresholds learned from validation or via meta-learning to reduce manual grid search.",
        "Add uncertainty estimation (e.g., MC Dropout, Deep Ensembles) to aid unknown detection and calibration.",
        "Use metric-learning margins (ArcFace/CosFace) in the classifier to enlarge inter-class angular margins and tighten intra-class variance."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "PyTorch (1.10.0)"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Server with Intel i7-8700 CPU, 128GB RAM, NVIDIA RTX 3090 GPU; multi-head attention encoder with 8 heads (d=64), FFN size 1024; BertAdam LR=5e-5, warmup=0.03; threshold σ selected via grid search (0.7 used)."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes OWCP, the first method using pre-training to solve open-world ETC.",
      "Discovers non-homogeneous marginal flows in spherical feature space and synthesizes simulated unknown flows via GANs.",
      "Adds an unknown Softmax node and two-stage recognition to improve known classification and unknown detection.",
      "Extensive experiments on three publicly available datasets (CrossPlatform, ISCX17, USTC-TFC), outperforming existing ETC and generic open-world methods; includes ablation and sensitivity analyses."
    ]
  },
  {
    "arxiv_id": "2308.00074v1",
    "title": "Using Kernel SHAP XAI Method to optimize the Network Anomaly Detection Model",
    "authors": "Khushnaseeb Roshan; Aasim Zafar",
    "abstract": "Anomaly detection and its explanation is important in many research areas such as intrusion detection, fraud detection, unknown attack detection in network traffic and logs. It is challenging to identify the cause or explanation of why one instance is an anomaly? and the other is not due to its unbounded and lack of supervisory nature. The answer to this question is possible with the emerging technique of explainable artificial intelligence (XAI). XAI provides tools and techniques to interpret and explain the output and working of complex models such as Deep Learning (DL). This paper aims to detect and explain network anomalies with XAI, kernelSHAP method. The same approach is used to improve the network anomaly detection model in terms of accuracy, recall, precision and f score. The experiment is conduced with the latest CICIDS2017 dataset. Two models are created (Model_1 and OPT_Model) and compared. The overall accuracy and F score of OPT_Model (when trained in unsupervised way) are 0.90 and 0.76, respectively.",
    "published_date": "2023-07-31",
    "pdf_link": "https://arxiv.org/pdf/2308.00074v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Network Anomaly Detection",
      "specific_problem": "Unsupervised network anomaly detection with autoencoder optimized via kernelSHAP-based feature selection and explanation",
      "attack_types": [
        "DDoS"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Autoencoder",
        "specific": "Feedforward AE with MSE reconstruction error",
        "novel_contribution": "Uses kernelSHAP on reconstruction error to select a subset of features without labels and retrains an optimized AE (OPT_Model) for improved detection"
      },
      {
        "type": "primary",
        "category": "XAI/Model-agnostic explainer",
        "specific": "Kernel SHAP",
        "novel_contribution": "Unsupervised feature selection by computing SHAP values of AE reconstruction error using an attack-only background set; selects top contributing features to build OPT_Model"
      },
      {
        "type": "baseline",
        "category": "Autoencoder",
        "specific": "Model_1 (all 78 features)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Clustering",
        "specific": "k-means (for summarizing the 200 attack background instances before SHAP)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "CICIDS2017",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Model_1 (Autoencoder, all 78 features)",
        "paper_reference": null,
        "metric": "AUC",
        "their_result": "0.95 (OPT_Model)",
        "baseline_result": "0.803"
      },
      {
        "method_name": "Model_1 (Autoencoder, all 78 features)",
        "paper_reference": null,
        "metric": "G-mean",
        "their_result": "0.934 (OPT_Model)",
        "baseline_result": "0.804"
      },
      {
        "method_name": "Model_1 (Autoencoder, all 78 features)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "0.90 (OPT_Model, unsupervised)",
        "baseline_result": null
      },
      {
        "method_name": "Model_1 (Autoencoder, all 78 features)",
        "paper_reference": null,
        "metric": "F1-score",
        "their_result": "0.76 (OPT_Model, unsupervised)",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "Precision",
      "Recall",
      "F1-score",
      "ROC-AUC",
      "Specificity",
      "G-mean",
      "Confusion Matrix"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can kernelSHAP-derived feature importance on AE reconstruction error (without labels) improve unsupervised network anomaly detection?",
        "Which features contribute most to AE reconstruction error for attack instances, and can selecting them yield a better model?",
        "Can XAI be used not only to explain but also to optimize DL-based anomaly detectors?"
      ],
      "gaps_identified": [
        "Lack of methods that both detect anomalies and explain the cause of anomalous instances in network traffic.",
        "Reconstruction error alone explains anomalies only to some extent; it does not reveal true feature contributions.",
        "Limited exploration of XAI to improve (not just explain) unsupervised DL anomaly detectors."
      ],
      "limitations": [
        "KernelSHAP time complexity on complex/high-dimensional datasets; larger background sets increase computation time.",
        "Results depend on selecting an appropriate background set, which may affect model performance.",
        "Evaluation uses a subset of CICIDS2017 with only DDoS attack in testing, limiting generalizability across attack types."
      ],
      "future_work": [
        "Apply other model-agnostic or model-specific, global or local explanation techniques to explain and improve unsupervised DL models for anomaly detection in network traffic and other real-world applications."
      ],
      "motivation": "Use XAI to make black-box DL models for anomaly detection transparent and to leverage explanations (via kernelSHAP) to improve performance by selecting truly contributory features without labels.",
      "potential_research_ideas": [
        "Extend evaluation to all CICIDS2017 attack classes and cross-day scenarios to assess generalization of SHAP-based feature selection.",
        "Compare SHAP-selected AE against other unsupervised baselines (Isolation Forest, One-Class SVM, Deep SVDD, DAGMM, LSTM-AE) with unified protocols.",
        "Develop adaptive background set selection strategies (e.g., prototype selection, clustering validity criteria) to stabilize SHAP values and reduce compute.",
        "Investigate streaming/online anomaly detection with approximate SHAP (e.g., sampling, model distillation) for near real-time explanations.",
        "Integrate counterfactual or contrastive explanations to complement SHAP and aid analyst actionability.",
        "Leverage SHAP interaction values or feature grouping (protocol/flow-level) to capture interactions important for attacks.",
        "Use SHAP-driven drift detection to update feature subsets and thresholds under concept drift in live networks."
      ],
      "architectural_improvement_recommendations": [
        "Replace/augment AE with denoising or variational autoencoders, sparse regularization (L1), or robust loss (Huber) to improve anomaly separation.",
        "Adopt Deep SVDD or hypersphere-based objectives on latent space for tighter normal modelling.",
        "Apply EVT-based thresholding on reconstruction error rather than ROC-derived fixed thresholds.",
        "Introduce dimensionality reduction (e.g., PCA) or feature grouping before AE to reduce SHAP computational burden.",
        "Use temporal models (LSTM/GRU autoencoders) to capture sequence dynamics in flows.",
        "Compute SHAP interaction values and group SHAP to stabilize explanations and improve feature subset selection.",
        "Optimize kernelSHAP with better sampling kernels or background summarization (e.g., k-medoids) and GPU-accelerated SHAP."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "Python 3.7",
        "Keras",
        "scikit-learn",
        "SHAP",
        "Google Colab"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "GPU-enabled Google Colab; AE trained for 100 epochs, batch size 8192; kernelSHAP computed on 200 attack instances summarized with k-means."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "High time complexity of kernelSHAP on high-dimensional data and larger background sets",
        "Sensitivity to background set selection for SHAP computations",
        "Threshold selection and stability for converting AE reconstruction error to binary decisions"
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes an unsupervised feature selection approach using kernelSHAP on AE reconstruction error without labels.",
      "Computes SHAP values per feature on attack instances to estimate true contributions to reconstruction error.",
      "Selects top contributing features and trains an optimized autoencoder (OPT_Model) that improves detection performance.",
      "Demonstrates improved metrics on CICIDS2017 subset: AUC 0.95 (vs 0.803), G-mean 0.934 (vs 0.804), and reports overall accuracy 0.90 and F1-score 0.76 for OPT_Model.",
      "Provides instance-level explanations highlighting features causing high reconstruction error for anomalous samples."
    ]
  },
  {
    "arxiv_id": "2308.08803v1",
    "title": "An Effective Deep Learning Based Multi-Class Classification of DoS and DDoS Attack Detection",
    "authors": "Arun Kumar Silivery; Kovvur Ram Mohan Rao; L K Suresh Kumar",
    "abstract": "In the past few years, cybersecurity is becoming very important due to the rise in internet users. The internet attacks such as Denial of service (DoS) and Distributed Denial of Service (DDoS) attacks severely harm a website or server and make them unavailable to other users. Network Monitoring and control systems have found it challenging to identify the many classes of DoS and DDoS attacks since each operates uniquely. Hence a powerful technique is required for attack detection. Traditional machine learning techniques are inefficient in handling extensive network data and cannot extract high-level features for attack detection. Therefore, an effective deep learning-based intrusion detection system is developed in this paper for DoS and DDoS attack classification. This model includes various phases and starts with the Deep Convolutional Generative Adversarial Networks (DCGAN) based technique to address the class imbalance issue in the dataset. Then a deep learning algorithm based on ResNet-50 extracts the critical features for each class in the dataset. After that, an optimized AlexNet-based classifier is implemented for detecting the attacks separately, and the essential parameters of the classifier are optimized using the Atom search optimization algorithm. The proposed approach was evaluated on benchmark datasets, CCIDS2019 and UNSW-NB15, using key classification metrics and achieved 99.37% accuracy for the UNSW-NB15 dataset and 99.33% for the CICIDS2019 dataset. The investigational results demonstrate that the suggested approach performs superior to other competitive techniques in identifying DoS and DDoS attacks.",
    "published_date": "2023-08-17",
    "pdf_link": "https://arxiv.org/pdf/2308.08803v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Multi-class intrusion detection with focus on DoS and DDoS attack detection in network traffic",
      "attack_types": [
        "DoS",
        "DDoS",
        "DNS-based DDoS",
        "NTP-based DDoS",
        "NetBIOS-based DDoS",
        "SYN-based DDoS",
        "MSSQL-based DDoS",
        "UDP-Lag DDoS",
        "LDAP-based DDoS",
        "SNMP-based DDoS"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "GAN",
        "specific": "DCGAN",
        "novel_contribution": "Used to oversample minority attack classes to address class imbalance in UNSW-NB15 and CICIDS2019."
      },
      {
        "type": "primary",
        "category": "CNN (Residual Network)",
        "specific": "ResNet-50 (1D)",
        "novel_contribution": "Applied as a 1D feature extractor with residual blocks to derive critical features from preprocessed flow data; dropout 0.2."
      },
      {
        "type": "primary",
        "category": "CNN",
        "specific": "AlexNet (1D)",
        "novel_contribution": "Customized lightweight 1D AlexNet (3 conv, 3 pooling, 2 FC, 2 LRN, Softmax) as the final classifier."
      },
      {
        "type": "primary",
        "category": "Metaheuristic Optimization",
        "specific": "Atom Search Optimization (ASO)",
        "novel_contribution": "Optimizes classifier hyperparameters (momentum, weight decay, epochs, initial learning rate, mini-batch size)."
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised (GAN for augmentation)"
    ],
    "datasets": [
      {
        "name": "UNSW-NB15",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CICIDS2019 (CSE-CIC-IDS2019)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "accuracy",
      "precision",
      "recall",
      "f1-score"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Traditional machine learning techniques are inefficient in handling extensive, high-dimensional network data and cannot extract high-level features for attack detection.",
        "Existing techniques predominantly perform binary classification rather than multi-class attack detection.",
        "Imbalanced class distributions in benchmark IDS datasets hinder effective training and evaluation."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Develop a deep learning-based IDS that can perform multi-class detection of DoS and DDoS attacks, address dataset class imbalance via augmentation, and improve feature extraction and classification performance over traditional ML approaches.",
      "potential_research_ideas": [
        "Evaluate cross-dataset generalization and domain adaptation (train on one dataset, test on another) to assess robustness.",
        "Replace DCGAN with tabular-specific generative models (e.g., CTGAN, TVAE, or WGAN-GP) and quantify augmentation quality and downstream gains.",
        "Introduce cost-sensitive/focal loss or class-balanced reweighting as an alternative or complement to oversampling and compare against DCGAN.",
        "Incorporate temporal modeling (TCN, LSTM/GRU, or Transformers) over flow sequences for attacks with temporal signatures.",
        "Adversarial robustness study for IDS (evasion/poisoning attacks) and robust training (adversarial training, randomized smoothing).",
        "Explainability for security analysts (e.g., SHAP/Integrated Gradients) and measure trust/calibration of predictions.",
        "Real-time streaming deployment study with throughput/latency optimization and concept drift handling (online learning).",
        "Uncertainty estimation and selective classification for high-stakes alerts (MC-Dropout, deep ensembles).",
        "Privacy-preserving training/inference (federated learning or DP-SGD) for cross-organizational IDS collaboration.",
        "Lightweight architectures or pruning/quantization/distillation for edge or high-speed networks."
      ],
      "architectural_improvement_recommendations": [
        "Use a single end-to-end architecture (e.g., 1D ResNet/ResNeXt with SE/CBAM attention) to jointly learn features and classification, potentially simplifying the pipeline.",
        "Adopt modern tabular deep learning models (TabNet, FT-Transformer, SAINT) which can better handle heterogeneous flow features than 1D CNNs on flattened vectors.",
        "Replace DCGAN with WGAN-GP or CTGAN specifically tailored for tabular data; add augmentation evaluation (TSTR/TSNE, detection of synthetic leakage).",
        "Introduce focal loss or class-balanced loss to mitigate imbalance without over-reliance on synthetic data.",
        "Apply Bayesian or population-based hyperparameter optimization (BOHB/Optuna) for more sample-efficient tuning than ASO; include k-fold CV.",
        "Add calibration techniques (temperature scaling) and report ECE/MCE along with accuracy/F1.",
        "Incorporate feature embedding layers with normalization and embedding dropout; try mixup/cutmix for tabular data.",
        "Evaluate and report inference latency/throughput; consider pruning/quantization and ONNX/TensorRT deployment."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "Keras",
        "TensorFlow"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Python + Keras (TensorFlow backend); Intel Core i7-7700 CPU, 32 GB RAM; no GPU reported; train/test split 70/30; 100 epochs; initial learning rate 0.001; mini-batch size 32; momentum 0.9; weight decay 0.005; dropout 0.2 in ResNet-50."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposed an effective deep learning-based IDS using a customized 1D AlexNet classifier for multi-class DoS/DDoS detection.",
      "Applied DCGAN-based data augmentation to mitigate class imbalance on CICIDS2019 and UNSW-NB15.",
      "Used a 1D ResNet-50 as a feature extractor to obtain critical features for each class.",
      "Optimized classifier hyperparameters using Atom Search Optimization (ASO) to reduce learning complexity and improve accuracy.",
      "Conducted evaluations on UNSW-NB15 and CICIDS2019, reporting 99.37% accuracy on UNSW-NB15 and 99.33% on CICIDS2019, and stated superiority over competitive techniques."
    ]
  },
  {
    "arxiv_id": "2308.10236v2",
    "title": "FedSIS: Federated Split Learning with Intermediate Representation Sampling for Privacy-preserving Generalized Face Presentation Attack Detection",
    "authors": "Naif Alkhunaizi; Koushik Srivatsan; Faris Almalik; Ibrahim Almakky; Karthik Nandakumar",
    "abstract": "Lack of generalization to unseen domains/attacks is the Achilles heel of most face presentation attack detection (FacePAD) algorithms. Existing attempts to enhance the generalizability of FacePAD solutions assume that data from multiple source domains are available with a single entity to enable centralized training. In practice, data from different source domains may be collected by diverse entities, who are often unable to share their data due to legal and privacy constraints. While collaborative learning paradigms such as federated learning (FL) can overcome this problem, standard FL methods are ill-suited for domain generalization because they struggle to surmount the twin challenges of handling non-iid client data distributions during training and generalizing to unseen domains during inference. In this work, a novel framework called Federated Split learning with Intermediate representation Sampling (FedSIS) is introduced for privacy-preserving domain generalization. In FedSIS, a hybrid Vision Transformer (ViT) architecture is learned using a combination of FL and split learning to achieve robustness against statistical heterogeneity in the client data distributions without any sharing of raw data (thereby preserving privacy). To further improve generalization to unseen domains, a novel feature augmentation strategy called intermediate representation sampling is employed, and discriminative information from intermediate blocks of a ViT is distilled using a shared adapter network. The FedSIS approach has been evaluated on two well-known benchmarks for cross-domain FacePAD to demonstrate that it is possible to achieve state-of-the-art generalization performance without data sharing. Code: https://github.com/Naiftt/FedSIS",
    "published_date": "2023-08-20",
    "pdf_link": "https://arxiv.org/pdf/2308.10236v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Biometric Security",
      "subdomain": "Presentation Attack Detection (Face Anti-Spoofing)",
      "specific_problem": "Privacy-preserving federated domain generalization for face presentation attack detection using split learning and intermediate representation sampling",
      "attack_types": [
        "print/photo attacks",
        "video replay attacks",
        "3D mask attacks"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "Vision Transformer (ViT)",
        "novel_contribution": "Hybrid ViT architecture (CNN tokenizer + ViT encoder + linear head) with federated split learning for client alignment"
      },
      {
        "type": "primary",
        "category": "CNN",
        "specific": "Convolutional tokenizer",
        "novel_contribution": "CNN-based tokenizer to better capture micro-texture cues for FacePAD"
      },
      {
        "type": "primary",
        "category": "Adapter Modules",
        "specific": "Shared adapter network",
        "novel_contribution": "Distills discriminative information from intermediate ViT blocks across clients"
      },
      {
        "type": "primary",
        "category": "Federated Learning",
        "specific": "Federated averaging with client-specific tokenizer and head",
        "novel_contribution": "Combined with split learning of the shared encoder and adapter to handle non-iid client data"
      },
      {
        "type": "primary",
        "category": "Split Learning",
        "specific": "FeSTA-inspired federated split learning",
        "novel_contribution": "Server-side shared encoder and adapter trained via split learning to align domains"
      },
      {
        "type": "primary",
        "category": "Data Augmentation",
        "specific": "Intermediate Representation Sampling",
        "novel_contribution": "Randomly samples intermediate ViT block outputs as features during training and inference to improve generalization"
      },
      {
        "type": "baseline",
        "category": "Federated Learning",
        "specific": "FedAvg",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Federated Learning",
        "specific": "FedProx",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "ViT (ECCV'22 FacePAD method)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Federated Learning",
      "Split Learning"
    ],
    "datasets": [
      {
        "name": "OULU-NPU",
        "type": "public",
        "domain": "face_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CASIA-MFSD (CASIA-FASD)",
        "type": "public",
        "domain": "face_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Idiap Replay-Attack",
        "type": "public",
        "domain": "face_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "MSU-MFSD",
        "type": "public",
        "domain": "face_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CelebA-Spoof (auxiliary dataset)",
        "type": "public",
        "domain": "face_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "ViT (ECCV'22) [14]",
        "paper_reference": "[14] (ECCV 2022 ViT-based FacePAD)",
        "metric": "Avg. HTER on Benchmark 1 (O&C&I→M, O&M&I→C, O&C&M→I, I&C&M→O) with CelebA-Spoof auxiliary",
        "their_result": null,
        "baseline_result": "6.00"
      },
      {
        "method_name": "FedAvg [35]",
        "paper_reference": "[35]",
        "metric": "Avg. HTER on Benchmark 1",
        "their_result": null,
        "baseline_result": "10.15"
      },
      {
        "method_name": "FedProx [23]",
        "paper_reference": "[23]",
        "metric": "Avg. HTER on Benchmark 1",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Shao et al. (feature disentanglement federated FacePAD) [43, 42]",
        "paper_reference": "[43, 42]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "HTER",
      "AUC",
      "TPR@FPR=1%"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can a privacy-preserving collaborative learning framework (without data sharing) achieve strong domain generalization for FacePAD?",
        "Does combining federated learning with split learning alleviate non-iid client distributions while enabling generalization to unseen domains?",
        "Can sampling intermediate ViT representations and distilling them via a shared adapter improve cross-domain performance?"
      ],
      "gaps_identified": [
        "Most DG-FacePAD methods assume centralized access to multi-domain data, which is unrealistic due to privacy/legal constraints.",
        "Standard FL struggles with non-iid client data and generalization to unseen target domains.",
        "Prior federated FacePAD approaches rely on cumbersome feature disentanglement, auxiliary depth supervision, and reconstruction losses."
      ],
      "limitations": [
        "Predictions are non-deterministic because intermediate representation sampling is also applied during inference.",
        "Relies on a server-hosted shared encoder/adapter; privacy is by architecture (no raw data sharing) without formal guarantees stated."
      ],
      "future_work": [],
      "motivation": "Enable privacy-preserving training of generalized FacePAD models across multiple entities with heterogeneous data, overcoming non-iid distributions and unseen-domain generalization challenges.",
      "potential_research_ideas": [
        "Incorporate formal privacy guarantees (e.g., secure aggregation, differential privacy, or homomorphic encryption) to quantify privacy leakage from intermediate tokens and gradients.",
        "Develop uncertainty-aware inference to handle non-determinism from block sampling and improve decision calibration across domains.",
        "Extend intermediate representation sampling to learned policies (e.g., reinforcement learning or gating) that adaptively select blocks per sample/domain.",
        "Personalization layers per client (e.g., lightweight adapters) to balance global generalization with local domain specificity in federated settings.",
        "Self-supervised or multi-task pretraining of the shared encoder using large-scale spoof-related proxy tasks to boost generalization.",
        "Communication- and computation-efficient split strategies (e.g., compressed token transmission, quantization) for bandwidth-constrained clients."
      ],
      "architectural_improvement_recommendations": [
        "Add a domain-adversarial or contrastive alignment loss on server-side features to complement the implicit alignment from split learning.",
        "Replace random block sampling with a learnable sampler or mixture-of-experts over blocks and aggregate via attention/ensembling at inference to reduce variance.",
        "Introduce stochastic adapters (e.g., dropout-style or stochastic depth) and ensemble inference to stabilize non-deterministic predictions.",
        "Incorporate lightweight calibration heads (temperature scaling) and per-domain batch norm/adapters to mitigate distribution shifts.",
        "Apply gradient and activation compression/quantization on client-server interfaces to reduce leakage and bandwidth usage."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/Naiftt/FedSIS",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Handling non-iid client data distributions in federated settings",
        "Generalizing to unseen domains/attacks at inference time",
        "Communication overhead for split learning between clients and server",
        "Privacy concerns from transmitting intermediate representations and gradients without formal guarantees"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": true,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces FedSIS, a federated split learning framework with intermediate representation sampling for privacy-preserving domain generalization in FacePAD.",
      "Designs a hybrid ViT (CNN tokenizer + ViT encoder + linear classifier) where client-specific tokenizer/head are trained via FL and the shared encoder via split learning.",
      "Proposes a feature augmentation strategy by sampling intermediate ViT block representations and distilling through a shared adapter network.",
      "Demonstrates state-of-the-art cross-domain FacePAD performance without data sharing on two benchmarks; experiments repeated with multiple seeds; releases code."
    ]
  },
  {
    "arxiv_id": "2308.03314v3",
    "title": "GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis",
    "authors": "Yuqiang Sun; Daoyuan Wu; Yue Xue; Han Liu; Haijun Wang; Zhengzi Xu; Xiaofei Xie; Yang Liu",
    "abstract": "Smart contracts are prone to various vulnerabilities, leading to substantial financial losses over time. Current analysis tools mainly target vulnerabilities with fixed control or data-flow patterns, such as re-entrancy and integer overflow. However, a recent study on Web3 security bugs revealed that about 80% of these bugs cannot be audited by existing tools due to the lack of domain-specific property description and checking. Given recent advances in Large Language Models (LLMs), it is worth exploring how Generative Pre-training Transformer (GPT) could aid in detecting logicc vulnerabilities.   In this paper, we propose GPTScan, the first tool combining GPT with static analysis for smart contract logic vulnerability detection. Instead of relying solely on GPT to identify vulnerabilities, which can lead to high false positives and is limited by GPT's pre-trained knowledge, we utilize GPT as a versatile code understanding tool. By breaking down each logic vulnerability type into scenarios and properties, GPTScan matches candidate vulnerabilities with GPT. To enhance accuracy, GPTScan further instructs GPT to intelligently recognize key variables and statements, which are then validated by static confirmation. Evaluation on diverse datasets with around 400 contract projects and 3K Solidity files shows that GPTScan achieves high precision (over 90%) for token contracts and acceptable precision (57.14%) for large projects like Web3Bugs. It effectively detects ground-truth logic vulnerabilities with a recall of over 70%, including 9 new vulnerabilities missed by human auditors. GPTScan is fast and cost-effective, taking an average of 14.39 seconds and 0.01 USD to scan per thousand lines of Solidity code. Moreover, static confirmation helps GPTScan reduce two-thirds of false positives.",
    "published_date": "2023-08-07",
    "pdf_link": "https://arxiv.org/pdf/2308.03314v3",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Blockchain/Smart Contract Security",
      "subdomain": "Smart Contract Vulnerability Detection",
      "specific_problem": "Detection of logic vulnerabilities in Solidity smart contracts by combining LLM-based code understanding with static analysis",
      "attack_types": [
        "Price manipulation (via AMM/reserves)",
        "Price manipulation by buying tokens",
        "ID-related violations",
        "Erroneous state updates (e.g., Approval Not Cleared)",
        "Atomicity violation",
        "Privilege escalation",
        "Erroneous accounting",
        "Risky First Deposit",
        "Wrong Interest Rate Order",
        "Wrong Checkpoint Order",
        "Front-running",
        "Flashloan-based vote manipulation",
        "Slippage",
        "Unauthorized transfer"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "GPT-3.5-turbo",
        "novel_contribution": "GPT used as a code-understanding engine with scenario- and property-based matching, guided extraction of key variables/statements, and a “mimic-in-the-background” prompting trick (self-consistency style majority answer) combined with static confirmation."
      }
    ],
    "learning_paradigm": [
      "Zero-shot prompting"
    ],
    "datasets": [
      {
        "name": "Top200",
        "type": "public",
        "domain": "smart_contract_source",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Web3Bugs",
        "type": "public",
        "domain": "smart_contract_source",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "DefiHacks",
        "type": "public",
        "domain": "smart_contract_source",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "precision",
      "recall",
      "F1 score",
      "false positive rate (FPR)",
      "scan time per KLOC",
      "cost per KLOC"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How can GPT be effectively combined with static analysis to detect logic vulnerabilities in smart contracts?",
        "Can breaking vulnerability types into code-level scenarios and properties enable GPT to match code-level semantics reliably?",
        "How to efficiently narrow candidate functions for GPT to mitigate context/cost limits while maintaining recall?",
        "Can static confirmation modules reliably validate GPT-identified key variables/statements and reduce false positives?"
      ],
      "gaps_identified": [
        "Existing tools mainly target fixed control/data-flow patterns (e.g., reentrancy, integer overflow) and miss ≈80% of Web3 bugs related to business logic.",
        "Naive GPT approaches with high-level descriptions lead to very high false positives and require advanced models (e.g., GPT-4) and large context.",
        "LLMs have limited logical reasoning for fine-grained code properties and order of operations; need verification by program analysis.",
        "Feeding entire multi-file projects to LLMs is infeasible due to context size and cost constraints."
      ],
      "limitations": [
        "Manually defined scenarios and properties currently cover ten logic vulnerability types; coverage is limited.",
        "LLM (GPT) may produce unreliable answers or miss nuances (e.g., misunderstanding statement order like “before/after”) without static confirmation.",
        "Context-window and cost constraints prevent analyzing whole large projects directly with GPT; requires filtering.",
        "Lower precision on large projects (e.g., 57.14% on Web3Bugs) compared to token contracts.",
        "Reliance on proprietary LLM (GPT-3.5-turbo) and its pre-trained knowledge; behavior may change over time."
      ],
      "future_work": [
        "Scale to more logic vulnerability types using a GPT-4-based pipeline to automatically derive scenario/property sentences from past reports and validate them.",
        "Broaden applicability and address current limitations (e.g., multi-file reasoning, larger contexts)."
      ],
      "motivation": "Detect business-logic vulnerabilities in smart contracts that are largely missed by existing static analysis tools, by leveraging LLM code understanding combined with program analysis.",
      "potential_research_ideas": [
        "Automatically mine and maintain a living knowledge base of scenario/property templates from new audits and incidents, with continual validation on ground-truth code.",
        "Integrate symbolic execution or SMT-based validation targeted at the GPT-extracted key variables/statements for stronger confirmation.",
        "Incorporate inter-contract and cross-transaction reasoning (e.g., DeFi protocol interactions, MEV contexts) via trace- or simulation-based augmentation to GPT prompts.",
        "Use retrieval-augmented prompting over project-specific documentation, ABI/specs, and known patterns to improve GPT’s semantic grounding.",
        "Evaluate and fine-tune open-source code LLMs for Solidity with instruction tuning on vulnerability reasoning steps and structured outputs.",
        "Extend to other languages and ecosystems (Vyper, Move, Rust/Solana) and multi-chain contract interactions.",
        "Adopt self-consistency with confidence calibration and structured, verifiable intermediate steps (program-of-thought) for variable/statement mapping.",
        "Develop cost-aware scheduling across files/functions using static heuristics and learned prioritization (budgeted detection)."
      ],
      "architectural_improvement_recommendations": [
        "Add a retrieval layer to assemble minimal, semantically relevant slices (program slicing + call/context summaries) before prompting the LLM.",
        "Adopt longer-context or hierarchical prompting (file -> function -> slice) with caching and deduplication to handle large projects efficiently.",
        "Combine LLM reasoning with symbolic execution/smt checks for properties like value bounds, order constraints, and privilege preconditions.",
        "Use constrained decoding (grammar-guided) to force structured outputs for variables/statements and evidence paths usable by static analyzers.",
        "Introduce uncertainty estimation (e.g., self-consistency votes + agreement scoring) to gate static confirmation and reduce unnecessary checks.",
        "Integrate dynamic validation via lightweight fuzzing/property-based tests auto-generated from detected scenarios/properties.",
        "Leverage inter-contract CFG/DFG graphs and graph-based ranking to prioritize suspicious functions prior to LLM matching."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "OpenAI GPT-3.5 API",
        "ANTLR",
        "crytic-compiler"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "GPT-3.5-turbo, default 4k context, temperature set to 0; average 14.39 seconds and ~$0.01 per 1K lines of Solidity; ~0.018 USD and ~20s per KLOC on larger projects (Web3Bugs/DefiHacks)."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Cloud-based security scanning platform (MetaScan integration)",
      "scalability_discussed": true,
      "inference_time": "Average 14.39 seconds per 1K lines of code",
      "deployment_challenges": [
        "LLM context window and cost constraints for multi-file projects",
        "LLM non-determinism and potential reasoning errors",
        "Need for static confirmation to reduce false positives",
        "Efficient candidate function filtering to control latency and cost"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduce GPTScan, the first tool combining GPT with static analysis for detecting logic vulnerabilities in smart contracts.",
      "Break down logic vulnerabilities into code-level scenarios and properties for direct GPT matching.",
      "Multi-dimensional filtering to narrow candidate functions for LLM analysis.",
      "Guided extraction of key variables/statements via GPT, followed by specialized static confirmation modules.",
      "A “mimic-in-the-background” prompting trick to reduce randomness and improve answer reliability.",
      "Comprehensive evaluation on three datasets (~400 projects, ~3K Solidity files, 62 ground-truth logic vulnerabilities).",
      "High precision on token contracts (>90%), acceptable precision on large projects (57.14%), recall >70%; F1: 67.8% (Web3Bugs), 80% (DefiHacks).",
      "Static confirmation reduces ~65.84% of false positives on Web3Bugs.",
      "Fast and cost-effective scanning (~14.39s and ~$0.01 per KLOC).",
      "Integration into MetaScan platform; evaluation data available online."
    ]
  },
  {
    "arxiv_id": "2308.16570v1",
    "title": "MONDEO: Multistage Botnet Detection",
    "authors": "Duarte Dias; Bruno Sousa; Nuno Antunes",
    "abstract": "Mobile devices have widespread to become the most used piece of technology. Due to their characteristics, they have become major targets for botnet-related malware. FluBot is one example of botnet malware that infects mobile devices. In particular, FluBot is a DNS-based botnet that uses Domain Generation Algorithms (DGA) to establish communication with the Command and Control Server (C2). MONDEO is a multistage mechanism with a flexible design to detect DNS-based botnet malware. MONDEO is lightweight and can be deployed without requiring the deployment of software, agents, or configuration in mobile devices, allowing easy integration in core networks. MONDEO comprises four detection stages: Blacklisting/Whitelisting, Query rate analysis, DGA analysis, and Machine learning evaluation. It was created with the goal of processing streams of packets to identify attacks with high efficiency, in the distinct phases. MONDEO was tested against several datasets to measure its efficiency and performance, being able to achieve high performance with RandomForest classifiers. The implementation is available at github.",
    "published_date": "2023-08-31",
    "pdf_link": "https://arxiv.org/pdf/2308.16570v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Botnet Detection",
      "specific_problem": "DNS-based botnet detection for mobile malware using DGA (e.g., FluBot) via multistage analysis of DNS requests only",
      "attack_types": [
        "DGA-based C2 communication",
        "Botnet infection/command-and-control",
        "DDoS facilitation (via botnet)",
        "Ransomware-related botnet activity"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Ensemble (Tree-based)",
        "specific": "Random Forest (scikit-learn)",
        "novel_contribution": "Used as Phase 4 classifier in a multistage DNS-only pipeline; reported as achieving high performance"
      },
      {
        "type": "baseline",
        "category": "Anomaly Detection (Tree-based)",
        "specific": "Isolation Forest (scikit-learn)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "SVM",
        "specific": "Support Vector Machine (scikit-learn)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "MLP/Neural Network",
        "specific": "MLPClassifier (scikit-learn) (referred as MPLC in the text)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "DGA Detector",
        "specific": "Intel DGA",
        "novel_contribution": "Integrated as Phase 3 with acceptance/rejection thresholds and feedback to lists; off-the-shelf detector used within the pipeline"
      },
      {
        "type": "primary",
        "category": "DGA Detector",
        "specific": "DGA Detective (SOCCRATES)",
        "novel_contribution": "Integrated as Phase 3 with acceptance/rejection thresholds and feedback to lists; off-the-shelf detector used within the pipeline"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised",
      "Rule-based"
    ],
    "datasets": [
      {
        "name": "Volunteer DNS traffic captured on ISC BIND server (about 20 users over three months)",
        "type": "private",
        "domain": "network_traffic (DNS queries)",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "FluBot Android malware DNS captures (UPS/Correos/DHL samples in Android Studio sandbox)",
        "type": "proprietary",
        "domain": "network_traffic (DNS queries)",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "Alexa Top 1 Million-derived benign DNS requests",
        "type": "public",
        "domain": "network_traffic (DNS queries derived from domain list)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Isolation Forest (scikit-learn)",
        "paper_reference": null,
        "metric": "Accuracy, Precision, Recall, F1-score",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "SVM (scikit-learn)",
        "paper_reference": null,
        "metric": "Accuracy, Precision, Recall, F1-score",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "MLPClassifier (scikit-learn)",
        "paper_reference": null,
        "metric": "Accuracy, Precision, Recall, F1-score",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Precision",
      "Recall",
      "F1-Score",
      "Accuracy",
      "Support",
      "Packets Processed (%) per phase",
      "Processing Time (ms) per phase",
      "Final Classification (Benign/Infected)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Majority of DNS-based botnet detection works rely on DNS response features, which take longer for accurate detection; MONDEO uses DNS requests only.",
        "Many existing botnet detection approaches require agents/software on end devices; MONDEO avoids device-side deployment.",
        "Protocol-independent methods like BotMark lack white/blacklisting support, risking false negatives for legitimate apps with botnet-like patterns."
      ],
      "limitations": [
        "Adding to a whitelist without manual confirmation can be dangerous from a security standpoint.",
        "Phase 4 (ML) takes more time and should evaluate fewer requests to reduce impact on detection latency.",
        "PoC whitelist/blacklist implementation uses linear search (O(n)) with FLD stripping, which is less efficient than hash-based or binary structures."
      ],
      "future_work": [],
      "motivation": "Provide a lightweight, deployable, DNS-request-only multistage mechanism for detecting DGA-based botnets (e.g., FluBot) in operator networks without requiring agents on mobile devices.",
      "potential_research_ideas": [
        "Sequence modeling of per-host DNS query streams (e.g., temporal Transformers or RNNs) to capture DGA bursts and periodicity beyond per-request features.",
        "Online learning with concept drift detection for evolving DGA families and changing benign traffic patterns.",
        "Federated or split learning across multiple operator DNS resolvers to improve generalization without sharing raw DNS logs.",
        "Active learning loop for uncertain domains to minimize labeling effort and safely expand black/whitelists.",
        "Hybrid DGA detection that ensembles multiple DGA detectors with meta-learning calibration for robust thresholds.",
        "Leverage side-channel features (e.g., EDNS0 sizes, QNAME entropy, NXDOMAIN rates) while still avoiding DNS answers.",
        "Benchmarking on public DGA corpora and mixed benign traffic (e.g., DGArchive-like feeds) to enable fair comparison.",
        "Adversarial robustness evaluation against DGA-hardened malware that mimics benign lexical statistics or throttles query rates."
      ],
      "architectural_improvement_recommendations": [
        "Replace PoC linear search for white/blacklists with hash-based sets or compressed Bloom filters (plus periodic false-positive audits).",
        "Introduce a streaming feature store and message bus (e.g., Kafka + RocksDB) so Phase 2/4 can operate statefully and horizontally scale.",
        "Calibrate the Phase 4 classifier outputs (e.g., Platt scaling or isotonic regression) and set cost-sensitive thresholds for operator triage.",
        "Adopt an incremental/online Random Forest or gradient boosted trees (e.g., XGBoost/LightGBM) for better latency-accuracy trade-offs.",
        "Implement a per-subscriber rate limiter and token-bucket tracking in Phase 2 to normalize across different user activity levels.",
        "Add explainability hooks (feature importances, SHAP) and per-decision rationale to assist SOC analysts.",
        "Harden the feedback loop with human-in-the-loop approvals and decay/TTL for whitelist entries to reduce long-lived poisoning risk."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": null,
      "frameworks": [
        "Python",
        "Flask",
        "scikit-learn",
        "Docker",
        "ISC BIND",
        "Android Studio"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Evaluated on a VM with 4 vCPUs and 16 GB RAM; two Docker containers (core and DGA components); RESTful API PoC."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Operator/core network DNS infrastructure; tested on a deployed DNS server with volunteer traffic and in a VM.",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Safely managing automatic whitelist/blacklist updates via feedback loop (risk of poisoning or premature whitelisting).",
        "Efficient state management for high-rate DNS streams in Phase 2 without excessive memory/time overhead.",
        "Balancing latency and accuracy given that the ML phase is slower and should process fewer requests.",
        "Operating solely on DNS requests may miss signals present only in DNS responses in some scenarios."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "A flexible, multistage DNS-request-only pipeline (black/whitelisting, query-rate analysis, DGA analysis, ML classifier) for botnet detection targeting mobile malware (e.g., FluBot).",
      "Lightweight deployment design that avoids installing agents/software on mobile devices; suitable for operator core networks.",
      "Integration of off-the-shelf DGA detectors (Intel DGA, DGA Detective) with configurable accept/reject thresholds and a feedback loop to update lists.",
      "Feature set and supervised ML evaluation with multiple classifiers; RandomForest reported as achieving high performance.",
      "Open-source PoC implemented with Python Flask and Docker; evaluated on real volunteer DNS traffic and lab-generated FluBot samples."
    ]
  },
  {
    "arxiv_id": "2308.04467v1",
    "title": "EPS: Distinguishable IQ Data Representation for Domain-Adaptation Learning of Device Fingerprints",
    "authors": "Abdurrahman Elmaghbub; Bechir Hamdaoui",
    "abstract": "Deep learning (DL)-based RF fingerprinting (RFFP) technology has emerged as a powerful physical-layer security mechanism, enabling device identification and authentication based on unique device-specific signatures that can be extracted from the received RF signals. However, DL-based RFFP methods face major challenges concerning their ability to adapt to domain (e.g., day/time, location, channel, etc.) changes and variability. This work proposes a novel IQ data representation and feature design, termed Double-Sided Envelope Power Spectrum or EPS, that is proven to overcome the domain adaptation problems significantly. By accurately capturing device hardware impairments while suppressing irrelevant domain information, EPS offers improved feature selection for DL models in RFFP. Experimental evaluations demonstrate its effectiveness, achieving over 99% testing accuracy in same-day/channel/location evaluations and 93% accuracy in cross-day evaluations, outperforming the traditional IQ representation. Additionally, EPS excels in cross-location evaluations, achieving a 95% accuracy. The proposed representation significantly enhances the robustness and generalizability of DL-based RFFP methods, thereby presenting a transformative solution to IQ data-based device fingerprinting.",
    "published_date": "2023-08-08",
    "pdf_link": "https://arxiv.org/pdf/2308.04467v1",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Wireless Security",
      "subdomain": "Physical-layer Device Fingerprinting",
      "specific_problem": "Improving domain adaptation/generalization for deep-learning RF fingerprinting via a new IQ data representation",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Feature Representation",
        "specific": "Double-Sided Envelope Power Spectrum (EPS)",
        "novel_contribution": "Introduces EPS, a new RF IQ data representation that captures oscillator-induced hardware impairments (e.g., CFO) while suppressing domain-specific information to improve generalization."
      },
      {
        "type": "primary",
        "category": "CNN",
        "specific": null,
        "novel_contribution": "Uses standard CNNs as the classifier; the novelty is in the input representation (EPS) rather than architecture."
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": null,
        "novel_contribution": "Baseline uses the same CNN trained on conventional time-domain IQ samples."
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "IEEE 802.11b WiFi RF dataset of 15 Pycom devices (8TB)",
        "type": "public",
        "domain": "rf_iq_samples",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "Wired Setup subset (part of Pycom 802.11b dataset)",
        "type": "public",
        "domain": "rf_iq_samples",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "Wireless Setup subset (part of Pycom 802.11b dataset)",
        "type": "public",
        "domain": "rf_iq_samples",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "Different Locations Setup subset (part of Pycom 802.11b dataset)",
        "type": "public",
        "domain": "rf_iq_samples",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "Random Deployment Setup subset (part of Pycom 802.11b dataset)",
        "type": "public",
        "domain": "rf_iq_samples",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "CNN on conventional time-domain IQ representation",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "95% (cross-location testing accuracy using EPS with the same CNN)",
        "baseline_result": "55% (cross-location testing accuracy using conventional IQ with the same CNN)"
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "Testing accuracy",
      "Average testing accuracy"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can a new RF IQ data representation that emphasizes oscillator-related impairments improve cross-domain generalization for DL-based RF fingerprinting?",
        "What is the relationship between carrier frequency offset (CFO) and the envelope behavior of IQ signals, and can it be exploited for robust device fingerprints?",
        "How do domain changes (day/time, location, channel) and hardware warm-up affect RF fingerprinting performance?"
      ],
      "gaps_identified": [
        "DL-based RFFP approaches relying on raw IQ often overfit to domain-specific factors and fail to generalize to new domains.",
        "Channel equalization can remove device-specific discriminative features, harming RFFP performance.",
        "Impairment compensation techniques target specific impairments and lack generalizability across environments.",
        "Adversarial/domain adaptation techniques do not scale well to medium/large device counts and adapt to specific target domains, struggling on unseen domains."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Improve robustness and generalizability of DL-based RF fingerprinting by designing an RF data representation that captures device-specific hardware impairments (notably oscillator/CFO) while suppressing domain-specific information.",
      "potential_research_ideas": [
        "Extend EPS-based fingerprinting to other wireless protocols and bands (e.g., LoRa, BLE, 5G NR) and multi-rate modulations.",
        "Combine EPS with complementary representations (e.g., cyclostationary features, higher-order spectra) for multi-view learning.",
        "Integrate domain generalization objectives (e.g., IRM, CORAL, MMD) on top of EPS features to further reduce domain leakage.",
        "Explore self-supervised or contrastive pretraining on EPS to reduce labeled data needs and improve few-shot device enrollment.",
        "Evaluate robustness against sophisticated impersonation/spoofing with replay or generative RF attacks targeting EPS features.",
        "Leverage multi-receiver diversity or cooperative sensing to stabilize EPS features under severe channel conditions.",
        "Automate warm-up detection and dynamic segment selection to maximize stable EPS feature extraction during deployment."
      ],
      "architectural_improvement_recommendations": [
        "Adopt a multi-branch network that jointly ingests EPS and raw IQ (or spectrogram) with late fusion to capture complementary cues.",
        "Use lightweight transformer or ConvNeXt backbones tailored for 1D/2D spectral inputs to improve capacity without heavy compute.",
        "Introduce contrastive learning or metric learning losses (e.g., triplet, SupCon) on EPS embeddings to sharpen inter-device separation.",
        "Incorporate domain-invariant regularizers (e.g., CORAL/MMD) during training to explicitly penalize domain leakage even with EPS.",
        "Apply adaptive instance normalization or feature-wise linear modulation conditioned on estimated CFO statistics.",
        "Develop a streaming EPS extraction pipeline with on-device DSP optimizations (fixed-point FFT/envelope extraction) for real-time use."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Lab/testbed with 15 Pycom IEEE 802.11b WiFi devices and USRP B210 receiver; cross-day and cross-location indoor scenarios.",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Domain shift across day/time, channel, and location",
        "Wireless channel variability and receiver hardware differences",
        "Oscillator warm-up and stabilization impacting CFO and features",
        "Risk of overfitting to environment-specific cues with conventional IQ inputs"
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes EPS, a novel RF signal representation that substantially enhances accuracy, robustness, and generalizability across domains.",
      "Demonstrates distinguishability and reliability of EPS across time, channel, and location domains.",
      "Releases an 8TB IEEE 802.11b WiFi dataset of 15 Pycom devices (raw and processed files; >5000 packets per device) across four scenarios.",
      "Assesses EPS with standard CNNs showing >99% same-domain accuracy, ~93% cross-day, and 95% cross-location accuracy.",
      "Highlights the impact of local oscillator frequency instability during hardware warm-up on DL-based RFFP performance."
    ]
  },
  {
    "arxiv_id": "2308.10055v1",
    "title": "Robust Fraud Detection via Supervised Contrastive Learning",
    "authors": "Vinay M. S.; Shuhan Yuan; Xintao Wu",
    "abstract": "Deep learning models have recently become popular for detecting malicious user activity sessions in computing platforms. In many real-world scenarios, only a few labeled malicious and a large amount of normal sessions are available. These few labeled malicious sessions usually do not cover the entire diversity of all possible malicious sessions. In many scenarios, possible malicious sessions can be highly diverse. As a consequence, learned session representations of deep learning models can become ineffective in achieving a good generalization performance for unseen malicious sessions. To tackle this open-set fraud detection challenge, we propose a robust supervised contrastive learning based framework called ConRo, which specifically operates in the scenario where only a few malicious sessions having limited diversity is available. ConRo applies an effective data augmentation strategy to generate diverse potential malicious sessions. By employing these generated and available training set sessions, ConRo derives separable representations w.r.t open-set fraud detection task by leveraging supervised contrastive learning. We empirically evaluate our ConRo framework and other state-of-the-art baselines on benchmark datasets. Our ConRo framework demonstrates noticeable performance improvement over state-of-the-art baselines.",
    "published_date": "2023-08-19",
    "pdf_link": "https://arxiv.org/pdf/2308.10055v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Fraud Detection",
      "subdomain": "Insider Threat Detection",
      "specific_problem": "Open-set fraud detection on user activity sessions with few labeled malicious samples and high malicious-session diversity",
      "attack_types": [
        "insider threat",
        "account misuse",
        "data exfiltration",
        "removable media misuse",
        "suspicious web access/job-seeking",
        "vandalism (Wikipedia)",
        "cloud misuse (OpenStack)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Contrastive Learning",
        "specific": "Supervised Contrastive Learning (SupCon)",
        "novel_contribution": "Two-stage supervised contrastive framework (ConRo) tailored for open-set fraud detection with limited malicious labels; alternating optimization with DeepSVDD to shape representation topology"
      },
      {
        "type": "primary",
        "category": "RNN/LSTM",
        "specific": "LSTM session encoder (2 hidden layers, mean-pooled)",
        "novel_contribution": "Session encoder for sequential user activities to produce representations suitable for supervised contrastive and DeepSVDD objectives"
      },
      {
        "type": "primary",
        "category": "One-class/Deep SVDD",
        "specific": "DeepSVDD",
        "novel_contribution": "Used jointly (alternating) with supervised contrastive loss to compact normal sessions into a minimal hypersphere and separate seen-malicious sessions"
      },
      {
        "type": "primary",
        "category": "Data Augmentation",
        "specific": "Mixup (representation-space)",
        "novel_contribution": "Generates similar potential malicious sessions around seen-malicious exemplars to address intra-class diversity (MO1)"
      },
      {
        "type": "primary",
        "category": "Data Augmentation",
        "specific": "Affine combination with filtering",
        "novel_contribution": "Generates diverse potential malicious sessions using affine combinations across malicious exemplars with false-positive filtering via hypersphere radius (MO2) to approximate unseen clusters"
      },
      {
        "type": "baseline",
        "category": "One-class/Deep SVDD",
        "specific": "DeepSVDD (Ruff et al., 2018)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Semi-supervised anomaly detection",
        "specific": "Deep SAD (Ruff et al., 2020)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Deviation-based anomaly detection",
        "specific": "Deviation Network (Pang et al., 2019; 2021a)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Open-set anomaly detection",
        "specific": "Ding et al., 2022 (open-set deep anomaly detection for images)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "CERT",
        "type": "public",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "UMD-Wikipedia",
        "type": "public",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Open-stack",
        "type": "public",
        "domain": "log_files",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "DeepSVDD",
        "paper_reference": "Ruff et al., 2018",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Deep SAD",
        "paper_reference": "Ruff et al., 2020",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Deviation Network (DevNet/Deviation-based)",
        "paper_reference": "Pang et al., 2019; Pang et al., 2021a",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Open-set deep anomaly detection (image domain)",
        "paper_reference": "Ding et al., 2022",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "How to learn session representations that generalize to unseen malicious sessions when only a few labeled malicious sessions with limited diversity are available?",
        "Can supervised contrastive learning, combined with representation-space augmentation, improve open-set fraud detection?",
        "How to generate and filter augmented malicious sessions to approximate unseen malicious clusters without introducing excessive label noise?"
      ],
      "gaps_identified": [
        "Existing deep anomaly detection methods using few anomalies (metric learning or deviation loss) overfit to seen anomalies and generalize poorly to unseen anomalies.",
        "Open-set anomaly detection methods tailored to images do not directly address session-log fraud detection where even normal sessions are diverse.",
        "Conventional augmentations in contrastive learning are too weak to generate diverse malicious sessions representative of unseen clusters."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Malicious user activity sessions are highly diverse, and only a few labeled malicious sessions are typically available; this mismatch harms generalization to unseen malicious sessions, making fraud detection an open-set problem requiring robust representation learning.",
      "potential_research_ideas": [
        "Incorporate generative models (e.g., conditional VAEs/flow models) to synthesize malicious session embeddings conditioned on learned cluster prototypes for broader unseen coverage.",
        "Prototype- or centroid-based supervised contrastive learning with adaptive hard-negative mining to better separate overlapping normal/unseen-malicious regions.",
        "Distributionally robust optimization or worst-case contrastive training over uncertainty sets around normal hypersphere boundaries to reduce overlap risk.",
        "Leverage semi-supervised or self-supervised pretraining on large unlabeled logs, followed by supervised contrastive fine-tuning with few anomalies.",
        "Temporal-structure-aware augmentations (e.g., subsequence shuffling constraints, time-warping) that preserve session semantics while inducing diversity.",
        "Hybrid encoder architectures (Transformer + temporal convolution) for long-range dependencies in sessions, compared to LSTM-only encoders.",
        "Online/streaming open-set fraud detection with continual contrastive updates and replay/memory of hard negatives.",
        "Calibrated open-set decision rules using energy-based scores or conformal prediction for reliable thresholds under distribution shift.",
        "Multi-view contrastive training combining heterogeneous activity modalities (e.g., file, email, web, device) with view-specific encoders and co-regularization.",
        "Graph-enhanced session modeling (user-resource interaction graphs) with GNN encoders and contrastive objectives across sequence and graph views."
      ],
      "architectural_improvement_recommendations": [
        "Replace or augment LSTM with Transformer encoder plus attention pooling to capture long-range dependencies and improve representation quality.",
        "Introduce prototype loss alongside supervised contrastive loss to tighten intra-class clusters (normal and malicious) and stabilize training.",
        "Implement hard-negative mining near the normal hypersphere boundary to focus contrastive learning on ambiguous regions.",
        "Use a learnable soft-boundary DeepSVDD (with radius as a parameter) or center-loss variants and jointly optimize with SupCon instead of alternating only.",
        "Add energy-based or Mahalanobis distance scoring heads for OOD/open-set calibration and combine with contrastive embeddings.",
        "Adopt curriculum-based augmentation strength scheduling (anneal λ ranges) to gradually expose the model to more diverse synthetic anomalies.",
        "Leverage self-supervised pretraining on sessions (e.g., masked activity modeling) to initialize the encoder before the two-stage ConRo training.",
        "Perform augmentation in both representation and input spaces with semantics-preserving constraints (e.g., activity-type aware mixup)."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces ConRo, a robust supervised contrastive learning framework for open-set fraud detection with limited malicious labels.",
      "Designs an LSTM-based session encoder trained with alternating supervised contrastive and DeepSVDD losses to shape a separable representation space.",
      "Proposes representation-space augmentation strategies (similar via mixup and diverse via affine combinations) plus false-positive filtering to approximate unseen malicious sessions.",
      "Provides theoretical generalization analysis and empirical evaluation on three benchmark datasets (CERT, UMD-Wikipedia, Open-stack) demonstrating noticeable improvements over state-of-the-art baselines."
    ]
  },
  {
    "arxiv_id": "2307.15915v1",
    "title": "JFinder: A Novel Architecture for Java Vulnerability Identification Based Quad Self-Attention and Pre-training Mechanism",
    "authors": "Jin Wang; Zishan Huang; Hui Xiao; Yinhao Xiao",
    "abstract": "Software vulnerabilities pose significant risks to computer systems, impacting our daily lives, productivity, and even our health. Identifying and addressing security vulnerabilities in a timely manner is crucial to prevent hacking and data breaches. Unfortunately, current vulnerability identification methods, including classical and deep learning-based approaches, exhibit critical drawbacks that prevent them from meeting the demands of the contemporary software industry. To tackle these issues, we present JFinder, a novel architecture for Java vulnerability identification that leverages quad self-attention and pre-training mechanisms to combine structural information and semantic representations. Experimental results demonstrate that JFinder outperforms all baseline methods, achieving an accuracy of 0.97 on the CWE dataset and an F1 score of 0.84 on the PROMISE dataset. Furthermore, a case study reveals that JFinder can accurately identify four cases of vulnerabilities after patching.",
    "published_date": "2023-07-29",
    "pdf_link": "https://arxiv.org/pdf/2307.15915v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Vulnerability Detection",
      "specific_problem": "Java source code vulnerability identification (classifying code snippets as vulnerable or non-vulnerable)",
      "attack_types": [
        "CWE-15 External Control of System or Configuration Setting",
        "CWE-23 Relative Path Traversal",
        "CWE-36 Absolute Path Traversal",
        "CWE-89 SQL Injection",
        "CWE-259 Use of Hard-coded Password",
        "CWE-606 Unchecked Input for Loop Condition"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "UniXcoder (pre-trained programming language model)",
        "novel_contribution": "Use of a pre-trained code model to generate Code Snippet Sequence (CSS) embeddings integrated alongside structural code graphs for vulnerability detection."
      },
      {
        "type": "primary",
        "category": "Attention",
        "specific": "Quad Self-Attention (composed of four Multi-View Self-Attention Encoders)",
        "novel_contribution": "A quad self-attention layer to fuse AST, CFG, DFG, and CSS representations into a single matrix capturing structural and semantic information."
      },
      {
        "type": "primary",
        "category": "CNN",
        "specific": null,
        "novel_contribution": "CNN applied after quad self-attention fusion to extract features for final classification."
      },
      {
        "type": "primary",
        "category": "MLP",
        "specific": null,
        "novel_contribution": "Fully connected layers for final vulnerability classification after CNN."
      },
      {
        "type": "primary",
        "category": "Graph augmentation",
        "specific": "MetaPath (reverse-edge augmentation)",
        "novel_contribution": "Add reverse edges to AST/CFG/DFG to enhance connectivity and completeness of extracted MetaPaths before attention fusion."
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Transfer Learning (Pre-trained embeddings)"
    ],
    "datasets": [
      {
        "name": "SARD CWE-15",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "SARD CWE-23",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "SARD CWE-36",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "SARD CWE-89",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "SARD CWE-259",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "SARD CWE-606",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "PROMISE Camel (Java)",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "PROMISE jEdit (Java)",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "PROMISE Lucene (Java)",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "PROMISE POI (Java)",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "PROMISE Synapse (Java)",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "PROMISE Xalan (Java)",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "PROMISE Xerces (Java)",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Accuracy",
      "Precision",
      "Recall",
      "F1-score"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Existing methods often rely only on structural information (AST/CFG/DFG) or only on semantic embeddings, missing complementary cues.",
        "Prior approaches have shown unsatisfactory performance on real software vulnerability datasets and are unsuitable for industrial application.",
        "Manual feature engineering is time-consuming and deep models tend to overfit small datasets; better use of pre-training is needed."
      ],
      "limitations": [
        "Input code length limited to 512 tokens due to the pre-trained model tokenizer; longer code must be truncated.",
        "Tokenizer issues with UniXcoder (e.g., splitting tokens improperly) significantly degraded accuracy before vocabulary expansion.",
        "CFG/DFG and AST extraction pipelines add preprocessing complexity; robustness across diverse Java codebases not fully assessed.",
        "Evaluation limited to Java; cross-language generalization not studied.",
        "Only aggregate results reported in extract; per-CWE or project-level detailed analysis not shown here."
      ],
      "future_work": [],
      "motivation": "Address critical drawbacks of existing vulnerability identification by fusing structural and semantic information via quad self-attention and leveraging pre-training to achieve industrially viable performance.",
      "potential_research_ideas": [
        "Extend JFinder to multi-language support (e.g., C/C++, Python) with language-agnostic IRs or code property graphs.",
        "Incorporate graph transformers or GNN encoders directly over AST/CFG/DFG rather than reverse-edge augmentation plus attention.",
        "Perform fine-tuning or domain-adaptive pre-training of the code model on vulnerability-focused corpora (e.g., CWE-labeled code, commit diffs).",
        "Add vulnerability localization (span/line-level) in addition to classification via multi-task learning.",
        "Introduce contrastive learning between structural and semantic views to improve representation alignment and robustness.",
        "Leverage long-sequence models (e.g., Longformer/FlashAttention/Transformer-XL) to overcome 512-token limit for large methods/classes.",
        "Integrate static analysis alerts and taint analysis as auxiliary inputs for attention fusion.",
        "Evaluate and adapt the model for just-in-time detection on code changes (commit-level predictions).",
        "Create a hard-negative mining pipeline using patched/unpatched pairs to better capture subtle vulnerability patterns.",
        "Study transfer to real-world industrial repositories and CI/CD integration."
      ],
      "architectural_improvement_recommendations": [
        "Replace or augment MVSA with a Graph Transformer that directly consumes heterogeneous graphs (AST/CFG/DFG) with type-aware attention.",
        "Adopt cross-attention between CSS and each structural graph, followed by gated fusion instead of simple concatenation.",
        "Use hierarchical encoders (function-level, file-level, project-level) with pooling to capture broader context beyond 512 tokens.",
        "Introduce multi-task objectives: vulnerability type (CWE) classification jointly with binary detection to encourage discriminative features.",
        "Apply curriculum learning and hard example mining using patched/unpatched pairs from version histories.",
        "Incorporate uncertainty estimation (MC Dropout/Deep Ensembles) to flag low-confidence predictions.",
        "Use sequence-length-aware architectures (e.g., Performer/FlashAttention-2) to scale attention over long code.",
        "Add adversarial training or data augmentation (identifier renaming, dead-code insertion) to improve robustness."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/WJ-8/JFinder",
      "frameworks": [
        "TensorFlow",
        "Keras",
        "HuggingFace Transformers"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Experiments run on 16-core 2.10 GHz Intel Xeon CPU, 256 GB RAM, NVIDIA RTX 3090 (24 GB VRAM). Source sequence length capped at 512 tokens; training uses cross-entropy loss."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Preprocessing complexity (AST/CFG/DFG extraction) may impact pipeline latency.",
        "512-token input limit can truncate large methods, potentially affecting accuracy.",
        "Tokenizer/vocabulary mismatches can degrade performance; requires vocabulary extension/normalization.",
        "GPU memory/time overhead from multi-view attention and graph fusion.",
        "Generalization to varied industrial codebases and build systems not yet validated."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes JFinder, a novel Java vulnerability identification architecture combining structural (AST/CFG/DFG) and semantic (pre-trained code model) information via a quad self-attention mechanism; code released on GitHub.",
      "Extensive experiments on 13 datasets (SARD CWEs and PROMISE Java projects) showing superior performance: “accuracy of 0.97 on the CWE dataset and an F1 score of 0.84 on the PROMISE dataset.”",
      "Case studies on four patched vulnerabilities demonstrating JFinder can recognize vulnerabilities and that patched code is no longer flagged, indicating learned vulnerability patterns."
    ]
  },
  {
    "arxiv_id": "2308.00373v1",
    "title": "Physical-Layer Authentication of Commodity Wi-Fi Devices via Micro-Signals on CSI Curves",
    "authors": "Ruiqi Kong; He Chen",
    "abstract": "This paper presents a new radiometric fingerprint that is revealed by micro-signals in the channel state information (CSI) curves extracted from commodity Wi-Fi devices. We refer to this new fingerprint as \"micro-CSI\". Our experiments show that micro-CSI is likely to be caused by imperfections in the radio-frequency circuitry and is present in Wi-Fi 4/5/6 network interface cards (NICs). We conducted further experiments to determine the most effective CSI collection configuration to stabilize micro-CSI. To extract micro-CSI from varying CSI curves, we developed a signal space-based extraction algorithm that effectively separates distortions caused by wireless channels and hardware imperfections under line-of-sight (LoS) scenarios. Finally, we implemented a micro-CSI-based device authentication algorithm that uses the k-Nearest Neighbors (KNN) method to identify 11 COTS Wi-Fi NICs from the same manufacturer in typical indoor environments. Our experimental results demonstrate that the micro-CSI-based authentication algorithm can achieve an average attack detection rate of over 99% with a false alarm rate of 0%.",
    "published_date": "2023-08-01",
    "pdf_link": "https://arxiv.org/pdf/2308.00373v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Wireless/Network Security",
      "subdomain": "Physical-Layer Security / Device Authentication",
      "specific_problem": "Physical-layer authentication of commodity Wi‑Fi devices using CSI-based micro-signals (micro-CSI) as radiometric fingerprints under LoS conditions",
      "attack_types": [
        "device impersonation",
        "spoofing",
        "rogue device authentication bypass"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Nearest Neighbor / Distance-based",
        "specific": "k-Nearest Neighbors (KNN) with thresholding",
        "novel_contribution": "Applies a simple KNN anomaly detection scheme over the proposed micro-CSI fingerprint; computes Euclidean distance to K nearest neighbors of claimed identity and thresholds for accept/reject."
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Anomaly Detection (one-class thresholding against legitimate device library)"
    ],
    "datasets": [
      {
        "name": "Atheros Wi‑Fi NIC CSI measurements (11 NICs, 2 rooms, LoS)",
        "type": "private",
        "domain": "wireless_csi",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "SDR-collected CSI from different-brand Wi‑Fi 4/5/6 NICs (existence study)",
        "type": "private",
        "domain": "wireless_csi",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Attack Detection Rate (ADR)",
      "False Alarm Rate (FAR)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can stable, device-unique radiometric fingerprints be extracted from commodity Wi‑Fi CSI without access to raw PHY samples?",
        "What CSI collection configuration stabilizes micro-CSI and preserves device-unique properties?",
        "Can a signal-space method separate wireless channel effects from hardware-induced micro-CSI under LoS?",
        "How many CSI packets (Ncsi) are needed to achieve high ADR at low FAR in indoor LoS environments?"
      ],
      "gaps_identified": [
        "Most radiometric fingerprinting requires raw physical-layer samples and expensive instrumentation (e.g., vector signal analyzers/SDRs), which hinders deployment on COTS systems.",
        "COTS devices do not expose PHY samples to higher layers for authentication purposes.",
        "Existing CSI-based fingerprints (e.g., CFO, nonlinear phase error, PA-induced variance) require many CSI measurements and have limited performance; a complementary fingerprint could improve PLA."
      ],
      "limitations": [
        "Evaluation and extraction method target strong line-of-sight (LoS) scenarios; performance in rich multipath/NLoS is not established.",
        "Transmitter is required to operate in single-antenna mode during authentication to avoid mixing fingerprints across RF chains.",
        "Micro-CSI is small in scale and requires averaging of multiple CSI packets (Ncsi up to 100–200) to suppress noise and stabilize the fingerprint.",
        "Main controlled experiments use Atheros NICs and the Atheros CSI tool; generalization across broader hardware/ecosystems is only partially evidenced.",
        "Threshold-based decision depends on environment and Ncsi; sensitivity to threshold selection and operational dynamics is not fully characterized."
      ],
      "future_work": [],
      "motivation": "Enable practical physical-layer authentication on commodity Wi‑Fi by leveraging CSI (readily available at higher layers) to extract hardware-induced radiometric fingerprints, avoiding the need for raw PHY samples and expensive test equipment.",
      "potential_research_ideas": [
        "Extend micro-CSI extraction to non-LoS and rich multipath via multipath-resilient signal-space modeling or joint channel–fingerprint separation.",
        "Multi-AP and spatial diversity fusion to improve robustness and reduce Ncsi per decision.",
        "Learned metric/Siamese representation for micro-CSI to replace Euclidean+KNN, enabling better generalization to unseen environments.",
        "Adaptive Ncsi selection and online calibration to meet latency constraints while maintaining ADR/FAR targets.",
        "Cross-band/cross-standard generalization (e.g., 5 GHz/6 GHz, 40/80 MHz channels, 802.11ax/11be) and subcarrier selection strategies.",
        "Robustness against active spoofers attempting to emulate micro-CSI (e.g., waveform shaping with SDRs); design and evaluation of attack/defense strategies.",
        "Temperature/aging compensation models for long-term stability; domain adaptation across time and devices.",
        "Federated or privacy-preserving enrollment of device fingerprints across enterprise environments."
      ],
      "architectural_improvement_recommendations": [
        "Replace KNN with a learned embedding (Siamese/Triplet networks) and probabilistic thresholding for improved discrimination under same-model devices.",
        "Joint optimization of channel estimation and fingerprint extraction (e.g., constrained deconvolution) to better separate channel and micro-CSI in non-LoS.",
        "Subcarrier weighting/attention or sparse subcarrier selection focusing on highly discriminative micro-CSI regions.",
        "Temporal modeling over sequences of micro-CSI estimates (e.g., simple HMM or RNN) to exploit dynamics and reduce Ncsi.",
        "Ensemble anomaly detection (e.g., one-class SVM, Isolation Forest) combined with KNN to improve robustness without heavy compute.",
        "Adaptive thresholding calibrated per environment with confidence estimation/uncertainty quantification."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Commodity mini-PC and COTS Wi‑Fi NICs; no GPU required. Euclidean distance over 56 subcarriers; averaging over Ncsi=10–200 packets per decision; very low compute footprint."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Indoor LoS rooms (research office and common room) with a COTS AP (AR9580, 2 antennas) and transmitters configured as single-antenna; 2.4 GHz, 20 MHz, 802.11n.",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Requires installing a CSI tool and access to CSI at higher layers.",
        "Transmitter must be switched to single-antenna mode during authentication (challenge–response).",
        "Performance currently demonstrated for strong LoS; robustness in NLoS unknown.",
        "Needs collection of multiple packets per decision (Ncsi up to 200) which impacts latency.",
        "Threshold tuning may be environment-dependent."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Identifies and defines a new radiometric fingerprint (micro-CSI) manifested as micro-signals on CSI curves of COTS Wi‑Fi devices.",
      "Empirically shows micro-CSI likely stems from RF circuitry imperfections and is present across Wi‑Fi 4/5/6 NICs.",
      "Determines CSI collection configurations that stabilize micro-CSI (single-antenna transmitter; SIMO at receiver for noise suppression).",
      "Proposes a signal space-based extraction algorithm that separates channel effects from hardware imperfections under strong LoS.",
      "Implements a micro-CSI-based device authentication method using KNN and evaluates on 11 Atheros NICs in two indoor environments.",
      "Reports “over 99% attack detection rate with a false alarm rate of 0%” when Ncsi is sufficiently large; detailed table shows ADR improving with Ncsi (e.g., 99.21% ADR at FAR=0% for Ncsi=200)."
    ]
  },
  {
    "arxiv_id": "2307.15555v1",
    "title": "All-for-One and One-For-All: Deep learning-based feature fusion for Synthetic Speech Detection",
    "authors": "Daniele Mari; Davide Salvi; Paolo Bestagini; Simone Milani",
    "abstract": "Recent advances in deep learning and computer vision have made the synthesis and counterfeiting of multimedia content more accessible than ever, leading to possible threats and dangers from malicious users. In the audio field, we are witnessing the growth of speech deepfake generation techniques, which solicit the development of synthetic speech detection algorithms to counter possible mischievous uses such as frauds or identity thefts. In this paper, we consider three different feature sets proposed in the literature for the synthetic speech detection task and present a model that fuses them, achieving overall better performances with respect to the state-of-the-art solutions. The system was tested on different scenarios and datasets to prove its robustness to anti-forensic attacks and its generalization capabilities.",
    "published_date": "2023-07-28",
    "pdf_link": "https://arxiv.org/pdf/2307.15555v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Digital Forensics",
      "subdomain": "Deepfake Detection",
      "specific_problem": "Synthetic speech (audio deepfake) detection via multi-feature fusion",
      "attack_types": [
        "Text-To-Speech (TTS)",
        "Voice Conversion (VC)",
        "Anti-forensic post-processing: MP3 compression",
        "Anti-forensic post-processing: additive Gaussian noise"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "MLP/Feedforward Neural Network",
        "specific": "Fully Connected (FC) networks with LeakyReLU and Softmax",
        "novel_contribution": "End-to-end deep learning feature-fusion of three heterogeneous handcrafted feature sets (FD, STLT, bicoherence) using learned embedding extractors to balance dimensionality before concatenation."
      },
      {
        "type": "primary",
        "category": "Representation Learning",
        "specific": "Embedding extractors (FC projections) for dimensionality reduction",
        "novel_contribution": "Learned embeddings (32/64/16 dims) from imbalanced feature vectors (416/800/8) to enable effective fusion."
      },
      {
        "type": "baseline",
        "category": "Ensemble/Fusion",
        "specific": "Late Fusion of individually trained models",
        "novel_contribution": "Considered as an alternative; found inferior to end-to-end joint training in this paper."
      },
      {
        "type": "baseline",
        "category": "Random Forest",
        "specific": null,
        "novel_contribution": "Baseline from [12]: FD (First Digit/Benford law errors on MFCCs) + RF classifier."
      },
      {
        "type": "baseline",
        "category": "Classical Supervised Classifier",
        "specific": null,
        "novel_contribution": "Baseline from [6]: STLT features modeling multi-order autoregressive speech used with supervised classifier."
      },
      {
        "type": "baseline",
        "category": "Classical Supervised Classifier",
        "specific": null,
        "novel_contribution": "Baseline from [1]: Detector using higher-order bispectral (bicoherence) correlations."
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "ASVspoof 2019 (Logical Access)",
        "type": "public",
        "domain": "audio_speech",
        "link": "https://www.asvspoof.org/asvspoof2019/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "LJSpeech (LJS)",
        "type": "public",
        "domain": "audio_speech",
        "link": "https://keithito.com/LJ-Speech-Dataset/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "LibriSpeech (train-clean-100 subset)",
        "type": "public",
        "domain": "audio_speech",
        "link": "https://www.openslr.org/12/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Cloud2019 (from [11])",
        "type": "public",
        "domain": "audio_speech",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      },
      {
        "name": "VidTIMIT (audio)",
        "type": "public",
        "domain": "audio_speech",
        "link": "https://conradsanderson.id.au/vidtimit/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "FD + Random Forest (Benford First Digit on MFCCs)",
        "paper_reference": "[12]",
        "metric": "AUC, Balanced Accuracy, ROC",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "STLT-based supervised detector",
        "paper_reference": "[6]",
        "metric": "AUC, Balanced Accuracy, ROC",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Bicoherence-based detector",
        "paper_reference": "[1]",
        "metric": "AUC, Balanced Accuracy, ROC",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Late Fusion of individually trained models (embeddings)",
        "paper_reference": "[17] (fusion concept)",
        "metric": "AUC, Balanced Accuracy",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "ROC",
      "AUC",
      "Balanced Accuracy"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Does deep-learning-based fusion of heterogeneous features (FD, STLT, bicoherence) improve synthetic speech detection over single-feature detectors?",
        "How robust is the fused detector to common anti-forensic post-processing (Gaussian noise, MP3 compression)?",
        "Can the fused detector generalize across datasets and distributions beyond ASVspoof 2019 when trained with mixed real-speech sources?"
      ],
      "gaps_identified": [
        "Existing detectors often focus on a single footprint (silence artifacts, speech dynamics, or higher-order correlations), potentially missing complementary cues.",
        "Deepfake detectors can be fragile to real-world post-processing (compression, noise) reducing effectiveness in-the-wild.",
        "Distribution shifts across datasets (e.g., ASVspoof vs. LibriSpeech reals) cause training instability and can harm generalization."
      ],
      "limitations": [
        "Training on mixed real datasets (ASVspoof + LibriSpeech) showed unstable optimization due to feature distribution mismatch; addressed via per-sample weighting.",
        "Relies on pre-defined handcrafted feature extractors (FD, STLT, bicoherence), which may miss other discriminative cues.",
        "Computational/training resource requirements and inference speeds are not reported.",
        "No explicit interpretability/attribution analysis provided."
      ],
      "future_work": [],
      "motivation": "Rising threat of speech deepfakes (TTS/VC) requires robust detectors; fusing complementary forensic cues should improve accuracy, robustness to anti-forensics, and generalization.",
      "potential_research_ideas": [
        "Extend fusion with additional complementary features (e.g., phase-based cues, glottal flow, prosody/temporal rhythm, CQCC/LFCC, spectro-temporal modulation).",
        "End-to-end learnable front-ends (e.g., SincNet/LEAF) to replace or complement handcrafted features.",
        "Self-supervised or contrastive pretraining on large-scale real speech to improve generalization and reduce reliance on labeled fakes.",
        "Domain adaptation/invariant representation learning to mitigate cross-dataset shifts (e.g., CORAL, MMD, adversarial domain adaptation).",
        "Adversarial/augmentation-based training against post-processing (compression, noise, resampling, codecs) for robustness.",
        "Calibration and open-set/OOD detection to handle unseen synthesis methods.",
        "Lightweight/edge deployable variants (quantization, pruning, knowledge distillation).",
        "Temporal attention or sequence modeling (Transformer/RNN) over frame-wise features to capture longer-range inconsistencies.",
        "Uncertainty estimation to flag low-confidence decisions in forensics pipelines."
      ],
      "architectural_improvement_recommendations": [
        "Replace simple concatenation with attention-based or gating-based fusion (e.g., cross-attention, FiLM, gating networks).",
        "Use Mixture-of-Experts with expert per feature family and learned router.",
        "Add metric learning/contrastive objectives between embeddings (e.g., supervised contrastive loss) to enhance class separation and domain invariance.",
        "Incorporate multi-task learning (e.g., auxiliary codec/noise detection) to encourage robust representations.",
        "Introduce normalization layers tailored per-feature distribution and learnable per-feature scaling to handle imbalance.",
        "Evaluate sequence models (BiLSTM/Transformer) on temporally structured versions of features.",
        "Add feature-dropout/modality dropout during training to improve robustness if one feature degrades.",
        "Use differentiable data augmentations (codec simulation layers) and adversarial training (e.g., PGD in feature space) against anti-forensics."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Quality degradation and post-processing (e.g., MP3 compression, additive noise) can reduce detection accuracy in the wild.",
        "Cross-dataset distribution shifts between real-speech sources can destabilize training and affect generalization."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": true,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes a deep learning-based feature fusion detector combining FD, STLT, and bicoherence features for synthetic speech detection.",
      "Introduces learned embedding extractors to mitigate feature dimensionality imbalance and enable effective fusion.",
      "End-to-end training of feature extractors and fusion classifier; shows late fusion performs worse.",
      "Demonstrates improved performance over state-of-the-art single-feature detectors and tests robustness to Gaussian noise and MP3 compression, as well as cross-dataset generalization.",
      "Analyzes feature correlations showing low cross-correlation among the three feature families, motivating fusion."
    ]
  },
  {
    "arxiv_id": "2308.08367v1",
    "title": "Diff-CAPTCHA: An Image-based CAPTCHA with Security Enhanced by Denoising Diffusion Model",
    "authors": "Ran Jiang; Sanfeng Zhang; Linfeng Liu; Yanbing Peng",
    "abstract": "To enhance the security of text CAPTCHAs, various methods have been employed, such as adding the interference lines on the text, randomly distorting the characters, and overlapping multiple characters. These methods partly increase the difficulty of automated segmentation and recognition attacks. However, facing the rapid development of the end-to-end breaking algorithms, their security has been greatly weakened. The diffusion model is a novel image generation model that can generate the text images with deep fusion of characters and background images. In this paper, an image-click CAPTCHA scheme called Diff-CAPTCHA is proposed based on denoising diffusion models. The background image and characters of the CAPTCHA are treated as a whole to guide the generation process of a diffusion model, thus weakening the character features available for machine learning, enhancing the diversity of character features in the CAPTCHA, and increasing the difficulty of breaking algorithms. To evaluate the security of Diff-CAPTCHA, this paper develops several attack methods, including end-to-end attacks based on Faster R-CNN and two-stage attacks, and Diff-CAPTCHA is compared with three baseline schemes, including commercial CAPTCHA scheme and security-enhanced CAPTCHA scheme based on style transfer. The experimental results show that diffusion models can effectively enhance CAPTCHA security while maintaining good usability in human testing.",
    "published_date": "2023-08-16",
    "pdf_link": "https://arxiv.org/pdf/2308.08367v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Web Security",
      "subdomain": "Bot Mitigation / CAPTCHA",
      "specific_problem": "Design and generation of image-based CAPTCHA resistant to ML-based automated attacks",
      "attack_types": [
        "segmentation-based attacks",
        "end-to-end object detection attacks",
        "transfer learning-based attacks"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Diffusion Model",
        "specific": "DDPM",
        "novel_contribution": "Guided denoising diffusion CAPTCHA generation that fuses characters and background by injecting a guiding image during the reverse process to weaken machine-learnable character features and increase feature diversity."
      },
      {
        "type": "primary",
        "category": "U-Net",
        "specific": null,
        "novel_contribution": "U-Net as the noise prediction network with time embedding, residual and self-attention blocks; 1000 timesteps; tailored for CAPTCHA confusion rather than photorealism."
      },
      {
        "type": "baseline",
        "category": "Object Detection",
        "specific": "Faster R-CNN",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "ResNet-50",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Style Transfer",
        "specific": null,
        "novel_contribution": "Used as a comparative CAPTCHA generation baseline (security-enhanced CAPTCHA based on style transfer)."
      }
    ],
    "learning_paradigm": [
      "Unsupervised",
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Oxford 102 Flowers (flower102) – 724-image subset",
        "type": "public",
        "domain": "natural_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Pascal VOC2012 (backgrounds for guiding images)",
        "type": "public",
        "domain": "natural_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "YiDun-CAPTCHA (NetEase)",
        "type": "proprietary",
        "domain": "captcha_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "DingXiang-CAPTCHA",
        "type": "proprietary",
        "domain": "captcha_images",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Diff-CAPTCHA generated images",
        "type": "synthetic",
        "domain": "captcha_images",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Commercial CAPTCHA scheme (e.g., YiDun-CAPTCHA)",
        "paper_reference": null,
        "metric": "attack success rate",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Commercial CAPTCHA scheme (e.g., DingXiang-CAPTCHA)",
        "paper_reference": null,
        "metric": "attack success rate",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Security-enhanced CAPTCHA based on style transfer",
        "paper_reference": null,
        "metric": "attack success rate",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "attack success rate",
      "recognition success rate",
      "human usability success rate"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can denoising diffusion models enhance CAPTCHA security while maintaining human usability?",
        "Does treating background and characters as a whole during diffusion reduce machine-learnable character features and lower attack success rates?",
        "How do end-to-end and two-stage attacks (Faster R-CNN; Faster R-CNN + ResNet-50) perform against Diff-CAPTCHA compared to commercial and style-transfer CAPTCHAs?"
      ],
      "gaps_identified": [
        "Text-based and many image-based CAPTCHAs are vulnerable to modern end-to-end ML attacks.",
        "Existing image-based CAPTCHAs often overlay text on backgrounds, leaving discernible character edges/features exploitable by neural networks.",
        "Adversarial CAPTCHAs can be defeated via perturbation removal and adversarial training.",
        "Limited exploration of diffusion models for CAPTCHA generation."
      ],
      "limitations": [
        "Diffusion model trained on a single-type background dataset (724 images from flower102) to ensure stability, which may limit background diversity.",
        "Usability sensitive to guiding-image brightness/contrast; requires a brightness balancing algorithm to avoid loss of character details.",
        "Focus on increasing confusion over photorealism; relatively simple U-Net and small training budget (200–400 iterations) may limit generation quality.",
        "Fixed image sizes (train 128x128, generate 256x256); broader resolutions not discussed."
      ],
      "future_work": [],
      "motivation": "Enhance CAPTCHA security against powerful end-to-end ML attacks by deeply fusing characters with backgrounds via diffusion to weaken learnable features while preserving human usability.",
      "potential_research_ideas": [
        "Use richer, domain-diverse background corpora and font/character domains to improve robustness and variety.",
        "Incorporate conditional controls (e.g., ControlNet, text/segmentation guidance) to regulate character placement and legibility while maintaining fusion.",
        "Evaluate against stronger modern solvers (e.g., latest DETR variants, YOLOv8/RT-DETR, transformer-based OCR, multimodal LLMs) to map robustness envelope.",
        "Adaptive generation that reacts to detected solver capabilities (meta-robustness) and rotates guidance strategies.",
        "Integrate perceptual legibility models to optimize the usability–security trade-off automatically.",
        "Design synthetic data generators to adversarially train attacker models and better quantify margins."
      ],
      "architectural_improvement_recommendations": [
        "Adopt faster samplers (e.g., DDIM, DPM-Solver) to reduce generation latency while preserving fusion quality.",
        "Add ControlNet or conditioning via semantic/layout masks for character regions to improve placement and controllable legibility.",
        "Use multi-scale U-Net with cross-attention to the guiding image to strengthen targeted fusion and reduce residual character edges.",
        "Apply classifier-free guidance tuning to balance security (fusion) vs. usability (readability).",
        "Train on multiple diverse background datasets and augmentations; include hard negatives from attacker feedback loops.",
        "Employ mixed-resolution training and test-time super-resolution to maintain clarity after diffusion."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes Diff-CAPTCHA, an image-based CAPTCHA generation scheme using denoising diffusion models that treat characters and background as a whole to weaken machine-learnable character features and increase diversity.",
      "Optimizes the reverse diffusion process and guiding-image design (including brightness balancing) to improve generation stability and usability for CAPTCHA.",
      "Conducts comprehensive security validation using end-to-end attacks (Faster R-CNN) and two-stage attacks (Faster R-CNN + ResNet-50) and compares against commercial and style-transfer CAPTCHA schemes, with human usability testing indicating maintained usability."
    ]
  },
  {
    "arxiv_id": "2308.02182v3",
    "title": "AutoML4ETC: Automated Neural Architecture Search for Real-World Encrypted Traffic Classification",
    "authors": "Navid Malekghaini; Elham Akbari; Mohammad A. Salahuddin; Noura Limam; Raouf Boutaba; Bertrand Mathieu; Stephanie Moteau; Stephane Tuffin",
    "abstract": "Deep learning (DL) has been successfully applied to encrypted network traffic classification in experimental settings. However, in production use, it has been shown that a DL classifier's performance inevitably decays over time. Re-training the model on newer datasets has been shown to only partially improve its performance. Manually re-tuning the model architecture to meet the performance expectations on newer datasets is time-consuming and requires domain expertise. We propose AutoML4ETC, a novel tool to automatically design efficient and high-performing neural architectures for encrypted traffic classification. We define a novel, powerful search space tailored specifically for the early classification of encrypted traffic using packet header bytes. We show that with different search strategies over our search space, AutoML4ETC generates neural architectures that outperform the state-of-the-art encrypted traffic classifiers on several datasets, including public benchmark datasets and real-world TLS and QUIC traffic collected from the Orange mobile network. In addition to being more accurate, AutoML4ETC's architectures are significantly more efficient and lighter in terms of the number of parameters. Finally, we make AutoML4ETC publicly available for future research.",
    "published_date": "2023-08-04",
    "pdf_link": "https://arxiv.org/pdf/2308.02182v3",
    "paper_types": [
      "benchmark",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Traffic Analysis",
      "specific_problem": "Encrypted Traffic Classification (early service- and application-level classification of TLS and QUIC traffic using packet header bytes)",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Neural Architecture Search (NAS)",
        "specific": "ENAS-inspired cell-based search with controller composing Normal and Reduction cells",
        "novel_contribution": "Novel, domain-tailored NAS search space for ETC enabling early classification from minimal packets (TLS first 3 handshake packets; QUIC singleton ClientHello)"
      },
      {
        "type": "primary",
        "category": "CNN",
        "specific": "Depthwise separable convolutions with ReLU+SeparableConv+BatchNorm(+Dropout) blocks, Average/Max pooling, Factorized Reduction, Filter Alignment hyper-layers",
        "novel_contribution": "Lightweight cell design with Loose Ends aggregation; hyper-layers for filter alignment and factorized reduction tailored to packet-bytes input"
      },
      {
        "type": "primary",
        "category": "Reinforcement Learning",
        "specific": "Controller RNN choosing cell inputs/ops (identity, sep-conv k={3,5}, pooling k=3), rewarded by validation accuracy",
        "novel_contribution": "Applied as one of multiple search strategies over the proposed ETC-specific search space"
      },
      {
        "type": "primary",
        "category": "Evolutionary Algorithm",
        "specific": "Aging evolution over NASNet-style cell space (as per Real et al.)",
        "novel_contribution": "Compared as a search strategy over the proposed ETC-specific search space"
      },
      {
        "type": "primary",
        "category": "Monte Carlo Tree Search",
        "specific": "MCTS/UCT-inspired controller (layer-by-layer hyperparameter selection)",
        "novel_contribution": "Discussed/considered as a search strategy; balances exploration–exploitation for architecture selection"
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Orange Mobile Network TLS Traffic",
        "type": "private",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Orange Mobile Network QUIC Traffic",
        "type": "private",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Unspecified public benchmark datasets for ETC",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "UWOrange",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "accuracy",
      "validation_accuracy",
      "number_of_parameters"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "In production, deep ETC classifiers suffer performance decay over time due to data drift.",
        "Periodic re-training on newer datasets only partially recovers performance.",
        "Manual re-tuning of architectures and hyperparameters is time-consuming and requires domain expertise.",
        "There is a need for efficient, lightweight models capable of accurate early classification from the first few packets.",
        "NAS typically requires domain expertise to define an effective search space; choosing building blocks is non-trivial."
      ],
      "limitations": [
        "NAS can be computationally expensive; reward evaluation requires training child models.",
        "Designing the search space still depends on expert-curated building blocks (alleviated here but inherently a design choice)."
      ],
      "future_work": [],
      "motivation": "Automate the design of efficient, high-performing ETC models that remain effective on newer datasets under data drift and support accurate early classification, reducing manual expert effort.",
      "potential_research_ideas": [
        "Incorporate continual/domain-adaptive learning to reduce the frequency or depth of NAS reruns under data drift.",
        "Multi-objective NAS that jointly optimizes accuracy, parameter count, latency, and robustness to protocol/version drift.",
        "Federated/privately-preserving NAS across ISPs to leverage diverse data distributions without sharing raw traffic.",
        "Self-supervised pretraining on packet-byte streams to initialize child models before NAS for improved data efficiency.",
        "Drift-aware online NAS that triggers lightweight architecture updates based on distribution shift detectors.",
        "Extend the search space to include attention-based blocks (e.g., lightweight transformers) and temporal convolutions.",
        "Explainability-constrained NAS that enforces interpretable motifs or produces saliency on packet bytes/fields.",
        "Adversarially robust NAS incorporating perturbation- or evasion-resistance objectives against traffic morphing.",
        "Cross-protocol transferability studies (e.g., TLS→QUIC) to design protocol-agnostic architectures."
      ],
      "architectural_improvement_recommendations": [
        "Add multi-fidelity/differentiable NAS (e.g., DARTS/Proxyless) to reduce search cost on large datasets.",
        "Include squeeze-and-excitation or lightweight attention within cells to improve channel selection on packet bytes.",
        "Introduce learned early-exit heads to support adaptive early classification with packet budget constraints.",
        "Use Bayesian optimization or BOHB/Hyperband scheduling for controller hyperparameters and training budgets.",
        "Augment training with protocol-version and cipher-suite augmentation to improve generalization to new TLS/QUIC versions."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/OrangeUW/AutoML4ETC",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "ISP mobile network (Orange) data; early classification setting at network operator",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Data drift causing performance decay over time in production.",
        "Requirement for early classification from very few packets (TLS first 3 packets; QUIC ClientHello).",
        "Need for lightweight, resource-efficient models suitable for operator environments."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes AutoML4ETC, an automated and resource-effective NAS tool for encrypted traffic classification that yields lighter architectures with higher accuracy than state-of-the-art ETC models.",
      "Introduces a novel, powerful search space tailored for early classification on packet raw bytes, achieving high accuracy from the first three TLS handshake packets and, for the first time, from the singleton QUIC ClientHello.",
      "Validates AutoML4ETC on real-world Orange mobile TLS and QUIC traffic and public datasets, demonstrating superior performance; releases the tool publicly for research."
    ]
  },
  {
    "arxiv_id": "2307.10195v1",
    "title": "ChatGPT for Digital Forensic Investigation: The Good, The Bad, and The Unknown",
    "authors": "Mark Scanlon; Frank Breitinger; Christopher Hargreaves; Jan-Niclas Hilgert; John Sheppard",
    "abstract": "The disruptive application of ChatGPT (GPT-3.5, GPT-4) to a variety of domains has become a topic of much discussion in the scientific community and society at large. Large Language Models (LLMs), e.g., BERT, Bard, Generative Pre-trained Transformers (GPTs), LLaMA, etc., have the ability to take instructions, or prompts, from users and generate answers and solutions based on very large volumes of text-based training data. This paper assesses the impact and potential impact of ChatGPT on the field of digital forensics, specifically looking at its latest pre-trained LLM, GPT-4. A series of experiments are conducted to assess its capability across several digital forensic use cases including artefact understanding, evidence searching, code generation, anomaly detection, incident response, and education. Across these topics, its strengths and risks are outlined and a number of general conclusions are drawn. Overall this paper concludes that while there are some potential low-risk applications of ChatGPT within digital forensics, many are either unsuitable at present, since the evidence would need to be uploaded to the service, or they require sufficient knowledge of the topic being asked of the tool to identify incorrect assumptions, inaccuracies, and mistakes. However, to an appropriately knowledgeable user, it could act as a useful supporting tool in some circumstances.",
    "published_date": "2023-07-10",
    "pdf_link": "https://arxiv.org/pdf/2307.10195v1",
    "paper_types": [
      "position",
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Digital Forensics",
      "subdomain": "Forensic Analysis and Investigation",
      "specific_problem": "Assessing the applicability of ChatGPT (GPT-4/GPT-3.5) for digital forensic investigation tasks including artefact identification, evidence/keyword searching, code generation, anomaly detection in logs, incident response, and education/training",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer LLM",
        "specific": "GPT-4 (ChatGPT)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Transformer LLM",
        "specific": "GPT-3.5 (ChatGPT)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Zero-shot prompting",
      "In-context learning"
    ],
    "datasets": [
      {
        "name": "ChatGPT-for-Digital-Forensics prompts and responses",
        "type": "public",
        "domain": "text_prompts_and_model_responses_for_digital_forensics_use_cases",
        "link": "https://github.com/markscanlonucd/ChatGPT-for-Digital-Forensics",
        "is_new_contribution": true,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can GPT-4/ChatGPT assist with artefact understanding and identification on systems (e.g., Windows artefacts, cloud interaction traces)?",
        "Can ChatGPT assist in searching for digital evidence (e.g., keyword lists, regular expressions)?",
        "Can ChatGPT generate useful and correct code for forensic activities?",
        "Can ChatGPT support anomaly detection within log files?",
        "Can ChatGPT aid incident response workflows?",
        "How suitable is ChatGPT for self-directed learning and education in digital forensics across different levels (introductory, advanced/expert, research/workshops)?"
      ],
      "gaps_identified": [
        "LLM reliability: OpenAI states GPT-4 \"is not fully reliable (it hallucinates facts and makes reasoning errors)\" and reliability is critical in forensics.",
        "Evidence privacy and legal admissibility: Many uses would require uploading evidence to a third-party service, raising chain-of-custody, legal, and ethical concerns.",
        "Non-determinism: \"Since ChatGPT responses are non-deterministic given identical prompts\" complicating reproducibility and consistent guidance.",
        "Knowledge currency: \"the knowledge cut-off of the model is September 2021,\" limiting usefulness for latest tools/practices.",
        "Hallucinated tools/paths: Provided links to tools that do not exist and inconsistent file paths (e.g., nonexistent Eric Zimmerman tools) and overemphasis on certain artefacts (Windows Event Logs) while missing others initially.",
        "Educational limitations: Difficulty providing accurate citations/authors, weaknesses in creating practical labs/exercises, and occasional technical errors in explanations (e.g., invalid FAT32 attribute byte).",
        "Regex/keyword generation quality issues: Produced incorrect regexes and mismatched examples; limited ability to cover edge cases without careful prompting."
      ],
      "limitations": [
        "Evaluations are illustrative and do not cover all potential DF use cases: \"these domains do not provide full coverage of all possible uses of LLMs for digital forensic.\"",
        "Model non-determinism affects repeatability: \"Since ChatGPT responses are non-deterministic given identical prompts.\"",
        "Model knowledge cut-off (September 2021) limits up-to-date guidance (Section 5.3).",
        "Mixed depth/accuracy across artefacts; some responses contain inaccuracies, broken links, or oversights (Sections 4 and 6)."
      ],
      "future_work": [
        "Paper states it \"proposes future directions for the utilisation of LLM-based AI in digital forensics\" (specifics not provided in the supplied text)."
      ],
      "motivation": "Assess the impact and potential impact (strengths, risks, and suitability) of ChatGPT/GPT-4 for digital forensic investigation tasks and education.",
      "potential_research_ideas": [
        "Develop an on-premise, privacy-preserving DF assistant LLM to avoid evidence exfiltration and maintain chain-of-custody.",
        "Create a retrieval-augmented generation (RAG) system grounded on vetted forensic artefact documentation and tool manuals to reduce hallucinations and improve citation of sources.",
        "Design a benchmark suite and gold-standard dataset for LLMs on DF tasks (artefact path resolution, log anomaly explanations, IR playbook generation, regex/keyword quality).",
        "Integrate tool-augmented LLMs that call verified DF utilities (e.g., Volatility, Autopsy, plaso, Eric Zimmerman tools) via constrained APIs and return verifiable outputs.",
        "Build a verification layer for LLM-generated regex/keyword lists (unit-test harness with synthetic and real DF corpora) to auto-detect and correct errors.",
        "Investigate chain-of-custody-aware LLM workflows that produce auditable reasoning traces and citations to accepted forensic references.",
        "Explore domain-adapted fine-tuning or instruction-tuning of open LLMs on curated DF corpora to improve accuracy on artefact locations and procedures.",
        "Evaluate multi-agent or self-critique approaches where a second agent verifies tool paths, references, and commands before presenting to the investigator."
      ],
      "architectural_improvement_recommendations": [
        "Adopt RAG with curated, versioned DF knowledge bases and enforce source citation in outputs.",
        "Incorporate function-calling to vetted forensic tool APIs with schema validation and sandboxed execution; return parsed, signed results.",
        "Implement deterministic decoding and response templates for high-stakes guidance (reduce variance), with uncertainty calibration and refusal when confidence is low.",
        "Add regex and command validators (linters, unit tests) in the generation loop; auto-suggest fixes for failing tests.",
        "Deploy on-prem or air-gapped inference for sensitive evidence; consider parameter-efficient fine-tuning on DF corpora.",
        "Use policy and safety layers to prevent fabrication of non-existent tools/paths; require retrieval-confirmation before suggesting specific paths."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/markscanlonucd/ChatGPT-for-Digital-Forensics",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Evidence confidentiality and privacy (uploading to third-party service).",
        "Chain-of-custody and admissibility concerns when using LLM outputs.",
        "Hallucinations and inaccuracies requiring expert oversight.",
        "Non-determinism leading to inconsistent guidance.",
        "Knowledge cut-off and staleness of model information.",
        "Broken/incorrect tool references and file paths.",
        "Potential legal/ethical issues and data sovereignty constraints."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": true,
      "fairness_bias": true
    },
    "contributions": [
      "Evaluated GPT-4/ChatGPT across multiple DF contexts: learning about DF topics, artefact identification, evidence searching, code generation for forensic activities, anomaly detection in log files, incident response, and creating educational storyboards.",
      "Outlined strengths, risks, and occasional benefits of LLMs for each area tested.",
      "Released a static repository of prompts and responses used in experiments for transparency.",
      "Drew general conclusions and highlighted limitations; proposed future directions for LLM-based AI in digital forensics."
    ]
  },
  {
    "arxiv_id": "2308.11042v1",
    "title": "Unlocking Hardware Security Assurance: The Potential of LLMs",
    "authors": "Xingyu Meng; Amisha Srivastava; Ayush Arunachalam; Avik Ray; Pedro Henrique Silva; Rafail Psiakis; Yiorgos Makris; Kanad Basu",
    "abstract": "System-on-Chips (SoCs) form the crux of modern computing systems. SoCs enable high-level integration through the utilization of multiple Intellectual Property (IP) cores. However, the integration of multiple IP cores also presents unique challenges owing to their inherent vulnerabilities, thereby compromising the security of the entire system. Hence, it is imperative to perform hardware security validation to address these concerns. The efficiency of this validation procedure is contingent on the quality of the SoC security properties provided. However, generating security properties with traditional approaches often requires expert intervention and is limited to a few IPs, thereby resulting in a time-consuming and non-robust process. To address this issue, we, for the first time, propose a novel and automated Natural Language Processing (NLP)-based Security Property Generator (NSPG). Specifically, our approach utilizes hardware documentation in order to propose the first hardware security-specific language model, HS-BERT, for extracting security properties dedicated to hardware design. To evaluate our proposed technique, we trained the HS-BERT model using sentences from RISC-V, OpenRISC, MIPS, OpenSPARC, and OpenTitan SoC documentation. When assessedb on five untrained OpenTitan hardware IP documents, NSPG was able to extract 326 security properties from 1723 sentences. This, in turn, aided in identifying eight security bugs in the OpenTitan SoC design presented in the hardware hacking competition, Hack@DAC 2022.",
    "published_date": "2023-08-21",
    "pdf_link": "https://arxiv.org/pdf/2308.11042v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Hardware Security",
      "subdomain": "Hardware Security Verification",
      "specific_problem": "Automated extraction/generation of security properties for SoC/RTL verification from hardware documentation",
      "attack_types": [
        "security flow violations",
        "privilege and access control issues",
        "reset control issues",
        "memory and storage misuse",
        "peripherals and on-chip fabric misuse",
        "debug/test interface misuse",
        "information flow policy violations"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "BERT (domain-specific pretraining)",
        "novel_contribution": "HS-BERT: first hardware security-specific language model pre-trained on hardware documentation for security property extraction"
      },
      {
        "type": "primary",
        "category": "Masked Language Modeling",
        "specific": "BERT MLM",
        "novel_contribution": "Self-supervised in-domain pretraining on hardware documentation to learn hardware/security terminology and context"
      },
      {
        "type": "primary",
        "category": "Sequence Classification",
        "specific": "BERT sequence classifier",
        "novel_contribution": "Fine-tuned classifier to identify security-property-related sentences vs non-property sentences in SoC documentation"
      },
      {
        "type": "primary",
        "category": "Data Augmentation",
        "specific": null,
        "novel_contribution": "Hardware-aware augmentation (random swap, random deletion, synonym replacement using WordNet, random insertion of common adverbs) constrained to preserve design entities/relations"
      },
      {
        "type": "baseline",
        "category": "Large Language Model",
        "specific": "ChatGPT",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Self-supervised",
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Dpre (Hardware documentation for MLM pretraining)",
        "type": "public",
        "domain": "hardware_documentation_text",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Dcls (Labeled sentences for sequence classification)",
        "type": "public",
        "domain": "hardware_documentation_text (OpenTitan, MIPS, OpenSPARC, RISC-V non-privileged docs)",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "Dval (Validation set of unseen documentation)",
        "type": "public",
        "domain": "hardware_documentation_text (OpenTitan AES/ADC, OpenRISC, RISC-V trace)",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "Five unseen OpenTitan IP documents (evaluation)",
        "type": "public",
        "domain": "hardware_documentation_text",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "OpenTitan SoC design (Hack@DAC 2022)",
        "type": "public",
        "domain": "hardware_rtl_design",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "WordNet lexical database",
        "type": "public",
        "domain": "language_lexicon",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "ChatGPT",
        "paper_reference": null,
        "metric": "identification accuracy of security-property-related sentences in OpenTitan documentation",
        "their_result": "“NSPG shows 15% improvement on identifying security properties in OpenTitan SoC documentation”",
        "baseline_result": null
      },
      {
        "method_name": "BERT MLM (Original documents only, no DA)",
        "paper_reference": null,
        "metric": "perplexity (validation)",
        "their_result": "4.795 (HS-BERT with Random Insertion DA)",
        "baseline_result": "6.018"
      },
      {
        "method_name": "BERT MLM + Random Swap (DA)",
        "paper_reference": null,
        "metric": "perplexity (validation)",
        "their_result": "4.795 (HS-BERT with Random Insertion DA)",
        "baseline_result": "5.018"
      },
      {
        "method_name": "BERT MLM + Random Deletion (DA)",
        "paper_reference": null,
        "metric": "perplexity (validation)",
        "their_result": "4.795 (HS-BERT with Random Insertion DA)",
        "baseline_result": "5.046"
      },
      {
        "method_name": "BERT MLM + Synonym Replacement (DA)",
        "paper_reference": null,
        "metric": "perplexity (validation)",
        "their_result": "4.795 (HS-BERT with Random Insertion DA)",
        "baseline_result": "5.029"
      }
    ],
    "performance_metrics_used": [
      "perplexity",
      "runtime",
      "properties_extracted_count",
      "bugs_found",
      "identification_accuracy_improvement"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can leveraging large language models foster hardware security assurance?",
        "Can an NLP framework automatically extract security-property-related sentences from SoC documentation to aid RTL security verification?"
      ],
      "gaps_identified": [
        "Security property generation is manual, expert-dependent, time-consuming, and non-robust for complex SoCs.",
        "Hardware documentation is underutilized as a source for systematic security property mining.",
        "Lack of a domain-specific language model tailored for hardware security documentation."
      ],
      "limitations": [
        "Limited in-domain data; relies on open-source documentation and requires data augmentation.",
        "The paper focuses on sentence-level extraction; the extent to which conversion to formal SystemVerilog Assertions is fully automated is not clearly specified.",
        "Evaluation reported on five OpenTitan IP docs and Hack@DAC 2022 design; broader generalization across vendors/architectures not shown in provided text."
      ],
      "future_work": [],
      "motivation": "Improve efficiency and robustness of SoC hardware security validation by automating high-quality security property generation from documentation using NLP/LLMs.",
      "potential_research_ideas": [
        "End-to-end generation of formal SystemVerilog Assertions from extracted sentences via sequence-to-sequence models (e.g., T5/CodeT5).",
        "Joint multimodal learning over documentation and RTL/netlists to ground properties to actual design signals and hierarchies.",
        "CWE-aware multi-label classification to categorize properties by weakness type and enforce coverage goals.",
        "Active learning/human-in-the-loop labeling to expand Dcls with minimal expert effort across new IPs.",
        "Transfer learning and domain adaptation to proprietary SoC documentation (closed-source) using privacy-preserving techniques.",
        "Contrastive and retrieval-augmented training using documentation sections, diagrams, and register maps for better context.",
        "Self-training or weak supervision (Snorkel) to bootstrap labels from patterns and templates in docs.",
        "Evaluate across more architectures (ARM, PowerPC) and peripherals; build benchmarks for property extraction quality.",
        "Robustness studies against noisy/ambiguous documentation and adversarial phrasing; develop detectors for unreliable extractions.",
        "Integrate NSPG outputs into commercial EDA security verification flows with automated coverage and triage pipelines."
      ],
      "architectural_improvement_recommendations": [
        "Adopt stronger backbones (RoBERTa/DeBERTa/Longformer) and hierarchical transformers for long document context.",
        "Use prompt-based or instruction-tuned LLMs with few-shot examples for zero-shot generalization to unseen IPs.",
        "Introduce contrastive learning with sentence/document anchors (SimCSE/SupCon) to separate property vs non-property contexts.",
        "Add domain-specific NER to tag signals/registers/interfaces and feed as features to the classifier.",
        "Employ back-translation and contextual augmentation instead of WordNet-based synonyms to reduce semantic drift.",
        "Implement a two-stage pipeline: sentence classification followed by structured IE to populate property templates and autogenerate SVA.",
        "Leverage retrieval-augmented generation over a documentation index to ground extractions and improve faithfulness.",
        "Calibrate confidence estimates and abstention to reduce false positives in verification workflows."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": "Reported MLM pretraining runtime: ~57–114 minutes depending on augmentation size; specific hardware not stated."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Applied to OpenTitan documentation and used to identify bugs in OpenTitan SoC (Hack@DAC 2022) within a hardware verification workflow",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Effectiveness depends on quality and completeness of documentation.",
        "Mapping extracted sentences to precise RTL signals and generating formal assertions may require additional tooling or human oversight.",
        "Generalization to proprietary IP documentation with different styles and terminology may require adaptation.",
        "Integration with existing EDA security verification tools and workflows."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces NSPG, an automated NLP-based framework to mine security-property-related sentences from SoC documentation to aid RTL vulnerability detection.",
      "Presents HS-BERT, the first hardware security-specific language model, including domain-specific data augmentation and MLM pretraining to improve performance.",
      "Evaluates on five unseen OpenTitan IP documents, extracting 326 security properties from 1723 sentences; properties validated and used to identify eight bugs in OpenTitan (Hack@DAC 2022).",
      "Shows 15% improvement over ChatGPT in identifying security properties in OpenTitan documentation and analyzes data augmentation effects on MLM perplexity."
    ]
  },
  {
    "arxiv_id": "2308.02805v2",
    "title": "Meta-Analysis and Systematic Review for Anomaly Network Intrusion Detection Systems: Detection Methods, Dataset, Validation Methodology, and Challenges",
    "authors": "Ziadoon K. Maseer; Robiah Yusof; Baidaa Al-Bander; Abdu Saif; Qusay Kanaan Kadhim",
    "abstract": "Intrusion detection systems (IDSs) built on artificial intelligence (AI) are presented as latent mechanisms for actively detecting fresh attacks over a complex network. Although review papers are used the systematic review or simple methods to analyse and criticize the anomaly NIDS works, the current review uses a traditional way as a quantitative description to find current gaps by synthesizing and summarizing the data comparison without considering algorithms performance. This paper presents a systematic and meta-analysis study of AI for network intrusion detection systems (NIDS) focusing on deep learning (DL) and machine learning (ML) approaches in network security. Deep learning algorithms are explained in their structure, and data intrusion network is justified based on an infrastructure of networks and attack types. By conducting a meta-analysis and debating the validation of the DL and ML approach by effectiveness, used dataset, detected attacks, classification task, and time complexity, we offer a thorough benchmarking assessment of the current NIDS-based publications-based systematic approach. The proposed method is considered reviewing works for the anomaly-based network intrusion detection system (anomaly-NIDS) models. Furthermore, the effectiveness of proposed algorithms and selected datasets are discussed for the recent direction and improvements of ML and DL to the NIDS. The future trends for improving an anomaly-IDS for continuing detection in the evolution of cyberattacks are highlighted in several research studies.",
    "published_date": "2023-08-05",
    "pdf_link": "https://arxiv.org/pdf/2308.02805v2",
    "paper_types": [
      "benchmark",
      "empirical_analysis",
      "survey"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Anomaly-based Network Intrusion Detection Systems (NIDS) using ML/DL: methods, datasets, validation, effectiveness, and challenges (2017–2022)",
      "attack_types": [
        "unknown attacks (zero-day/novel)",
        "network scanning",
        "port scanning",
        "denial-of-service (DoS)",
        "malware propagation",
        "unauthorized access"
      ]
    },
    "ml_techniques": [
      {
        "type": "baseline",
        "category": "Artificial Neural Network",
        "specific": "FFNN",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Decision Tree",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "K-Nearest Neighbors",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Naive Bayes",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Random Forest",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Support Vector Machine",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "K-means",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Expectation-Maximization",
        "specific": "EM",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Self-Organizing Map",
        "specific": "SOM",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Deep Neural Network",
        "specific": "DNN",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Deep Belief Network",
        "specific": "DBN",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Restricted Boltzmann Machine",
        "specific": "RBM",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Autoencoder",
        "specific": "AE",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Convolutional Neural Network",
        "specific": "CNN",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Recurrent Neural Network",
        "specific": "RNN",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Long Short-Term Memory",
        "specific": "LSTM",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Probabilistic Neural Network",
        "specific": "PNN",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "KDD CUP 99",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "NSL-KDD",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "accuracy",
      "time complexity",
      "execution time"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "What are the effectiveness, used datasets, detected attacks, classification tasks, and time complexity of ML/DL anomaly-based NIDS from 2017 to 2022?",
        "Which public datasets are used to train and test anomaly-NIDS and how have they evolved?",
        "What validation methodologies are employed (e.g., cross-validation) and how do they impact results?",
        "What current issues and future trends exist for improving anomaly-based IDS in the face of evolving cyberattacks?"
      ],
      "gaps_identified": [
        "Prior review articles often lacked systematic review and/or meta-analysis focused on anomaly NIDS and did not comprehensively analyze current gaps and trends.",
        "Need for quantitative analysis synthesizing effectiveness, detected attack types, datasets, classification tasks, and time complexity.",
        "Many NIDS evaluations rely on imbalanced datasets; multi-attack (multi-class) detection is needed rather than binary-only evaluations.",
        "Execution time/time complexity considerations are essential to avoid overhead and packet loss but are not consistently addressed.",
        "Anomaly-based NIDS face higher false positive rates; robust validation and methodology reporting are needed."
      ],
      "limitations": [],
      "future_work": [
        "Highlight and discuss future trends to improve anomaly-IDS for ongoing detection as cyberattacks evolve."
      ],
      "motivation": "Rapid growth of networked/cloud/IoT data and evolving threats strain traditional defenses; anomaly-based NIDS with ML/DL are promising, but prior reviews inadequately synthesized effectiveness, datasets, and validation. This work provides a systematic review and meta-analysis (2017–2022) to benchmark methods, datasets, tasks, and time complexity and identify challenges and trends.",
      "potential_research_ideas": [
        "Design a standardized, reproducible benchmarking protocol for anomaly-NIDS including unified train/test splits, metrics, and reporting across common datasets.",
        "Develop updated, representative, and balanced public datasets capturing modern traffic (e.g., IoT/5G/cloud) with well-defined attack taxonomies.",
        "Investigate cross-dataset generalization and domain adaptation for NIDS to address dataset bias and distribution shift.",
        "Explore online/streaming and continual learning for anomaly-NIDS with concept drift handling and low-latency constraints.",
        "Integrate explainability into anomaly-NIDS to reduce false positives and support analyst triage.",
        "Evaluate and harden anomaly-NIDS against adversarial manipulation of traffic features.",
        "Combine supervised and self-/unsupervised representation learning for rare and novel attack detection."
      ],
      "architectural_improvement_recommendations": [
        "Use hybrid architectures (e.g., CNN+LSTM or AE+CNN) to capture spatial-temporal patterns in flows and sequences.",
        "Apply cost-sensitive learning, focal loss, or rebalancing strategies to handle class imbalance in multi-attack settings.",
        "Adopt efficient feature extraction and model compression (pruning/quantization/knowledge distillation) to meet time complexity constraints.",
        "Leverage self-supervised pretraining on large unlabeled traffic to improve feature robustness and reduce labeled data needs.",
        "Implement robust validation (k-fold, nested CV) with strict separation of flows/sessions across splits to prevent leakage."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Time complexity and execution time causing potential overhead and packet loss",
        "Higher false positive rates in anomaly-based detection",
        "Evolving/novel attack techniques challenging detection",
        "Resource constraints in networks with large volumes of traffic/devices"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Systematic literature review and meta-analysis of ML/DL-based anomaly NIDS from 2017 to 2022.",
      "Explanation of ML/DL methods and identification/explanation of available network intrusion datasets.",
      "Quantitative synthesis of effectiveness, used datasets, detected attacks, classification tasks (binary vs multi-class), and time complexity.",
      "Highlighting current issues and future trends for anomaly-based NIDS."
    ]
  },
  {
    "arxiv_id": "2308.00156v2",
    "title": "On the Impact of the Hardware Warm-Up Time on Deep Learning-Based RF Fingerprinting",
    "authors": "Abdurrahman Elmaghbub; Vincent Immler; Bechir Hamdaoui",
    "abstract": "Deep learning-based RF fingerprinting offers great potential for improving the security robustness of various emerging wireless networks. Although much progress has been done in enhancing fingerprinting methods, the impact of device hardware stabilization and warm-up time on the achievable fingerprinting performances has not received adequate attention. As such, this paper focuses on addressing this gap by investigating and shedding light on what could go wrong if the hardware stabilization aspects are overlooked. Specifically, our experimental results show that when the deep learning models are trained with data samples captured after the hardware stabilizes but tested with data captured right after powering on the devices, the device classification accuracy drops below 37%. However, when both the training and testing data are captured after the stabilization period, the achievable average accuracy exceeds 99%, when the model is trained and tested on the same day, and achieves 88% and 96% when the model is trained on one day but tested on another day, for the wireless and wired scenarios, respectively. Additionally, in this work, we leverage simulation and testbed experimentation to explain the cause behind the I/Q signal behavior observed during the device hardware warm-up time that led to the RF fingerprinting performance degradation. Furthermore, we release a large WiFi dataset, containing both unstable (collected during the warm-up period) and stable (collected after the warm-up period) captures across multiple days. Our work contributes datasets, explanations, and guidelines to enhance the robustness of RF fingerprinting in securing emerging wireless networks.",
    "published_date": "2023-07-31",
    "pdf_link": "https://arxiv.org/pdf/2308.00156v2",
    "paper_types": [
      "empirical_analysis",
      "new_dataset"
    ],
    "security_domain": {
      "primary": "Wireless Security",
      "subdomain": "RF fingerprinting and device authentication",
      "specific_problem": "Impact of transceiver hardware warm-up/stabilization time on deep learning-based RF device fingerprinting accuracy",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "CNN",
        "specific": "CNN model employed in [10] with 2x8192 I/Q input",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "NetSTAR Lab Pycom WiFi I/Q Warm-Up Dataset (stable and unstable captures, wired and wireless, multi-day)",
        "type": "public",
        "domain": "rf_iq_signals",
        "link": "http://research.engr.oregonstate.edu/hamdaoui/datasets",
        "is_new_contribution": true,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "accuracy"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "What is the impact of device hardware warm-up/stabilization time on the accuracy of DL-based RF fingerprinting?",
        "What goes wrong when training on stabilized data but testing on data captured immediately after power-on?",
        "Is the commonly observed cross-time (cross-day) performance degradation due to wireless channel variations or hardware instability during warm-up?",
        "What causes the observed time-domain I/Q signal behavior during warm-up that leads to accuracy degradation?"
      ],
      "gaps_identified": [
        "Hardware stabilization and warm-up time have not received adequate attention in DL-based RF fingerprinting data collection and evaluation.",
        "Cross-day performance drops are often attributed to wireless channel effects; the role of hardware warm-up instability has been overlooked."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Improve robustness of RF fingerprinting by understanding and quantifying the effect of hardware warm-up/stabilization on I/Q-based deep learning classifiers and providing datasets and guidelines.",
      "potential_research_ideas": [
        "Design warm-up aware training protocols that detect and exclude or separately model early power-on (unstable) segments.",
        "Develop domain adaptation or transfer learning methods to bridge unstable-to-stable distribution shifts across power cycles.",
        "Physics-informed models that explicitly parameterize/correct CFO and oscillator drift during warm-up within the network.",
        "Self-supervised pretraining on unlabeled I/Q streams to learn invariances to warm-up-induced dynamics.",
        "Data augmentation that simulates warm-up effects (time-varying CFO/phase/amplitude envelopes) to improve robustness.",
        "Online calibration modules that estimate stabilization state and gate classification decisions until stability is reached.",
        "Cross-hardware generalization studies across chipsets and wireless standards (e.g., 802.11g/n, BLE, LoRa) to validate generality.",
        "Multi-receiver fusion to disambiguate channel versus hardware warm-up effects in the wild."
      ],
      "architectural_improvement_recommendations": [
        "Add a front-end signal conditioning layer that estimates and compensates time-varying CFO/phase drift before the CNN.",
        "Use a two-branch model: one branch for stable captures, one for unstable, with a learned gating mechanism based on a stabilization detector.",
        "Incorporate temporal models (e.g., 1D CNN + Transformer) over longer I/Q windows to capture evolving warm-up dynamics.",
        "Adopt domain-adversarial training to reduce the distribution gap between min1 (unstable) and min12 (stable) regimes.",
        "Joint multitask learning to predict device ID and stabilization state, encouraging the network to disentangle warm-up effects."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "IoT testbed with 15 Pycom transmitters and a USRP B210 receiver; wireless (1 m line-of-sight) and wired (SMA) setups",
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Instability during hardware warm-up causes large accuracy drops if not accounted for.",
        "Need to wait approximately 12 minutes after power-on for stabilization to achieve high accuracy.",
        "Potential misattribution of performance degradation to channel effects rather than hardware warm-up.",
        "Operational requirement to detect stabilization state before authenticating devices."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Experimental study quantifying the impact of hardware warm-up on DL-based RF fingerprinting accuracy.",
      "Evidence that training on stabilized data but testing on early power-on data drops accuracy below 37% (about 30% wireless, 20% wired).",
      "When both training and testing on stabilized captures, same-day accuracy exceeds 99%; cross-day achieves 88% (wireless) and 96% (wired).",
      "Simulation and testbed explanations of the I/Q signal behavior during warm-up (e.g., CFO/oscillator impairments) that drive performance degradation.",
      "Release of a large WiFi I/Q dataset including unstable and stable captures across multiple days, for both wired and wireless scenarios (public link provided).",
      "Guidelines to account for hardware stabilization to enhance robustness of RF fingerprinting."
    ]
  },
  {
    "arxiv_id": "2307.10256v1",
    "title": "Hidden Markov Models with Random Restarts vs Boosting for Malware Detection",
    "authors": "Aditya Raghavan; Fabio Di Troia; Mark Stamp",
    "abstract": "Effective and efficient malware detection is at the forefront of research into building secure digital systems. As with many other fields, malware detection research has seen a dramatic increase in the application of machine learning algorithms. One machine learning technique that has been used widely in the field of pattern matching in general-and malware detection in particular-is hidden Markov models (HMMs). HMM training is based on a hill climb, and hence we can often improve a model by training multiple times with different initial values. In this research, we compare boosted HMMs (using AdaBoost) to HMMs trained with multiple random restarts, in the context of malware detection. These techniques are applied to a variety of challenging malware datasets. We find that random restarts perform surprisingly well in comparison to boosting. Only in the most difficult \"cold start\" cases (where training data is severely limited) does boosting appear to offer sufficient improvement to justify its higher computational cost in the scoring phase.",
    "published_date": "2023-07-17",
    "pdf_link": "https://arxiv.org/pdf/2307.10256v1",
    "paper_types": [
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Malware Analysis",
      "subdomain": "Malware Detection",
      "specific_problem": "Opcode-sequence-based binary classification of malware vs benign using HMMs; comparison of HMMs with multiple random restarts versus AdaBoosted HMM ensembles under standard, morphing, and cold-start scenarios",
      "attack_types": [
        "Trojan (Cridex)",
        "Backdoor (Harebot)",
        "Spyware (Security Shield)",
        "Trojan (Zbot/Zeus)",
        "Trojan with rootkit (ZeroAccess)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Hidden Markov Model",
        "specific": null,
        "novel_contribution": "Systematic use of 1000 random restarts for HMM training and selecting the best model for malware detection; authors note this approach has not been explicitly applied in malware detection before."
      },
      {
        "type": "primary",
        "category": "Boosting / Ensemble",
        "specific": "AdaBoost",
        "novel_contribution": "Boosting HMM-based classifiers with AdaBoost for malware detection; authors note boosted HMMs have not received much attention in information security."
      },
      {
        "type": "baseline",
        "category": "Hidden Markov Model",
        "specific": null,
        "novel_contribution": "A single, typically trained HMM (the paper’s “average HMM”) used as the reference baseline."
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Malicia malware dataset (families: Cridex, Harebot, Security Shield, Zbot, ZeroAccess)",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Benign Windows System32 executables (fresh install)",
        "type": "private",
        "domain": "benign_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Average HMM (single model)",
        "paper_reference": null,
        "metric": "AUC (ROC) and Accuracy",
        "their_result": "\"the differences between the average HMM and the best of the random restarts model is generally significant\"",
        "baseline_result": "Average HMM underperforms compared to selecting the best model from 1000 random restarts in most families and settings."
      },
      {
        "method_name": "Boosted HMMs (AdaBoost) vs Random Restarts — Initial experiments",
        "paper_reference": null,
        "metric": "AUC (ROC)",
        "their_result": "\"boosting had little effect on the results for Harebot, Zbot, and ZeroAccess\"; \"for Cridex and Security Shield, boosting does provide some measurable improvement\"",
        "baseline_result": "Random restarts generally competitive; boosting offers only modest gains where non-boosted models are weakest."
      },
      {
        "method_name": "Boosted HMMs (AdaBoost) vs Random Restarts — Morphing experiments",
        "paper_reference": null,
        "metric": "AUC (ROC)",
        "their_result": "\"Cridex at 10% morphing and ZeroAccess at 50% morphing both show substantial improvement for the boosted models, as compared to random restarts.\"",
        "baseline_result": "In most morphing cases, differences between random restarts and boosted are not large; boosting sometimes helps."
      },
      {
        "method_name": "Boosted HMMs (AdaBoost) vs Random Restarts — Cold start",
        "paper_reference": null,
        "metric": "AUC (ROC) and Accuracy",
        "their_result": "\"the advantage of boosting over multiple random restarts ... is generally greatest for the cases where the training data is very limited\"",
        "baseline_result": "Random restarts consistently outperform average HMM; boosting yields notable gains primarily in low-data regimes."
      },
      {
        "method_name": "Computational efficiency comparison (scoring phase)",
        "paper_reference": null,
        "metric": "Qualitative efficiency",
        "their_result": "\"boosting can significantly increase the work factor at the scoring phase\"; \"For random restarts, we simply select the best model, and hence the scoring phase is no more costly than a single HMM.\"",
        "baseline_result": "Random restarts preferred unless boosting provides significant accuracy gains to justify higher scoring cost."
      }
    ],
    "performance_metrics_used": [
      "ROC",
      "AUC",
      "Accuracy",
      "TPR (Recall)",
      "FPR"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How do HMMs trained with multiple random restarts compare to AdaBoosted HMMs for malware detection?",
        "Under code morphing (benign opcode insertion) and cold-start (limited training data), which approach provides better accuracy/AUC and cost trade-offs?",
        "Is the improvement from boosting sufficient to justify its higher computational cost during scoring?"
      ],
      "gaps_identified": [
        "Boosted HMMs appear to have received little prior attention in information security; their utility relative to simpler random-restart selection is unclear.",
        "Random-restart HMM selection had not been explicitly evaluated for malware detection despite HMMs’ hill-climb training nature.",
        "Sensitivity to limited data (cold start) and code morphing/obfuscation remains a practical challenge for opcode-based detection.",
        "Model instability observed for some families (e.g., Zbot) suggests sensitivity to training sample selection."
      ],
      "limitations": [
        "Single feature modality: mnemonic opcode sequences only (top 30 per family plus 'other').",
        "Code morphing is simulated by inserting benign opcode sequences into malware, which may not capture all real-world obfuscation/metamorphism behaviors.",
        "Experiments restricted to Malicia families plus a locally collected benign set; generalization to broader malware/ecosystems not demonstrated.",
        "No public code; implementation details like exact HMM hyperparameters (e.g., number of states) not fully specified in the excerpt.",
        "Boosting vs random restarts compared on per-family setups; cross-family generalization or multi-family models not explored."
      ],
      "future_work": [],
      "motivation": "Evaluate whether simple HMM random restarts can match or surpass AdaBoosted HMMs for malware detection, and identify when boosting’s added scoring cost is justified, especially in challenging settings (morphing, cold start).",
      "potential_research_ideas": [
        "Develop hybrid ensembles that combine best-of-random-restart selection with a small boosted subset selected for maximum diversity to minimize scoring cost.",
        "Investigate representation learning for opcode sequences (e.g., opcode embeddings or n-gram transformers) as inputs to HMMs or sequence discriminative models to improve robustness under obfuscation.",
        "Extend to dynamic behavior features (API call sequences, system calls) and evaluate multi-modal HMM or mixed graphical models vs boosting.",
        "Design adversarially robust training (e.g., data augmentation with realistic obfuscation transformations, adversarial training) tailored to opcode sequences.",
        "Study cross-family or universal models that generalize to unseen families, including few-shot or meta-learning approaches for cold start.",
        "Analyze and mitigate model instability (e.g., for Zbot) via bagging, cross-fold model aggregation, or stability selection."
      ],
      "architectural_improvement_recommendations": [
        "Use diversity-driven selection of HMMs for boosting (e.g., select weak learners with low pairwise correlation) to enhance AdaBoost gains while limiting ensemble size.",
        "Adopt discriminative sequence models (e.g., Conditional Random Fields, HCRF) or discriminative training of HMMs (MMI/MCE) to improve separability vs standard generative HMMs.",
        "Incorporate opcode embedding layers and learnable emission distributions (e.g., neural HMMs) to capture richer context beyond top-30 opcode buckets.",
        "Apply bagging or stacked generalization over random-restart HMMs to stabilize families like Zbot and reduce variance.",
        "Integrate realistic obfuscation/morphing pipelines (packer/unpacker traces, code reordering, junk insertion) for training-time augmentation to improve robustness.",
        "Calibrate scores (e.g., Platt scaling/Isotonic) per-family to harmonize thresholds and improve ROC operating points across families."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Training: 1000 random restarts per family; Scoring: boosted HMMs require evaluating multiple HMMs per sample, increasing work factor compared to a single best HMM; 5-fold cross validation used."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Boosted HMMs increase scoring-time computational cost compared to selecting a single best HMM.",
        "Cold-start scenarios with very limited training data reduce accuracy; boosting helps but at higher cost.",
        "Code morphing/obfuscation (even simulated) degrades separability between malware and benign.",
        "Model instability for some families (e.g., Zbot) indicates sensitivity to training sample selection."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": true,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Comprehensive empirical comparison of HMMs with multiple random restarts versus AdaBoosted HMMs for malware detection across five malware families.",
      "Demonstrates that random restarts generally perform surprisingly well relative to boosting; boosting offers notable gains primarily in the most difficult cold-start cases.",
      "Evaluates robustness under simulated code morphing at 10%, 50%, and 100% benign-insertion levels, showing mixed benefits of boosting.",
      "Analyzes computational trade-offs, noting boosted HMMs’ higher scoring-phase cost versus single best-model selection after random restarts."
    ]
  },
  {
    "arxiv_id": "2308.01237v3",
    "title": "LSF-IDM: Automotive Intrusion Detection Model with Lightweight Attribution and Semantic Fusion",
    "authors": "Pengzhou Cheng; Lei Hua; Haobin Jiang; Gongshen Liu",
    "abstract": "Autonomous vehicles (AVs) are more vulnerable to network attacks due to the high connectivity and diverse communication modes between vehicles and external networks. Deep learning-based Intrusion detection, an effective method for detecting network attacks, can provide functional safety as well as a real-time communication guarantee for vehicles, thereby being widely used for AVs. Existing works well for cyber-attacks such as simple-mode but become a higher false alarm with a resource-limited environment required when the attack is concealed within a contextual feature. In this paper, we present a novel automotive intrusion detection model with lightweight attribution and semantic fusion, named LSF-IDM. Our motivation is based on the observation that, when injected the malicious packets to the in-vehicle networks (IVNs), the packet log presents a strict order of context feature because of the periodicity and broadcast nature of the CAN bus. Therefore, this model first captures the context as the semantic feature of messages by the BERT language framework. Thereafter, the lightweight model (e.g., BiLSTM) learns the fused feature from an input packet's classification and its output distribution in BERT based on knowledge distillation. Experiment results demonstrate the effectiveness of our methods in defending against several representative attacks from IVNs. We also perform the difference analysis of the proposed method with lightweight models and Bert to attain a deeper understanding of how the model balance detection performance and model complexity.",
    "published_date": "2023-08-02",
    "pdf_link": "https://arxiv.org/pdf/2308.01237v3",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Automotive Security",
      "subdomain": "In-Vehicle Network Intrusion Detection",
      "specific_problem": "Lightweight intrusion detection on CAN bus via contextual semantic features and knowledge distillation for resource-constrained deployment",
      "attack_types": [
        "DoS (Denial of Service)",
        "Fuzzy (random ID/DATA injection)",
        "RPM injection",
        "GEAR injection"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "BERT",
        "novel_contribution": "Teacher model to capture contextual semantic features of CAN messages; used for distillation and semantic fusion"
      },
      {
        "type": "primary",
        "category": "Knowledge Distillation",
        "specific": "Teacher: BERT; Student: BiLSTM/DNN",
        "novel_contribution": "Fuses teacher output distribution with student features via a controlled hyper-parameter to balance detection performance and model complexity"
      },
      {
        "type": "primary",
        "category": "RNN",
        "specific": "BiLSTM",
        "novel_contribution": "Lightweight student classifier receiving fused features"
      },
      {
        "type": "primary",
        "category": "DNN",
        "specific": null,
        "novel_contribution": "Alternative lightweight student model evaluated for generalization"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Transfer Learning",
      "Knowledge Distillation",
      "Fine-tuning"
    ],
    "datasets": [
      {
        "name": "HCRL Car Hacking (Car Hacking for Intrusion Detection)",
        "type": "public",
        "domain": "can_bus_logs",
        "link": "https://ocslab.hksecurity.net/Datasets/car-hacking-dataset",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "BiLSTM-only (lightweight model without BERT fusion)",
        "paper_reference": null,
        "metric": "accuracy, FAR",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "DNN-only (lightweight model without BERT fusion)",
        "paper_reference": null,
        "metric": "accuracy, FAR",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "BERT-only (teacher without lightweight fusion)",
        "paper_reference": "Alkhatib et al. (CAN-BERT) cited as prior use of BERT for IVN-IDS",
        "metric": "accuracy, FAR",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "accuracy",
      "false alarm rate (FAR)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How to capture contextual semantic features of CAN messages to detect attacks concealed within context while meeting resource constraints?",
        "Can knowledge distillation from a pre-trained BERT teacher to lightweight students (BiLSTM/DNN) balance detection performance and model complexity?",
        "Does semantic fusion of teacher output distribution with lightweight features reduce false alarm rates on IVN intrusion detection?"
      ],
      "gaps_identified": [
        "Pre-trained NLP models (e.g., BERT, GPT) achieve strong performance but are too heavy for resource-limited IVNs",
        "Lightweight models suffer from high false alarm rates and performance bottlenecks for context-dependent attacks",
        "Many prior works rely on increasingly complex deep structures that are impractical for real-time in-vehicle deployment"
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Provide an IDS for AVs that leverages contextual semantics of CAN traffic while remaining lightweight enough for resource-constrained IVNs, reducing false alarms compared to prior lightweight methods.",
      "potential_research_ideas": [
        "Domain-adaptive pretraining of smaller Transformer models on large unlabeled CAN logs followed by distillation to ultra-light students",
        "Online/continual learning to adapt to vehicle-specific CAN patterns and mitigate dataset shift while maintaining low FAR",
        "Federated or split learning across fleets to improve generalization without sharing raw CAN data",
        "Adversarial robustness evaluation and defenses against targeted CAN message perturbations or mimicry attacks",
        "Explainable IDS mechanisms (e.g., attention-based or counterfactual explanations) to localize suspicious IDs/bytes for incident response",
        "Multi-view fusion of temporal frequency features (inter-arrival times) with semantic token sequences for improved context sensitivity",
        "Confidence calibration and uncertainty-aware detection thresholds to lower FAR in deployment",
        "Self-supervised/contrastive pretraining on CAN sequences to reduce labeled data requirements before distillation"
      ],
      "architectural_improvement_recommendations": [
        "Replace full BERT with compact domain-specific Transformers (e.g., DistilBERT, TinyBERT, MobileBERT) plus structured distillation (logits + intermediate layer/attention matching)",
        "Augment tokenization with explicit CAN-structure embeddings (ID embedding, byte-position embeddings, time-gap/periodicity embeddings)",
        "Use lightweight temporal convolutions or Temporal Convolutional Networks in the student for faster inference versus BiLSTM",
        "Apply quantization and pruning to the student and the distilled teacher head for edge deployment",
        "Incorporate temperature scaling and loss-balancing hyper-parameters for more stable knowledge distillation and FAR control",
        "Introduce memory modules or periodicity-aware positional encodings to better capture CAN bus regularities"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Teacher model described as BERT-base (12 Transformer blocks, hidden size 768, ~110M parameters). Student models are lightweight (BiLSTM/DNN). Exact training hardware and timings not specified."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "In-vehicle networks (CAN bus) with resource constraints",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Limited computing and storage resources in IVNs",
        "High real-time and reliability requirements for in-vehicle systems",
        "High FAR in lightweight models without context-aware features",
        "Large pre-trained language models are impractical for on-vehicle deployment"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Propose LSF-IDM, a lightweight IDS that fuses semantic features from a BERT teacher with a lightweight student via knowledge distillation",
      "Leverage BERT to capture contextual semantic features of CAN messages; transfer this knowledge to BiLSTM/DNN students",
      "Demonstrate improved detection accuracy and reduced false alarm rates on the HCRL car hacking dataset versus traditional ML methods",
      "Analyze trade-offs between detection performance and model complexity by comparing lightweight models and BERT"
    ]
  },
  {
    "arxiv_id": "2308.00121v3",
    "title": "Getting pwn'd by AI: Penetration Testing with Large Language Models",
    "authors": "Andreas Happe; Jürgen Cito",
    "abstract": "The field of software security testing, more specifically penetration testing, is an activity that requires high levels of expertise and involves many manual testing and analysis steps. This paper explores the potential usage of large-language models, such as GPT3.5, to augment penetration testers with AI sparring partners. We explore the feasibility of supplementing penetration testers with AI models for two distinct use cases: high-level task planning for security testing assignments and low-level vulnerability hunting within a vulnerable virtual machine. For the latter, we implemented a closed-feedback loop between LLM-generated low-level actions with a vulnerable virtual machine (connected through SSH) and allowed the LLM to analyze the machine state for vulnerabilities and suggest concrete attack vectors which were automatically executed within the virtual machine. We discuss promising initial results, detail avenues for improvement, and close deliberating on the ethics of providing AI-based sparring partners.",
    "published_date": "2023-07-24",
    "pdf_link": "https://arxiv.org/pdf/2308.00121v3",
    "paper_types": [
      "position",
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Software and Systems Security",
      "subdomain": "Penetration Testing",
      "specific_problem": "LLM-assisted high-level task planning and low-level automated privilege escalation on a Linux host",
      "attack_types": [
        "Privilege Escalation",
        "Password Spraying",
        "Kerberoasting",
        "AS-REP Roasting",
        "Active Directory Certificate Services abuse",
        "Unconstrained Delegation abuse",
        "Group Policy abuse",
        "Phishing",
        "OSINT-based reconnaissance",
        "SUID binary abuse",
        "sudo abuse/GTFObins",
        "Reverse shell"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "GPT-3.5-turbo",
        "novel_contribution": "Closed-feedback loop where the LLM emits concrete shell commands executed via SSH on a vulnerable Linux VM; command outputs are iteratively fed back as context to drive further exploitation steps."
      },
      {
        "type": "baseline",
        "category": "Autonomous LLM agent framework",
        "specific": "AgentGPT",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Autonomous LLM agent framework",
        "specific": "AutoGPT",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Zero-shot prompting",
      "Autonomous LLM agent planning"
    ],
    "datasets": [
      {
        "name": "lin.security vulnerable Linux VM",
        "type": "public",
        "domain": "host_os",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "“To what extent can we automate security testing with LLMs?”"
      ],
      "gaps_identified": [
        "Chronic lack of skilled penetration testers; need for ‘sparring partners’.",
        "LLM hallucinations and weak multi-step planning in low-level exploitation.",
        "Instability and non-determinism across runs compared to scripted enumeration tools.",
        "Limited prompt/context window constrains memory of system state.",
        "Cloud-hosted models raise data sharing and moderation constraints; lack of evaluation of local models.",
        "Ethical moderation inconsistently triggered; easy to bypass with prompt phrasing."
      ],
      "limitations": [
        "“Singular prototype runs were not stable” and showed variation in command sequences.",
        "Evaluation primarily on a single deliberately vulnerable Linux VM; no broad quantitative study.",
        "Reliance on GPT-3.5-turbo via cloud API; results sensitive to moderation and prompt phrasing.",
        "No actual execution of high-level offensive operations against a real organization (ethical filters prevented scans/phishing).",
        "Limited multi-step planning noted: “indicating lacking multi-step planning capabilities of either our script or the underlying model.”",
        "Memory limited by context window; simplistic memory strategy in prototype.",
        "No comparative baselines or statistical success metrics reported."
      ],
      "future_work": [
        "Integrate high-level task planning and low-level exploitation into a unified system.",
        "Evaluate cloud models versus locally run LLMs (e.g., LLaMA-family) for cost, privacy, and capability.",
        "Improve memory with verification/reflection, multi-stream memories, and summarized state to reduce hallucinations.",
        "Automate prompt generation and optimization (with human oversight).",
        "Explore customer-specific fine-tuning over successive engagements and right-size model parameter counts."
      ],
      "motivation": "Address workforce shortage in penetration testing by augmenting testers with AI ‘sparring partners’ to improve efficiency and training for novices while keeping humans-in-the-loop.",
      "potential_research_ideas": [
        "Create a benchmark suite for automated pentesting agents across diversified targets (Linux/Windows/AD/cloud) with standardized success and safety metrics.",
        "Develop planning-aware LLM agents that interleave tool-driven enumeration with goal-conditioned reasoning, leveraging MITRE ATT&CK as a structured ontology.",
        "Investigate reflective memory architectures that extract and persist only salient security facts, hypotheses, and outstanding tests to cut hallucinations.",
        "Evaluate detectability trade-offs by pairing offensive LLM agents with blue-team telemetry to measure pattern predictability and IDS evasion.",
        "Study responsible-automation controls (policy engines, constrained command whitelists, execution sandboxes) for dual-use LLM agents.",
        "Assess effectiveness of locally fine-tuned, domain-specific LLMs trained on sanitized pentest notes and CTF write-ups while enforcing safety filters."
      ],
      "architectural_improvement_recommendations": [
        "Add a task/planning module that explicitly models tactics→techniques→procedures and decomposes goals into verifiable subgoals with success checks.",
        "Introduce multi-stream memory (recent commands, extracted findings, hypothesized system model) with periodic reflection/summary updates.",
        "Implement tool-use orchestration (enumeration scripts, package/database queries) with function-calling and schema-validated outputs.",
        "Incorporate verification loops (e.g., self-consistency checks, external retrieval from vetted knowledge bases) before executing high-impact commands.",
        "Support local models for privacy and cost control; add adapters/fine-tuning for environment-specific jargon and configurations.",
        "Add safety guardrails: allowlisted commands, rate limits, sandbox isolation, and human-in-the-loop approvals for risky actions."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/ipa-lab/hackingBuddyGPT",
      "frameworks": [
        "Python",
        "OpenAI API"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Low; uses a cloud LLM API and a single vulnerable VM over SSH. No GPU training required."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Local vulnerable Linux VM accessed via SSH; high-level planning tested against a real organization’s public web presence with approval (no intrusive actions performed).",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Ethical/safety moderation causing denials or inconsistent behavior.",
        "Data privacy concerns when sending context to cloud LLMs.",
        "Instability and non-determinism across runs.",
        "Hallucinations and weak multi-step planning.",
        "Context window limits constraining memory.",
        "Dual-use risk and potential misuse.",
        "Cost and API dependency for cloud-hosted models."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Formulates and investigates the feasibility of using LLMs as ‘sparring partners’ for penetration testing across high-level planning and low-level exploitation.",
      "Implements a closed-loop prototype where GPT-3.5 generates shell commands executed over SSH on a vulnerable Linux VM with outputs fed back as context.",
      "Demonstrates practical success in gaining root on the target VM via common vectors (e.g., sudo -l plus GTFObins).",
      "Publishes source code and documentation (hackingBuddyGPT).",
      "Provides qualitative analysis of grounding vs. hallucinations, stability/reproducibility, and ethical moderation behavior.",
      "Outlines a vision and concrete avenues for improving memory, planning, integration of high/low-level workflows, and evaluation of local models."
    ]
  },
  {
    "arxiv_id": "2307.05437v1",
    "title": "Improving the Security of Smartwatch Payment with Deep Learning",
    "authors": "George Webber",
    "abstract": "Making contactless payments using a smartwatch is increasingly popular, but this payment medium lacks traditional biometric security measures such as facial or fingerprint recognition. In 2022, Sturgess et al. proposed WatchAuth, a system for authenticating smartwatch payments using the physical gesture of reaching towards a payment terminal. While effective, the system requires the user to undergo a burdensome enrolment period to achieve acceptable error levels. In this dissertation, we explore whether applications of deep learning can reduce the number of gestures a user must provide to enrol into an authentication system for smartwatch payment. We firstly construct a deep-learned authentication system that outperforms the current state-of-the-art, including in a scenario where the target user has provided a limited number of gestures. We then develop a regularised autoencoder model for generating synthetic user-specific gestures. We show that using these gestures in training improves classification ability for an authentication system. Through this technique we can reduce the number of gestures required to enrol a user into a WatchAuth-like system without negatively impacting its error rates.",
    "published_date": "2023-07-11",
    "pdf_link": "https://arxiv.org/pdf/2307.05437v1",
    "paper_types": [
      "empirical_analysis",
      "reproducibility",
      "new_technique"
    ],
    "security_domain": {
      "primary": "IoT/Wearable Security",
      "subdomain": "User Authentication (Behavioral Biometrics)",
      "specific_problem": "Smartwatch payment authentication using payment-gesture biometrics with reduced enrollment burden",
      "attack_types": [
        "Impersonation/Impostor attempts (unauthorized user attempting payment)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Deep sequence modeling (CNN/RNN)",
        "specific": null,
        "novel_contribution": "Supervised deep-learned authentication model on inertial sensor payment-gesture data that outperforms WatchAuth, including in low-enrollment settings"
      },
      {
        "type": "primary",
        "category": "Autoencoder (regularised)",
        "specific": "VAE/WAE-style regularised autoencoder",
        "novel_contribution": "User-specific gesture generation via a regularised autoencoder with latent-space regularisation and user-based clustering to augment training and reduce enrollment samples"
      },
      {
        "type": "baseline",
        "category": "Random Forest",
        "specific": "WatchAuth RF on handcrafted features",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Statistical features + classical ML",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Dynamic Time Warping",
        "specific": "DTW distance for time-series comparison (preliminaries; may be used as comparator)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "WatchAuth dataset",
        "type": "public",
        "domain": "inertial_sensor_timeseries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "WatchAuth (Random Forest on handcrafted features)",
        "paper_reference": "Sturgess et al., 2022 (WatchAuth)",
        "metric": "FAR@0, EER, AUROC",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "FAR (False Acceptance Rate)",
      "FRR (False Rejection Rate)",
      "EER (Equal Error Rate)",
      "AUROC (Area Under ROC)",
      "FAR@0 (FAR when FRR=0)"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can deep learning reduce the number of payment-gesture samples a user must provide to enroll in a smartwatch payment authentication system without degrading error rates?",
        "Can a deep-learned authentication model outperform the WatchAuth baseline, especially under limited enrollment data?",
        "Can a regularised autoencoder generate user-specific synthetic payment gestures that improve authentication performance when used for training?"
      ],
      "gaps_identified": [
        "WatchAuth requires a burdensome enrollment phase (on the order of 100 gestures) to achieve acceptable error rates.",
        "Deep learning approaches for smartwatch authentication using inertial sensors are relatively unexplored.",
        "No published work found on generative data augmentation for biometric authentication with inertial sensors."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Smartwatch contactless payments lack built-in biometric safeguards (e.g., face/fingerprint). Reducing enrollment burden while maintaining security would improve usability and adoption of gesture-based authentication.",
      "potential_research_ideas": [
        "Adversarial robustness evaluation of gesture-based authentication against spoofed or coached gestures and sensor-replay attacks.",
        "Cross-device and cross-domain generalisation: domain adaptation to different smartwatch models, sampling rates, or terminal configurations.",
        "Few-shot/meta-learning approaches to further reduce enrollment to single-digit gestures while maintaining FAR@0.",
        "Multimodal fusion (e.g., adding PPG/heart-rate or magnetometer) to improve robustness with minimal additional power.",
        "Continual learning for gradual personalization from real-world transactions without catastrophic forgetting.",
        "Privacy-preserving synthetic data generation (DP-VAEs/DP-WAEs) to share models/datasets safely."
      ],
      "architectural_improvement_recommendations": [
        "Evaluate specific sequence models (e.g., 1D ResNet + bidirectional GRU/LSTM) and attention mechanisms for temporal alignment.",
        "Contrastive/self-supervised pretraining (e.g., SimCLR/TS-TCC) on unlabeled gesture segments prior to supervised fine-tuning.",
        "Conditional generative modeling (cVAE or conditional diffusion) conditioned on user ID and terminal position to synthesize more diverse, controllable gestures.",
        "Latent regularisation with metric learning (triplet/ArcFace losses) to enforce user separation while preserving intra-user variability.",
        "Augmentation policies tailored to inertial signals (time-warp, jitter, permutation, magnitude warping) combined with synthetic data.",
        "On-device model compression (quantization/pruning/knowledge distillation) to meet smartwatch compute and energy constraints."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Enrollment burden (number of gestures required).",
        "Battery/compute constraints for on-device inference on smartwatches.",
        "Potential variability across smartwatch hardware and sensor quality.",
        "Usability trade-offs when tuning thresholds for FAR vs FRR."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Survey of biometric authentication and deep generative modeling for data augmentation.",
      "Reproduction of WatchAuth methodology and key findings.",
      "Evaluation of supervised deep learning architectures for authentication on WatchAuth, achieving performance that outperforms WatchAuth, including with limited enrollment data.",
      "Design of a regularised autoencoder that generates user-specific synthetic payment gestures.",
      "Demonstration that training with generated gestures improves authentication and reduces required enrollment without degrading error rates."
    ]
  },
  {
    "arxiv_id": "2308.16453v3",
    "title": "Listen to Minority: Encrypted Traffic Classification for Class Imbalance with Contrastive Pre-Training",
    "authors": "Xiang Li; Juncheng Guo; Qige Song; Jiang Xie; Yafei Sang; Shuyuan Zhao; Yongzheng Zhang",
    "abstract": "Mobile Internet has profoundly reshaped modern lifestyles in various aspects. Encrypted Traffic Classification (ETC) naturally plays a crucial role in managing mobile Internet, especially with the explosive growth of mobile apps using encrypted communication. Despite some existing learning-based ETC methods showing promising results, three-fold limitations still remain in real-world network environments, 1) label bias caused by traffic class imbalance, 2) traffic homogeneity caused by component sharing, and 3) training with reliance on sufficient labeled traffic. None of the existing ETC methods can address all these limitations. In this paper, we propose a novel Pre-trAining Semi-Supervised ETC framework, dubbed PASS. Our key insight is to resample the original train dataset and perform contrastive pre-training without using individual app labels directly to avoid label bias issues caused by class imbalance, while obtaining a robust feature representation to differentiate overlapping homogeneous traffic by pulling positive traffic pairs closer and pushing negative pairs away. Meanwhile, PASS designs a semi-supervised optimization strategy based on pseudo-label iteration and dynamic loss weighting algorithms in order to effectively utilize massive unlabeled traffic data and alleviate manual train dataset annotation workload. PASS outperforms state-of-the-art ETC methods and generic sampling approaches on four public datasets with significant class imbalance and traffic homogeneity, remarkably pushing the F1 of Cross-Platform215 with 1.31%, ISCX-17 with 9.12%. Furthermore, we validate the generality of the contrastive pre-training and pseudo-label iteration components of PASS, which can adaptively benefit ETC methods with diverse feature extractors.",
    "published_date": "2023-08-31",
    "pdf_link": "https://arxiv.org/pdf/2308.16453v3",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Traffic Analysis",
      "specific_problem": "Encrypted traffic classification (ETC) for mobile app identification under severe class imbalance and traffic homogeneity with limited labeled data",
      "attack_types": [
        "traffic analysis"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "Multi-head self-attention encoder (stacked N=6)",
        "novel_contribution": "Used as the backbone feature extractor for encrypted flow sequences built from multi-granularity inputs (raw payload 2-byte tokens + signed packet lengths)"
      },
      {
        "type": "primary",
        "category": "Contrastive Learning",
        "specific": "InfoNCE",
        "novel_contribution": "Contrastive pre-training with tailored positive/negative sampling that uses strong negatives (different classes) and weak negatives (same class, different communication info such as TLS certificate or destination IP:port), with weighted loss emphasizing strong negatives"
      },
      {
        "type": "primary",
        "category": "Pseudo-labeling",
        "specific": "Iterative pseudo-labeling with confidence threshold",
        "novel_contribution": "Semi-supervised fine-tuning with iterative pseudo-labeling, weighted sampling for class rebalancing, and dynamic loss weighting between labeled and pseudo-labeled data"
      },
      {
        "type": "primary",
        "category": "Sampling/Rebalancing",
        "specific": "Positive–negative pairs resampling; class-proportional weighted sampling",
        "novel_contribution": "Custom sampling to build contrastive pairs and to re-balance pseudo-labeled data by inverse class frequency"
      },
      {
        "type": "primary",
        "category": "Loss Reweighting",
        "specific": "Dynamic loss weighting",
        "novel_contribution": "Set w1=1.0 for labeled data and w2=0.5 for pseudo-labeled data to mitigate noise"
      },
      {
        "type": "primary",
        "category": "Embedding/Projection",
        "specific": "Two-layer MLP projection head with ReLU",
        "novel_contribution": "Projection head connects encoder features to contrastive objective to preserve informative features during pre-training"
      }
    ],
    "learning_paradigm": [
      "Self-supervised",
      "Semi-supervised",
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Cross-Platform215 (C-P215)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "ISCX-17 (VPN/non-VPN app traffic, 17 classes)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CST-TLS1.3 (C-TLS1.3)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CIC2019",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "State-of-the-art ETC methods",
        "paper_reference": null,
        "metric": "F1",
        "their_result": "“remarkably pushing the F1 of Cross-Platform215 with 1.31%, ISCX-17 with 9.12%”",
        "baseline_result": null
      },
      {
        "method_name": "Generic sampling approaches (oversampling/undersampling)",
        "paper_reference": null,
        "metric": "F1",
        "their_result": "PASS outperforms generic sampling approaches on four public datasets",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "F1"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "RQ1: Compared to existing ETC methods, can the proposed PASS achieve better classification performance, especially in combating class imbalance and homogeneous traffic?",
        "RQ2: How does each part of PASS contribute to the performance?",
        "RQ3: How do sensitivity parameters, such as the number of pre-training samples, affect the performance?",
        "RQ4: For existing ETC methods, can PASS provide some guidance to improve their performance?"
      ],
      "gaps_identified": [
        "Label bias caused by traffic class imbalance in real-world environments",
        "Traffic homogeneity caused by component sharing across apps (shared repos, domains, CDNs)",
        "Training relies on sufficient labeled traffic; labeling at scale is difficult",
        "Lack of research explicitly targeting homogeneous traffic in ETC",
        "Most prior ETC evaluations rely on balanced/idealized datasets"
      ],
      "limitations": [
        "Assumes TCP flows and focuses on encrypted TCP (e.g., HTTPS) only",
        "Assumes plaintext payload and destination features cannot be used in classification; communication info used only for pre-training negatives",
        "Semi-supervised step assumes unlabeled data come from the same scenario/distribution as labeled training data",
        "Dynamic loss weights (w1=1, w2=0.5) are chosen based on experience and preliminary experiments",
        "Performance of pseudo-labeling depends on confidence threshold selection obtained via cross-validation"
      ],
      "future_work": [],
      "motivation": "Current learning-based ETC struggles with real-world class imbalance, traffic homogeneity across apps, and the scarcity/cost of labels; the authors aim to build a framework that mitigates label bias, disambiguates homogeneous traffic, and leverages abundant unlabeled data.",
      "potential_research_ideas": [
        "Design adaptive confidence calibration and thresholding for pseudo-labeling (e.g., temperature scaling, class-wise thresholds, or uncertainty-aware selection)",
        "Introduce supervised contrastive learning or class-prototype alignment to further mitigate class imbalance and improve separation of homogeneous traffic",
        "Explore hard negative mining strategies and dynamic negative weighting based on similarity or curriculum schedules",
        "Pre-train on massive, diverse, cross-network unlabeled traffic and evaluate domain generalization and transfer across networks/regions",
        "Incorporate session-level or user-level temporal context (multi-flow sequences) and hierarchical app-family→app classifiers",
        "Combine consistency regularization (e.g., temporal ensembling, Mean Teacher) with pseudo-labeling for more robust semi-supervised training",
        "Use memory-bank or momentum encoders (e.g., MoCo) to scale contrastive pre-training with large negative sets",
        "Leverage graph-based representations (e.g., communication endpoint graphs) to detect and discount shared third-party domains during representation learning",
        "Investigate robustness to evasion (traffic morphing/padding) via augmentation policies that simulate protocol obfuscations",
        "Study privacy-preserving pre-training (federated/self-supervised) across organizations without sharing raw traffic"
      ],
      "architectural_improvement_recommendations": [
        "Replace fixed loss weights with learnable or schedule-based weights; adopt focal or class-balanced losses during fine-tuning",
        "Adopt supervised contrastive loss during fine-tuning with class prototypes to better separate minority classes",
        "Implement momentum contrast (MoCo) or queue-based negatives to stabilize and scale contrastive pre-training",
        "Use adaptive temperature or margin parameters conditioned on negative type (strong vs weak) or class frequency",
        "Augment multi-granularity inputs with flow inter-arrival time statistics and TLS handshake features (when permissible) for richer context",
        "Add multi-level heads (family-level and app-level) for hierarchical classification to reduce confusion from shared components",
        "Incorporate uncertainty estimation (e.g., Monte Carlo dropout) to guide pseudo-label selection and weighting",
        "Evaluate alternative backbones (e.g., lightweight CNN-Transformers) for better inference efficiency without sacrificing accuracy"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Severe class imbalance in real network traffic leading to label bias",
        "Traffic homogeneity due to shared third-party components/domains across apps",
        "Obtaining sufficiently labeled traffic at scale is costly and difficult",
        "Assumptions that destination features and payload plaintext are unavailable may limit feature choices in some deployments",
        "Need for large unlabeled traffic from the target environment to realize semi-supervised gains"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "A new ETC paradigm (PASS) combining multi-granularity traffic sequence construction with a multi-head self-attention encoder and contrastive pre-training to jointly address class imbalance and traffic homogeneity",
      "A semi-supervised optimization strategy with iterative pseudo-labeling, weighted sampling, and dynamic loss weighting to effectively leverage unlabeled traffic and reduce labeling needs",
      "Extensive experiments on four public datasets with significant class imbalance and homogeneity showing PASS surpasses state-of-the-art ETC methods and generic sampling approaches; reported improvements include “pushing the F1 of Cross-Platform215 with 1.31%, ISCX-17 with 9.12%”; components generalize to different feature extractors"
    ]
  },
  {
    "arxiv_id": "2308.13062v1",
    "title": "ZeroLeak: Using LLMs for Scalable and Cost Effective Side-Channel Patching",
    "authors": "M. Caner Tol; Berk Sunar",
    "abstract": "Security critical software, e.g., OpenSSL, comes with numerous side-channel leakages left unpatched due to a lack of resources or experts. The situation will only worsen as the pace of code development accelerates, with developers relying on Large Language Models (LLMs) to automatically generate code. In this work, we explore the use of LLMs in generating patches for vulnerable code with microarchitectural side-channel leakages. For this, we investigate the generative abilities of powerful LLMs by carefully crafting prompts following a zero-shot learning approach. All generated code is dynamically analyzed by leakage detection tools, which are capable of pinpointing information leakage at the instruction level leaked either from secret dependent accesses or branches or vulnerable Spectre gadgets, respectively. Carefully crafted prompts are used to generate candidate replacements for vulnerable code, which are then analyzed for correctness and for leakage resilience. From a cost/performance perspective, the GPT4-based configuration costs in API calls a mere few cents per vulnerability fixed. Our results show that LLM-based patching is far more cost-effective and thus provides a scalable solution. Finally, the framework we propose will improve in time, especially as vulnerability detection tools and LLMs mature.",
    "published_date": "2023-08-24",
    "pdf_link": "https://arxiv.org/pdf/2308.13062v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Hardware Security and Trusted Computing",
      "subdomain": "Side-Channel and Speculative Execution Defenses",
      "specific_problem": "Automated source-level patching of non-constant-time code and Spectre v1 gadgets using LLMs with dynamic/speculative leakage verification",
      "attack_types": [
        "Execution-time (timing) side-channel leakage",
        "Cache/memory access side-channel leakage",
        "Secret-dependent branches/accesses",
        "Spectre v1 (Bounds Check Bypass)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer LLM",
        "specific": "GPT-4",
        "novel_contribution": "Zero-shot, prompt-engineered patch generation integrated with leakage detection tools; stacked prompts and heuristics to produce correct and leakage-resilient code"
      },
      {
        "type": "baseline",
        "category": "Transformer LLM",
        "specific": "GPT-3.5",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer LLM",
        "specific": "Google PaLM 2 (chat-bison)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer LLM",
        "specific": "Meta LLaMA/LLaMA2",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Zero-shot"
    ],
    "datasets": [
      {
        "name": "Microbenchmark of C code compiled from known vulnerabilities (33 leakage points)",
        "type": "synthetic",
        "domain": "software_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Kocher's Spectre v1 gadget suite (15 code snippets)",
        "type": "public",
        "domain": "software_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "GPT-3.5",
        "paper_reference": null,
        "metric": "Leakage patch success rate across all vulnerabilities (microbenchmark)",
        "their_result": "“GPT4 excels in patching 97% of all leakages successfully …”",
        "baseline_result": "“GPT3.5 was able to fix 62% of the leakage points …”"
      },
      {
        "method_name": "Google PaLM 2 (chat-bison)",
        "paper_reference": null,
        "metric": "Leakage patch success rate across all vulnerabilities (microbenchmark)",
        "their_result": "“GPT4 excels in patching 97% of all leakages …”",
        "baseline_result": "“Google chat-bison … patch 56% … across all vulnerabilities …”"
      },
      {
        "method_name": "Meta LLaMA2",
        "paper_reference": null,
        "metric": "Leakage patch success rate across all vulnerabilities (microbenchmark)",
        "their_result": "“GPT4 excels in patching 97% of all leakages …”",
        "baseline_result": "“Meta LLaMa2 patch 35% across all vulnerabilities …”"
      },
      {
        "method_name": "clang LFENCE injection (compiler-based Spectre v1 mitigation)",
        "paper_reference": null,
        "metric": "Runtime overhead of Spectre v1 mitigation",
        "their_result": "“up to 10× faster than Spectre v1 patches generated with existing methods … our toolchain significantly outperforms compiler-based techniques such as in clang lfence injection.”",
        "baseline_result": "Compiler-inserted lfence incurs up to 10× higher overhead vs. GPT-4-generated patches (exact baseline numbers not provided)."
      },
      {
        "method_name": "GPT-3.5",
        "paper_reference": null,
        "metric": "API cost to patch 33 leaks",
        "their_result": "“the total cost of patching 33 leaks is at $1.34” (GPT-4)",
        "baseline_result": "“GPT3.5 … costing 19 times less than GPT4.”"
      }
    ],
    "performance_metrics_used": [
      "Patch success rate (% of leakage points/gadgets fixed)",
      "Number of leaks fixed",
      "Runtime/efficiency overhead vs. compiler lfence mitigation",
      "API cost per vulnerability/total cost",
      "Leakage detection pass/fail using dynamic (Microwalk) and speculative (Spectector, KLEESpectre, Pitchfork) tools",
      "Correctness of generated code (functional validation)"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can zero-shot LLMs generate correct and leakage-resilient patches for microarchitectural side-channel vulnerabilities?",
        "How should prompts be crafted and sequenced to maximize correctness and side-channel security?",
        "Are LLM-generated patches cost-effective and efficient compared to existing compiler-based mitigations?"
      ],
      "gaps_identified": [
        "Numerous side-channel leakages in critical software remain unpatched due to lack of resources/expertise.",
        "Limited adoption and capability of automated constant-time/leakage detection in practice; developers often unaware or do not use such tools.",
        "Existing automated program repair focuses on syntactic/build errors or basic security bugs, not microarchitectural side-channel vulnerabilities.",
        "Compiler-based Spectre mitigations can be inefficient (e.g., lfence overuse).",
        "Open-source projects (e.g., crypto libraries) lack scalable, reliable patch automation and verification pipelines."
      ],
      "limitations": [
        "“Our framework is only limited by the capability of the detection tools, e.g., false positives and negatives …”",
        "Cost varies depending on vulnerability type and code length.",
        "Scope limited to software-enabled leakages; does not address physical side-channel leakages (power/EM).",
        "Assumes software is bug-free functionally (does not address memory safety bugs such as buffer overflows).",
        "Focuses on zero-shot LLM usage; does not train custom models."
      ],
      "future_work": [
        "“the framework we propose will improve in time, especially as vulnerability detection tools and LLMs mature.”",
        "Continuous improvement as hardware/software stacks evolve via CI/CD integration."
      ],
      "motivation": "Large volume of unpatched microarchitectural side-channel vulnerabilities in widely used software and the need for scalable, cost-effective patching as LLM-driven code generation accelerates.",
      "potential_research_ideas": [
        "Create a public benchmark suite and leaderboard for LLM-based side-channel patching across diverse languages, compilers, and microarchitectures (including Spectre v2, Meltdown, MDS).",
        "Fine-tune domain-specific LLMs on verified constant-time patterns and Spectre-safe idioms; compare against zero-shot.",
        "Integrate formal verification (e.g., verified constant-time and SNI proofs) with LLM patch generation via counterexample-guided repair loops.",
        "Multi-objective patch optimization to jointly minimize runtime overhead, code churn, and leakage risk.",
        "Use program analysis/AST-guided constrained decoding to prevent reintroducing secret-dependent control/data flows.",
        "Active learning: mine patches that pass detectors to iteratively refine an internal policy or retrieval-augmented prompt memory.",
        "Extend to automatic repair in large real-world codebases (OpenSSL, NSS) with automated test harness generation."
      ],
      "architectural_improvement_recommendations": [
        "Adopt a closed-loop patching system: LLM proposes edits -> static/dynamic/speculative analyzers evaluate -> feedback prompts drive revised patches (with automated ranking).",
        "Constrained decoding using taint-aware constraints (disallow secret-dependent branches/memory accesses) during generation.",
        "AST-diff and semantics-preserving transformations with equivalence checking and fuzzing to ensure functional correctness.",
        "Hybrid detectors: combine Microwalk (dynamic MI), Pitchfork (constant-time), Spectector/KLEESpectre (SNI/speculative) to reduce false positives/negatives.",
        "Cost-aware prompt planning: adapt model choice and prompt length based on vulnerability class to minimize API spend.",
        "Cache a retrieval library of verified patch templates and inject them via in-context learning."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "LLM via API; reported cost example: GPT-4 patched 33 leaks for $1.34; GPT-3.5 ~19× cheaper; no custom training or GPU required."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Intended for CI/CD pipelines where source is patched and binaries are tested on target machines",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Dependence on accuracy/coverage of leakage detectors (false positives/negatives).",
        "Ensuring functional correctness while eliminating leakages.",
        "Cost variability by vulnerability type and code length.",
        "Compiler/platform-specific behavior affecting leakage and performance.",
        "Generalization from microbenchmarks to large, complex codebases."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "First comprehensive study of state-of-the-art LLMs (GPT, PaLM2, LLaMA) to automatically patch microarchitectural vulnerabilities (non-constant time and Spectre v1).",
      "A toolchain that tests binaries with leakage/Spectre detection tools (Microwalk, Pitchfork, Spectector, KLEESpectre) and automatically generates source-level patches using LLMs.",
      "CI/CD-oriented workflow: patch source while testing binaries, capturing compiler/platform effects.",
      "On a microbenchmark, GPT-4 patched 97% of all leakages; patched 33 leaks at $1.34 total cost; GPT-3.5 fixed 62%; PaLM2 chat-bison 56%; LLaMA2 35%.",
      "Framework improvement tied to better detection tools and advancing LLMs.",
      "Efficiency: up to ~10× faster patches than clang lfence Spectre v1 mitigation.",
      "Generated patches include comments to aid maintainability."
    ]
  },
  {
    "arxiv_id": "2310.05935v1",
    "title": "Vulnerability Clustering and other Machine Learning Applications of Semantic Vulnerability Embeddings",
    "authors": "Mark-Oliver Stehr; Minyoung Kim",
    "abstract": "Cyber-security vulnerabilities are usually published in form of short natural language descriptions (e.g., in form of MITRE's CVE list) that over time are further manually enriched with labels such as those defined by the Common Vulnerability Scoring System (CVSS). In the Vulnerability AI (Analytics and Intelligence) project, we investigated different types of semantic vulnerability embeddings based on natural language processing (NLP) techniques to obtain a concise representation of the vulnerability space. We also evaluated their use as a foundation for machine learning applications that can support cyber-security researchers and analysts in risk assessment and other related activities. The particular applications we explored and briefly summarize in this report are clustering, classification, and visualization, as well as a new logic-based approach to evaluate theories about the vulnerability space.",
    "published_date": "2023-08-23",
    "pdf_link": "https://arxiv.org/pdf/2310.05935v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Vulnerability Management and Risk Assessment",
      "specific_problem": "Learning semantic embeddings of CVE/NVD vulnerability descriptions to support clustering, classification (e.g., CVSS/CWE/CPE label prediction), visualization, and logic-based evaluation of vulnerability-space theories",
      "attack_types": [
        "Cross-Site Scripting (CWE-79)",
        "SQL Injection (CWE-89)",
        "Improper Privilege Management (CWE-269)",
        "Out-of-bounds Read (CWE-125)",
        "Out-of-bounds Write (CWE-787)",
        "Exposure of Sensitive Information (CWE-200)",
        "Remote Code Execution"
      ]
    },
    "ml_techniques": [
      {
        "type": "baseline",
        "category": "Word/Sentence Embeddings",
        "specific": "FastText (pretrained on Common Crawl)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "RNN",
        "specific": "bi-LSTM",
        "novel_contribution": "Refined NLP model operating on sequences of token embeddings for vulnerability descriptions"
      },
      {
        "type": "primary",
        "category": "RNN",
        "specific": "bi-GRU",
        "novel_contribution": "Refined NLP model operating on sequences of token embeddings"
      },
      {
        "type": "primary",
        "category": "CNN",
        "specific": "1D ConvNet",
        "novel_contribution": "Refined NLP model consuming sequences of FastText embeddings"
      },
      {
        "type": "primary",
        "category": "Attention",
        "specific": "Self-attention over bi-LSTM",
        "novel_contribution": "Leverages typical structure of CVE descriptions to improve accuracy"
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "Transformer network (sequence model)",
        "novel_contribution": "Applied to vulnerability description embeddings"
      },
      {
        "type": "baseline",
        "category": "Dimensionality Reduction",
        "specific": "PCA",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Dimensionality Reduction",
        "specific": "UMAP",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Autoencoder",
        "specific": "Feed-forward AE (encoder 300->500->2000->code; symmetric decoder; ReLU + dropout; MSE loss; Adam)",
        "novel_contribution": "Autoencoder-based semantic vulnerability representation yielded superior accuracy among unsupervised DR methods"
      },
      {
        "type": "baseline",
        "category": "Variational Autoencoder",
        "specific": "VAE (normal latent prior)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "GAN",
        "specific": "Generator/Discriminator + inverter for DR",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Dimensionality Reduction",
        "specific": "LDA",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Dimensionality Reduction",
        "specific": "Neighborhood Component Analysis (NCA)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Visualization/DR",
        "specific": "t-SNE",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Visualization/DR",
        "specific": "Parametric t-SNE",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Clustering",
        "specific": "K-Means",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Clustering",
        "specific": "Hierarchical clustering (Ward)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Clustering",
        "specific": "HDBSCAN",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Clustering",
        "specific": "OPTICS",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Classifier",
        "specific": "Naive Bayes",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Classifier",
        "specific": "Logistic Regression",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Classifier",
        "specific": "k-Nearest Neighbors",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Classifier",
        "specific": "SVM (RBF kernel)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Gradient Boosting",
        "specific": "XGBoost",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "MLP",
        "specific": "Feed-forward MLP (1–3 hidden layers) for classification and supervised DR",
        "novel_contribution": "Functional classification MLP tailored to CVSS label prediction and supervised dimensionality reduction; shows best performance; evidence of transfer from CVSS v2 to v3"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised",
      "Transfer Learning"
    ],
    "datasets": [
      {
        "name": "NIST National Vulnerability Database (NVD)",
        "type": "public",
        "domain": "vulnerability_descriptions_and_labels (CVSS, CWE, CPE)",
        "link": "https://nvd.nist.gov/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "MITRE CVE list",
        "type": "public",
        "domain": "vulnerability_descriptions",
        "link": "https://cve.mitre.org/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Common Platform Enumeration (CPE) dictionary",
        "type": "public",
        "domain": "product_platform_labels",
        "link": "https://nvd.nist.gov/products/cpe",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Common Weakness Enumeration (CWE)",
        "type": "public",
        "domain": "weakness_taxonomy_labels",
        "link": "https://cwe.mitre.org/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Common Vulnerability Scoring System (CVSS)",
        "type": "public",
        "domain": "vulnerability_scoring_labels",
        "link": "https://www.first.org/cvss/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Common Crawl (for FastText pretrained embeddings)",
        "type": "public",
        "domain": "web_text_corpus",
        "link": "https://commoncrawl.org/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Naive Bayes (classification)",
        "paper_reference": null,
        "metric": "accuracy, balanced accuracy, precision, recall, F1",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Logistic Regression (classification)",
        "paper_reference": null,
        "metric": "accuracy, balanced accuracy, precision, recall, F1",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "k-Nearest Neighbors (classification)",
        "paper_reference": null,
        "metric": "accuracy, balanced accuracy, precision, recall, F1",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "SVM with RBF kernel (classification)",
        "paper_reference": null,
        "metric": "accuracy, balanced accuracy, precision, recall, F1",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "XGBoost (classification)",
        "paper_reference": "Chen & Guestrin 2016",
        "metric": "accuracy, balanced accuracy, precision, recall, F1",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "PCA (dimensionality reduction)",
        "paper_reference": null,
        "metric": "downstream classification accuracy; clustering metrics (NMI/HOM/COM)",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "K-Means (clustering)",
        "paper_reference": null,
        "metric": "NMI, homogeneity (HOM), completeness (COM)",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Hierarchical clustering (Ward)",
        "paper_reference": null,
        "metric": "NMI, HOM, COM",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "HDBSCAN (clustering)",
        "paper_reference": "Campello et al. 2013/2015",
        "metric": "NMI, HOM, COM",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "OPTICS (clustering)",
        "paper_reference": "Ankerst et al. 1999",
        "metric": "NMI, HOM, COM",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "accuracy",
      "balanced accuracy",
      "precision",
      "recall",
      "F1-score",
      "NMI (Normalized Mutual Information)",
      "homogeneity",
      "completeness"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can concise semantic embeddings of CVE/NVD descriptions be learned to capture key vulnerability properties (e.g., CVSS, CWE, CPE)?",
        "Which dimensionality reduction techniques produce the most useful vulnerability representations for downstream classification and clustering?",
        "Can classifiers trained on CVSS v2 transfer to v3 labels?",
        "How effective are semantic embeddings for visualizing and navigating the vulnerability space over time?",
        "Can a logic-based approach be used to evaluate theories about the vulnerability space?"
      ],
      "gaps_identified": [
        "Subject-matter-expert labels (especially CWE) are often incomplete and sometimes not very informative; a few general CWE classes dominate the dataset.",
        "Derived CVSS numeric scores (0–10) are not reliably reflected in empirical data; qualitative base metrics are preferable for ML.",
        "Concept drift over time is expected but unquantified; models may need temporal adaptation.",
        "CVSS v3 labels are only available for recent years, limiting supervised learning for the full timeline.",
        "Large fractions of CVEs lack CWE labels; many vulnerabilities are unlabeled."
      ],
      "limitations": [
        "Training/validation used data up to 2020 with a 90/10 split; partial 2021 data was not utilized.",
        "Transfer learning from CWE to CVSS did not improve performance, likely due to low information content and label imbalance/noise in CWE annotations.",
        "GAN and VAE underperform for classification/clustering relative to simpler autoencoder and MLP setups.",
        "Risk of overfitting if adding complex temporal weighting; temporal drift not yet modeled.",
        "Linear models and RBF-SVMs were insufficient to capture the complexity of the learned embedding space."
      ],
      "future_work": [
        "Use 2021 CVEs as an additional validation set and periodically retrain all stages to compensate for concept drift.",
        "Quantify temporal drift and explore temporal weighting or time-aware models.",
        "Refine heuristics for temporal trend visualizations (e.g., beyond top-n clusters).",
        "Leverage generative models to sample from the vulnerability space for additional applications.",
        "Extend the workflow with advanced capabilities (including the logic-based theory evaluation)."
      ],
      "motivation": "Support the Vulnerabilities Equities Process and cyber risk assessment by providing a data-driven, ML-based foundation to represent, classify, cluster, and visualize the evolving vulnerability space and place new CVEs in context.",
      "potential_research_ideas": [
        "Domain-adaptive pretraining and fine-tuning of contextual language models (e.g., BERT/RoBERTa) on CVE/NVD corpora for improved embeddings and downstream tasks.",
        "Time-aware or continual-learning models to handle concept drift in vulnerability distributions and labels.",
        "Multi-task learning jointly predicting CVSS, CWE, and CPE labels with hierarchical constraints and label interdependencies.",
        "Graph-based modeling of the CVE–CWE–CPE ecosystem with knowledge-graph embeddings and GNNs for improved inference and explainability.",
        "Semi-supervised and active learning to leverage abundant unlabeled CVEs and to request targeted expert labels for ambiguous cases.",
        "Contrastive learning on CVE text (e.g., SimCSE/Sentence-BERT-style) to build robust sentence embeddings of vulnerability descriptions.",
        "Causal or logic-guided models that integrate the proposed logic-based theory evaluation with learned embeddings.",
        "Forecasting models for trend prediction of vulnerability clusters and emergence of new clusters."
      ],
      "architectural_improvement_recommendations": [
        "Replace or augment FastText with contextual transformer encoders (e.g., BERT/RoBERTa) with domain-adaptive pretraining on NVD/CVE text.",
        "Adopt hierarchical multi-label classifiers that encode CWE/CVSS hierarchies and dependencies; use focal loss and class-balanced sampling.",
        "Introduce time-aware embedding layers (positional or decay features) and periodic fine-tuning to address drift.",
        "Use contrastive objectives (InfoNCE) for representation learning on CVE pairs/augmentations to improve clustering separability.",
        "Incorporate structured metadata (CPE, vendor/product) via feature fusion or cross-attention to text embeddings.",
        "Build a joint text–graph model (GNN over CVE–CWE–CPE graph) with text encoders providing node features.",
        "Calibrate classifier outputs (temperature scaling/Platt) and provide explanation via attention rollout or post-hoc methods (LIME/SHAP)."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "JupyterFlow",
        "FastText",
        "XGBoost",
        "UMAP (umap-learn)"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Autoencoder/MLP trained with Adam; AE/VAE/GAN trained for ~10,000 epochs (batch sizes 100–1000) on ~150K CVEs; no specific GPU/CPU details provided."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Concept drift over time requiring periodic retraining.",
        "Incomplete/noisy labels (especially CWE), causing class imbalance and reduced transferability.",
        "Need to scale clustering/classification to ~150K+ CVEs; dimensionality reduction used to improve scalability.",
        "Potential overfitting risk when incorporating temporal weighting.",
        "Handling unlabeled vulnerabilities while maintaining utility for analysts."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "A complete ML workflow to generate and evaluate semantic vulnerability embeddings from CVE/NVD descriptions using FastText and refined sequence models.",
      "Comprehensive empirical evaluation of dimensionality reduction techniques; autoencoder-based representations outperform other unsupervised methods for downstream accuracy.",
      "Supervised MLP-based dimensionality reduction and classification tuned for CVSS labels; best performance and evidence of transfer from CVSS v2 to v3.",
      "Interactive visualization of the vulnerability space with label overlays (CWE, CPE, CVSS) and temporal evolution/cluster trend analyses.",
      "Exploration of a new logic-based approach to evaluate theories about the vulnerability space.",
      "Scalable clustering of ~150K CVEs using UMAP + density-based methods (HDBSCAN/OPTICS) and hierarchical overlays."
    ]
  },
  {
    "arxiv_id": "2307.13787v1",
    "title": "The GANfather: Controllable generation of malicious activity to improve defence systems",
    "authors": "Ricardo Ribeiro Pereira; Jacopo Bono; João Tiago Ascensão; David Aparício; Pedro Ribeiro; Pedro Bizarro",
    "abstract": "Machine learning methods to aid defence systems in detecting malicious activity typically rely on labelled data. In some domains, such labelled data is unavailable or incomplete. In practice this can lead to low detection rates and high false positive rates, which characterise for example anti-money laundering systems. In fact, it is estimated that 1.7--4 trillion euros are laundered annually and go undetected. We propose The GANfather, a method to generate samples with properties of malicious activity, without label requirements. We propose to reward the generation of malicious samples by introducing an extra objective to the typical Generative Adversarial Networks (GANs) loss. Ultimately, our goal is to enhance the detection of illicit activity using the discriminator network as a novel and robust defence system. Optionally, we may encourage the generator to bypass pre-existing detection systems. This setup then reveals defensive weaknesses for the discriminator to correct. We evaluate our method in two real-world use cases, money laundering and recommendation systems. In the former, our method moves cumulative amounts close to 350 thousand dollars through a network of accounts without being detected by an existing system. In the latter, we recommend the target item to a broad user base with as few as 30 synthetic attackers. In both cases, we train a new defence system to capture the synthetic attacks.",
    "published_date": "2023-07-25",
    "pdf_link": "https://arxiv.org/pdf/2307.13787v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Financial Security",
      "subdomain": "Anti-Money Laundering (AML)",
      "specific_problem": "Generate malicious-like samples without labels to train a robust detector; optionally learn to bypass existing rule-based systems. Also demonstrated for recommender system injection attacks.",
      "attack_types": [
        "Money laundering layering using mule accounts",
        "Recommender system shilling/injection to promote a target item"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "GAN",
        "specific": "Wasserstein GAN-style training with custom generator objective",
        "novel_contribution": "Adds a use-case-specific differentiable malicious objective to the generator loss and an optional differentiable alert-system penalty; uses the discriminator as the final defense detector trained without labeled malicious data."
      },
      {
        "type": "primary",
        "category": "CNN",
        "specific": "Transposed convolutions in generator; convolutions in discriminator (AML use-case)",
        "novel_contribution": "3D tensor representation of dynamic transaction graphs with dual-branch generator (amounts and probabilities) and permutation-invariant ops in discriminator."
      },
      {
        "type": "baseline",
        "category": "Rule-based system",
        "specific": "Existing AML rules (5 alert scenarios); differentiable proxy network",
        "novel_contribution": "Hard-coded neural proxy mimicking rules to provide gradients to the generator."
      },
      {
        "type": "primary",
        "category": "Collaborative Filtering",
        "specific": "Cosine-similarity based user-based CF used inside the malicious objective",
        "novel_contribution": "Objective pushes predicted ratings of a target item higher for all users to synthesize shilling attacks."
      }
    ],
    "learning_paradigm": [
      "Unsupervised",
      "Adversarial Training"
    ],
    "datasets": [
      {
        "name": "Real-world financial transactions dataset (undisclosed)",
        "type": "proprietary",
        "domain": "network_traffic-like transaction graphs (bank transfers)",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Existing AML rule-based alert system",
        "paper_reference": null,
        "metric": "Detection performance at matched alert rate (recall/precision not fully reported); also qualitative: ability to move funds without detection",
        "their_result": "“moves cumulative amounts close to 350 thousand dollars through a network of accounts without being detected by an existing system.” Also trained detector (DM) evaluated on real test set; results in Table 1 (numbers not visible).",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Accuracy (discriminator distinguishing real vs generated: “100% accuracy” reported for many settings)",
      "Alert rate (threshold matched to rules)",
      "Amount of money moved undetected (up to ~$350,000)",
      "Number of synthetic attackers needed to broadly recommend target item (as few as 30)",
      "Distributional comparisons (money flow, amounts, counts)",
      "Qualitative bypass rate of existing alerts"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can we generate malicious-like samples from predominantly legitimate, unlabeled data and train an effective detection system without any labeled malicious examples?",
        "Can we encourage a generator to bypass existing (rule-based) detection systems to expose weaknesses and use this to improve a complementary detector?",
        "How does adding a malicious objective to the GAN loss change training dynamics and the discriminator’s utility as a detector?"
      ],
      "gaps_identified": [
        "Labeled malicious data in domains like AML is unavailable or incomplete; feedback is sparse and delayed.",
        "High false positives and low detection rates in current AML systems.",
        "Adversarial nature of many illicit activities where attacker and defender co-adapt.",
        "Existing rule-based systems are non-differentiable, hindering gradient-based adversarial evaluation."
      ],
      "limitations": [
        "Assumes unlabeled data is predominantly legitimate.",
        "Requires a differentiable formulation of the malicious objective and, if used, a differentiable proxy for existing alert systems.",
        "3D tensor representation for AML may limit the size/complexity of generated cases (authors note most investigations involve up to 5 accounts).",
        "Evaluation details for some metrics (e.g., full Table 1 results) are not fully reported in the provided text; dataset cannot be shared.",
        "Generalization to other domains may need substantial domain-specific adaptations."
      ],
      "future_work": [
        "Apply the framework to other adversarial domains lacking labeled data, with domain-specific objective functions.",
        "Include temporal information in recommender systems using a setup similar to the AML use case.",
        "Explore integration of non-differentiable existing systems via improved differentiable proxies."
      ],
      "motivation": "Improve detection of illicit activity in domains with scarce or delayed labels by generating malicious-like samples and training a robust detector without labeled malicious data; reveal and fix weaknesses in existing defenses.",
      "potential_research_ideas": [
        "Design richer, multi-objective malicious objectives (e.g., blend stealth constraints, cost/benefit, and cross-institution coordination) and study their impact on detector robustness.",
        "Evaluate transferability: do detectors trained on synthetic attacks catch real-world novel laundering patterns across institutions and geographies?",
        "Develop principled methods to learn differentiable proxies of non-differentiable alert systems (e.g., imitation learning/distillation with safety guarantees).",
        "Incorporate federated or privacy-preserving training so multiple financial institutions can collaboratively learn robust discriminators without sharing raw data.",
        "Stress-test adversarial training with multi-generator ensembles that emulate diverse attacker archetypes.",
        "Formalize risk-sensitive objectives (e.g., CVaR) to bias generation toward high-impact but plausible attacks while controlling distributional shift.",
        "Benchmark against graph-native approaches and extend to cross-bank multi-view settings where detectors see only partial graphs.",
        "Establish standardized evaluation protocols and datasets for synthetic laundering and shilling attacks to assess generalization and overfitting to synthetic artifacts."
      ],
      "architectural_improvement_recommendations": [
        "Replace/augment 3D tensor AML representation with graph neural networks (GNNs) for dynamic, variable-size transaction graphs; consider temporal GNNs.",
        "Adopt WGAN-GP or spectral normalization to stabilize training; use curriculum on alpha (α) to gradually increase malicious pressure.",
        "Use conditional generation and constraints (e.g., diffusion models with constraints) to control attack properties while staying realistic.",
        "Model partial observability explicitly (multi-view generators vs single-view discriminators for per-FI visibility).",
        "Leverage reinforcement learning or constrained optimization for objectives with non-differentiable components, combined with differentiable surrogates.",
        "Train improved rule proxies via supervised distillation from the true rule engine outputs on large samples; add uncertainty calibration.",
        "Ensemble multiple discriminators specialized in different attack facets (volume, topology, temporal patterns) to improve coverage."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": null,
      "frameworks": [
        "PyTorch"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Need differentiable objectives and proxies for existing non-differentiable systems.",
        "Assumption that unlabeled data is mostly legitimate can be violated, biasing training.",
        "Potential distributional gaps between synthetic and real attacks; risk of detectors overfitting to synthetic artifacts.",
        "Integration with existing rule-based workflows and threshold calibration at matched alert rates.",
        "Data confidentiality and limited dataset sharing hinder external validation."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": true,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces The GANfather, a GAN-based framework that adds a malicious objective to the generator loss to synthesize malicious-like samples without requiring labeled malicious data.",
      "Optionally incorporates an existing detection system via a differentiable proxy to encourage the generator to evade it, exposing weaknesses.",
      "Uses the discriminator as a robust defense detector trained against synthetic malicious data and real data.",
      "Provides a 3D tensor representation for dynamic transaction graphs and tailored generator/discriminator architectures for AML.",
      "Demonstrates on AML that the generator can move “close to 350 thousand dollars” undetected by existing rules while enabling training of a new detector.",
      "Demonstrates on recommender systems that a target item can be broadly recommended using “as few as 30 synthetic attackers.”",
      "Offers a theoretical analysis showing how the malicious objective shifts the generator distribution and changes GAN dynamics (discriminator may converge to perfect classification)."
    ]
  },
  {
    "arxiv_id": "2307.08547v1",
    "title": "Metadata-based Malware Detection on Android using Machine Learning",
    "authors": "Alexander Hefter; Christoph Sendner; Alexandra Dmitrienko",
    "abstract": "In the digitized world, smartphones and their apps play an important role. To name just a few examples, some apps offer possibilities for entertainment, others for online banking, and others offer support for two-factor authentication. Therefore, with smartphones also, sensitive information is shared; thus, they are a desirable target for malware. The following technical report gives an overview of how machine learning, especially neural networks, can be employed to detect malicious Android apps based on their metadata. Detection based on the metadata is necessary since not all of an app's information is readable from another app due to the security layout of Android. To do so, a comparable big dataset of metadata of apps has been collected for learning and evaluation in this work. The first section, after the introduction, presents the related work, followed by the description of the sources of the dataset and the selection of the features used for machine learning, in this case, only the app permissions. Afterward, a free available dataset is used to find an efficient and effective neural network model for learning and evaluation. Here, the fully connected network type consisting of dense layers is chosen. Then this model is trained and evaluated on the new, more extensive dataset to obtain a representative result. It turns out that this model detects malware with an accuracy of 92.93% based on an app's permissions.",
    "published_date": "2023-07-17",
    "pdf_link": "https://arxiv.org/pdf/2307.08547v1",
    "paper_types": [
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Mobile Security",
      "subdomain": "Android Malware Detection",
      "specific_problem": "Detecting malicious Android apps using static metadata (requested permissions) only",
      "attack_types": [
        "generic Android malware"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "MLP/Feedforward Dense Network",
        "specific": "Fully connected neural network with three dense layers (sizes reported: 1024×2048×1024; also tested 4096×8192×4096) with dropout and L2 regularization; sigmoid output",
        "novel_contribution": "Empirical finding that a simple dense network is both effective and far more time-efficient than CNN/RNN variants for bag-of-permissions input; feature preselection procedure for permissions"
      },
      {
        "type": "baseline",
        "category": "RNN",
        "specific": "GRU (single layer, 150 units, dropout 20%) feeding dense layers",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "RNN",
        "specific": "LSTM (two layers, 100 units each, dropout 25%) feeding dense layers",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "1D CNN (three conv layers; filters 5×80×30 with kernel sizes 10×5×3, zero padding + max pooling) feeding dense layers",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Hybrid (RNN+CNN)",
        "specific": "Parallel GRU (80 units, dropout 25%) and CNN feature extractors feeding dense layers",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Assembled Android app metadata dataset (this work)",
        "type": "proprietary",
        "domain": "android_app_metadata/permissions",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "Android Permissions dataset (2019)",
        "type": "public",
        "domain": "android_app_metadata/permissions",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "AndroZoo corpus",
        "type": "public",
        "domain": "android_apks",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "VirusTotal malware dataset (~30k apps; Google Drive)",
        "type": "public",
        "domain": "android_apks",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CCCS-CIC-AndMal-2020",
        "type": "public",
        "domain": "android_malware_metadata/permissions",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Contagio Mobile Mini Dump",
        "type": "public",
        "domain": "android_apks",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "APK Auditor dataset (Talha et al.)",
        "type": "unknown",
        "domain": "android_app_metadata/permissions",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Giang et al. permissions dataset",
        "type": "unknown",
        "domain": "android_app_metadata/permissions",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "AndroTracker (subset used by Karabey Aksakalli)",
        "type": "public",
        "domain": "android_app_metadata/permissions",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Rahali et al. static-features image dataset (~200k malware + ~200k benign)",
        "type": "unknown",
        "domain": "android_app_metadata (permissions, activities, receivers, providers)",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "CDMC 2016 (Cyber Security Data Mining Competition)",
        "type": "public",
        "domain": "android_apks/permissions",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "SeqDroid training/evaluation dataset (Lee et al.)",
        "type": "unknown",
        "domain": "android_apks/metadata (package name, certificate owner, permissions, intent actions)",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Drebin",
        "type": "public",
        "domain": "android_malware (static features)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "GRU-based network",
        "paper_reference": null,
        "metric": "Accuracy (Android Permissions dataset 2019; threshold 0.5)",
        "their_result": "NN: 0.9547",
        "baseline_result": "GRU: 0.9533"
      },
      {
        "method_name": "LSTM-based network",
        "paper_reference": null,
        "metric": "Accuracy (Android Permissions dataset 2019; threshold 0.5)",
        "their_result": "NN: 0.9547",
        "baseline_result": "LSTM: 0.9500"
      },
      {
        "method_name": "CNN-based network",
        "paper_reference": null,
        "metric": "Accuracy (Android Permissions dataset 2019; threshold 0.5)",
        "their_result": "NN: 0.9547",
        "baseline_result": "CNN: 0.9560 (highest)"
      },
      {
        "method_name": "GRU+CNN hybrid",
        "paper_reference": null,
        "metric": "Accuracy (Android Permissions dataset 2019; threshold 0.5)",
        "their_result": "NN: 0.9547",
        "baseline_result": "GRU+CNN: 0.9547"
      },
      {
        "method_name": "GRU-based network",
        "paper_reference": null,
        "metric": "AUC (validation; Android Permissions dataset 2019)",
        "their_result": "NN: 0.9149",
        "baseline_result": "GRU: 0.9216"
      },
      {
        "method_name": "LSTM-based network",
        "paper_reference": null,
        "metric": "AUC (validation; Android Permissions dataset 2019)",
        "their_result": "NN: 0.9149",
        "baseline_result": "LSTM: 0.8971"
      },
      {
        "method_name": "CNN-based network",
        "paper_reference": null,
        "metric": "AUC (validation; Android Permissions dataset 2019)",
        "their_result": "NN: 0.9149",
        "baseline_result": "CNN: 0.9058"
      },
      {
        "method_name": "GRU+CNN hybrid",
        "paper_reference": null,
        "metric": "AUC (validation; Android Permissions dataset 2019)",
        "their_result": "NN: 0.9149",
        "baseline_result": "GRU+CNN: 0.9032"
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "Recall",
      "Precision",
      "F1",
      "ROC curve",
      "AUC",
      "Training epoch time"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can Android malware be effectively detected using only metadata (permissions) accessible to other apps under Android’s security model?",
        "What neural network architecture is both effective and computationally efficient for classifying apps by permissions?",
        "How to construct a sufficiently large and representative metadata dataset for training and evaluation?"
      ],
      "gaps_identified": [
        "Few freely available, sufficiently large, up-to-date Android metadata datasets (permissions) for training/evaluation; existing public sets are small or dated.",
        "Dynamic analysis can improve detection but incurs high system overhead, making it impractical for on-device or large-scale use.",
        "Convolutional architectures are ill-suited to unordered, bag-of-permissions inputs; imposed order can affect results.",
        "Memory constraints prevent using the entire raw permission universe (~502,331 unique permissions)."
      ],
      "limitations": [
        "Only permissions were used as features despite extracting broader metadata.",
        "Rare permissions and class-exclusive permissions were removed, which may discard informative features and may not generalize to unseen apps.",
        "No dynamic features were used due to overhead considerations.",
        "Model selection and detailed metrics were done on a smaller public dataset; final large-scale evaluation reports only overall accuracy (92.93%) without detailed breakdown.",
        "Training set balancing involved duplicating malware samples on the smaller dataset, which may bias learning.",
        "Computational constraints led to choosing NN over potentially better-performing GRU in some settings.",
        "No threshold optimization beyond 0.5 was performed for test reporting on model-selection dataset."
      ],
      "future_work": [],
      "motivation": "Smartphones process sensitive data via apps; Android’s security model restricts inter-app data access, so detection must rely on accessible metadata (e.g., permissions). The work assembles a large metadata corpus and studies neural models for permissions-based malware detection.",
      "potential_research_ideas": [
        "Augment permissions with additional static metadata (API calls, components, intent filters, certificate/Signer info, package relations) to improve accuracy and robustness.",
        "Learn embeddings for permissions (e.g., using co-occurrence graphs or self-supervised objectives) and apply attention-based or transformer-style models tailored to unordered feature sets.",
        "Handle the very large permission space via hashing trick, feature hashing embeddings, or sparse input layers to avoid discarding rare but informative permissions.",
        "Combine static metadata with lightweight dynamic signals (e.g., limited syscall/behavioral summaries) collected on-device or via sandbox snapshots to improve recall while keeping overhead low.",
        "Apply domain adaptation/continual learning to handle distribution drift across app stores, time, and Android versions.",
        "Calibrate outputs and optimize thresholds for specific operational points (e.g., maximize TPR at low FPR); use cost-sensitive learning to prioritize malware recall.",
        "Integrate explainability (e.g., SHAP on binary permission vectors) to identify high-risk permissions and support analyst decisions.",
        "Evaluate robustness to adversarial evasion (e.g., permission stuffing/removal) and design regularization or adversarial training for stability.",
        "Explore privacy-preserving deployment (on-device inference, federated learning) to avoid sharing app data."
      ],
      "architectural_improvement_recommendations": [
        "Replace raw dense input with learnable sparse embeddings for permissions, summing/attending over present permissions (set encoder).",
        "Adopt a Wide & Deep approach combining linear permission weights with deep interactions for better calibration and interpretability.",
        "Introduce attention/pooling mechanisms invariant to permutation of permissions (DeepSets/Set Transformer).",
        "Use class-imbalance handling (focal loss, weighted loss) to improve malware recall without sacrificing precision.",
        "Perform threshold calibration (Platt scaling/temperature scaling) and ROC-based operating point selection for deployment.",
        "Leverage regularization and dropout tuning informed by validation AUC rather than accuracy; early stopping with patience.",
        "Evaluate larger but efficient architectures (e.g., sparse MLPs, mixtures-of-experts) to scale with feature size while keeping latency low."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "TensorFlow",
        "Keras"
      ],
      "reproducibility_score": "low",
      "computational_requirements": "Not specified. Reported per-epoch training times on the smaller dataset: NN ~0.29s; CNN ~1.1s; GRU ~18s; LSTM ~16s; GRU+CNN ~13s (hardware unspecified)."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Android security model restricts accessible features; relying only on permissions may limit detection power.",
        "Large and evolving permission space (over 500k unique entries observed) complicates feature management and memory footprint.",
        "Distribution shift across app sources/time/Android versions may degrade performance without continual updates.",
        "Dynamic analysis improves accuracy but imposes high overhead and is impractical for on-device detection.",
        "Risk of over-reliance on class-exclusive or rare permissions; filtering may remove useful signals."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Assembled a very large Android app metadata corpus (1,447,566 benign; 1,397,986 malware) from multiple sources (AndroZoo, VirusTotal, CCCS-CIC-AndMal-2020, Contagio) and derived a permissions dataset.",
      "Designed a permissions feature selection pipeline: removed extremely rare (<26 apps) and class-exclusive permissions, yielding 2,137 permission features.",
      "Empirically compared several neural architectures (MLP, CNN, LSTM, GRU, GRU+CNN) on a public Android permissions dataset; found dense MLP offers strong accuracy with superior training efficiency for bag-of-permissions.",
      "Trained and evaluated the selected dense network on the large assembled dataset, reporting 92.93% accuracy for permissions-based Android malware detection."
    ]
  },
  {
    "arxiv_id": "2307.06616v3",
    "title": "SecureFalcon: Are We There Yet in Automated Software Vulnerability Detection with LLMs?",
    "authors": "Mohamed Amine Ferrag; Ammar Battah; Norbert Tihanyi; Ridhi Jain; Diana Maimut; Fatima Alwahedi; Thierry Lestable; Narinderjit Singh Thandi; Abdechakour Mechri; Merouane Debbah; Lucas C. Cordeiro",
    "abstract": "Software vulnerabilities can cause numerous problems, including crashes, data loss, and security breaches. These issues greatly compromise quality and can negatively impact the market adoption of software applications and systems. Traditional bug-fixing methods, such as static analysis, often produce false positives. While bounded model checking, a form of Formal Verification (FV), can provide more accurate outcomes compared to static analyzers, it demands substantial resources and significantly hinders developer productivity. Can Machine Learning (ML) achieve accuracy comparable to FV methods and be used in popular instant code completion frameworks in near real-time? In this paper, we introduce SecureFalcon, an innovative model architecture with only 121 million parameters derived from the Falcon-40B model and explicitly tailored for classifying software vulnerabilities. To achieve the best performance, we trained our model using two datasets, namely the FormAI dataset and the FalconVulnDB. The FalconVulnDB is a combination of recent public datasets, namely the SySeVR framework, Draper VDISC, Bigvul, Diversevul, SARD Juliet, and ReVeal datasets. These datasets contain the top 25 most dangerous software weaknesses, such as CWE-119, CWE-120, CWE-476, CWE-122, CWE-190, CWE-121, CWE-78, CWE-787, CWE-20, and CWE-762. SecureFalcon achieves 94% accuracy in binary classification and up to 92% in multiclassification, with instant CPU inference times. It outperforms existing models such as BERT, RoBERTa, CodeBERT, and traditional ML algorithms, promising to push the boundaries of software vulnerability detection and instant code completion frameworks.",
    "published_date": "2023-07-13",
    "pdf_link": "https://arxiv.org/pdf/2307.06616v3",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Vulnerability Detection",
      "specific_problem": "Automated detection and classification of vulnerabilities in C/C++ source code (binary and multi-class over top CWE categories) with near real-time inference suitable for code completion frameworks",
      "attack_types": [
        "CWE-119",
        "CWE-120",
        "CWE-476",
        "CWE-122",
        "CWE-190",
        "CWE-121",
        "CWE-78",
        "CWE-787",
        "CWE-20",
        "CWE-762"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "SecureFalcon (derived from Falcon-40B)",
        "novel_contribution": "Lightweight 121M-parameter architecture distilled/derived from Falcon-40B with decoder layers using RoPE and a classification head for binary and multi-class vulnerability classification; fine-tuned on FormAI and FalconVulnDB; optimized for instant CPU inference."
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "BERT",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "RoBERTa",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer (code-pretrained)",
        "specific": "CodeBERT",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "k-Nearest Neighbors",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Logistic Regression",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Naive Bayes",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Support Vector Machine",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Random Forest",
        "specific": "RF / RRF (Regularized/Rotation Random Forest variant as named in paper)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Decision Tree",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Linear Discriminant Analysis",
        "specific": "LDA",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Transfer Learning",
      "Fine-tuning"
    ],
    "datasets": [
      {
        "name": "FormAI dataset",
        "type": "synthetic",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      },
      {
        "name": "FalconVulnDB",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": true,
        "availability": ""
      },
      {
        "name": "SySeVR framework dataset",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Draper VDISC",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "BigVul",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "DiverseVul",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "SARD Juliet",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "ReVeal",
        "type": "public",
        "domain": "source_code",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "CodeBERT",
        "paper_reference": null,
        "metric": "accuracy",
        "their_result": "94% (binary), 92% (multi-class)",
        "baseline_result": "88% (accuracy; best Transformer baseline mentioned)"
      },
      {
        "method_name": "BERT",
        "paper_reference": null,
        "metric": "accuracy",
        "their_result": "94% (binary), 92% (multi-class)",
        "baseline_result": null
      },
      {
        "method_name": "RoBERTa",
        "paper_reference": null,
        "metric": "accuracy",
        "their_result": "94% (binary), 92% (multi-class)",
        "baseline_result": null
      },
      {
        "method_name": "Random Forest (RF) / RRF",
        "paper_reference": null,
        "metric": "accuracy",
        "their_result": "94% (binary), 92% (multi-class)",
        "baseline_result": "81% (RF accuracy)“},{"
      }
    ],
    "performance_metrics_used": [
      "accuracy"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can Machine Learning (ML) achieve accuracy comparable to Formal Verification (FV) methods and be used in instant code completion frameworks in near real-time?",
        "Can we develop a model that detects vulnerabilities efficiently without the lengthy processing times of BMC while maintaining high accuracy?"
      ],
      "gaps_identified": [
        "Existing datasets for C/C++ vulnerability detection are often fully synthetic or non-compilable or have skewed class distributions.",
        "Labels can be noisy: manual labeling introduces human error; static-analysis-based labels have high false positives.",
        "DL approaches often suffer from inadequate models, learning irrelevant features, data duplication, and imbalance.",
        "Formal verification (BMC) is accurate but computationally expensive, impractical for near real-time usage."
      ],
      "limitations": [
        "It is unrealistic to expect LLMs to match the results/assurances of BMC tools at this stage.",
        "FormAI lacks real project data; FalconVulnDB aggregation was introduced to compensate.",
        "Overall program verification is undecidable; hence full guarantees cannot be provided by ML."
      ],
      "future_work": [
        "Integrate a robust BMC tool with a pre-trained LLM to enhance the pace and accuracy of vulnerability detection in a hybrid pipeline.",
        "Further minimize the performance gap to formal methods while maintaining instant inference for IDE integration."
      ],
      "motivation": "Provide a compact, fast, and accurate vulnerability detector that can operate with near-instant inference in code completion workflows, narrowing the speed-accuracy gap between ML and formal verification.",
      "potential_research_ideas": [
        "Hybrid verification pipeline: LLM pre-filtering followed by targeted BMC on high-risk snippets with dynamic bound selection.",
        "Uncertainty-aware detection with calibrated confidence and abstention to trigger formal checks when uncertain.",
        "Multi-representation learning: fuse code tokens with AST/DFG/CFG graphs (e.g., GraphCodeBERT/GNN hybrid) for richer semantics.",
        "Contrastive and curriculum learning using CWE taxonomy to improve multi-class separation and rare-class performance.",
        "Retrieval-augmented vulnerability detection using CWE examples and known vulnerable code templates for in-context guidance.",
        "Active learning loop with developer-in-the-loop labeling on hard/ambiguous cases from real repositories.",
        "Domain adaptation from synthetic/aggregated datasets to real-world repos via self-training and test-time adaptation.",
        "Lightweight quantization/distillation to sub-INT8 or sparsified models for on-device IDE inference."
      ],
      "architectural_improvement_recommendations": [
        "Adopt a hybrid Transformer-GNN encoder to incorporate structural code properties (AST/DFG/PDG) alongside token sequences.",
        "Introduce a confidence calibration head (e.g., temperature scaling) and selective classification to defer uncertain cases.",
        "Use class-balanced focal loss or re-weighting and mixup/cutmix for imbalanced CWE classes.",
        "Apply retrieval-augmented attention over a CWE/example memory bank for pattern grounding.",
        "Leverage parameter-efficient tuning (LoRA/IA3) for continual adaptation to new CWEs and projects.",
        "Add static analysis features (taint, alias, pointer analysis summaries) as auxiliary inputs for multi-task learning."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Model size ~121M parameters; reported instant CPU inference; training/inference hardware and run times not fully specified in provided text."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Intended for integration into code completion frameworks/IDEs (not deployed in the paper)",
      "scalability_discussed": true,
      "inference_time": "\"instant CPU inference times\" (as stated)",
      "deployment_challenges": [
        "Accuracy-speed trade-off vs. formal verification; ensuring low false positives in real projects.",
        "Dataset shift from synthetic/aggregated datasets to real-world enterprise codebases.",
        "Integration with IDEs and CI pipelines while preserving developer productivity.",
        "Handling unseen/rare CWE types and evolving vulnerability patterns."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces SecureFalcon, a lightweight 121M-parameter LLM derived from Falcon-40B tailored for vulnerability classification with instant CPU inference.",
      "Fine-tunes on FormAI (ESBMC-labeled, compilable, balanced C code) to learn accurate vulnerability signals.",
      "Creates FalconVulnDB by aggregating multiple public datasets (SySeVR, Draper VDISC, BigVul, DiverseVul, SARD Juliet, ReVeal) to cover top-25 CWE weaknesses and complement FormAI.",
      "Reports 94% accuracy in binary classification and up to 92% in multi-class classification; outperforms traditional ML (e.g., RF 81%) and LLM baselines (e.g., CodeBERT 88%)."
    ]
  },
  {
    "arxiv_id": "2308.02581v1",
    "title": "Cream Skimming the Underground: Identifying Relevant Information Points from Online Forums",
    "authors": "Felipe Moreno-Vera; Mateus Nogueira; Cainã Figueiredo; Daniel Sadoc Menasché; Miguel Bicudo; Ashton Woiwood; Enrico Lovat; Anton Kocheturov; Leandro Pfleger de Aguiar",
    "abstract": "This paper proposes a machine learning-based approach for detecting the exploitation of vulnerabilities in the wild by monitoring underground hacking forums. The increasing volume of posts discussing exploitation in the wild calls for an automatic approach to process threads and posts that will eventually trigger alarms depending on their content. To illustrate the proposed system, we use the CrimeBB dataset, which contains data scraped from multiple underground forums, and develop a supervised machine learning model that can filter threads citing CVEs and label them as Proof-of-Concept, Weaponization, or Exploitation. Leveraging random forests, we indicate that accuracy, precision and recall above 0.99 are attainable for the classification task. Additionally, we provide insights into the difference in nature between weaponization and exploitation, e.g., interpreting the output of a decision tree, and analyze the profits and other aspects related to the hacking communities. Overall, our work sheds insight into the exploitation of vulnerabilities in the wild and can be used to provide additional ground truth to models such as EPSS and Expected Exploitability.",
    "published_date": "2023-08-03",
    "pdf_link": "https://arxiv.org/pdf/2308.02581v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Threat Intelligence",
      "subdomain": "Vulnerability Management and Exploit Monitoring",
      "specific_problem": "Classifying underground forum threads citing CVEs into PoC, Weaponization, or Exploitation to detect exploitation in the wild",
      "attack_types": [
        "Vulnerability exploitation in the wild",
        "Exploit weaponization",
        "Proof-of-Concept exploit discussion",
        "Fully Undetectable (FUD) evasion of AV"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Random Forest",
        "specific": null,
        "novel_contribution": "Applied to classify CVE-citing underground forum threads into PoC/Weaponization/Exploitation; achieved very high performance for Exploitation vs Non-exploitation and strong performance for three-class classification."
      },
      {
        "type": "baseline",
        "category": "Decision Tree",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Text Embedding",
        "specific": "Bag-of-Words (BoW)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Text Embedding",
        "specific": "TF-IDF",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Text Embedding",
        "specific": "Doc2Vec",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "CrimeBB (Hackforums subset; threads/posts citing CVEs)",
        "type": "public",
        "domain": "forum_posts",
        "link": null,
        "is_new_contribution": false,
        "availability": "available_on_request"
      },
      {
        "name": "National Vulnerability Database (NVD) CVE data and CVSS",
        "type": "public",
        "domain": "vulnerability_database",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "EPSS (Exploit Prediction Scoring System) scores",
        "type": "public",
        "domain": "vulnerability_risk_scores",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Russian underground market exploit dataset (from prior work [2])",
        "type": "private",
        "domain": "underground_market_posts",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Decision Tree + TF-IDF (three-class)",
        "paper_reference": null,
        "metric": "Accuracy / Precision / Recall / F1",
        "their_result": "0.86 / 0.87 / 0.86 / 0.86 (Random Forest + TF-IDF)",
        "baseline_result": "0.73 / 0.73 / 0.74 / 0.72"
      },
      {
        "method_name": "Decision Tree + Doc2Vec (three-class)",
        "paper_reference": null,
        "metric": "Accuracy / Precision / Recall / F1",
        "their_result": "0.86 / 0.90 / 0.86 / 0.86 (Random Forest + Doc2Vec)",
        "baseline_result": "0.74 / 0.74 / 0.74 / 0.73"
      },
      {
        "method_name": "Decision Tree + TF-IDF (Exploitation vs Non-exploitation)",
        "paper_reference": null,
        "metric": "Accuracy / Precision / Recall / F1",
        "their_result": "0.98 / 0.98 / 0.98 / 0.98 (Random Forest + TF-IDF)",
        "baseline_result": "0.91 / 0.91 / 0.91 / 0.91"
      },
      {
        "method_name": "Decision Tree + Doc2Vec (Exploitation vs Non-exploitation)",
        "paper_reference": null,
        "metric": "Accuracy / Precision / Recall / F1",
        "their_result": "0.99 / 0.99 / 0.99 / 0.99 (Random Forest + Doc2Vec)",
        "baseline_result": "0.92 / 0.93 / 0.92 / 0.92"
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "Precision",
      "Recall",
      "F1"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can underground forum threads citing CVEs be automatically classified into PoC, Weaponization, or Exploitation to detect exploitation in the wild?",
        "What are the price distributions of hacking tools/exploits discussed in CrimeBB compared to a Russian underground market?",
        "What are the delays between NVD publication of CVEs and discussions in underground forums?",
        "How do CVSS and EPSS risk scores distribute across PoC, Weaponization, and Exploitation threads?"
      ],
      "gaps_identified": [
        "Exploitation in the wild is much less studied than weaponization due to sensitive data and NDAs.",
        "No prior work using CrimeBB to understand exploitation in the wild.",
        "EPSS uses private sources; there is a need for additional public ground truth regarding exploitation activity."
      ],
      "limitations": [
        "Analysis focuses on Hackforums; Antichat (a major forum) was discarded due to primarily Russian-language posts.",
        "Slang and abbreviations typical of underground communities were not accounted for in labeling or features.",
        "Manual labeling required and 314 threads did not fit target categories and were excluded.",
        "Dataset is class-imbalanced (PoC 244, Weaponization 397, Exploitation 102).",
        "EPSS scores used are from 2021–2023, creating a temporal gap relative to 2007–2019 forum posts."
      ],
      "future_work": [
        "Leverage additional features such as CVSS and EPSS scores of vulnerabilities and prices of hacking tools in the classifier.",
        "Account for slang and abbreviations typical in underground communities.",
        "Extend beyond English-only forums to include multilingual forums such as Antichat."
      ],
      "motivation": "Early detection of weaponization and exploitation in the wild is key to defense; underground forums provide timely, privileged signals on exploit availability, pricing, and active use that can augment vulnerability risk models like EPSS and Expected Exploitability.",
      "potential_research_ideas": [
        "Develop transformer-based models (e.g., domain-adapted BERT) fine-tuned on forum text to improve three-class classification and generalization across forums.",
        "Build a temporally-aware early-warning system that detects pre-NVD chatter about CVEs and forecasts exploitation risk.",
        "Create a multilingual pipeline with machine translation and cross-lingual embeddings to incorporate Russian and other non-English forums.",
        "Use weak supervision and distant supervision (e.g., heuristic patterns, CVE linking) to scale labeling and reduce manual effort.",
        "Perform event extraction to identify concrete exploitation claims (actor, target, exploit, outcome) and link to CVEs/products.",
        "Integrate forum-derived signals as features in EPSS/Expected Exploitability models and evaluate lift.",
        "Study adversarial obfuscation (FUD, evasion slang) and robustness of classifiers to text manipulation common in underground communities."
      ],
      "architectural_improvement_recommendations": [
        "Replace bag-of-words/TF-IDF/doc2vec with pre-trained transformer encoders and hierarchical thread models to capture context across posts.",
        "Address class imbalance via class-weighting, focal loss, or data augmentation (e.g., back-translation) and evaluate calibration of predicted probabilities.",
        "Incorporate structured features (CVSS, EPSS, price, product names) into a multimodal model alongside text embeddings.",
        "Model thread structure and temporal progression with hierarchical or sequence models (e.g., HAN, temporal transformers).",
        "Add character-level and subword features to handle obfuscation and slang; integrate language detection and translation.",
        "Use explainability tools (e.g., SHAP) on tree/transformer models to extract human-interpretable rules for security analysts."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://tinyurl.com/crimebbpaper",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Forum posts are unstructured and noisy.",
        "Slang and abbreviations not yet handled can hinder accuracy.",
        "Multilingual content (e.g., Russian) excluded limits coverage.",
        "Access to CrimeBB/underground data can require agreements and may be restricted.",
        "Class imbalance and evolving terminology in underground communities."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Longitudinal empirical analysis of underground forum activity about vulnerabilities, including exploit/tool pricing and timing relative to NVD publication.",
      "Supervised classifier to label CVE-citing threads as PoC, Weaponization, or Exploitation; Random Forest achieves accuracy/precision/recall above 0.99 for Exploitation vs Non-exploitation and ~0.86 for three-class classification.",
      "Insights into differences between weaponization and exploitation via interpretable rules (decision tree interpretation) and keyword analysis.",
      "Provision of additional ground truth signals that can augment models like EPSS and Expected Exploitability."
    ]
  },
  {
    "arxiv_id": "2307.11000v1",
    "title": "BehaveFormer: A Framework with Spatio-Temporal Dual Attention Transformers for IMU enhanced Keystroke Dynamics",
    "authors": "Dilshan Senerath; Sanuja Tharinda; Maduka Vishwajith; Sanka Rasnayaka; Sandareka Wickramanayake; Dulani Meedeniya",
    "abstract": "Continuous Authentication (CA) using behavioural biometrics is a type of biometric identification that recognizes individuals based on their unique behavioural characteristics, like their typing style. However, the existing systems that use keystroke or touch stroke data have limited accuracy and reliability. To improve this, smartphones' Inertial Measurement Unit (IMU) sensors, which include accelerometers, gyroscopes, and magnetometers, can be used to gather data on users' behavioural patterns, such as how they hold their phones. Combining this IMU data with keystroke data can enhance the accuracy of behavioural biometrics-based CA. This paper proposes BehaveFormer, a new framework that employs keystroke and IMU data to create a reliable and accurate behavioural biometric CA system. It includes two Spatio-Temporal Dual Attention Transformer (STDAT), a novel transformer we introduce to extract more discriminative features from keystroke dynamics. Experimental results on three publicly available datasets (Aalto DB, HMOG DB, and HuMIdb) demonstrate that BehaveFormer outperforms the state-of-the-art behavioural biometric-based CA systems. For instance, on the HuMIdb dataset, BehaveFormer achieved an EER of 2.95\\%. Additionally, the proposed STDAT has been shown to improve the BehaveFormer system even when only keystroke data is used. For example, on the Aalto DB dataset, BehaveFormer achieved an EER of 1.80\\%. These results demonstrate the effectiveness of the proposed STDAT and the incorporation of IMU data for behavioural biometric authentication.",
    "published_date": "2023-07-03",
    "pdf_link": "https://arxiv.org/pdf/2307.11000v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Mobile Security",
      "subdomain": "User Authentication",
      "specific_problem": "Continuous authentication on smartphones using keystroke dynamics enhanced with IMU sensors",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "Spatio-Temporal Dual Attention Transformer (STDAT)",
        "novel_contribution": "Single-transformer dual-attention over temporal and channel axes; integrates Gaussian Range Encoder positional encoding and a Multi-Scale 2D CNN within each block; used for both keystroke and IMU modalities with feature-level fusion"
      },
      {
        "type": "primary",
        "category": "CNN",
        "specific": "Multi-Scale 2D CNN",
        "novel_contribution": "2D convolutions with multiple kernel sizes (1,3,5) inside STDAT to jointly extract temporal×channel patterns"
      },
      {
        "type": "primary",
        "category": "Metric Learning",
        "specific": "Triplet Loss",
        "novel_contribution": "Metric learning objective for user-discriminative embedding space for free-text keystroke/IMU sequences"
      },
      {
        "type": "baseline",
        "category": "RNN",
        "specific": "LSTM (TypeNet, HuMINet/DuoNet)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "TypeFormer",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Metric Learning",
      "Transfer Learning"
    ],
    "datasets": [
      {
        "name": "Aalto DB",
        "type": "public",
        "domain": "keystroke_dynamics",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "HMOG DB",
        "type": "public",
        "domain": "keystroke_dynamics + imu + touch",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "HuMIdb",
        "type": "public",
        "domain": "keystroke_dynamics + imu + touch + mobile_context_sensors",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "TypeNet",
        "paper_reference": "[2]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "HuMINet",
        "paper_reference": "[23]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "DuoNet",
        "paper_reference": "[24]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "TypeFormer",
        "paper_reference": "[21]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "EER",
      "Usability",
      "Time to Correct Reject (TCR)",
      "False Reject Worse Interval (FRWI)",
      "False Accept Worse Interval (FAWI)",
      "FAR",
      "FRR",
      "DET curve",
      "Silhouette score"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Existing keystroke/touch-stroke CA systems have limited accuracy and reliability",
        "Prior multimodal CA works used LSTMs with score-level fusion; no study used transformers for multimodal behavioural biometrics-based CA",
        "Limited use of IMU fusion at feature level with keystroke dynamics",
        "Prior works did not evaluate with continuous CA metrics (Usability, TCR, FRWI, FAWI)"
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Improve reliability and accuracy of mobile continuous authentication by combining keystroke dynamics with smartphone IMU and introducing a dual-attention transformer to extract more discriminative spatio-temporal features.",
      "potential_research_ideas": [
        "Cross-modal attention or co-attention fusion between keystroke and IMU streams instead of simple concatenation",
        "Self-supervised pretraining on large unlabeled mobile sensor streams to reduce labeled data needs and improve generalization",
        "On-device efficient variants via pruning/quantization/distillation for real-time CA with low energy use",
        "Domain adaptation and personalization to handle device heterogeneity and user behavior drift over time",
        "Adversarial imitation/spoofing detection for behavioural biometrics (e.g., detect robotic/scripted key/touch patterns)",
        "Federated or privacy-preserving training to keep raw behavioural data on-device",
        "Continual learning to adapt embeddings as user behavior evolves without catastrophic forgetting"
      ],
      "architectural_improvement_recommendations": [
        "Introduce cross-attention blocks that explicitly align keystroke tokens with temporally-synchronized IMU segments",
        "Replace simple feature concatenation with gated fusion or Mixture-of-Experts conditioned on context (activity, posture)",
        "Add temporal convolution or dilated attention for longer contexts with reduced compute",
        "Use learnable per-sensor positional encodings and sensor-specific adapters before fusion",
        "Employ contrastive multi-view losses (e.g., InfoNCE) alongside triplet loss for stronger embeddings",
        "Calibrate scores with user-specific thresholds or Platt scaling for improved operational performance"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/DilshanSenarath/BehaveFormer",
      "frameworks": [
        "PyTorch"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces BehaveFormer, a multimodal CA framework that fuses keystroke and IMU at feature level",
      "Proposes Spatio-Temporal Dual Attention Transformer (STDAT) with temporal and channel attention in a single encoder",
      "Uses Gaussian Range Encoder positional encoding and Multi-Scale 2D CNN within STDAT",
      "Demonstrates superior performance over prior behavioural-biometric CA systems across three public datasets",
      "Reports strong results including EER=1.80% on Aalto DB (keystroke only) and EER=2.95% on HuMIdb (keystroke+IMU)",
      "Evaluates with both traditional and continuous CA metrics (Usability, TCR, FRWI, FAWI)",
      "Provides code for reproducibility"
    ]
  },
  {
    "arxiv_id": "2307.01494v1",
    "title": "Review of Deep Learning-based Malware Detection for Android and Windows System",
    "authors": "Nazmul Islam; Seokjoo Shin",
    "abstract": "Differentiating malware is important to determine their behaviors and level of threat; as well as to devise defensive strategy against them. In response, various anti-malware systems have been developed to distinguish between different malwares. However, most of the recent malware families are Artificial Intelligence (AI) enable and can deceive traditional anti-malware systems using different obfuscation techniques. Therefore, only AI-enabled anti-malware system is robust against these techniques and can detect different features in the malware files that aid in malicious activities. In this study we review two AI-enabled techniques for detecting malware in Windows and Android operating system, respectively. Both the techniques achieved perfect accuracy in detecting various malware families.",
    "published_date": "2023-07-04",
    "pdf_link": "https://arxiv.org/pdf/2307.01494v1",
    "paper_types": [
      "survey"
    ],
    "security_domain": {
      "primary": "Malware Analysis",
      "subdomain": "Malware Detection",
      "specific_problem": "Review and comparative analysis of deep learning-based malware detection for Windows and Android systems",
      "attack_types": [
        "general malware families",
        "Android malware",
        "botnet (CTU-13 includes botnet traffic)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Gradient Boosting (Ensemble)",
        "specific": "XGBoost",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Autoencoder",
        "specific": "Autoencoder (AE) for feature learning",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Data Imbalance Handling",
        "specific": "SMOTEENN (Synthetic Minority Oversampling + Edited Nearest Neighbors)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Metric Learning / Siamese Network",
        "specific": "Siamese Neural Network (SN)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Optimization",
        "specific": "Adam",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Activation Functions",
        "specific": "ReLU, Tanh, Softmax (output)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised",
      "Metric learning"
    ],
    "datasets": [
      {
        "name": "CTU-13",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "UNSW-NB15",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "MALIMG (Malware Image)",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Accuracy",
      "Precision",
      "Recall",
      "F1",
      "Loss"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Windows method: \"DL/ML Analysis of only one dataset\"",
        "Windows method: \"No comparison of train and test analysis\"",
        "Android method: \"No precision, recall and F1 score\"",
        "Android method: \"No comparison of train and test analysis\"",
        "General: Need AI-enabled approaches to counter obfuscation used by modern malware; traditional signature/anomaly-only systems can be deceived"
      ],
      "limitations": [
        "The review considers only two techniques (one Windows, one Android)",
        "Reported results rely on the respective source studies; no independent reproduction",
        "Limited metric reporting for the Android SN study (no precision/recall/F1)",
        "Lack of cross-dataset and train/test split analysis in the reviewed works"
      ],
      "future_work": [],
      "motivation": "Recent malware use AI and obfuscation to evade traditional defenses; review AI-enabled malware detection techniques for Windows and Android to highlight their performance.",
      "potential_research_ideas": [
        "Cross-dataset and cross-platform evaluation benchmark for malware detection across Windows and Android with standardized splits and metrics",
        "Robustness studies against obfuscation and adversarial manipulations (e.g., packing, code morphing, adversarial perturbations on images/byte sequences)",
        "Multimodal malware detection combining static (bytes, API, permissions) and dynamic (behavior/network) features",
        "Privacy-preserving collaborative training (e.g., federated learning) for organizations to share signals without raw data",
        "Continual/online learning to adapt to evolving malware families without catastrophic forgetting",
        "Explainable malware detection to highlight indicative features/regions/APIs aiding analyst triage",
        "Lightweight on-device Android detectors with energy/latency constraints and model compression",
        "Open, unified benchmark that aligns malware family labels across datasets and reports precision/recall/F1, calibration, and OOD detection"
      ],
      "architectural_improvement_recommendations": [
        "For Windows pipeline: replace plain AE with supervised or variational AE and add class-balanced/focal loss in downstream classifiers",
        "Evaluate alternative gradient boosting (LightGBM/CatBoost) and deep tabular models with proper calibration",
        "Adopt end-to-end byte-level CNN/Transformer encoders over PE streams to reduce feature engineering",
        "Use improved imbalance handling (class-balanced loss, reweighting) beyond SMOTEENN; validate with ablations",
        "For Android SN: employ a CNN backbone (e.g., ResNet or EfficientNet) within the Siamese setup; use triplet loss with hard negative mining and margin tuning",
        "Add OOD detection and confidence calibration to handle unseen families",
        "Incorporate API-call graphs/CFGs with GNNs and fuse with image/byte features (late or attention-based fusion)",
        "Standardize evaluation with fixed train/val/test splits and report precision/recall/F1, ROC-AUC, PR-AUC, and calibration metrics"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Handling malware obfuscation and evolution",
        "Dataset imbalance and representativeness",
        "Generalization to unseen families and across platforms",
        "Lack of standardized evaluation splits and comprehensive metrics in prior studies"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Concise review of two AI-enabled malware detection techniques: a Windows framework using AE+SMOTEENN+XGBoost [5] and an Android Siamese Network approach [6]",
      "Summarizes system architectures and data pipelines (Figures 1 and 2)",
      "Compiles reported performance and notes advantages and limitations for each method (Table I)"
    ]
  },
  {
    "arxiv_id": "2307.03679v1",
    "title": "Undecimated Wavelet Transform for Word Embedded Semantic Marginal Autoencoder in Security improvement and Denoising different Languages",
    "authors": "Shreyanth S",
    "abstract": "By combining the undecimated wavelet transform within a Word Embedded Semantic Marginal Autoencoder (WESMA), this research study provides a novel strategy for improving security measures and denoising multiple languages. The incorporation of these strategies is intended to address the issues of robustness, privacy, and multilingualism in data processing applications. The undecimated wavelet transform is used as a feature extraction tool to identify prominent language patterns and structural qualities in the input data. The proposed system may successfully capture significant information while preserving the temporal and geographical links within the data by employing this transform. This improves security measures by increasing the system's ability to detect abnormalities, discover hidden patterns, and distinguish between legitimate content and dangerous threats. The Word Embedded Semantic Marginal Autoencoder also functions as an intelligent framework for dimensionality and noise reduction. The autoencoder effectively learns the underlying semantics of the data and reduces noise components by exploiting word embeddings and semantic context. As a result, data quality and accuracy are increased in following processing stages. The suggested methodology is tested using a diversified dataset that includes several languages and security scenarios. The experimental results show that the proposed approach is effective in attaining security enhancement and denoising capabilities across multiple languages. The system is strong in dealing with linguistic variances, producing consistent outcomes regardless of the language used. Furthermore, incorporating the undecimated wavelet transform considerably improves the system's ability to efficiently address complex security concerns",
    "published_date": "2023-07-06",
    "pdf_link": "https://arxiv.org/pdf/2307.03679v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Security Analytics",
      "subdomain": "Anomaly Detection",
      "specific_problem": "Security enhancement and denoising of multilingual textual data using wavelet-based features and an autoencoder to detect anomalies/hidden patterns and distinguish legitimate vs. malicious content",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Autoencoder",
        "specific": "Word Embedded Semantic Marginal Autoencoder (WESMA)",
        "novel_contribution": "Introduces WESMA and integrates it with Undecimated Wavelet Transform for multilingual denoising and security anomaly detection."
      },
      {
        "type": "primary",
        "category": "Word Embeddings",
        "specific": null,
        "novel_contribution": "Leverages semantic word embeddings within the autoencoder to preserve meaning across languages during denoising and anomaly detection."
      },
      {
        "type": "primary",
        "category": "Signal Processing / Wavelets",
        "specific": "Undecimated Wavelet Transform (UWT)",
        "novel_contribution": "Uses UWT as a shift-invariant feature extractor preserving temporal/spatial relations in multilingual data; combined with WESMA."
      }
    ],
    "learning_paradigm": [
      "Unsupervised",
      "Self-supervised"
    ],
    "datasets": [
      {
        "name": "Diversified multilingual dataset (unspecified)",
        "type": "private",
        "domain": "text_corpus",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can integrating Undecimated Wavelet Transform with a Word Embedded Semantic Marginal Autoencoder improve security (anomaly/hidden pattern detection) in multilingual data?",
        "Can the combined approach effectively denoise multilingual linguistic data while preserving key language traits and temporal/spatial relationships?",
        "Can the system distinguish between legitimate content and potentially malicious/threatening content across different languages?",
        "How robust is the approach across diverse languages and security scenarios?"
      ],
      "gaps_identified": [
        "Traditional security systems often fail to detect anomalies and hidden patterns in multilingual data, risking security breaches.",
        "Noise in multilingual data reduces accuracy and reliability of downstream processing.",
        "Discrete Wavelet Transform suffers from shift-variance due to decimation; UWT preserves temporal/spatial relationships and phase information.",
        "Lack of robust, language-independent security solutions that handle multilingual variability."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Address robustness, privacy, and multilingual processing challenges by combining UWT feature extraction with a semantic, word-embedding-driven marginal autoencoder for denoising and security enhancement.",
      "potential_research_ideas": [
        "Establish a clear threat model and evaluate on well-defined security tasks (e.g., phishing/spam/hate speech/cyberbullying detection) across multiple languages.",
        "Benchmark on public multilingual datasets with security relevance to enable reproducibility (e.g., multilingual hate speech, phishing emails, spam, malware-related text).",
        "Augment with contextual multilingual embeddings (e.g., mBERT, XLM-R) and compare against static embeddings.",
        "Incorporate anomaly detection heads (e.g., one-class objectives, deep SVDD) atop the learned latent space for calibrated detection.",
        "Evaluate adversarial robustness for text (paraphrase, synonym, character-level noise) and wavelet-domain perturbations.",
        "Perform ablations to quantify UWT’s contribution vs. standard DWT and no-wavelet baselines.",
        "Extend to cross-domain generalization (train on one language/domain, test on others) with domain adaptation techniques.",
        "Integrate differential privacy or federated learning for privacy-preserving training on sensitive text."
      ],
      "architectural_improvement_recommendations": [
        "Replace or complement static word embeddings with contextual multilingual models (mBERT/XLM-R) as the embedding layer for WESMA.",
        "Introduce attention mechanisms in the autoencoder (transformer-based autoencoder) to better capture long-range semantics.",
        "Use a variational or denoising diffusion/score-based variant to improve denoising and uncertainty estimation.",
        "Multi-resolution fusion: concatenate UWT coefficients at multiple scales with embedding features before encoding.",
        "Add an explicit anomaly scoring module (e.g., energy-based or one-class head) over the latent representation.",
        "Regularize with contrastive/self-supervised objectives (e.g., SimCLR/InfoNCE) across languages to improve language-invariant features.",
        "Calibrate outputs and add conformal prediction for reliable anomaly detection under distribution shift."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes a combined framework integrating Undecimated Wavelet Transform with a Word Embedded Semantic Marginal Autoencoder (WESMA).",
      "Uses UWT for shift-invariant, multi-scale feature extraction preserving temporal/spatial and phase information in multilingual data.",
      "Uses WESMA for semantic denoising and dimensionality reduction leveraging word embeddings.",
      "Claims effective security enhancement via improved anomaly/hidden pattern detection and denoising across multiple languages.",
      "Discusses conceptual privacy advantages by avoiding downsampling and purportedly reducing leakage (no formal privacy technique)."
    ]
  },
  {
    "arxiv_id": "2307.14540v1",
    "title": "Lateral-Direction Localization Attack in High-Level Autonomous Driving: Domain-Specific Defense Opportunity via Lane Detection",
    "authors": "Junjie Shen; Yunpeng Luo; Ziwen Wan; Qi Alfred Chen",
    "abstract": "Localization in high-level Autonomous Driving (AD) systems is highly security critical. While the popular Multi-Sensor Fusion (MSF) based design can be more robust against single-source sensor spoofing attacks, it is found recently that state-of-the-art MSF algorithms is vulnerable to GPS spoofing alone due to practical factors, which can cause various road hazards such as driving off road or onto the wrong way. In this work, we perform the first systematic exploration of the novel usage of lane detection (LD) to defend against such attacks. We first systematically analyze the potentials of such a domain-specific defense opportunity, and then design a novel LD-based defense approach, $LD^3$, that aims at not only detecting such attacks effectively in the real time, but also safely stopping the victim in the ego lane upon detection considering the absence of onboard human drivers.   We evaluate $LD^3$ on real-world sensor traces and find that it can achieve effective and timely detection against existing attack with 100% true positive rates and 0% false positive rates. Results also show that $LD^3$ is robust to diverse environmental conditions and is effective at steering the AD vehicle to safely stop within the current traffic lane. We implement $LD^3$ on two open-source high-level AD systems, Baidu Apollo and Autoware, and validate its defense capability in both simulation and the physical world in end-to-end driving. We further conduct adaptive attack evaluations and find that $LD^3$ is effective at bounding the deviations from reaching the attack goals in stealthy attacks and is robust to latest LD-side attack.",
    "published_date": "2023-07-26",
    "pdf_link": "https://arxiv.org/pdf/2307.14540v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Autonomous Vehicle Security",
      "subdomain": "Localization and Sensor Fusion Security",
      "specific_problem": "Defense against lateral-direction localization attacks on multi-sensor fusion (MSF) in high-level autonomous driving using lane detection",
      "attack_types": [
        "GPS spoofing",
        "Lateral-direction localization attack (FusionRipper)",
        "Adaptive stealthy attack against LD3",
        "LD-side attack against lane detection"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "CNN",
        "specific": null,
        "novel_contribution": "Novel use of lane detection output as a defense signal for real-time detection of lateral localization attacks and for safety-driven response to stop within lane (LD3)."
      },
      {
        "type": "primary",
        "category": "Sensor Fusion",
        "specific": null,
        "novel_contribution": "Safety-driven fusion between LD and MSF that penalizes the source more aggressive in causing lateral deviations to bound attack impact during attack response."
      },
      {
        "type": "baseline",
        "category": "Physical-invariant state estimation",
        "specific": null,
        "novel_contribution": "Adaptation of prior GPS validation approaches for small robots (e.g., SAVIOR, CI) to AD context used as baseline; shown to have very high false positives and near random guessing."
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Rule-based"
    ],
    "datasets": [
      {
        "name": "Real-world AD sensor traces (multi-sensor: GPS, LiDAR, IMU, cameras) used for LD3 evaluation",
        "type": "proprietary",
        "domain": "autonomous_driving_multisensor_traces",
        "link": "https://sites.google.com/view/cav-sec/LD3",
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Night-time driving trace (low visibility) for robustness evaluation",
        "type": "proprietary",
        "domain": "autonomous_driving_multisensor_traces",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Industrial-grade AD simulator scenarios (for Apollo/Autoware end-to-end)",
        "type": "synthetic",
        "domain": "simulation_autonomous_driving",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "FusionRipper attack traces from prior work used for coverage analysis",
        "type": "proprietary",
        "domain": "autonomous_driving_multisensor_traces",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "SAVIOR (physical-invariant GPS validation for small robotic vehicles)",
        "paper_reference": "SAVIOR [24]",
        "metric": "Attack detection effectiveness (e.g., false positive rate, true positive rate)",
        "their_result": "Authors state LD3 achieves 100% TPR and 0% FPR; baseline adaptation suffers very high false positives, close to random guessing.",
        "baseline_result": null
      },
      {
        "method_name": "CI (physical-invariant-based detector for small robotic vehicles)",
        "paper_reference": "CI [25]",
        "metric": "Attack detection effectiveness (e.g., false positive rate, true positive rate)",
        "their_result": "Authors report baseline adaptation has limited effectiveness in AD context with very high false positives.",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "True Positive Rate (TPR)",
      "False Positive Rate (FPR)",
      "Detection timeliness/latency (detection before touching lane boundaries)",
      "Lateral deviation at final stopping position",
      "Robustness across environmental conditions (e.g., night-time/low visibility)",
      "Bounded deviation under adaptive attacks"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can lane detection (LD) be used as a domain-specific, real-time defense signal to detect lateral-direction localization attacks on high-level AD systems?",
        "How effectively and timely can an LD-based defense detect attacks compared to physical-invariant detectors adapted from small robotics?",
        "Can an LD-based response safely stop the vehicle within the current lane after detection in the absence of onboard human drivers?",
        "How robust is an LD-based defense to diverse environmental conditions and adaptive attacks, including stealthy and LD-side attacks?"
      ],
      "gaps_identified": [
        "No software-based defenses exist for GPS spoofing-based lateral-direction localization attacks on high-level AD MSF systems.",
        "Prior physical-invariant based defenses for small robots do not transfer well to high-speed, curvy-road AD contexts and small yet safety-critical deviations.",
        "Existing works largely ignore attack response design suitable for high-level AD where human takeover is absent.",
        "Defense coverage concerns where lane markings are absent (e.g., intersections) had not been analyzed in prior work."
      ],
      "limitations": [
        "Effectiveness of LD-based defense depends on the presence and visibility of lane markings; limited or no capability in regions without lane lines (e.g., certain intersections).",
        "Simultaneous attacks on both localization and lane detection are not considered in the threat model.",
        "Independence between LD and MSF inputs may diminish under adaptive attackers that also target LD.",
        "Results for baseline comparison are qualitative in the provided text; quantitative baseline numbers are not reported here."
      ],
      "future_work": [
        "Extend defense to scenarios without clear lane markings and at intersections.",
        "Strengthen robustness against coordinated attacks that target both localization (e.g., GPS) and LD simultaneously.",
        "Broaden evaluation across more adverse weather conditions and diverse road environments.",
        "Open-source implementation and datasets to enhance reproducibility and benchmarking."
      ],
      "motivation": "High-level AD localization is vulnerable to GPS spoofing even with MSF; lateral deviations are safety-critical. Lane detection offers a domain-specific opportunity to detect and respond to such attacks using existing cameras and mature LD models.",
      "potential_research_ideas": [
        "Integrate additional vision cues (road edges, lane topology, curb/guardrail detection, HD map lane priors) to generalize defense beyond explicit lane markings.",
        "Combine LD with map-matching and road semantics to infer lane center in intersections and occluded regions.",
        "Develop simultaneous multi-channel attack detection that jointly monitors LD, MSF, and vehicle kinematics via causal consistency checks.",
        "Adversarial robustness enhancements for LD (adversarial training, ensemble LD models, uncertainty estimation) to resist LD-side attacks.",
        "Formal verification of the safety-driven fusion and stopping policy to guarantee bounded deviations under attack.",
        "Online calibration cross-checks between cameras and IMU/LiDAR to detect spoofing-induced inconsistencies."
      ],
      "architectural_improvement_recommendations": [
        "Augment LD3 with uncertainty-aware LD outputs and MSF confidence to adapt detection thresholds dynamically.",
        "Fuse LD with semantic segmentation of drivable area and road boundaries to maintain defense when lane paints are degraded.",
        "Introduce a fail-operational fallback (e.g., reduced-speed, lane-keeping via road-edge detection) when lane markings are missing.",
        "Implement temporal smoothing and change-point detection on LD-MS F discrepancy to improve timeliness while maintaining low FPR.",
        "Use robust sensor fusion frameworks (e.g., factor graphs with safety penalties) to explicitly penalize lateral deviation sources during attack response."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "Baidu Apollo",
        "Autoware"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "High-level AD systems (Baidu Apollo, Autoware) in industrial-grade simulator and physical-world end-to-end tests on a Level-4 development chassis",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Absence or poor visibility of lane markings (e.g., intersections, adverse weather, worn paint).",
        "Potential adaptive or simultaneous attacks targeting LD and localization.",
        "Ensuring safe stopping within lane under diverse traffic scenarios.",
        "Reliance on camera quality, lighting, and calibration for LD performance."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": true,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "First systematic exploration of lane detection as a defense for lateral-direction localization attacks in high-level AD.",
      "Design of LD3, including real-time attack detection at MSF output level and a safety-driven response to stop within lane.",
      "Empirical evaluation on real-world sensor traces achieving \"100% true positive rates and 0% false positive rates\" with timely detection; robust under night-time low visibility.",
      "Implementation on Baidu Apollo and Autoware with validation in simulation and physical-world end-to-end driving.",
      "Adaptive attack evaluation showing bounded deviations under a stealthy attack and robustness to a recent LD-side attack."
    ]
  },
  {
    "arxiv_id": "2308.03554v2",
    "title": "TemporalFED: Detecting Cyberattacks in Industrial Time-Series Data Using Decentralized Federated Learning",
    "authors": "Ángel Luis Perales Gómez; Enrique Tomás Martínez Beltrán; Pedro Miguel Sánchez Sánchez; Alberto Huertas Celdrán",
    "abstract": "Industry 4.0 has brought numerous advantages, such as increasing productivity through automation. However, it also presents major cybersecurity issues such as cyberattacks affecting industrial processes. Federated Learning (FL) combined with time-series analysis is a promising cyberattack detection mechanism proposed in the literature. However, the fact of having a single point of failure and network bottleneck are critical challenges that need to be tackled. Thus, this article explores the benefits of the Decentralized Federated Learning (DFL) in terms of cyberattack detection and resource consumption. The work presents TemporalFED, a software module for detecting anomalies in industrial environments using FL paradigms and time series. TemporalFED incorporates three components: Time Series Conversion, Feature Engineering, and Time Series Stationary Conversion. To evaluate TemporalFED, it was deployed on Fedstellar, a DFL framework. Then, a pool of experiments measured the detection performance and resource consumption in a chemical gas industrial environment with different time-series configurations, FL paradigms, and topologies. The results showcase the superiority of the configuration utilizing DFL and Semi-Decentralized Federated Learning (SDFL) paradigms, along with a fully connected topology, which achieved the best performance in anomaly detection. Regarding resource consumption, the configuration without feature engineering employed less bandwidth, CPU, and RAM than other configurations.",
    "published_date": "2023-08-07",
    "pdf_link": "https://arxiv.org/pdf/2308.03554v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Industrial Control Systems Security",
      "subdomain": "Anomaly/Intrusion Detection",
      "specific_problem": "Federated learning-based anomaly detection on industrial time-series (ICS) data without a central server bottleneck/single point of failure",
      "attack_types": [
        "cyberattacks causing anomalies in industrial processes (unspecified types)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "RNN/LSTM",
        "specific": "LSTM",
        "novel_contribution": "Used within TemporalFED on Fedstellar to perform anomaly detection on converted/engineered time-series in decentralized and semi-decentralized FL setups"
      },
      {
        "type": "primary",
        "category": "Feature Engineering",
        "specific": "Autocorrelation (AC) features",
        "novel_contribution": "Windowed autocorrelation features to capture periodic/repetitive industrial behavior for anomaly detection in FL"
      },
      {
        "type": "primary",
        "category": "Feature Engineering",
        "specific": "Discrete Fourier Transform (DFT) features",
        "novel_contribution": "Windowed DFT to extract dominant frequencies; designed to surface changes induced by cyberattacks"
      },
      {
        "type": "primary",
        "category": "Time-series preprocessing",
        "specific": "Stationarity conversion (detrending/seasonality removal)",
        "novel_contribution": "Module to transform non-stationary series into stationary for improved model learning in FL"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Federated (DFL, SDFL, CFL)"
    ],
    "datasets": [
      {
        "name": "Tennessee Eastman Process (TE/TEP)",
        "type": "public",
        "domain": "industrial_process_time_series",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Centralized Federated Learning (CFL)",
        "paper_reference": null,
        "metric": "F1-score",
        "their_result": "0.9406 (SDFL, fully connected); 0.9405 (DFL, fully connected)",
        "baseline_result": "0.8762–0.9082 (CFL, various configs)"
      }
    ],
    "performance_metrics_used": [
      "F1-score",
      "CPU usage",
      "RAM usage",
      "Bytes exchanged (bandwidth)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Does decentralized or semi-decentralized federated learning (DFL/SDFL) improve anomaly detection performance over CFL for industrial time-series?",
        "How do different federation topologies (e.g., fully connected) affect anomaly detection performance?",
        "What is the resource consumption (CPU, RAM, bandwidth) trade-off for different TemporalFED configurations (with/without feature engineering, stationarity) and FL paradigms (CFL/DFL/SDFL)?",
        "Can a modular time-series processing pipeline (conversion, feature engineering, stationarity) be effectively integrated into a DFL platform (Fedstellar) for ICS security?"
      ],
      "gaps_identified": [
        "Centralized FL has a single point of failure and an aggregation bottleneck in Industry 4.0 scenarios.",
        "No DFL-oriented work considering time series analysis to detect cyberattacks affecting Industry 4.0.",
        "No prior work evaluating both detection performance and resource consumption of DFL versus CFL using time-series data in industrial settings."
      ],
      "limitations": [
        "Evaluation is on a simulated but realistic industrial testbed (Tennessee Eastman process), not a live industrial deployment.",
        "Comparisons are across FL paradigms/topologies using the same model within one platform; no cross-paper baselines reported.",
        "No explicit consideration of poisoning/adversarial robustness, privacy mechanisms (e.g., DP), or explainability in the experiments."
      ],
      "future_work": [],
      "motivation": "Eliminate CFL single point of failure and bottleneck while enabling time-series-centric anomaly detection in industrial environments via decentralized federated learning.",
      "potential_research_ideas": [
        "Incorporate attention-based or transformer time-series models (e.g., Informer/TimesNet) within TemporalFED to test if they outperform LSTM under DFL/SDFL.",
        "Evaluate robustness to data/model poisoning and byzantine clients in DFL/SDFL with secure/robust aggregation methods.",
        "Add differential privacy or secure aggregation to quantify the privacy-utility-resource trade-offs in ICS settings.",
        "Self-supervised pretraining on unlabeled ICS time-series (e.g., contrastive/forecasting tasks) to reduce labeled data dependence and improve cross-site generalization.",
        "Online/continual learning with concept drift detection to handle non-stationary industrial processes over time.",
        "Topology-aware training using graph-based models (e.g., GNN for participant topology) and adaptive peer selection.",
        "Edge resource-aware compression/quantization of updates (e.g., sparsification, sketching) to further reduce bandwidth in D2D links.",
        "Explainability for operators (e.g., frequency/lag attributions from DFT/AC features) to aid incident response.",
        "Validate on multiple real-world ICS datasets and with real deployments to test generalization and operational constraints."
      ],
      "architectural_improvement_recommendations": [
        "Replace/augment LSTM with transformer-based time-series encoders and compare under identical FL settings.",
        "Introduce robust aggregation (e.g., coordinate-wise median, Krum, RSA) for DFL/SDFL to mitigate byzantine/poisoned updates.",
        "Integrate differential privacy (DP-SGD) and/or secure aggregation protocols into Fedstellar to provide formal privacy guarantees.",
        "Implement adaptive, topology-aware peer selection and weighted model mixing based on link quality and data similarity.",
        "Add self-supervised pretraining and fine-tuning pipeline within TemporalFED to leverage unlabeled industrial streams.",
        "Incorporate model/update compression (quantization/sparsification) natively in the DFL pipeline with tunable budgets.",
        "Provide built-in drift detection and incremental re-stationarization to maintain performance on evolving processes."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "PyTorch",
        "PyTorch Lightning"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Industrial process simulation (Tennessee Eastman Process) on Fedstellar DFL/SDFL/CFL setups",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Single point of failure and aggregation bottleneck in CFL",
        "Bandwidth constraints and communication overhead in decentralized (D2D) settings"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Creation of TemporalFED, a time-series analysis module for Fedstellar with three components: Time Series Conversion, Feature Engineering (DFT and autocorrelation), and Stationary Conversion.",
      "Validation in a simulated but realistic chemical gas industrial scenario (Tennessee Eastman process) with multiple simulations per normal/abnormal class.",
      "Experimental evaluation across FL paradigms and topologies showing best anomaly detection with fully connected SDFL (F1 = 0.9406) and DFL (F1 = 0.9405), while CFL achieved 0.8762–0.9082 F1.",
      "Resource consumption study measuring CPU, RAM, and exchanged bytes; CFL exchanges more bytes between participants; analysis across TemporalFED configurations (with/without feature engineering)."
    ]
  },
  {
    "arxiv_id": "2307.10191v1",
    "title": "A Lightweight Approach for Network Intrusion Detection based on Self-Knowledge Distillation",
    "authors": "Shuo Yang; Xinran Zheng; Zhengzhuo Xu; Xingjun Wang",
    "abstract": "Network Intrusion Detection (NID) works as a kernel technology for the security network environment, obtaining extensive research and application. Despite enormous efforts by researchers, NID still faces challenges in deploying on resource-constrained devices. To improve detection accuracy while reducing computational costs and model storage simultaneously, we propose a lightweight intrusion detection approach based on self-knowledge distillation, namely LNet-SKD, which achieves the trade-off between accuracy and efficiency. Specifically, we carefully design the DeepMax block to extract compact representation efficiently and construct the LNet by stacking DeepMax blocks. Furthermore, considering compensating for performance degradation caused by the lightweight network, we adopt batch-wise self-knowledge distillation to provide the regularization of training consistency. Experiments on benchmark datasets demonstrate the effectiveness of our proposed LNet-SKD, which outperforms existing state-of-the-art techniques with fewer parameters and lower computation loads.",
    "published_date": "2023-07-09",
    "pdf_link": "https://arxiv.org/pdf/2307.10191v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Lightweight network intrusion detection on resource-constrained devices with high accuracy",
      "attack_types": [
        "DoS",
        "DDoS",
        "Probe/PortScan",
        "R2L (Remote-to-Local)",
        "U2R (User-to-Root)",
        "Brute Force",
        "Web Attacks",
        "Botnet"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "CNN",
        "specific": "Lightweight CNN with Depthwise Separable Convolution and Max-Feature-Map (MFM) activation",
        "novel_contribution": "DeepMax block (DSConv + MFM + pooling) to extract compact representations with reduced FLOPs and parameters"
      },
      {
        "type": "primary",
        "category": "Knowledge Distillation",
        "specific": "Batch-wise Self-Knowledge Distillation (SKD) without teacher",
        "novel_contribution": "First use of self-knowledge distillation for NID; uses previous-iteration logits as soft targets to regularize training consistency"
      },
      {
        "type": "primary",
        "category": "Loss/Training",
        "specific": "Class-balanced cross-entropy; KL divergence with temperature; cosine annealing; SGD with momentum",
        "novel_contribution": "Combines class-balanced loss with batch-wise SKD via trade-off factor λ"
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "Standard convolutional CNN (architecture similar to LNet but with standard conv)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "LNet (DeepMax) without SKD",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "LNet− (DeepMax without MFM)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "DNN/MLP",
        "specific": "Feedforward neural network",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "RNN",
        "specific": "Recurrent Neural Network (e.g., LSTM)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "DBN",
        "specific": "Deep Belief Network (stacked RBMs) [6]",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN + KD",
        "specific": "KD-TCNN (teacher-student KD for IoT) [14]",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Self-Knowledge Distillation (regularization)"
    ],
    "datasets": [
      {
        "name": "NSL-KDD",
        "type": "public",
        "domain": "network_traffic",
        "link": "https://www.unb.ca/cic/datasets/nsl.html",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CICIDS2017 (CIC-IDS2017)",
        "type": "public",
        "domain": "network_traffic",
        "link": "https://www.unb.ca/cic/datasets/ids-2017.html",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "DNN (NSL-KDD)",
        "paper_reference": null,
        "metric": "Accuracy and F1 (macro) on NSL-KDD",
        "their_result": "LNet-SKD: Acc 98.66, F1 89.03",
        "baseline_result": "Acc 94.23, F1 66.09"
      },
      {
        "method_name": "RNN (NSL-KDD)",
        "paper_reference": null,
        "metric": "Accuracy and F1 (macro) on NSL-KDD",
        "their_result": "LNet-SKD: Acc 98.66, F1 89.03",
        "baseline_result": "Acc 95.75, F1 68.20"
      },
      {
        "method_name": "DBN [6] (NSL-KDD)",
        "paper_reference": "[6] DBN-based IDS",
        "metric": "Accuracy and F1 (macro) on NSL-KDD",
        "their_result": "LNet-SKD: Acc 98.66, F1 89.03",
        "baseline_result": "Acc 95.88, F1 71.25"
      },
      {
        "method_name": "KD-TCNN [14] (NSL-KDD)",
        "paper_reference": "[14] Knowledge distillation CNN for IoT",
        "metric": "Accuracy and F1 (macro) on NSL-KDD",
        "their_result": "LNet-SKD: Acc 98.66, F1 89.03",
        "baseline_result": "Acc 98.20, F1 86.63"
      },
      {
        "method_name": "DNN (CICIDS2017)",
        "paper_reference": null,
        "metric": "Accuracy and F1 (macro) on CICIDS2017",
        "their_result": "LNet-SKD: Acc 99.90, F1 96.74",
        "baseline_result": "Acc 98.10, F1 71.74"
      },
      {
        "method_name": "RNN (CICIDS2017)",
        "paper_reference": null,
        "metric": "Accuracy and F1 (macro) on CICIDS2017",
        "their_result": "LNet-SKD: Acc 99.90, F1 96.74",
        "baseline_result": "Acc 98.11, F1 73.17"
      },
      {
        "method_name": "DBN [6] (CICIDS2017)",
        "paper_reference": "[6] DBN-based IDS",
        "metric": "Accuracy and F1 (macro) on CICIDS2017",
        "their_result": "LNet-SKD: Acc 99.90, F1 96.74",
        "baseline_result": "Acc 99.84, F1 92.75"
      },
      {
        "method_name": "KD-TCNN [14] (CICIDS2017)",
        "paper_reference": "[14] Knowledge distillation CNN for IoT",
        "metric": "Accuracy and F1 (macro) on CICIDS2017",
        "their_result": "LNet-SKD: Acc 99.90, F1 96.74",
        "baseline_result": "Acc 99.64, F1 96.02"
      }
    ],
    "performance_metrics_used": [
      "Accuracy (macro)",
      "Precision (macro)",
      "Recall (macro)",
      "F1 score (macro)",
      "Parameters (K)",
      "FLOPs (K)"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "How to improve detection accuracy while reducing computational cost and model storage for deployment on resource-constrained devices?",
        "Can a lightweight CNN block (DeepMax: DSConv + MFM) extract compact representations effectively for NID?",
        "Can batch-wise self-knowledge distillation compensate for the performance degradation of lightweight models without requiring a heavy teacher?"
      ],
      "gaps_identified": [
        "Lightweight CNNs that only use depthwise convolution may ignore inter-channel correlations and hurt accuracy.",
        "Conventional knowledge distillation requires carefully designed teacher models and adds training burden.",
        "Existing CNN-based NID models have grown large, hindering deployment on edge/resource-constrained devices.",
        "Class imbalance in NID datasets challenges minority-class detection."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Achieve a better trade-off between accuracy and efficiency for NID on resource-constrained devices by introducing a lightweight architecture and teacher-free self-distillation.",
      "potential_research_ideas": [
        "Evaluate LNet-SKD on additional modern datasets (UNSW-NB15, CSE-CIC-IDS2018, Bot-IoT) and in streaming settings.",
        "Incorporate temporal modeling (e.g., lightweight temporal convolutions or Conv-LSTM) for flows/sequences.",
        "Explore attention/transformer-lite modules (e.g., MobileViT, Performer) within or after DeepMax blocks.",
        "Combine SKD with contrastive/self-supervised pretext tasks to improve representations under class imbalance.",
        "Apply quantization-aware training and structured pruning to further compress LNet-SKD without accuracy loss.",
        "Investigate calibration and OOD/novel attack detection atop LNet-SKD outputs.",
        "Add federated/distillation-on-device training for privacy-preserving deployment across organizations.",
        "Study adversarial robustness (evasion/poisoning) and add robust training or certified defenses.",
        "Integrate explainability (feature attribution on flow features) for analyst trust and debugging."
      ],
      "architectural_improvement_recommendations": [
        "Replace or augment MFM with squeeze-and-excitation or lightweight channel attention to better model inter-channel correlations.",
        "Use depthwise separable convolutions with variable kernel sizes (mixed DW kernels) to capture multi-scale patterns.",
        "Adopt EMA of logits as the teacher for SKD (temporal ensembling) to stabilize targets versus single previous-iteration logits.",
        "Use class-imbalance-aware losses (focal loss, LDAM-DRW) alongside the class-balanced loss.",
        "Add label smoothing and temperature calibration for better generalization and minority-class performance.",
        "Perform hardware-aware NAS to co-optimize accuracy, latency, and memory on target edge devices.",
        "Apply 8-bit quantization and low-bit activations; fuse BatchNorm; profile end-to-end latency.",
        "Introduce dynamic inference (early-exit heads) for easy samples to reduce average compute."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "PyTorch"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Python 3.7, PyTorch 1.12.0; trained on a 2.4GHz Intel Core i9 CPU with 16GB RAM; SGD (momentum 0.9, weight decay 1e-4), initial LR 0.1 with cosine annealing; typical hyperparameters τ=3, λ∈{1,2}."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Deploying on resource-constrained/edge devices with limited compute and storage",
        "Maintaining accuracy when compressing models (avoiding loss from ignored channel correlations)",
        "Handling class imbalance typical in NID datasets",
        "Avoiding heavy teacher models and extra training burden for KD"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "DeepMax block combining depthwise separable convolution and Max-Feature-Map to reduce parameters and FLOPs while preserving accuracy.",
      "LNet: a lightweight CNN built by stacking DeepMax blocks; achieves strong performance with low compute/storage.",
      "Batch-wise self-knowledge distillation to regularize training without a teacher; claimed first application of SKD to NID.",
      "Extensive experiments on NSL-KDD and CICIDS2017 showing superior accuracy/F1 with fewer parameters and lower FLOPs compared to SOTA (e.g., KD-TCNN)."
    ]
  },
  {
    "arxiv_id": "2307.05529v1",
    "title": "Keystroke Dynamics for User Identification",
    "authors": "Atharva Sharma; Martin Jureček; Mark Stamp",
    "abstract": "In previous research, keystroke dynamics has shown promise for user authentication, based on both fixed-text and free-text data. In this research, we consider the more challenging multiclass user identification problem, based on free-text data. We experiment with a complex image-like feature that has previously been used to achieve state-of-the-art authentication results over free-text data. Using this image-like feature and multiclass Convolutional Neural Networks, we are able to obtain a classification (i.e., identification) accuracy of 0.78 over a set of 148 users. However, we find that a Random Forest classifier trained on a slightly modified version of this same feature yields an accuracy of 0.93.",
    "published_date": "2023-07-07",
    "pdf_link": "https://arxiv.org/pdf/2307.05529v1",
    "paper_types": [
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Identity and Access Management",
      "subdomain": "Behavioral Biometrics (Keystroke Dynamics)",
      "specific_problem": "Multiclass user identification from free-text keystroke timing data using engineered image-like features (KDI)",
      "attack_types": [
        "Masquerade/Impersonation detection (continuous authentication context)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "CNN",
        "specific": "Multiclass CNN with cutout regularization on Keystroke Dynamics Image (KDI)",
        "novel_contribution": "Applies the KDI image-like representation from prior work to the harder multiclass user identification setting; evaluates impact of subsequence length and uses cutout regularization."
      },
      {
        "type": "primary",
        "category": "Random Forest",
        "specific": null,
        "novel_contribution": "Trains on a flattened and slightly modified KDI feature; achieves substantially higher accuracy (0.93) than CNN on 148-user identification."
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Buffalo Keystroke Dataset (free-text)",
        "type": "public",
        "domain": "keystroke_timings",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "CNN (multiclass on KDI with cutout)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "0.78 (148 users)",
        "baseline_result": "0.93 (Random Forest)"
      },
      {
        "method_name": "Random Forest (flattened, slightly modified KDI)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "0.93 (148 users)",
        "baseline_result": "0.78 (CNN)"
      }
    ],
    "performance_metrics_used": [
      "Accuracy"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can the image-like KDI feature used for authentication yield strong performance for multiclass user identification on free-text data?",
        "How does keystroke subsequence length affect identification accuracy with CNNs on KDI features?",
        "Do classic ML methods (e.g., Random Forest) on flattened/modified KDI outperform CNNs for user identification?"
      ],
      "gaps_identified": [
        "User identification (multiclass) with free-text has been relatively less studied compared to authentication.",
        "Common datasets (e.g., Buffalo) provide limited modalities (no pressure), constraining feature richness.",
        "Need for approaches that are robust across keyboards/sessions and feasible for continuous authentication."
      ],
      "limitations": [
        "Work limited to one dataset (Buffalo free-text).",
        "Dataset lacks pressure-based signals; only timing features are used.",
        "CNN approach underperforms Random Forest on this task; suggests architecture-feature misalignment.",
        "To reduce sparsity they limit keys to 42 common keys, potentially discarding information.",
        "Trade-off between subsequence length (latency/resources) and accuracy; no real-time evaluation reported."
      ],
      "future_work": [],
      "motivation": "Improve inclusive, continuous user identification for security/intrusion detection using keystroke dynamics on realistic free-text data, and assess whether image-like features with CNNs or classic ML yield better performance.",
      "potential_research_ideas": [
        "Open-set/unknown-user detection layered on multiclass identification for intrusion scenarios.",
        "Domain adaptation across keyboards/devices and sessions (e.g., adversarial/domain-invariant training) to handle cross-hardware variability.",
        "Self-supervised or contrastive pretraining on keystroke streams to learn robust representations before supervised fine-tuning.",
        "Sequence-level modeling of KDI streams (e.g., 3D CNN/Temporal CNN/Transformer over successive KDIs) for richer temporal dynamics.",
        "Metric learning for user-specific embeddings enabling both identification and verification with a shared model.",
        "Federated or privacy-preserving training of keystroke models across clients without centralizing raw timing data.",
        "Robustness studies and defenses against imitation/spoofing and adversarial perturbations of timing data.",
        "Fairness auditing across gender/age/ability with bias mitigation if disparities exist."
      ],
      "architectural_improvement_recommendations": [
        "Replace plain CNN with ResNet-style CNNs or Vision Transformers tailored to sparse KDI tensors.",
        "Combine CNN features with temporal models (GRU/LSTM/Transformer) over sliding windows of KDI images.",
        "Graph/keyboard-layout aware models (e.g., GNN over key adjacency) to better encode spatial relations between keys.",
        "Feature engineering for sparsity: log-scaling, masking, or learned embeddings for rare key-pairs; add n-graph statistics beyond pairs.",
        "Ensemble tree methods (RF + Gradient Boosted Trees) with calibrated probabilities; perform feature selection/importance-driven pruning.",
        "Hyperparameter sweeps on subsequence length and cutout scheduling; curriculum learning from short to long sequences.",
        "Evaluate class-balanced losses and label smoothing to help CNNs with many classes."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Need sufficient keystroke subsequence length for reliable identification, adding latency for continuous use.",
        "Potential sensitivity to keyboard/device changes across sessions/users.",
        "Handling sparsity in high-dimensional KDI tensors and coverage of less frequent key-pairs.",
        "Privacy concerns around continuous monitoring of typing behavior (not addressed)."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Evaluates KDI image-like features for the multiclass user identification problem on free-text keystrokes.",
      "Introduces a slightly modified, flattened KDI feature enabling a Random Forest to achieve 0.93 accuracy on 148 users.",
      "Shows a multiclass CNN with cutout regularization achieves 0.78 accuracy; contrasts deep vs classic ML on same features.",
      "Explores the impact of keystroke subsequence length and discusses feature sparsity handling by restricting to 42 common keys.",
      "Claims strongest reported identification accuracy on the Buffalo free-text dataset to date."
    ]
  },
  {
    "arxiv_id": "2307.01570v1",
    "title": "Machine Learning-Based Intrusion Detection: Feature Selection versus Feature Extraction",
    "authors": "Vu-Duc Ngo; Tuan-Cuong Vuong; Thien Van Luong; Hung Tran",
    "abstract": "Internet of things (IoT) has been playing an important role in many sectors, such as smart cities, smart agriculture, smart healthcare, and smart manufacturing. However, IoT devices are highly vulnerable to cyber-attacks, which may result in security breaches and data leakages. To effectively prevent these attacks, a variety of machine learning-based network intrusion detection methods for IoT networks have been developed, which often rely on either feature extraction or feature selection techniques for reducing the dimension of input data before being fed into machine learning models. This aims to make the detection complexity low enough for real-time operations, which is particularly vital in any intrusion detection systems. This paper provides a comprehensive comparison between these two feature reduction methods of intrusion detection in terms of various performance metrics, namely, precision rate, recall rate, detection accuracy, as well as runtime complexity, in the presence of the modern UNSW-NB15 dataset as well as both binary and multiclass classification. For example, in general, the feature selection method not only provides better detection performance but also lower training and inference time compared to its feature extraction counterpart, especially when the number of reduced features K increases. However, the feature extraction method is much more reliable than its selection counterpart, particularly when K is very small, such as K = 4. Additionally, feature extraction is less sensitive to changing the number of reduced features K than feature selection, and this holds true for both binary and multiclass classifications. Based on this comparison, we provide a useful guideline for selecting a suitable intrusion detection type for each specific scenario, as detailed in Tab. 14 at the end of Section IV.",
    "published_date": "2023-07-04",
    "pdf_link": "https://arxiv.org/pdf/2307.01570v1",
    "paper_types": [
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Comparison of feature selection versus feature extraction for machine-learning-based NIDS on IoT network traffic",
      "attack_types": [
        "Analysis",
        "Backdoor",
        "DoS",
        "Exploits",
        "Fuzzers",
        "Generic",
        "Reconnaissance",
        "Shellcode",
        "Worms",
        "Normal (binary: Normal vs Abnormal)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Feature Selection",
        "specific": "Correlation-based feature selection",
        "novel_contribution": "Systematic empirical comparison vs PCA across accuracy and runtime under varying K (number of reduced features)"
      },
      {
        "type": "primary",
        "category": "Dimensionality Reduction",
        "specific": "PCA",
        "novel_contribution": "Used as the feature extraction counterpart in a controlled comparison focused on low-latency NIDS"
      },
      {
        "type": "baseline",
        "category": "Decision Tree",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Random Forest",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Multilayer Perceptron",
        "specific": null,
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "UNSW-NB15 (10% cleaned subset)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Precision",
      "Recall",
      "F1-score",
      "Detection accuracy",
      "Training time",
      "Inference time"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How do feature selection and feature extraction compare for NIDS on UNSW-NB15 in terms of precision, recall, F1-score, and accuracy?",
        "How do training and inference runtimes differ between feature selection and feature extraction under varying reduced feature counts K?",
        "How sensitive are these methods to the choice of K for both binary and multiclass classifications?",
        "How does per-attack-class detection accuracy change with K for each method?"
      ],
      "gaps_identified": [
        "“a comprehensive comparison between these two feature reduction methods has been overlooked in the literature.”",
        "Lack of a theoretical/practical guideline on when to use feature selection versus feature extraction for NIDS on modern datasets like UNSW-NB15",
        "Many prior feature selection methods (e.g., GA, PSO) have high computational cost for real-time IoT NIDS",
        "AE-based feature extraction offers capability but incurs high training and testing complexity compared to PCA and LDA"
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Provide a comprehensive, latency-aware comparison of feature selection vs. feature extraction for IoT NIDS on a modern dataset, and derive a guideline to select an appropriate approach per scenario.",
      "potential_research_ideas": [
        "Design a hybrid adaptive pipeline that selects between feature selection and feature extraction dynamically based on K, class distribution, and latency budget.",
        "Extend the comparison across multiple modern datasets (e.g., CICIDS2017, BoT-IoT) to assess dataset sensitivity and generalization.",
        "Investigate lightweight autoencoder variants with pruning/quantization to reduce latency, comparing against PCA at small K.",
        "Explore supervised dimensionality reduction (e.g., LDA, neural discriminant analysis) versus unsupervised PCA under class imbalance and per-class performance constraints.",
        "Develop online/incremental feature selection and incremental PCA to support streaming NIDS with concept drift in IoT networks.",
        "Incorporate cost-sensitive and per-class-weighted objectives to improve detection of minority attack classes while controlling false alarms.",
        "Evaluate and harden against adversarial examples targeting the feature-reduction stage (evasion via feature-space manipulations).",
        "Add explainability mechanisms to interpret selected features or principal components impacting each attack class."
      ],
      "architectural_improvement_recommendations": [
        "Implement a runtime-aware controller that chooses correlation-based selection for larger K and switches to PCA for very small K (e.g., K ≤ 4), as indicated by the paper’s findings.",
        "Use a two-stage reduction (filter selection to remove redundancies, then PCA on the remaining features) to combine benefits of both approaches.",
        "Apply mutual information or mRMR-based selection instead of pure correlation to capture non-linear dependencies with modest added cost.",
        "Adopt incremental PCA and streaming feature-correlation estimators for real-time deployment.",
        "Integrate model selection (DT/RF/MLP) with feature-reduction choice via simple AutoML constrained by latency targets.",
        "Calibrate classifiers and tune class thresholds per attack class to mitigate class imbalance effects observed in UNSW-NB15."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "NumPy",
        "Matplotlib",
        "Pandas",
        "SciPy",
        "scikit-learn",
        "scikit-plot",
        "time"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Intel Core i5-10400F (6 cores/12 threads, 12MB cache, 65W), 16GB RAM, Nvidia GTX 1650 OC-4G, Ubuntu 20.04.4 LTS"
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Resource constraints on IoT devices",
        "Requirement for low-latency, real-time detection",
        "High computational cost of some feature-selection and AE-based extraction methods"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Comprehensive empirical comparison of feature selection (correlation-based) versus feature extraction (PCA) for IoT NIDS on UNSW-NB15 under both binary and multiclass settings.",
      "Measured detection metrics (precision, recall, F1-score, accuracy) and runtime (training/inference) as functions of reduced feature count K.",
      "Key finding: “in general, the feature selection method not only provides better detection performance but also lower training and inference time compared to its feature extraction counterpart, especially when the number of reduced features K increases.”",
      "Key finding: “the feature extraction method is much more reliable than its selection counterpart, particularly when K is very small, such as K = 4.”",
      "Feature extraction is less sensitive to K than feature selection for both binary and multiclass classifications; per-attack-class behavior analyzed.",
      "Provided a theoretical/practical guideline (Table 14) for selecting an appropriate intrusion detection type per scenario.",
      "Highlighted that such a comparison and guideline over UNSW-NB15 had been overlooked in the literature."
    ]
  },
  {
    "arxiv_id": "2307.08206v3",
    "title": "Identifying Vulnerable Third-Party Java Libraries from Textual Descriptions of Vulnerabilities and Libraries",
    "authors": "Tianyu Chen; Lin Li; Bingjie Shan; Guangtai Liang; Ding Li; Qianxiang Wang; Tao Xie",
    "abstract": "To address security vulnerabilities arising from third-party libraries, security researchers maintain databases monitoring and curating vulnerability reports. Application developers can identify vulnerable libraries by directly querying the databases with their used libraries. However, the querying results of vulnerable libraries are not reliable due to the incompleteness of vulnerability reports. Thus, current approaches model the task of identifying vulnerable libraries as a named-entity-recognition (NER) task or an extreme multi-label learning (XML) task. These approaches suffer from highly inaccurate results in identifying vulnerable libraries with complex and similar names, e.g., Java libraries. To address these limitations, in this paper, we propose VulLibMiner, the first to identify vulnerable libraries from textual descriptions of both vulnerabilities and libraries, together with VulLib, a Java vulnerability dataset with their affected libraries. VulLibMiner consists of a TF-IDF matcher to efficiently screen out a small set of candidate libraries and a BERT-FNN model to identify vulnerable libraries from these candidates effectively. We evaluate VulLibMiner using four state-of-the-art/practice approaches of identifying vulnerable libraries on both their dataset named VeraJava and our VulLib dataset. Our evaluation results show that VulLibMiner can effectively identify vulnerable libraries with an average F1 score of 0.657 while the state-of-the-art/practice approaches achieve only 0.521.",
    "published_date": "2023-07-17",
    "pdf_link": "https://arxiv.org/pdf/2307.08206v3",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software Supply Chain Security",
      "subdomain": "Open-Source Dependency Risk/Vulnerability Management",
      "specific_problem": "Identify vulnerable third-party Java libraries by linking CVE descriptions to affected libraries using library descriptions",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Information Retrieval",
        "specific": "Weighted TF-IDF candidate screening",
        "novel_contribution": "Customized weighted TF-IDF using POS/NER-informed token weighting to efficiently shortlist candidate libraries from >300k corpus with few false negatives"
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "BERT",
        "novel_contribution": "Cross-encoder style BERT + feed-forward head to score coherence between CVE and library descriptions as an entity-linking task"
      },
      {
        "type": "primary",
        "category": "Feed-Forward Network",
        "specific": "FNN classifier head",
        "novel_contribution": "Final classification head over BERT representation for pairwise CVE–library matching"
      },
      {
        "type": "baseline",
        "category": "Extreme Multi-Label Learning (XML)",
        "specific": "FastXML",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Extreme Multi-Label Learning (XML)",
        "specific": "LightXML",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Extreme Multi-Label Learning (XML)",
        "specific": "Chronos",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Information Retrieval",
        "specific": "TF-IDF matcher",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "VulLib",
        "type": "public",
        "domain": "vulnerability_reports + library_descriptions (Java/Maven)",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "VeraJava",
        "type": "public",
        "domain": "vulnerability_reports + affected_libraries (Java)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "FastXML",
        "paper_reference": null,
        "metric": "F1",
        "their_result": "0.657 (average F1 of VulLibMiner)",
        "baseline_result": "0.521 (average F1 across state-of-the-art/practice approaches)"
      },
      {
        "method_name": "LightXML",
        "paper_reference": null,
        "metric": "F1",
        "their_result": "0.657 (average F1 of VulLibMiner)",
        "baseline_result": "0.521 (average F1 across state-of-the-art/practice approaches)"
      },
      {
        "method_name": "Chronos",
        "paper_reference": null,
        "metric": "F1",
        "their_result": "0.657 (average F1 of VulLibMiner)",
        "baseline_result": "0.521 (average F1 across state-of-the-art/practice approaches)"
      },
      {
        "method_name": "TF-IDF matcher (their own IR-only baseline)",
        "paper_reference": null,
        "metric": "F1",
        "their_result": "0.657 (average F1 of VulLibMiner)",
        "baseline_result": "0.521 (average F1 across state-of-the-art/practice approaches)"
      }
    ],
    "performance_metrics_used": [
      "F1 score",
      "Runtime per vulnerability (seconds)"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can incorporating library descriptions (not just names) improve the accuracy of identifying vulnerable libraries from CVE descriptions?",
        "How to balance efficiency and effectiveness when matching CVE descriptions with a very large corpus of libraries?",
        "How does the proposed two-stage (TF-IDF + BERT-FNN) entity-linking approach compare to NER-based and XML-based baselines?",
        "Can the approach generalize to zero-shot libraries not seen during training?"
      ],
      "gaps_identified": [
        "Existing NER/XML methods rely only on library names and struggle with complex/similar Java library names",
        "Vulnerability reports (e.g., NVD) are incomplete: many reports omit affected libraries or list them inaccurately",
        "Severe label sparsity for XML: many more libraries than vulnerabilities lead to poor discrimination among similar names",
        "Lack of datasets including library descriptions suitable for training semantic models"
      ],
      "limitations": [
        "Initial scarcity of labeled Java CVE–library pairs with descriptions (mitigated by constructing VulLib)",
        "Approach depends on availability and quality of library descriptions; poor or generic descriptions may reduce accuracy",
        "Focused on Java/Maven; generalization to other ecosystems not empirically validated in the provided text",
        "Requires two-stage pipeline where BERT inference remains the costly step without candidate filtering"
      ],
      "future_work": [],
      "motivation": "Improve reliability of identifying vulnerable third-party libraries, overcoming incompleteness of vulnerability reports and the ineffectiveness of name-only matching for complex and similar Java library names.",
      "potential_research_ideas": [
        "Extend to multi-ecosystem coverage (PyPI, npm, RubyGems) with ecosystem-specific description normalization and evaluation",
        "Incorporate code-level and metadata signals (e.g., repository topics, README, API docs, commit messages) with multi-modal fusion",
        "Dual-encoder (bi-encoder) retrieval with contrastive learning for sub-second retrieval over million-scale libraries, plus cross-encoder reranking",
        "Hard negative mining using libraries with highly similar names/artifacts/groups to sharpen discrimination",
        "Version-aware reasoning to predict affected version ranges, integrating semantic matching with version constraints",
        "Active learning or weak supervision to expand labeled CVE–library pairs and reduce manual validation",
        "Knowledge-augmented models leveraging CPE/CWE ontologies and project lineage graphs (groupId/artifactId hierarchy)",
        "Evaluate and improve zero-shot performance via domain-adaptive pretraining on security corpora",
        "Investigate robustness to noisy/incomplete descriptions and design noise-robust training objectives"
      ],
      "architectural_improvement_recommendations": [
        "Replace exhaustive cross-encoder BERT with a two-tower bi-encoder and ANN index (e.g., FAISS) for scalable retrieval, followed by a smaller cross-encoder reranker",
        "Use domain-adaptive pretraining (DAPT) on CVE/NVD/GitHub advisories and Maven descriptions to improve semantic alignment",
        "Integrate structured features (groupId/artifactId tokens, token edit distance, CPE hints) with learned embeddings via feature gating",
        "Introduce calibration (e.g., temperature scaling) for better decision thresholds and probability estimates",
        "Leverage prompt-tuned or instruction-tuned LLMs for few-shot linking and as a reranker to handle sparse descriptions",
        "Add POS/NER-informed token weighting to both retrieval and reranking with learnable weights via end-to-end training"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "BERT inference ~4 ms per CVE–library pair; with 512 candidates, end-to-end identification <2 seconds per vulnerability."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": "< 2 seconds per vulnerability with 512 candidates; ~4 ms per CVE–library pair",
      "deployment_challenges": [
        "Need to maintain up-to-date, comprehensive library description corpus at ecosystem scale",
        "Handling incomplete/noisy CVE reports and missing descriptions",
        "Balancing accuracy with inference cost for real-time querying at vulnerability database scale"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Propose identifying vulnerable libraries by incorporating library descriptions, not only names",
      "Design a two-stage approach (weighted TF-IDF candidate screening + BERT-FNN) for efficient and accurate entity linking between CVEs and libraries",
      "Construct and release VulLib, an open-source Java dataset with 5,028 CVE–Library pairs, manually validated",
      "Empirical evaluation on VeraJava and VulLib showing average F1 = 0.657 vs 0.521 for state-of-the-art/practice baselines; strong zero-shot gains; per-vulnerability inference under two seconds"
    ]
  },
  {
    "arxiv_id": "2307.05936v4",
    "title": "Introducing Packet-Level Analysis in Programmable Data Planes to Advance Network Intrusion Detection",
    "authors": "Roberto Doriguzzi-Corin; Luis Augusto Dias Knob; Luca Mendozzi; Domenico Siracusa; Marco Savi",
    "abstract": "Programmable data planes offer precise control over the low-level processing steps applied to network packets, serving as a valuable tool for analysing malicious flows in the field of intrusion detection. Albeit with limitations on physical resources and capabilities, they allow for the efficient extraction of detailed traffic information, which can then be utilised by Machine Learning (ML) algorithms responsible for identifying security threats. In addressing resource constraints, existing solutions in the literature rely on compressing network data through the collection of statistical traffic features in the data plane. While this compression saves memory resources in switches and minimises the burden on the control channel between the data and the control plane, it also results in a loss of information available to the Network Intrusion Detection System (NIDS), limiting access to packet payload, categorical features, and the semantic understanding of network communications, such as the behaviour of packets within traffic flows. This paper proposes P4DDLe, a framework that exploits the flexibility of P4-based programmable data planes for packet-level feature extraction and pre-processing. P4DDLe leverages the programmable data plane to extract raw packet features from the network traffic, categorical features included, and to organise them in a way that the semantics of traffic flows are preserved. To minimise memory and control channel overheads, P4DDLe selectively processes and filters packet-level data, so that only the features required by the NIDS are collected. The experimental evaluation with recent Distributed Denial of Service (DDoS) attack data demonstrates that the proposed approach is very efficient in collecting compact and high-quality representations of network flows, ensuring precise detection of DDoS attacks.",
    "published_date": "2023-07-12",
    "pdf_link": "https://arxiv.org/pdf/2307.05936v4",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Efficient packet-level feature extraction in programmable data planes to support ML-based DDoS detection in the SDN control plane",
      "attack_types": [
        "DDoS",
        "SYN Flood",
        "UDP Flood",
        "ICMP Flood"
      ]
    },
    "ml_techniques": [
      {
        "type": "baseline",
        "category": "CNN",
        "specific": null,
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "",
        "type": "",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": ""
      }
    ],
    "baselines": [
      {
        "method_name": "Naive sequential packet-level buffering (no selective processing/feature filtering)",
        "paper_reference": null,
        "metric": "False Negative Rate (FNR)",
        "their_result": "Significantly lower system FNR",
        "baseline_result": "Higher system FNR"
      }
    ],
    "performance_metrics_used": [
      "False Negative Rate (FNR)",
      "Control channel usage (qualitative)",
      "Memory/register utilization (qualitative)",
      "Collision probability sensitivity (via Bloom filter size)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can packet-level features, including categorical attributes, be efficiently extracted and organized in P4 data planes while preserving flow semantics and minimizing control channel overhead?",
        "How does selective packet-level feature collection in the data plane affect downstream ML-based DDoS detection performance (e.g., FNR) compared to naive buffering?",
        "What is the impact of counting Bloom filter sizing on collision probability and overall detection performance?"
      ],
      "gaps_identified": [
        "Most prior programmable data plane solutions compress to statistical features, losing categorical features, packet timestamps, and flow semantics needed by modern ML-based NIDS.",
        "In-network ML is constrained by P4 language/hardware (e.g., no floating-point/divisions), limiting feasible models and degrading inference performance.",
        "Existing approaches have race conditions on shared memory between data and control planes during feature extraction and reading.",
        "Hash-based flow indexing in prior work can cause collisions, corrupting flow statistics and degrading reliability.",
        "Approaches based on global/time-window statistics can only yield coarse binary attack indicators, not flow-level identification."
      ],
      "limitations": [
        "Focus of experimental evaluation is on DDoS detection; generalization to broader attack classes is not empirically demonstrated.",
        "Relies on SDN environment with programmable P4 devices; portability to heterogeneous hardware may vary.",
        "While packet payload references are discussed, the approach is constrained by data plane memory and arithmetic limitations; full payload capture is not feasible at scale.",
        "CNN-based NIDS is external to the contribution; end-to-end performance depends on downstream model choice and training quality."
      ],
      "future_work": [],
      "motivation": "Preserve rich packet-level and categorical features and flow semantics for ML-based NIDS while controlling memory and control-channel overhead in programmable data plane environments.",
      "potential_research_ideas": [
        "Extend P4DDLe’s packet-level extraction to support multi-attack, multi-protocol NIDS (e.g., brute force, fragmentation, QoS abuse) and evaluate across diverse public datasets.",
        "Learned in-switch feature selection policies that adaptively choose which packet-level fields to export based on current threat context and control-channel budget.",
        "Integrate lightweight payload sketching or content-defined sampling in the data plane to capture minimal payload semantics without overwhelming resources.",
        "Closed-loop controller that dynamically tunes Bloom filter/register sizes and export rates based on live collision/latency telemetry to maintain accuracy-latency SLAs.",
        "Combine packet-level representations with flow-level statistical summaries (hybrid multi-view features) to improve robustness and reduce export volume."
      ],
      "architectural_improvement_recommendations": [
        "Replace or complement counting Bloom filter with composite sketches (e.g., count–min sketch + conservative update) to further reduce collision-induced flow mixing.",
        "Adopt double-buffering with lock-free epoch switching plus per-flow sequence numbering to strengthen race-condition avoidance and loss accounting.",
        "Introduce per-tenant/flow-class quotas and priority-aware export queues to bound control-channel usage under attack load.",
        "Use programmable recirculation or digest batching strategies to coalesce feature exports and amortize control-channel overhead.",
        "Provide a pluggable schema/config for feature sets so the data plane extracts exactly the fields required by a given NIDS model, enabling rapid retargeting."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/risingfbk/p4ddle",
      "frameworks": [
        "P4"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "SDN controller with P4-programmable data plane (e.g., P4 switch) using control-channel digests/exports",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Control-channel bandwidth and latency bottlenecks between data and control planes",
        "Limited arithmetic operations and memory/register space in P4 devices",
        "Potential flow identifier collisions in sketches/filters",
        "Race conditions between data-plane write and control-plane read if not carefully designed",
        "Portability and heterogeneity across programmable hardware targets"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "P4DDLe: a framework for P4-based programmable data planes enabling packet-level feature extraction and preprocessing that preserves flow semantics and categorical features.",
      "A counting Bloom filter-based mechanism to efficiently group packet-level features into flows while minimizing identifier collisions.",
      "Comprehensive assessment including Bloom filter size sensitivity and evaluation of a CNN-based NIDS for realistic DDoS detection using P4DDLe’s representations; demonstrates lower system FNR versus a naive buffering strategy."
    ]
  },
  {
    "arxiv_id": "2307.11864v1",
    "title": "The Looming Threat of Fake and LLM-generated LinkedIn Profiles: Challenges and Opportunities for Detection and Prevention",
    "authors": "Navid Ayoobi; Sadat Shahriar; Arjun Mukherjee",
    "abstract": "In this paper, we present a novel method for detecting fake and Large Language Model (LLM)-generated profiles in the LinkedIn Online Social Network immediately upon registration and before establishing connections. Early fake profile identification is crucial to maintaining the platform's integrity since it prevents imposters from acquiring the private and sensitive information of legitimate users and from gaining an opportunity to increase their credibility for future phishing and scamming activities. This work uses textual information provided in LinkedIn profiles and introduces the Section and Subsection Tag Embedding (SSTE) method to enhance the discriminative characteristics of these data for distinguishing between legitimate profiles and those created by imposters manually or by using an LLM. Additionally, the dearth of a large publicly available LinkedIn dataset motivated us to collect 3600 LinkedIn profiles for our research. We will release our dataset publicly for research purposes. This is, to the best of our knowledge, the first large publicly available LinkedIn dataset for fake LinkedIn account detection. Within our paradigm, we assess static and contextualized word embeddings, including GloVe, Flair, BERT, and RoBERTa. We show that the suggested method can distinguish between legitimate and fake profiles with an accuracy of about 95% across all word embeddings. In addition, we show that SSTE has a promising accuracy for identifying LLM-generated profiles, despite the fact that no LLM-generated profiles were employed during the training phase, and can achieve an accuracy of approximately 90% when only 20 LLM-generated profiles are added to the training set. It is a significant finding since the proliferation of several LLMs in the near future makes it extremely challenging to design a single system that can identify profiles created with various LLMs.",
    "published_date": "2023-07-21",
    "pdf_link": "https://arxiv.org/pdf/2307.11864v1",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Online Social Network (OSN) Security",
      "subdomain": "Fake/Sybil Account Detection",
      "specific_problem": "Early detection of fake and LLM-generated LinkedIn profiles at registration time using only static profile text",
      "attack_types": [
        "Fake accounts (LinkedIn)",
        "LLM-generated fake profiles",
        "Impersonation/social engineering setup",
        "Spam/scam/phishing preparation"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Embedding manipulation / feature engineering for text",
        "specific": "Section and Subsection Tag Embedding (SSTE)",
        "novel_contribution": "Introduces subtractive embedding of section and subsection tags from section text embeddings, followed by pooling and classification to enhance discrimination between legitimate and fake/LLM-generated profiles."
      },
      {
        "type": "primary",
        "category": "Static word embedding",
        "specific": "GloVe",
        "novel_contribution": "Used within SSTE to evaluate effectiveness across embedding types; achieves ~95% accuracy with SSTE."
      },
      {
        "type": "primary",
        "category": "Contextual string embedding",
        "specific": "Flair",
        "novel_contribution": "Used within SSTE; shows comparable performance to other embeddings (~95% accuracy)."
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "BERT-base",
        "novel_contribution": "Used as contextual encoder within SSTE; part of comparison across embeddings (~95% accuracy)."
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": "RoBERTa",
        "novel_contribution": "Used as contextual encoder within SSTE; part of comparison across embeddings (~95% accuracy)."
      },
      {
        "type": "baseline",
        "category": "Feature-based classifier (numerical features only)",
        "specific": null,
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Ayoobi–Shahriar–Mukherjee LinkedIn Profiles Dataset (3600 profiles)",
        "type": "public",
        "domain": "social_network_profiles",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "Adikari and Dutta LinkedIn fake profile dataset",
        "type": "public",
        "domain": "social_network_profiles",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Numerical-features-only model (no textual SSTE)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "“about 95% across all word embeddings” for distinguishing legitimate and fake profiles; also ~70% on zero-shot LLM-generated profile detection; ~90% with only 20 LLM-generated profiles added to training.",
        "baseline_result": "17.79% lower accuracy than SSTE (~77.2% accuracy) on fake profile detection."
      }
    ],
    "performance_metrics_used": [
      "Accuracy"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can fake LinkedIn profiles be detected immediately at registration using only static textual profile information?",
        "Does subtracting section/subsection tag embeddings (SSTE) improve separability between legitimate and fake/LLM-generated profiles?",
        "Can a detector trained without LLM-generated profiles generalize to LLM-generated profiles, and how many LLM examples are needed to adapt effectively?"
      ],
      "gaps_identified": [
        "Scarcity of publicly available LinkedIn datasets for fake profile detection (prior public dataset reportedly includes only 17 fake profiles).",
        "Existing fake-account detectors often rely on dynamic/network features that are unavailable at registration time and require interaction with accounts.",
        "No prior approaches specifically target detection of LLM-generated fake accounts."
      ],
      "limitations": [
        "Collecting real fake LinkedIn profiles is extremely challenging, constraining the number of verified fake samples (600).",
        "LLM-generated profiles in the dataset are created with a single LLM (ChatGPT), which may limit evaluation across diverse LLMs.",
        "Focuses on pre-connection, publicly visible fields; does not leverage dynamic/network signals."
      ],
      "future_work": [
        "Refine the approach using more diverse LLM-generated profiles and prompts.",
        "Leverage the released dataset to explore additional features and models for early fake profile detection.",
        "Investigate improved generalization to unseen LLMs and prompt styles."
      ],
      "motivation": "Enable early, pre-connection detection of fake and LLM-generated LinkedIn profiles to protect users and platform integrity without relying on dynamic/network data.",
      "potential_research_ideas": [
        "Cross-LLM generalization study: train with diverse LLM-generated profiles (multiple models and prompts) to improve robustness to unseen generators.",
        "Multimodal detection by incorporating profile images and logos (with privacy safeguards) alongside text to catch inconsistencies.",
        "Contrastive learning across profile sections (e.g., About vs Experience) to detect semantic inconsistencies typical in fake/LLM-generated profiles.",
        "Open-set and out-of-distribution detection to flag novel LLM styles and unseen attack patterns at registration time.",
        "Adversarial robustness evaluation against prompt-engineered evasive LLM content; develop adversarial training with paraphrase and style perturbations.",
        "Cross-lingual and code-switching detection to handle global LinkedIn profiles.",
        "Weak/semi-supervised learning using large unlabeled LinkedIn profiles with small labeled sets to reduce manual labeling burden.",
        "Graph-agnostic temporal signals (e.g., section fill order/timestamps if available) without requiring connections to enhance early detection."
      ],
      "architectural_improvement_recommendations": [
        "Replace mean pooling with hierarchical transformers over sections, using learned section-tag embeddings rather than subtractive fixed tags.",
        "Add contrastive losses that push apart semantically inconsistent section pairs while pulling together coherent legitimate profiles.",
        "Calibrate outputs with uncertainty estimation (e.g., temperature scaling) and integrate an abstain option for high-risk cases at registration.",
        "Use lightweight adapters (LoRA/IA3) on BERT/RoBERTa for efficient fine-tuning with few LLM-generated examples.",
        "Incorporate style features (perplexity, burstiness, repetition) and plagiarism detection signals as auxiliary inputs to SSTE.",
        "Data augmentation with paraphrasing/back-translation and synthetic error injection to improve robustness."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Access to dynamic/network data is undesirable or unavailable at registration time.",
        "LinkedIn anti-scraping and data access restrictions complicate large-scale data collection.",
        "Manual verification of fake profiles is labor-intensive and limits dataset growth."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces the Section and Subsection Tag Embedding (SSTE) technique for profile-text-based fake account detection.",
      "Builds and releases a 3600-profile LinkedIn dataset (1800 legitimate, 600 fake, 1200 ChatGPT-generated) using only pre-connection public fields.",
      "Detects fake profiles immediately at registration without dynamic/network data.",
      "Demonstrates high accuracy (~95%) across GloVe, Flair, BERT, and RoBERTa within SSTE.",
      "Shows zero-shot promise on LLM-generated profiles (~70% without LLM training) and rapid adaptation to ~90% accuracy with only 20 LLM-generated training samples.",
      "Reports SSTE outperforms a numerical-features-only model by 17.79% accuracy."
    ]
  },
  {
    "arxiv_id": "2307.12469v5",
    "title": "How Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation",
    "authors": "Cen Zhang; Yaowen Zheng; Mingqiang Bai; Yeting Li; Wei Ma; Xiaofei Xie; Yuekang Li; Limin Sun; Yang Liu",
    "abstract": "LLM-based (Large Language Model) fuzz driver generation is a promising research area. Unlike traditional program analysis-based method, this text-based approach is more general and capable of harnessing a variety of API usage information, resulting in code that is friendly for human readers. However, there is still a lack of understanding regarding the fundamental issues on this direction, such as its effectiveness and potential challenges. To bridge this gap, we conducted the first in-depth study targeting the important issues of using LLMs to generate effective fuzz drivers. Our study features a curated dataset with 86 fuzz driver generation questions from 30 widely-used C projects. Six prompting strategies are designed and tested across five state-of-the-art LLMs with five different temperature settings. In total, our study evaluated 736,430 generated fuzz drivers, with 0.85 billion token costs ($8,000+ charged tokens). Additionally, we compared the LLM-generated drivers against those utilized in industry, conducting extensive fuzzing experiments (3.75 CPU-year). Our study uncovered that: - While LLM-based fuzz driver generation is a promising direction, it still encounters several obstacles towards practical applications; - LLMs face difficulties in generating effective fuzz drivers for APIs with intricate specifics. Three featured design choices of prompt strategies can be beneficial: issuing repeat queries, querying with examples, and employing an iterative querying process; - While LLM-generated drivers can yield fuzzing outcomes that are on par with those used in the industry, there are substantial opportunities for enhancement, such as extending contained API usage, or integrating semantic oracles to facilitate logical bug detection. Our insights have been implemented to improve the OSS-Fuzz-Gen project, facilitating practical fuzz driver generation in industry.",
    "published_date": "2023-07-24",
    "pdf_link": "https://arxiv.org/pdf/2307.12469v5",
    "paper_types": [
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Fuzzing and Software Testing",
      "specific_problem": "Effectiveness of LLM-based automated fuzz driver generation for C library APIs",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "LLM (Transformer)",
        "specific": "gpt-4-0613",
        "novel_contribution": "Evaluated with six designed prompting strategies (including iterative fix prompts); no new model introduced"
      },
      {
        "type": "primary",
        "category": "LLM (Transformer)",
        "specific": "gpt-3.5-turbo-0613",
        "novel_contribution": "Evaluated across multiple prompt strategies and temperatures"
      },
      {
        "type": "primary",
        "category": "LLM (Transformer)",
        "specific": "text-bison-001",
        "novel_contribution": "Evaluated as a closed-source baseline under multiple prompt strategies"
      },
      {
        "type": "primary",
        "category": "LLM (Transformer)",
        "specific": "codellama-34b-instruct",
        "novel_contribution": "Evaluated as an open-source code LLM under multiple prompt strategies"
      },
      {
        "type": "primary",
        "category": "LLM (Transformer)",
        "specific": "wizardcoder-15b-v1.0",
        "novel_contribution": "Evaluated as an open-source code LLM under multiple prompt strategies"
      },
      {
        "type": "primary",
        "category": "Prompt Engineering / In-context Learning",
        "specific": null,
        "novel_contribution": "Six prompt strategies (NAIVE-K, BACTX-K, DOCTX-K, UGCTX-K, BA-ITER-K, ALL-ITER-K) with key designs: repeated querying, extended info (docs/snippets), iterative generate-and-fix workflow with seven error-specific fix templates"
      }
    ],
    "learning_paradigm": [
      "Zero-shot prompting",
      "In-context learning (examples within prompt)",
      "Prompt engineering / iterative self-refinement"
    ],
    "datasets": [
      {
        "name": "86 fuzz driver generation questions from 30 OSS-Fuzz C projects",
        "type": "public",
        "domain": "source_code / fuzz_drivers",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "OSS-Fuzz manual/industry fuzz drivers for target projects",
        "type": "public",
        "domain": "fuzz_drivers",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Generated fuzz drivers (736,430) produced in this study",
        "type": "synthetic",
        "domain": "fuzz_drivers",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Industry/OSS-Fuzz manually written fuzz drivers",
        "paper_reference": null,
        "metric": "24-hour fuzzing outcomes (e.g., coverage/bugs)",
        "their_result": "\"LLM-generated drivers demonstrated com- parable fuzzing outcomes\"",
        "baseline_result": "Drivers used in industry (OSS-Fuzz) used as reference for comparison; no numeric gap reported in the excerpt"
      }
    ],
    "performance_metrics_used": [
      "Question resolve rate (fraction of APIs for which at least one effective driver is generated)",
      "Compilation success / linkage success",
      "Short-term fuzzing effectiveness (e.g., one-minute coverage increase; non-effective fuzzing detection)",
      "Manual semantic correctness checks for API usage",
      "24-hour fuzzing experiment outcomes (3.75 CPU-year)",
      "Token cost (0.85B tokens; $8,000+ charged)",
      "Effect of temperature and repetition count K on success"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "RQ1 To what extent can current LLMs generate effective fuzz drivers for software testing?",
        "RQ2 What are the primary challenges associated with generating effective fuzz drivers using LLMs?",
        "RQ3 What are the effectiveness and characteristics for different prompting strategies?",
        "RQ4 How do LLM-generated drivers perform comparing to those practically used in the industry?"
      ],
      "gaps_identified": [
        "High repetition cost to achieve success: \"71% of questions are resolved by repeating the entire query process at least five times and 45% require repeating the process ten times\"",
        "Ensuring semantic correctness in API usage: needed for 34% of APIs (29/86) and hard to validate automatically",
        "Complex API/environment dependencies (e.g., needing network servers/clients) block some targets (5/86)",
        "LLMs struggle with APIs with intricate specifics; tend to produce minimal API usage drivers",
        "API documentation often less helpful than example code; availability/quality of extended info varies"
      ],
      "limitations": [
        "Study scope limited to 30 OSS-Fuzz C projects and 86 API questions",
        "Evaluated five specific LLMs and five temperature settings; findings may not generalize beyond these models/configs",
        "Iterative fixing capped at five iterations; repetition K capped at 40",
        "Extended info availability uneven: docs present for 49/86 APIs; examples may be truncated to fit token limits",
        "Automatic validation relies on compilation and short-term fuzzing; some semantic issues require manual checking",
        "No explicit handling for complex environment setup beyond text prompts (e.g., external servers/clients)"
      ],
      "future_work": [
        "Extend contained API usage within generated drivers to exercise more behaviors",
        "Integrate semantic oracles/assertions to enable logical bug detection",
        "Explore advanced solutions for complex environment/context setup (e.g., automated network peer/server creation)",
        "Improve strategies to reduce repetition needs and token costs (automation efficiency)"
      ],
      "motivation": "Provide the first in-depth understanding of the effectiveness, challenges, and practical considerations of using LLMs to generate fuzz drivers, a promising but under-explored alternative to program-analysis-based generators.",
      "potential_research_ideas": [
        "Retrieval-augmented prompting with structured API schemas and verified usage patterns mined from large code corpora",
        "Hybrid pipeline combining static analysis (AST/points-to/CFD) to verify and repair API usage semantics before fuzzing",
        "Automatic environment/context builder synthesis (e.g., mock or real network/file system/server peers) guided by API specs and examples",
        "Oracle synthesis: infer semantic assertions from documentation/examples via spec mining and dynamic invariant detection",
        "Feedback-driven optimization: RL or bandit-based prompt selection using fuzzing coverage/bug signals as rewards",
        "Fine-tuning or continual pretraining of code LLMs on a corpus of high-quality fuzz drivers and API usage exemplars",
        "Multi-agent decomposition (planner/generator/tester) where agents iteratively propose, fuzz, diagnose, and repair drivers",
        "Constraint-guided generation using API contracts (pre/postconditions) expressed in a DSL to enforce correctness at generation time",
        "Active retrieval and ranking of example snippets with similarity/quality scoring to maximize usefulness while minimizing tokens",
        "Program repair loops augmented with sanitizer logs and symbolic execution traces to fix deeper semantic issues"
      ],
      "architectural_improvement_recommendations": [
        "Add a RAG layer: index API docs, headers, and curated example snippets; retrieve top-k with a budgeted token policy",
        "Implement a static checker pass (API-specific rules, resource lifecycle analyses) before running fuzzing to catch misuse",
        "Introduce a context/environment synthesis module (e.g., service stubs, file/dir scaffolding, credential/config generation)",
        "Adopt a planner-executor-refiner architecture with explicit subgoals: input arrangement, init, execution, extended usage, oracles, cleanup",
        "Use structured generation (templates/AST constraints) to ensure mandatory components (init/exec/cleanup) are present",
        "Leverage coverage- and crash-based feedback for adaptive prompt rewriting and targeted expansion of API usage paths",
        "Train lightweight adapters or preference models to prefer robust, non-minimal API usages",
        "Integrate semantic oracle generation via LLM + invariant mining (Daikon-style) and test oracles from docs/examples"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "0.85B tokens processed (~$8,000+ charged tokens); 736,430 generated drivers; 24-hour fuzzing per target totaling 3.75 CPU-year; evaluations across five LLMs and five temperatures; iteration cap 5; repetition K up to 40"
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "OSS-Fuzz ecosystem / industrial fuzzing workflows (comparison against OSS-Fuzz drivers; strategies integrated into OSS-Fuzz-Gen)",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "High token costs due to repetition and extended prompts",
        "Semantic correctness and robustness of API usage hard to ensure automatically",
        "Complex environment dependencies (e.g., network peers) not easily captured via text-only prompts",
        "LLM tendency to produce minimal API usages may limit fuzzing effectiveness",
        "Handling runtime errors (OOMS, timeouts, leaks, crashes) requires robust diagnose-and-fix loops"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "First in-depth empirical study on LLM-based fuzz driver generation, highlighting potentials and challenges",
      "Design and large-scale evaluation of six prompt strategies, including iterative generate-and-fix with seven error-specific fix templates",
      "Systematic comparison with industry (OSS-Fuzz) drivers via extensive fuzzing; summarized implications for improvement",
      "Ported strategies to improve OSS-Fuzz-Gen, enabling practical fuzz driver generation in industry"
    ]
  },
  {
    "arxiv_id": "2307.08549v1",
    "title": "G-Scan: Graph Neural Networks for Line-Level Vulnerability Identification in Smart Contracts",
    "authors": "Christoph Sendner; Ruisi Zhang; Alexander Hefter; Alexandra Dmitrienko; Farinaz Koushanfar",
    "abstract": "Due to the immutable and decentralized nature of Ethereum (ETH) platform, smart contracts are prone to security risks that can result in financial loss. While existing machine learning-based vulnerability detection algorithms achieve high accuracy at the contract level, they require developers to manually inspect source code to locate bugs. To this end, we present G-Scan, the first end-to-end fine-grained line-level vulnerability detection system evaluated on the first-of-its-kind real world dataset. G-Scan first converts smart contracts to code graphs in a dependency and hierarchy preserving manner. Next, we train a graph neural network to identify vulnerable nodes and assess security risks. Finally, the code graphs with node vulnerability predictions are mapped back to the smart contracts for line-level localization. We train and evaluate G-Scan on a collected real world smart contracts dataset with line-level annotations on reentrancy vulnerability, one of the most common and severe types of smart contract vulnerabilities. With the well-designed graph representation and high-quality dataset, G-Scan achieves 93.02% F1-score in contract-level vulnerability detection and 93.69% F1-score in line-level vulnerability localization. Additionally, the lightweight graph neural network enables G-Scan to localize vulnerabilities in 6.1k lines of code smart contract within 1.2 seconds.",
    "published_date": "2023-07-17",
    "pdf_link": "https://arxiv.org/pdf/2307.08549v1",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Smart Contract Security",
      "subdomain": "Vulnerability Detection",
      "specific_problem": "Line-level localization of reentrancy vulnerabilities in Ethereum Solidity smart contracts",
      "attack_types": [
        "Reentrancy"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "GNN",
        "specific": "GCN",
        "novel_contribution": "End-to-end node-to-line mapping via AST src attribute with a dependency- and hierarchy-preserving code graph; homogeneous directed graph built from AST hierarchy, control flow/ordering, reference, branching, loop, and break/continue/return edges; enriched node feature embeddings (function/statement types, variable properties, member access send/transfer/call); lightweight GCN for node classification enabling fast line-level localization."
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "G-Scan real-world smart contract line-level reentrancy dataset",
        "type": "private",
        "domain": "smart_contract_source_code",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "SCScan",
        "paper_reference": "SCScan [14]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "MANDO (two-stage GNN)",
        "paper_reference": "MANDO [26]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "DR-GCN",
        "paper_reference": "DR-GCN [19]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "F1-score"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can we accurately and efficiently localize smart contract vulnerabilities at the line level using GNNs?",
        "How can graph representations capture both code dependencies and hierarchies for fine-grained vulnerability detection?",
        "How to map vulnerable graph nodes back to exact source lines reliably?",
        "Can a lightweight GNN provide low-latency localization on large (thousands of LoC) contracts?"
      ],
      "gaps_identified": [
        "Lack of real-world fine-grained datasets with line-level annotations for training and evaluation.",
        "Prior graph representations focus on semantics but miss structural dependencies and code hierarchies.",
        "Existing methods often detect at contract/node level but do not map predictions back to source lines.",
        "Scalability and efficiency limitations in multi-stage or heavy DNN-based pipelines for large contracts."
      ],
      "limitations": [
        "Evaluation focuses on reentrancy vulnerability only (proof-of-concept on a single vulnerability class).",
        "Dataset, while real-world and large-scale, is not publicly available at time of writing.",
        "Results reported primarily in terms of F1-score; broader metrics and cross-method quantitative comparisons are not provided in the provided text."
      ],
      "future_work": [
        "Open-sourcing the code and the collected line-level annotated dataset.",
        "Adapting and evaluating G-Scan on additional vulnerability types beyond reentrancy."
      ],
      "motivation": "Enable precise, efficient line-level localization of smart contract vulnerabilities to reduce developer burden and support end-users in assessing contract risk with low inference overhead.",
      "potential_research_ideas": [
        "Extend to multi-vulnerability, multi-label line-level detection (reentrancy, arithmetic, access control) with shared representations.",
        "Cross-chain and cross-language generalization (e.g., Vyper, Move) via multilingual code graph pretraining.",
        "Combine static GNN-based analysis with dynamic traces (fuzzing/symbolic execution) for hybrid detection.",
        "Self-supervised pretraining on large unlabeled smart contracts using graph contrastive learning to reduce labeled data needs.",
        "Active learning or weak supervision to scale line-level annotation efficiently.",
        "Robustness against code transformations (refactoring, variable renaming, inlining) via data augmentation and invariance objectives.",
        "Uncertainty estimation and calibration for safer developer workflows (flagging high-confidence lines).",
        "Explainable vulnerability localization with attention/attribution over graph substructures.",
        "Online/streaming analysis for continuous contract auditing and version diff-aware localization."
      ],
      "architectural_improvement_recommendations": [
        "Adopt heterogeneous/relational GNNs (e.g., R-GCN, HAN) with typed edges to better model different edge semantics (AST, CFG, data deps).",
        "Incorporate edge features and attention (GAT with edge embeddings) to weight control/data dependencies.",
        "Use hierarchical graph pooling/readout to capture function-level and contract-level context jointly with node-level tasks.",
        "Multi-task learning that jointly optimizes contract-level vulnerability classification and line-level localization.",
        "Integrate inter-contract call graphs and library dependencies to capture cross-contract reentrancy flows.",
        "Sequence-aware heads (e.g., CRF over line order) to smooth line-level predictions within blocks.",
        "Leverage pretrained code models (e.g., CodeBERT/GraphCodeBERT) to initialize node/text embeddings before GNN layers.",
        "Knowledge distillation to maintain the lightweight property while increasing accuracy."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": "Lightweight model; reported inference time: localizes vulnerabilities in a 6.1k-line smart contract within 1.2 seconds. Training hardware/time not specified."
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": "1.2 seconds for a 6.1k LoC contract",
      "deployment_challenges": [
        "Dependence on high-quality line-level annotations for supervised training.",
        "Generalizing beyond reentrancy to other vulnerability types.",
        "Ensuring accurate AST-based node-to-line mapping across diverse coding styles and contract sizes.",
        "Maintaining low-latency inference as contract size and complexity grow."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "First end-to-end algorithm leveraging GNNs for fine-grained line-level detection of smart contract vulnerabilities.",
      "Scalable, efficient framework agnostic to contract size, localizing vulnerabilities within 1.2 seconds for >6.1k LoC contracts.",
      "Proof-of-concept evaluation on a first-of-its-kind real-world data collection of 13,773 contracts and 5,363,793 lines with line-level reentrancy labels.",
      "Achieves 93.02% F1-score (contract-level) and 93.69% F1-score (line-level) by incorporating variable dependencies and code hierarchies into graph modeling.",
      "Plans to open-source code and the collected dataset to promote research."
    ]
  },
  {
    "arxiv_id": "2307.10214v1",
    "title": "Time for aCTIon: Automated Analysis of Cyber Threat Intelligence in the Wild",
    "authors": "Giuseppe Siracusano; Davide Sanvito; Roberto Gonzalez; Manikantan Srinivasan; Sivakaman Kamatchi; Wataru Takahashi; Masaru Kawakita; Takahiro Kakumaru; Roberto Bifulco",
    "abstract": "Cyber Threat Intelligence (CTI) plays a crucial role in assessing risks and enhancing security for organizations. However, the process of extracting relevant information from unstructured text sources can be expensive and time-consuming. Our empirical experience shows that existing tools for automated structured CTI extraction have performance limitations. Furthermore, the community lacks a common benchmark to quantitatively assess their performance. We fill these gaps providing a new large open benchmark dataset and aCTIon, a structured CTI information extraction tool. The dataset includes 204 real-world publicly available reports and their corresponding structured CTI information in STIX format. Our team curated the dataset involving three independent groups of CTI analysts working over the course of several months. To the best of our knowledge, this dataset is two orders of magnitude larger than previously released open source datasets. We then design aCTIon, leveraging recently introduced large language models (GPT3.5) in the context of two custom information extraction pipelines. We compare our method with 10 solutions presented in previous work, for which we develop our own implementations when open-source implementations were lacking. Our results show that aCTIon outperforms previous work for structured CTI extraction with an improvement of the F1-score from 10%points to 50%points across all tasks.",
    "published_date": "2023-07-14",
    "pdf_link": "https://arxiv.org/pdf/2307.10214v1",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "benchmark",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Threat Intelligence",
      "subdomain": "CTI Automation",
      "specific_problem": "Structured CTI information extraction from unstructured text into STIX bundles",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "LLM",
        "specific": "GPT-3.5",
        "novel_contribution": "Two-step LLM pipeline with pre-processing/condensation to fit context limits and an extraction + self-verification prompting stage for selecting and classifying relevant CTI entities and attack patterns into STIX."
      },
      {
        "type": "primary",
        "category": "Transformer",
        "specific": null,
        "novel_contribution": "Zero-shot prompting and in-context learning applied to CTI extraction with constraints to reduce hallucinations and handle small context windows."
      },
      {
        "type": "baseline",
        "category": "Transformer",
        "specific": "BERT (various fine-tuned NER/classification models)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Sequence labeling / NER",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Sentence classification",
        "specific": null,
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Zero-shot",
      "In-context learning",
      "Supervised (for baseline fine-tuned models)"
    ],
    "datasets": [
      {
        "name": "aCTIon CTI-STIX Benchmark (204 reports)",
        "type": "public",
        "domain": "cti_reports_text",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "SecIE",
        "type": "public",
        "domain": "cti_reports_text",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "CASIE",
        "type": "public",
        "domain": "cti_reports_text",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "ThreatKG",
        "type": "public",
        "domain": "cti_reports_text",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "LADDER",
        "type": "public",
        "domain": "cti_reports_text",
        "link": null,
        "is_new_contribution": false,
        "availability": "available_on_request"
      },
      {
        "name": "SecBERT (attack pattern sentences)",
        "type": "public",
        "domain": "cti_reports_text",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "TRAM (attack pattern sentences)",
        "type": "public",
        "domain": "cti_reports_text",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "TTPDrill",
        "type": "public",
        "domain": "cti_reports_text",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "AttacKG",
        "type": "public",
        "domain": "cti_reports_text",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "rcATT",
        "type": "public",
        "domain": "cti_reports_text",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "SecIE",
        "paper_reference": null,
        "metric": "F1-score",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "ThreatKG",
        "paper_reference": null,
        "metric": "F1-score",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "LADDER",
        "paper_reference": null,
        "metric": "F1-score",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "SecBERT",
        "paper_reference": null,
        "metric": "F1-score",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "TRAM",
        "paper_reference": null,
        "metric": "F1-score",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "TTPDrill",
        "paper_reference": null,
        "metric": "F1-score",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "AttacKG",
        "paper_reference": null,
        "metric": "F1-score",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "rcATT",
        "paper_reference": null,
        "metric": "F1-score",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "BERT-based NER (generic transformer baselines)",
        "paper_reference": null,
        "metric": "F1-score",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Sentence-level CLF for ATT&CK technique extraction",
        "paper_reference": null,
        "metric": "F1-score",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "F1-score",
      "Precision",
      "Recall",
      "CTI-metrics (report-level, entity/relation correctness)"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "How to evaluate existing and future tools for structured CTI information extraction using CTI-metrics rather than proxy NLP metrics?",
        "Can LLMs (GPT-3.5) with tailored prompting and pipelines improve end-to-end structured CTI extraction into STIX?",
        "What is the performance of prior CTI extraction tools when evaluated on a large, expert-annotated benchmark of real-world reports?"
      ],
      "gaps_identified": [
        "Lack of a common benchmark with CTI-metrics for structured CTI extraction.",
        "Existing tools focus largely on IoC extraction and struggle with entities like threat actor, malware, target, and attack pattern.",
        "Prevailing evaluations use NLP token/sentence-level metrics that overestimate performance for end-to-end CTI extraction.",
        "Small or partially available datasets with limited report-level annotations due to high annotation costs."
      ],
      "limitations": [
        "LLMs suffer from hallucinations and small context windows, requiring special handling.",
        "Structured CTI extraction is nuanced and requires relevance filtering beyond standard NER; fully automating may still need analyst oversight."
      ],
      "future_work": [],
      "motivation": "Provide an open, large, real-world benchmark and a practical LLM-based method to improve and fairly evaluate structured CTI extraction into STIX.",
      "potential_research_ideas": [
        "Design a retrieval-augmented CTI extractor that grounds LLM outputs on ATT&CK and CTI knowledge bases to reduce hallucinations.",
        "Fine-tune an open-source long-context LLM on CTI corpora and STIX graph outputs for reproducible, offline deployment.",
        "Multi-document and temporal CTI aggregation: fuse multiple reports into a unified evolving STIX knowledge graph.",
        "Active learning with analyst-in-the-loop to iteratively refine relevance filtering and attack pattern attribution.",
        "Graph-aware decoding: generate STIX graphs directly with constrained decoding, followed by graph validation and repair.",
        "Evaluate adversarial robustness of CTI extraction (e.g., obfuscated text, injection) and develop defenses.",
        "Extend benchmark to cover more STIX entities/relations (e.g., Campaign, Tool, Course of Action) and multi-lingual reports.",
        "Incorporate table/image parsing and OCR to capture IoCs and TTPs embedded in non-textual elements."
      ],
      "architectural_improvement_recommendations": [
        "Add retrieval augmentation (RAG) over curated CTI KBs and MITRE ATT&CK pages with citation-based grounding.",
        "Use longer-context LLMs or chunk-linking strategies with hierarchical summarization and cross-chunk coreference resolution.",
        "Constrained JSON/STIX-schema decoding with function calling and validation to minimize formatting and entity-type errors.",
        "Self-consistency with multi-sample decoding and verifier/reflection steps specialized for CTI relevance and ATT&CK mapping.",
        "Confidence calibration and abstention thresholds to flag low-confidence extractions for analyst review.",
        "Integrate document structure parsing (headings, tables, IoC sections) to prioritize relevant sections and reduce noise.",
        "Add a relation extraction module to attribute attack patterns to the correct entities using entailment-based checks.",
        "Leverage open-source LLMs (e.g., instruction-tuned Llama/T5) to increase reproducibility and control costs."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "LLM hallucinations affecting entity/relationship correctness.",
        "Small context windows requiring pre-processing and summarization.",
        "Ambiguity and aliasing in CTI entity mentions demanding relevance filtering.",
        "Attribution of ATT&CK techniques across dispersed narrative text."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "New large open benchmark dataset: 204 real-world CTI reports with expert-curated STIX bundles (two orders of magnitude larger than prior open datasets).",
      "aCTIon: an LLM-based CTI extraction pipeline using GPT-3.5 with pre-processing and self-verification prompting to produce STIX.",
      "Comprehensive benchmark and replication of 10 prior solutions with consistent CTI-metric evaluation.",
      "Reported gains: “Our results show that aCTIon outperforms previous work for structured CTI extraction with an improvement of the F1-score from 10%points to 50%points across all tasks.”",
      "More detailed gains: “increasing the F1-score by 15-50% points for malware, threat actor and target entities extraction, and by about 10% points for attack pattern extraction.”"
    ]
  },
  {
    "arxiv_id": "2307.08946v1",
    "title": "EsaNet: Environment Semantics Enabled Physical Layer Authentication",
    "authors": "Ning Gao; Qiying Huang; Cen Li; Shi Jin; Michail Matthaiou",
    "abstract": "Wireless networks are vulnerable to physical layer spoofing attacks due to the wireless broadcast nature, thus, integrating communications and security (ICAS) is urgently needed for 6G endogenous security. In this letter, we propose an environment semantics enabled physical layer authentication network based on deep learning, namely EsaNet, to authenticate the spoofing from the underlying wireless protocol. Specifically, the frequency independent wireless channel fingerprint (FiFP) is extracted from the channel state information (CSI) of a massive multi-input multi-output (MIMO) system based on environment semantics knowledge. Then, we transform the received signal into a two-dimensional red green blue (RGB) image and apply the you only look once (YOLO), a single-stage object detection network, to quickly capture the FiFP. Next, a lightweight classification network is designed to distinguish the legitimate from the illegitimate users. Finally, the experimental results show that the proposed EsaNet can effectively detect physical layer spoofing attacks and is robust in time-varying wireless environments.",
    "published_date": "2023-07-18",
    "pdf_link": "https://arxiv.org/pdf/2307.08946v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Physical Layer Security",
      "specific_problem": "Physical layer authentication for spoofing/impersonation detection using channel fingerprints in massive MIMO-OFDM",
      "attack_types": [
        "physical layer spoofing",
        "impersonation",
        "MAC address spoofing (motivating example)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Object Detection CNN",
        "specific": "YOLOv5 (CSPDarknet53 backbone, FPN+PAN)",
        "novel_contribution": "Applied to detect frequency-independent channel fingerprint (FiFP) heat spots in angle-delay RGB images derived from received signals"
      },
      {
        "type": "primary",
        "category": "MLP (Fully-connected network)",
        "specific": "4-layer Dense network with Dropout; ReLU in third layer, sigmoid elsewhere; cross-entropy loss",
        "novel_contribution": "Lightweight threshold-free classifier on fused YOLO features (angles, delays, bounding box size of top paths)"
      },
      {
        "type": "primary",
        "category": "Feature Engineering",
        "specific": "Angle-delay transform of MIMO-OFDM channel; extraction of frequency-independent fingerprint (FiFP)",
        "novel_contribution": "Environment-semantics-based feature design to filter fast time-varying gain/phase and select sparse, interpretable angle-delay features"
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": "VGG-net (CSI-image based black-box detection)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Rule-based",
        "specific": "Fixed-threshold detector",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "YOLOv5 training channel images (synthetic)",
        "type": "synthetic",
        "domain": "wireless_channel_images (angle-delay RGB from received MIMO-OFDM signals)",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "Dataset A (synthetic, stationary distributions)",
        "type": "synthetic",
        "domain": "wireless_channel_images / fused YOLO features for classification",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      },
      {
        "name": "Dataset B (synthetic, shifted mean distributions for robustness)",
        "type": "synthetic",
        "domain": "wireless_channel_images / fused YOLO features for classification",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "VGG-net (CSI-image black-box)",
        "paper_reference": null,
        "metric": "Detection accuracy (%) across SNR {0,5,10,15,20} dB on Dataset A",
        "their_result": "EsaNet: [99.25, 99.5, 99.75, 99.75, 99.75]",
        "baseline_result": "VGG-net: [88.75, 92, 91.5, 90.75, 89.75]"
      },
      {
        "method_name": "Fixed threshold detector",
        "paper_reference": null,
        "metric": "Detection accuracy (%) across SNR {0,5,10,15,20} dB on Dataset A",
        "their_result": "EsaNet: [99.25, 99.5, 99.75, 99.75, 99.75]",
        "baseline_result": "Fixed threshold: [97.5, 96.8, 97.7, 97.3, 97.5]"
      },
      {
        "method_name": "VGG-net (CSI-image black-box)",
        "paper_reference": null,
        "metric": "Detection accuracy (%) across SNR {0,5,10,15,20} dB on Dataset B (robustness test with mean shifts)",
        "their_result": "EsaNet: [92.5, 92.7, 92.6, 93.5, 92.5]",
        "baseline_result": "VGG-net: [84, 81, 80, 81, 80]"
      },
      {
        "method_name": "Fixed threshold detector",
        "paper_reference": null,
        "metric": "Detection accuracy (%) across SNR {0,5,10,15,20} dB on Dataset B (robustness test with mean shifts)",
        "their_result": "EsaNet: [92.5, 92.7, 92.6, 93.5, 92.5]",
        "baseline_result": "Fixed threshold: [90.3, 91.5, 90, 90.4, 90.8]"
      }
    ],
    "performance_metrics_used": [
      "Detection accuracy",
      "ROC AUC"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can frequency-independent, environment-semantics (angle-delay) features from massive MIMO channels enable robust physical layer authentication in time-varying wireless environments?",
        "Can object detection (YOLO) rapidly capture sparse channel semantics to reduce data processing overhead and latency for PHY authentication?"
      ],
      "gaps_identified": [
        "Fixed/dynamic-threshold PHY authentication requires strong assumptions and accurate statistical models, which are infeasible in complex, unknown wireless environments (especially mmWave/THz).",
        "Directly feeding high-dimensional CSI into deep networks leads to long latency, poor interpretability, and unstable performance in time-varying environments.",
        "Massive MIMO exponentially increases CSI acquisition and processing complexity."
      ],
      "limitations": [
        "Evaluations are simulation-based with synthetic datasets; no over-the-air or hardware testbed results.",
        "Scenario assumes fixed locations for Alice/Eve and a single-antenna receiver in an indoor multi-scatterer setting.",
        "Performance degrades when the mean of the angle-delay distribution shifts (Dataset B shows ~6.85% accuracy drop compared to Dataset A).",
        "Results are reported for a specific parameterization (e.g., M=N=32, L=5, α=β=16, SNR range) and may not generalize without further validation.",
        "No explicit latency or runtime measurements; reduced overhead is argued but not quantified."
      ],
      "future_work": [
        "Positioned as a candidate paradigm for integrating communications and security (ICAS) in 6G; potential extension to practical systems is implied."
      ],
      "motivation": "Detect and prevent physical layer spoofing with a robust, interpretable, and low-latency method suitable for massive MIMO in complex, time-varying wireless environments.",
      "potential_research_ideas": [
        "Over-the-air validation on SDR testbeds (e.g., USRP) across bands (sub-6, mmWave) to quantify real-world robustness and latency.",
        "End-to-end differentiable pipeline that jointly learns the angle-delay transform, detection, and classification (replace two-stage YOLO+MLP with joint training).",
        "Temporal modeling for user identity tracking under mobility (e.g., sequence models or transformers over successive frames).",
        "Domain adaptation and continual learning to handle distribution shifts across environments, arrays, and frequencies.",
        "Adversarial robustness analysis at PHY (evasion via crafted waveforms) and defenses (adversarial training, certified defenses).",
        "Cross-device/cross-array generalization with calibration-invariant representations; multi-antenna receiver extensions.",
        "Lightweight edge deployment via quantization, pruning, and knowledge distillation while preserving accuracy.",
        "Uncertainty estimation and abstention for safety-critical decisions (e.g., selective authentication under low SNR)."
      ],
      "architectural_improvement_recommendations": [
        "Replace YOLOv5 with a lightweight heatmap regression/keypoint detector tailored to sparse spot localization, potentially improving speed and precision.",
        "Introduce a physics-informed layer for the angle-delay transform so it is differentiable and trainable (learned oversampling/basis).",
        "Joint training of detector and classifier with a multi-task loss to reduce error propagation and improve calibration.",
        "Incorporate temporal feature fusion (e.g., 1D/2D CNN-LSTM or transformer) across consecutive frames for robustness to noise and dynamics.",
        "Apply calibration-aware normalization and learned uncertainty (e.g., MC Dropout) in the classifier to improve reliability under distribution shifts."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "PyTorch",
        "YOLOv5"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Training performed with NVIDIA GeForce GTX 2080Ti; YOLOv5 trained with Adam (lr 1e-3, weight decay 5e-4, 300 iterations, batch size 16); classifier trained with RMSprop (lr 1e-3, 1200 iterations, batch size 64)."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces environment-semantics-based frequency-independent channel fingerprint (FiFP) using angle-delay features, improving interpretability and training efficiency.",
      "Transforms received MIMO-OFDM signals into RGB angle-delay images and uses YOLOv5 to rapidly capture sparse features, reducing processing overhead and latency.",
      "Designs a lightweight, threshold-free classifier on fused YOLO features to distinguish legitimate from illegitimate users.",
      "Demonstrates via simulations high detection accuracy and robustness across SNRs and under distribution shifts; argues compatibility with existing protocols and channel estimation components for ICAS in 6G."
    ]
  },
  {
    "arxiv_id": "2308.04704v2",
    "title": "A Feature Set of Small Size for the PDF Malware Detection",
    "authors": "Ran Liu; Charles Nicholas",
    "abstract": "Machine learning (ML)-based malware detection systems are becoming increasingly important as malware threats increase and get more sophisticated. PDF files are often used as vectors for phishing attacks because they are widely regarded as trustworthy data resources, and are accessible across different platforms. Therefore, researchers have developed many different PDF malware detection methods. Performance in detecting PDF malware is greatly influenced by feature selection. In this research, we propose a small features set that don't require too much domain knowledge of the PDF file. We evaluate proposed features with six different machine learning models. We report the best accuracy of 99.75% when using Random Forest model. Our proposed feature set, which consists of just 12 features, is one of the most conciseness in the field of PDF malware detection. Despite its modest size, we obtain comparable results to state-of-the-art that employ a much larger set of features.",
    "published_date": "2023-08-09",
    "pdf_link": "https://arxiv.org/pdf/2308.04704v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Malware Analysis",
      "subdomain": "Document Malware Detection",
      "specific_problem": "Static detection of malicious PDF files using small graph/tree-structure feature set",
      "attack_types": [
        "PDF malware",
        "phishing via PDF documents"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Feature Engineering",
        "specific": "Graph-theoretic features from PDF object tree",
        "novel_contribution": "Proposes a 12-feature, PDF-specific graph/tree feature set requiring minimal domain knowledge"
      },
      {
        "type": "primary",
        "category": "Random Forest",
        "specific": null,
        "novel_contribution": "Best-performing classifier using the proposed 12 graph features; achieves 99.75% accuracy"
      },
      {
        "type": "baseline",
        "category": "Decision Tree",
        "specific": "J48",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Gradient Boosting",
        "specific": "XGBoost",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Naive Bayes",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "MLP",
        "specific": "Multi-layer Perceptron",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Logistic Regression",
        "specific": "Simple Logistic",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Contagio (PDF malware dataset)",
        "type": "public",
        "domain": "pdf_documents",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Model[18] (unspecified in excerpt)",
        "paper_reference": "[18]",
        "metric": "Accuracy/Precision/Recall/F1",
        "their_result": "Acc 0.9975, Prec 0.9973, Rec 0.9974, F1 0.9974 (Random Forest, proposed features)",
        "baseline_result": "Acc 0.9965, Prec 0.9970, Rec 0.9970, F1 0.9970"
      },
      {
        "method_name": "PDF Malware Detection Based on Optimizable Decision Trees",
        "paper_reference": "Abu Al-Haija et al., 2022 [1]",
        "metric": "Accuracy/Precision/Recall/F1",
        "their_result": "Acc 0.9975, Prec 0.9973, Rec 0.9974, F1 0.9974",
        "baseline_result": "Acc 0.9884, Prec 0.9880, Rec 0.9890, F1 0.9885"
      },
      {
        "method_name": "PDF Malware Detection based on Stacking Learning",
        "paper_reference": "Issakhani et al., 2022 [3]",
        "metric": "Accuracy/Precision/Recall/F1",
        "their_result": "Acc 0.9975, Prec 0.9973, Rec 0.9974, F1 0.9974",
        "baseline_result": "Acc 0.9869, Prec 0.9888, Rec 0.9887, F1 0.9877"
      },
      {
        "method_name": "Hidost",
        "paper_reference": "[15]",
        "metric": "Accuracy/FPR",
        "their_result": "Accuracy 99.75%, FPR 0.23% (Random Forest with proposed features)",
        "baseline_result": "Accuracy 99.8%, FPR < 0.06%"
      },
      {
        "method_name": "PDFrate",
        "paper_reference": "[9], [12], [18]",
        "metric": "Accuracy/FPR",
        "their_result": "Accuracy 99.75%, FPR 0.23% (Random Forest with proposed features)",
        "baseline_result": "Accuracy 99.0%, FPR 0.2%"
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "Precision",
      "Recall",
      "F1 Score",
      "True Positive Rate (TPR)",
      "False Positive Rate (FPR)",
      "False Negative Rate (FNR)",
      "True Negative Rate (TNR)",
      "95% Confidence Intervals",
      "Confusion Matrix"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "Dynamic features require sandbox execution and are often not PDF-specific; sophisticated malware can evade by detecting sandbox environments.",
        "Common static features (e.g., keyword-based, JavaScript code) require extensive PDF domain knowledge and can be obfuscated by attackers.",
        "Large, complex feature sets may lead to overfitting; there is a need for small, simple PDF-specific features to reduce attack surface.",
        "Existing approaches often rely on features attackers may have already probed and exploited for evasion."
      ],
      "limitations": [
        "Vulnerable to evasive attacks via insertion, deletion, or alteration of subtrees in the PDF object graph.",
        "Effectiveness depends on quality of parsed PDF objects; parser failed to fully parse some malware specimens.",
        "Parser (pdfrw) could not extract path objects for all files, reducing usable dataset size."
      ],
      "future_work": [
        "Explore additional features to improve overall performance and enhance robustness.",
        "Enrich and diversify feature types to mitigate evasion (beyond current graph features)."
      ],
      "motivation": "Design a very small, PDF-specific feature set that requires minimal domain knowledge yet achieves accuracy comparable to or better than state-of-the-art methods using much larger feature sets.",
      "potential_research_ideas": [
        "Adversarially robust training using simulated subtree insert/delete/modify attacks on PDF object graphs.",
        "Combine the proposed graph features with a minimal set of obfuscation-resilient content/keyword features for improved robustness.",
        "Model the PDF object graph with graph kernels or graph neural networks and compare against the 12-feature approach.",
        "Multi-parser ensemble parsing to mitigate parser-specific failures (e.g., pdfrw + Poppler + PeePDF) before feature extraction.",
        "Unsupervised or semi-supervised anomaly detection on graph features to detect novel/zero-day PDF malware.",
        "Temporal drift analysis of PDF structural features to adapt models over time with online or incremental learning.",
        "Automated feature search to discover additional compact graph descriptors that are hard to manipulate without breaking PDF validity."
      ],
      "architectural_improvement_recommendations": [
        "Augment the 12-feature set with invariants derived from cross-reference tables, versioning chains, and trailer consistency to resist simple subtree perturbations.",
        "Use calibrated gradient boosting (e.g., XGBoost/LightGBM) with hyperparameter tuning and class-weighting to potentially improve FPR without sacrificing TPR.",
        "Integrate adversarial training by generating plausibly valid graph perturbations during training.",
        "Adopt a parsing ensemble and fallback strategy: attempt multiple parsers and reconcile object graphs to reduce parsing failures.",
        "Add uncertainty estimation (e.g., Platt scaling or conformal prediction) to flag samples for deeper analysis when confidence is low.",
        "Evaluate and possibly incorporate lightweight graph kernels or shallow GNNs trained on the object graph as an additional channel."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "scikit-learn",
        "XGBoost",
        "pdfrw"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Model can be evaded by subtree insertions/deletions/alterations in the PDF object graph.",
        "Dependence on successful parsing; parser failures degrade detection efficacy."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes one of the smallest PDF-specific feature sets (12 graph/tree features) requiring minimal PDF domain knowledge.",
      "Performs statistical analysis demonstrating significant differences between benign and malicious PDFs across the proposed graph features.",
      "Empirically evaluates six ML classifiers with 5-fold cross-validation, achieving up to 99.75% accuracy (Random Forest).",
      "Benchmarks against prior work, showing comparable or superior performance despite using a much smaller feature set."
    ]
  },
  {
    "arxiv_id": "2307.02289v1",
    "title": "Fuzzing with Quantitative and Adaptive Hot-Bytes Identification",
    "authors": "Tai D. Nguyen; Long H. Pham; Jun Sun",
    "abstract": "Fuzzing has emerged as a powerful technique for finding security bugs in complicated real-world applications. American fuzzy lop (AFL), a leading fuzzing tool, has demonstrated its powerful bug finding ability through a vast number of reported CVEs. However, its random mutation strategy is unable to generate test inputs that satisfy complicated branching conditions (e.g., magic-byte comparisons, checksum tests, and nested if-statements), which are commonly used in image decoders/encoders, XML parsers, and checksum tools. Existing approaches (such as Steelix and Neuzz) on addressing this problem assume unrealistic assumptions such as we can satisfy the branch condition byte-to-byte or we can identify and focus on the important bytes in the input (called hot-bytes) once and for all. In this work, we propose an approach called \\tool~which is designed based on the following principles. First, there is a complicated relation between inputs and branching conditions and thus we need not only an expressive model to capture such relationship but also an informative measure so that we can learn such relationship effectively. Second, different branching conditions demand different hot-bytes and we must adjust our fuzzing strategy adaptively depending on which branches are the current bottleneck. We implement our approach as an open source project and compare its efficiency with other state-of-the-art fuzzers. Our evaluation results on 10 real-world programs and LAVA-M dataset show that \\tool~achieves sustained increases in branch coverage and discovers more bugs than other fuzzers.",
    "published_date": "2023-07-05",
    "pdf_link": "https://arxiv.org/pdf/2307.02289v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Fuzz Testing",
      "specific_problem": "Coverage-guided greybox fuzzing that bypasses complex branching conditions via quantitative branch distance and adaptive hot-bytes identification",
      "attack_types": [
        "Denial of Service (via crashes found by fuzzing)",
        "Remote Code Execution (potential vulnerabilities discovered)",
        "Privilege Escalation (potential vulnerabilities discovered)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Feed-forward Neural Network",
        "specific": "Deep neural network for hot-byte prediction (architecture unspecified)",
        "novel_contribution": "Uses branch-distance bitmaps as supervision and trains adaptively on a min-Pareto seed set to predict input bytes whose mutation most reduces distance to just-missed branches"
      },
      {
        "type": "primary",
        "category": "Multi-objective Optimization",
        "specific": "Pareto optimality with min-Pareto seed selection",
        "novel_contribution": "Greedy algorithm to maintain a lightweight, adaptive seed pool aligned to targeted branches, reducing redundant mutations"
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Online"
    ],
    "datasets": [
      {
        "name": "LAVA-M",
        "type": "public",
        "domain": "program_binaries",
        "link": "https://github.com/panda-re/lava",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "10 real-world programs (jpg, png, xml, bz2, tiff, ttf formats)",
        "type": "public",
        "domain": "program_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "AFL",
        "paper_reference": "Michal Zalewski (AFL)",
        "metric": "branch coverage, unique crashes/bugs",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "FairFuzz",
        "paper_reference": "L. Lemieux and K. Sen, ISSTA 2018",
        "metric": "branch coverage, unique crashes/bugs",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Neuzz",
        "paper_reference": "S. She et al., NDSS 2019",
        "metric": "branch coverage, unique crashes/bugs",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "MOPT",
        "paper_reference": "F. Lyu et al., USENIX Security 2019",
        "metric": "branch coverage, unique crashes/bugs",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Angora",
        "paper_reference": "P. Chen and H. Chen, S&P 2018",
        "metric": "branch coverage, unique crashes/bugs",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "TortoiseFuzz",
        "paper_reference": "X. Chen et al., USENIX Security 2021",
        "metric": "branch coverage, unique crashes/bugs",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "branch coverage",
      "unique crashes",
      "number of bugs discovered",
      "coverage over time"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Q1: How to quantitatively compare t and t′ in terms of which one is more likely to cover a branch?",
        "Q2: How to adaptively minimize fuzzing queue to reduce the number of redundant mutations?"
      ],
      "gaps_identified": [
        "Existing approaches assume unrealistic assumptions such as we can satisfy the branch condition byte-to-byte or we can identify and focus on the important bytes in the input (called hot-bytes) once and for all.",
        "Lightweight taint algorithms are not able to capture the relation between inputs and branching conditions quantitatively; they typically return only byte offsets and give no clue on how to mutate hot-bytes.",
        "Neural-network-based guidance in prior work (e.g., Neuzz) is not adaptive as the guidance (i.e., gradient) quickly becomes outdated after several branches are covered."
      ],
      "limitations": [
        "Focuses only on branch distance (not approach distance) to avoid computational overhead, potentially reducing guidance quality for distant branches.",
        "Min-Pareto set is computed via a greedy approximation with O(|O|^2) complexity and may not be optimal.",
        "Architecture and training details of the neural network are not specified in the provided text, limiting reproducibility details."
      ],
      "future_work": [],
      "motivation": "Random mutations in CGF (e.g., AFL) struggle with complex branching conditions (magic bytes, checksums, nested ifs); need an expressive model and adaptive strategy to identify hot-bytes and guide mutations quantitatively.",
      "potential_research_ideas": [
        "Incorporate approach distance estimation (e.g., dynamic path approximations or lightweight CFG distance heuristics) alongside branch distance to better guide towards distant branches.",
        "Use program-structure-aware models (e.g., GNNs over CFG/DFG) to predict both hot bytes and mutation operators conditioned on targeted branches.",
        "Formulate seed and mutation scheduling as a reinforcement learning problem jointly optimizing coverage and crash yield.",
        "Neuro-symbolic hybrid: integrate lightweight constraint solving or concolic execution when predicted hot-bytes stagnate to solve hard conditions (e.g., checksums).",
        "Active learning to query informative seeds (e.g., diversity- or uncertainty-based sampling) from the broader pool beyond min-Pareto.",
        "Meta-learning to rapidly adapt hot-byte predictors across different targets/program formats.",
        "Differentiable fuzzing via IR lifting to enable gradient signals directly through some operations (e.g., lifted arithmetic/comparisons)."
      ],
      "architectural_improvement_recommendations": [
        "Multi-task model that predicts (a) hot-byte importance per branch and (b) recommended mutation operator and magnitude per byte.",
        "Curriculum training that gradually increases branch-condition complexity to stabilize online learning.",
        "Uncertainty-aware prediction (e.g., MC dropout) to guide exploration vs. exploitation in selecting bytes and seeds.",
        "Maintain a bounded, diversity-regularized min-Pareto set using clustering in distance space to prevent crowding and reduce O(|O|^2) costs.",
        "Periodic re-labeling/refresh of training data to mitigate stale supervision as program coverage evolves.",
        "Leverage byte n-gram or attention mechanisms to better capture checksum/magic sequence dependencies."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Local fuzzing of real-world open-source programs and LAVA-M targets",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Balancing learning overhead with fuzzing throughput",
        "Potential staleness of learned guidance as coverage evolves",
        "Complexity of maintaining and updating the min-Pareto set"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "An algorithm for fuzzing queue minimization (min-Pareto set) that eliminates redundant mutations.",
      "A novel machine learning approach for identifying hot-bytes quantitatively and adaptively using branch-distance supervision.",
      "An open-source fuzzer, Finch, evaluated on 10 real-world programs and LAVA-M, achieving sustained increases in branch coverage and discovering more bugs than other fuzzers."
    ]
  },
  {
    "arxiv_id": "2307.02663v1",
    "title": "Convergence of Communications, Control, and Machine Learning for Secure and Autonomous Vehicle Navigation",
    "authors": "Tengchan Zeng; Aidin Ferdowsi; Omid Semiari; Walid Saad; Choong Seon Hong",
    "abstract": "Connected and autonomous vehicles (CAVs) can reduce human errors in traffic accidents, increase road efficiency, and execute various tasks ranging from delivery to smart city surveillance. Reaping these benefits requires CAVs to autonomously navigate to target destinations. To this end, each CAV's navigation controller must leverage the information collected by sensors and wireless systems for decision-making on longitudinal and lateral movements. However, enabling autonomous navigation for CAVs requires a convergent integration of communication, control, and learning systems. The goal of this article is to explicitly expose the challenges related to this convergence and propose solutions to address them in two major use cases: Uncoordinated and coordinated CAVs. In particular, challenges related to the navigation of uncoordinated CAVs include stable path tracking, robust control against cyber-physical attacks, and adaptive navigation controller design. Meanwhile, when multiple CAVs coordinate their movements during navigation, fundamental problems such as stable formation, fast collaborative learning, and distributed intrusion detection are analyzed. For both cases, solutions using the convergence of communication theory, control theory, and machine learning are proposed to enable effective and secure CAV navigation. Preliminary simulation results are provided to show the merits of proposed solutions.",
    "published_date": "2023-07-05",
    "pdf_link": "https://arxiv.org/pdf/2307.02663v1",
    "paper_types": [
      "position",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Cyber-Physical Systems Security",
      "subdomain": "Autonomous/Connected Vehicles",
      "specific_problem": "Secure autonomous navigation via joint communication-control-ML design for stable path tracking/formation, robust control under data injection attacks, adaptive and collaborative learning, and distributed intrusion detection",
      "attack_types": [
        "sensor data injection",
        "malicious information injection",
        "cyber-physical attacks"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "RNN",
        "specific": "LSTM",
        "novel_contribution": "Used to learn temporal interdependencies in CAV state for robust data fusion under data injection attacks"
      },
      {
        "type": "primary",
        "category": "Reinforcement Learning",
        "specific": "Q-learning (Q-function based)",
        "novel_contribution": "Combined with LSTM summary to select sensor weighting actions that minimize long-term impact of data injection attacks"
      },
      {
        "type": "primary",
        "category": "Federated Learning",
        "specific": "Dynamic Federated Proximal (DFP)",
        "novel_contribution": "Novel FL algorithm accounting for mobility, fading, and non-IID/unbalanced data; coupled with an incentive mechanism to increase participation"
      },
      {
        "type": "primary",
        "category": "MLP/ANN",
        "specific": "ANN auto-tuner for PID",
        "novel_contribution": "ANN auto-tunes PID parameters for adaptive longitudinal control, trained via proposed FL (DFP)"
      },
      {
        "type": "primary",
        "category": "GAN",
        "specific": null,
        "novel_contribution": "Proposed distributed intrusion detection concept for coordinated CAVs using generator-discriminator across vehicles (as illustrated in Fig. 3)"
      }
    ],
    "learning_paradigm": [
      "Reinforcement",
      "Supervised",
      "Federated"
    ],
    "datasets": [
      {
        "name": "Berkeley DeepDrive (BDD100K)",
        "type": "public",
        "domain": "driving_videos / vehicular_data_traces",
        "link": "https://www.bdd100k.com/",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Separate optimization of communication and control systems",
        "paper_reference": null,
        "metric": "Number of vehicular communication links meeting delay requirements / path tracking stability",
        "their_result": "“the system with the joint design outperforms baselines optimizing the communication and control system separately.”",
        "baseline_result": null
      },
      {
        "method_name": "FL without DFP and incentive mechanism (generic baseline)",
        "paper_reference": null,
        "metric": "Convergence speed (rounds/time to converge)",
        "their_result": "“the convergence speed of the DFP algorithm with our incentive mechanism design can be improved by 40% compared with baseline”",
        "baseline_result": null
      },
      {
        "method_name": "Non-optimized controller parameters for platoon formation",
        "paper_reference": "[13]",
        "metric": "Wireless reliability under control delay constraints (plant/string stability)",
        "their_result": "Platoon with optimized controller design achieves higher wireless reliability than other parameter choices a, b∈[2,4]",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "stability (plant stability via Lyapunov-Krasovskii/Razumikhin)",
      "string stability (non-amplification of disturbances via transfer function)",
      "delay threshold constraints for stability",
      "wireless reliability (probability of meeting delay/packet loss requirements)",
      "number of links satisfying delay requirements",
      "convergence time/speed of FL",
      "spacing/trajectory deviation under attack (qualitative via sensor weights)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How do wireless delay and packet loss affect controller stability and path tracking in uncoordinated CAVs, and how to jointly design communication and control to ensure stability?",
        "How to design robust CAV control and state estimation under cyber-physical (data injection) attacks using ML?",
        "How to realize adaptive navigation controllers under heterogeneous, limited, and non-IID local data using federated learning while accounting for mobility and fading?",
        "How to maintain stable formation (plant and string stability) for coordinated CAVs under wireless impairments through joint communication-control design?",
        "How to enable fast collaborative learning among coordinated CAVs (e.g., drone swarms) with wireless constraints and privacy considerations?",
        "How to perform distributed intrusion detection for coordinated CAVs leveraging generative-discriminative models?"
      ],
      "gaps_identified": [
        "Prior works study communications, control, and ML largely in isolation and treat other components as black boxes.",
        "Existing path tracking security studies often ignore cyber-physical interdependence in CAVs.",
        "Standard FL assumes balanced IID data and stable connectivity, which does not hold under CAV mobility, fading, and participation variability.",
        "Cloud-based ML for coordinated CAVs is bandwidth- and privacy-inefficient."
      ],
      "limitations": [
        "Primarily preliminary simulation results; no real-world deployment or hardware-in-the-loop validation reported.",
        "Quantitative comparisons lack detailed numerical baselines and statistical analysis.",
        "Attack model evaluated focuses on sensor data injection with a simplified multi-sensor setting; broader adversarial models not assessed.",
        "Reproducibility details (code, hyperparameters, training settings) are not provided."
      ],
      "future_work": [
        "“Finally, we present future research directions and open problems to further improve the joint system design.”"
      ],
      "motivation": "Autonomous navigation requires a convergent integration of communication, control, and learning; current approaches typically treat these subsystems separately, which undermines stability, security, and adaptability.",
      "potential_research_ideas": [
        "Certified stability-aware co-design: jointly optimize communication scheduling and controller gains with formal guarantees under stochastic wireless channels.",
        "Attack-resilient FL for CAV control using robust aggregation (e.g., trimmed mean, median, Krum) and secure aggregation to mitigate poisoned or Byzantine participants.",
        "Privacy-preserving FL with differential privacy or homomorphic encryption tailored to CAV control tasks, quantifying the utility-privacy-stability trade-off.",
        "Multi-agent RL for formation control that incorporates communication delay models and safety shields for string stability.",
        "Event-triggered and semantic communication protocols for CAV control and FL to reduce bandwidth while preserving stability and convergence.",
        "Adversarial training and sensor fusion redundancy against spoofing/data injection across heterogeneous sensors (camera, LiDAR, radar) within the LSTM-RL framework.",
        "Joint design with 5G/NR sidelink and beam management policies optimizing reliability for FL rounds and control loops in platoons and drone swarms.",
        "Cross-layer intrusion detection using distributed GANs with consistency constraints across vehicles to detect coordinated anomalies."
      ],
      "architectural_improvement_recommendations": [
        "Augment DFP with robust/Byzantine-resilient aggregation and secure aggregation to tolerate malicious or low-quality clients.",
        "Integrate model-based observers (e.g., Kalman/Moving Horizon Estimation) with LSTM-RL fusion for physics-informed robustness and explicit uncertainty quantification.",
        "Adopt hierarchical FL (edge server between vehicles and cloud) with client selection based on data quality and control-criticality scoring.",
        "Incorporate event-triggered updates for both control inputs and FL model uploads to reduce delay and congestion while meeting stability margins.",
        "Use multi-sensor cross-checks and residual-based anomaly detection to complement GAN-based intrusion detection and reduce false positives.",
        "Co-optimize V2X resource allocation (power, RBs, MCS) with control Lyapunov/Barrier functions to enforce safety under communication constraints."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Wireless delay and packet loss affecting stability",
        "Mobility and fading reducing FL participation and reliability",
        "Non-IID and unbalanced local datasets across vehicles",
        "Energy and bandwidth constraints for model exchange",
        "Exposure to cyber-physical attacks (sensor data injection)",
        "Privacy concerns with sharing trajectory/sensing data"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": true,
      "privacy_preserving": true,
      "fairness_bias": false
    },
    "contributions": [
      "Comprehensive position-style study on convergence of communications, control, and ML for secure autonomous navigation in uncoordinated and coordinated CAVs.",
      "Joint communication-control design methodology using time-delay systems with Lyapunov-Krasovskii/Razumikhin analysis to derive delay thresholds and optimize both subsystems for stable path tracking and formation.",
      "ML-based robust control against cyber-physical (data injection) attacks via an LSTM + RL sensor fusion strategy.",
      "Adaptive navigation controller using FL; introduces a Dynamic Federated Proximal (DFP) algorithm and an incentive mechanism to handle mobility, fading, and non-IID data.",
      "Collaborative FL framework for drone swarms with convergence analysis under wireless impairments (e.g., antenna angle deviations).",
      "Concept and design for distributed intrusion detection in coordinated CAVs (generator-discriminator based), enabling detection using shared and locally generated data.",
      "Preliminary simulations demonstrating improved stability/reliability with joint design, a 40% improvement in FL convergence speed with DFP and incentives, and benefits for platoon reliability with optimized controller parameters."
    ]
  },
  {
    "arxiv_id": "2307.11454v2",
    "title": "Structure-Aware Code Vulnerability Analysis With Graph Neural Networks",
    "authors": "Ravil Mussabayev",
    "abstract": "This study explores the effectiveness of graph neural networks (GNNs) for vulnerability detection in software code, utilizing a real-world dataset of Java vulnerability-fixing commits. The dataset's structure, based on the number of modified methods in each commit, offers a natural partition that facilitates diverse investigative scenarios. The primary focus is to evaluate the general applicability of GNNs in identifying vulnerable code segments and distinguishing these from their fixed versions, as well as from random non-vulnerable code. Through a series of experiments, the research addresses key questions about the suitability of different configurations and subsets of data in enhancing the prediction accuracy of GNN models. Experiments indicate that certain model configurations, such as the pruning of specific graph elements and the exclusion of certain types of code representation, significantly improve performance. Additionally, the study highlights the importance of including random data in training to optimize the detection capabilities of GNNs.",
    "published_date": "2023-07-21",
    "pdf_link": "https://arxiv.org/pdf/2307.11454v2",
    "paper_types": [
      "empirical_analysis",
      "reproducibility",
      "new_dataset"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Vulnerability Detection (Static Analysis)",
      "specific_problem": "Method-level code vulnerability classification using graph neural networks; distinguishing vulnerable vs fixed code and vulnerable vs random safe code",
      "attack_types": [
        "software code vulnerabilities (various CWE categories)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "GNN",
        "specific": "GGNN (ReVeal architecture with representation learning and SMOTE components)",
        "novel_contribution": "No new architecture proposed; the paper evaluates and ablates ReVeal components and graph representations (AST/DDG/CFG), and studies pruning/partitioning strategies"
      },
      {
        "type": "baseline",
        "category": "GNN",
        "specific": "GGNN (ReVeal) without SMOTE and Representation Learning",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "GNN",
        "specific": "GGNN (ReVeal) without AST edges (DDG+CFG only)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "GNN",
        "specific": "GGNN (ReVeal) with pruning at operator nodes",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Imbalanced learning",
        "specific": "Majority class downsampling (vs SMOTE)",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "ReVeal C++ method-level vulnerability dataset (Linux Debian Kernel, Chromium)",
        "type": "public",
        "domain": "source_code (C/C++)",
        "link": "https://github.com/VulDetProject/ReVeal",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Java Vulnerability-Fixing Commits dataset (865 commits; method-level pairs and random safe code)",
        "type": "private",
        "domain": "source_code (Java)",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Baseline ReVeal configuration",
        "paper_reference": null,
        "metric": "Median F1; Median ROC AUC",
        "their_result": "Median F1=27.29; Median ROC AUC=0.696",
        "baseline_result": null
      },
      {
        "method_name": "ReVeal without SMOTE & Representation Learning",
        "paper_reference": null,
        "metric": "Median F1; Median ROC AUC",
        "their_result": "Median F1=21.45; Median ROC AUC=0.730",
        "baseline_result": "Baseline ReVeal: Median F1=27.29; ROC AUC=0.696"
      },
      {
        "method_name": "ReVeal without AST edges (DDG+CFG only)",
        "paper_reference": null,
        "metric": "Median F1; Median ROC AUC",
        "their_result": "Median F1=27.65; Median ROC AUC=0.706",
        "baseline_result": "Baseline ReVeal: Median F1=27.29; ROC AUC=0.696"
      },
      {
        "method_name": "ReVeal with pruning at operator nodes",
        "paper_reference": null,
        "metric": "Median F1; Median ROC AUC",
        "their_result": "Median F1=30.83; Median ROC AUC=0.724",
        "baseline_result": "Baseline ReVeal: Median F1=27.29; ROC AUC=0.696"
      },
      {
        "method_name": "ReVeal with majority class downsampling",
        "paper_reference": null,
        "metric": "Median F1; Median ROC AUC",
        "their_result": "Median F1=26.61; Median ROC AUC=0.678",
        "baseline_result": "Baseline ReVeal: Median F1=27.29; ROC AUC=0.696"
      }
    ],
    "performance_metrics_used": [
      "F1",
      "ROC AUC"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Research Question 1. How would one optimize the parameters in the following dimensions to achieve the best possible performance of the ReVeal model on the ReVeal dataset? (1) Include the SMOTE and representation learning modules or only use a single GGNN block; (2) Include information about AST edges into the input graphs or only use DDG and CFG edges; (3) Include the full graph, which can be too large and detailed, or use its pruned version at operator nodes instead; (4) Balance the training data by downsampling the majority class or keep the original class ratio.",
        "Research Question 2. Is Pi useful? (i = 1,2,3)",
        "Research Question 3. How difficult is task T1?",
        "Research Question 4. How difficult is task T2?",
        "Research Question 5. Does random code appear in P2 as k increases?",
        "Research Question 6. How does the size of P3 affect the overall performance?"
      ],
      "gaps_identified": [
        "Standard GNN-based approaches (ReVeal) underperform on task T1 (distinguishing vulnerable vs fixed versions), performing near random.",
        "Including all AST edges can degrade performance, suggesting overfitting to fine-grained or irrelevant features.",
        "Imbalanced data and label noise in multi-function commits (P2) complicate training; potential increase of random/noisy code in P2 with larger k.",
        "Training without random safe code (P3) severely hurts performance when test sets include random safe code.",
        "Limited trials and relatively small training sets for large GNNs hinder reliable conclusions."
      ],
      "limitations": [
        "Only one trial was performed for each k (RQ2–RQ5) and |P3| (RQ6), which may not capture randomness in training/splitting.",
        "Available training data may be insufficient to draw solid conclusions (e.g., ~3148 training examples for k=5 in P1∪P2∪P3).",
        "Findings are confined to a specific architecture (ReVeal GGNN) and may not generalize to other models.",
        "Java dataset labeling relies on commit-level heuristics; in multi-function commits, not all changes may correspond to vulnerability fixes, introducing noise."
      ],
      "future_work": [
        "Investigate other machine learning models for code vulnerability detection.",
        "Improve data augmentation techniques to enhance performance.",
        "Develop better models specifically tailored to tackle task T1 (vulnerable vs fixed)."
      ],
      "motivation": "Automate code vulnerability detection at scale and assess the general applicability and configuration sensitivity of GNNs (ReVeal) on real-world datasets, reproducing C++ results and extending to a new Java dataset with structure-aware partitions.",
      "potential_research_ideas": [
        "Design diff-aware or Siamese GNNs that jointly encode (f, f') pairs with contrastive or metric learning to directly optimize task T1.",
        "Introduce graph edit networks or graph matching networks to model fine-grained changes between vulnerable and fixed code.",
        "Self-supervised pretraining on large code graphs (e.g., masked node/edge prediction, contrastive graph views) followed by fine-tuning on vulnerability labels.",
        "Multi-task learning that jointly optimizes T1 and T2 with shared encoders and task-specific heads.",
        "Hard-negative mining and curriculum learning to gradually introduce increasingly similar fixed versions for robust discrimination.",
        "Data-centric approaches to reduce P2 noise: lineage/blame analysis, static analysis alerts, or developer annotations to identify which methods in multi-function commits truly fix vulnerabilities.",
        "Leverage token-level and line-level signals (e.g., integrate LineVul-like supervision) with graph encoders for finer granularity.",
        "Cross-language transfer (C++↔Java) via shared intermediate representations (CPG) and domain adaptation.",
        "Uncertainty estimation and risk-aware thresholds to support practical triage in CI pipelines."
      ],
      "architectural_improvement_recommendations": [
        "Adopt a two-tower (Siamese) encoder over (original, fixed) method graphs with contrastive/triplet losses to explicitly target T1.",
        "Incorporate edge-type-aware attention (e.g., R-GCN or edge-weighted GAT) to learn when to downweight AST edges rather than removing them heuristically.",
        "Use graph pooling/pruning learned end-to-end (TopK/SAGPool) to simplify graphs while retaining salient semantics.",
        "Fuse graph encoders with code-token transformers (e.g., CodeBERT/GraphCodeBERT) via late or cross-attention fusion.",
        "Replace SMOTE with class-balanced or focal losses and dynamic reweighting to handle imbalance without synthetic samples.",
        "Adopt program slicing to construct smaller, semantically-focused graphs (data/control slices) that reduce noise in P2.",
        "Train a three-class model (vulnerable vs fixed vs random safe) with calibrated outputs, then compose T0 via decision logic.",
        "Introduce contrastive pretraining using random safe code (P3) as negatives and patched pairs as positives to better separate distributions."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/VulDetProject/ReVeal",
      "frameworks": [
        "DGL",
        "Python",
        "NumPy",
        "Joern"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": "Intel Xeon Gold 6151 (32 cores), NVIDIA Tesla V100 16GB, 126 GB RAM; Python 3.10.7, NumPy 1.23.3, DGL 1.0.1+cu117, Joern 1.1.1495. Default hyperparams: lr=1e-4, weight decay=1e-3, embedding size=200, batch size=128, max batches=10000, grad accumulation=8, early stopping patience=50 (C++), 20 (Java)."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Class imbalance and need for careful handling (SMOTE or reweighting).",
        "Label noise in multi-function commits (P2) where not all changes correspond to vulnerability fixes.",
        "Difficulty of T1 (vulnerable vs fixed) with small code differences leads to near-random performance for current model.",
        "Dependency on random safe code (P3) during training to achieve good performance on mixed test sets.",
        "Potential overfitting when including full AST edges (too fine-grained, large graphs)."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Empirical reproduction and ablation of the ReVeal GGNN model on the ReVeal C++ dataset across SMOTE/RL usage, AST edge inclusion, graph pruning, and class downsampling.",
      "Collection and structuring of a Java dataset (865 vulnerability-fixing commits) with a partitioning scheme based on the number of modified methods (P1, P2, P3).",
      "Problem decomposition into tasks T1 (vulnerable vs fixed) and T2 (potentially vulnerable vs random safe) and evaluation under multiple train/test regimes.",
      "Finding that pruning at operator nodes and excluding AST edges improve performance, while naive downsampling harms it.",
      "Identifying that inclusion of random safe code (P3) in training is critical when test sets include random code; model performs well on T2 but underperforms on T1.",
      "Analysis of the effect of k (number of changed methods per commit) and P3 size on performance, with indications of increasing noise in P2 as k increases and little sensitivity to P3 size."
    ]
  },
  {
    "arxiv_id": "2307.11853v1",
    "title": "Exploring Security Commits in Python",
    "authors": "Shiyu Sun; Shu Wang; Xinda Wang; Yunlong Xing; Elisa Zhang; Kun Sun",
    "abstract": "Python has become the most popular programming language as it is friendly to work with for beginners. However, a recent study has found that most security issues in Python have not been indexed by CVE and may only be fixed by 'silent' security commits, which pose a threat to software security and hinder the security fixes to downstream software. It is critical to identify the hidden security commits; however, the existing datasets and methods are insufficient for security commit detection in Python, due to the limited data variety, non-comprehensive code semantics, and uninterpretable learned features. In this paper, we construct the first security commit dataset in Python, namely PySecDB, which consists of three subsets including a base dataset, a pilot dataset, and an augmented dataset. The base dataset contains the security commits associated with CVE records provided by MITRE. To increase the variety of security commits, we build the pilot dataset from GitHub by filtering keywords within the commit messages. Since not all commits provide commit messages, we further construct the augmented dataset by understanding the semantics of code changes. To build the augmented dataset, we propose a new graph representation named CommitCPG and a multi-attributed graph learning model named SCOPY to identify the security commit candidates through both sequential and structural code semantics. The evaluation shows our proposed algorithms can improve the data collection efficiency by up to 40 percentage points. After manual verification by three security experts, PySecDB consists of 1,258 security commits and 2,791 non-security commits. Furthermore, we conduct an extensive case study on PySecDB and discover four common security fix patterns that cover over 85% of security commits in Python, providing insight into secure software maintenance, vulnerability detection, and automated program repair.",
    "published_date": "2023-07-21",
    "pdf_link": "https://arxiv.org/pdf/2307.11853v1",
    "paper_types": [
      "new_dataset",
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Software Security",
      "subdomain": "Secure Software Maintenance and Vulnerability Management",
      "specific_problem": "Detection and characterization of security-related commits in Python repositories (including silent security fixes) and construction of a Python security-commit dataset",
      "attack_types": [
        "injection",
        "denial of service",
        "bypass",
        "exploit",
        "leakage",
        "overflow",
        "underflow",
        "smuggling",
        "spoofing",
        "unauthorized access",
        "access control issues",
        "open redirect",
        "race condition",
        "out of bounds",
        "directory traversal (dot dot slash)",
        "code execution"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Graph Representation",
        "specific": "CommitCPG",
        "novel_contribution": "A new commit graph representation that merges previous and current version Code Property Graphs (AST/CFG/PDG) with versioned nodes/edges and slicing to preserve sequential and structural semantics of code changes."
      },
      {
        "type": "primary",
        "category": "GNN with Attention",
        "specific": "SCOPY (GCN with multi-head attention)",
        "novel_contribution": "A multi-attributed graph learning model that embeds node statements with CodeBERT and edge attributes as one-hot vectors to learn from CommitCPG and identify security commit candidates."
      },
      {
        "type": "primary",
        "category": "Transformer (code model)",
        "specific": "CodeBERT",
        "novel_contribution": "Used to embed node statements (sequential semantics) within the SCOPY pipeline; not novel itself but integral to the proposed model."
      },
      {
        "type": "baseline",
        "category": "Topic Modeling",
        "specific": "Latent Dirichlet Allocation (LDA)",
        "novel_contribution": "Used to automatically extract security-related keywords from CVE/CWE/commit messages for candidate filtering; not a novel contribution."
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "PySecDB (overall)",
        "type": "public",
        "domain": "code_commits",
        "link": "https://github.com/SunLab-GMU/PySecDB",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "PySecDB - Base Dataset",
        "type": "public",
        "domain": "code_commits",
        "link": "https://github.com/SunLab-GMU/PySecDB",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "PySecDB - Pilot Dataset",
        "type": "public",
        "domain": "code_commits",
        "link": "https://github.com/SunLab-GMU/PySecDB",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "PySecDB - Augmented Dataset",
        "type": "public",
        "domain": "code_commits",
        "link": "https://github.com/SunLab-GMU/PySecDB",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "CVE records (MITRE) used for base dataset",
        "type": "public",
        "domain": "vulnerability_reports",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Wild GitHub commits (351 repositories)",
        "type": "public",
        "domain": "code_commits",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Random selection of commits (for candidate collection efficiency)",
        "paper_reference": null,
        "metric": "Efficiency ratio: security commits / total candidates",
        "their_result": "\"Compared with random selection, the keyword filtering method and SCOPY can improve the efficiency by 30 and 40 percentage points when constructing the pilot and augmented datasets, respectively.\"",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "efficiency ratio (security commits among candidates)",
      "percentage point improvement in collection efficiency"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How to identify hidden (silent) security commits in Python beyond those indexed by CVE?",
        "Can we construct a comprehensive Python security-commit dataset that captures diverse vulnerability types and real-world patterns?",
        "How to jointly capture sequential (token-level) and structural (dependency-level) semantics of code changes for commit-level security detection?",
        "What common security fix patterns exist in Python security commits and how broadly do they apply?"
      ],
      "gaps_identified": [
        "No existing Python-focused security commit datasets; prior datasets focus on C/C++ or Java.",
        "Existing feature extraction methods are language-dependent and cannot be directly migrated to Python.",
        "Deep learning approaches often treat code as natural language (losing structure) or focus only on code dependencies (losing sequential semantics).",
        "Learned features from deep models are often uninterpretable.",
        "Only 46% of indexed Python CVEs provide corresponding security commits; many fixes are silent and unindexed."
      ],
      "limitations": [
        "Pilot dataset construction depends on presence of security-related keywords in commit messages (mitigated by augmented dataset).",
        "Manual verification by three experts required to ensure data quality (48 man-hours), which may limit scalability.",
        "Augmented detection relies on static analysis tooling (CPG generation via Joern) which introduces computational overhead; slicing is used to reduce it.",
        "Focuses on Python and popular GitHub repositories; generalization to other ecosystems is untested in this paper."
      ],
      "future_work": [],
      "motivation": "Most Python security issues are not CVE-indexed and may be fixed by silent security commits, threatening downstream security; existing datasets and techniques are insufficient for Python and for capturing comprehensive code semantics with interpretability.",
      "potential_research_ideas": [
        "Extend CommitCPG+SCOPY to other languages and multi-language repositories to build cross-language security-commit datasets.",
        "Incorporate commit message and issue tracker signals into a multimodal graph-text model (e.g., joint CodeBERT + textual transformer fusion).",
        "Pretrain SCOPY with self-supervised objectives on unlabeled commit graphs (contrastive learning across versions/hunks) before supervised fine-tuning.",
        "Leverage large code models (e.g., CodeT5+, StarCoder2, GraphCodeBERT) for node/graph embeddings and compare against CodeBERT.",
        "Develop explainability methods for graph-based commit classifiers (e.g., subgraph/rationale extraction) and map them to fix patterns/CWEs.",
        "Automate extraction of security fix templates from discovered patterns and integrate with program repair tools for suggestion/auto-fix.",
        "Active learning pipeline to continuously mine new security commits with human-in-the-loop verification to expand PySecDB.",
        "Evaluate real-time detection in CI/CD with streaming commit analysis and incremental graph updates.",
        "Study adversarial robustness of commit-level detectors against obfuscation in commit diffs/messages."
      ],
      "architectural_improvement_recommendations": [
        "Add global graph pooling with hierarchical graph networks (e.g., Graph U-Nets) to better aggregate multi-function changes.",
        "Augment edge features with richer static analysis (taint/dataflow summaries, security API catalogs) and learned relation embeddings.",
        "Replace or complement CodeBERT with domain-adapted code LLMs fine-tuned on Python diffs to better capture patch semantics.",
        "Use cross-attention between previous and current version subgraphs to explicitly model change provenance instead of merged graphs only.",
        "Introduce multi-task learning to jointly predict security commit and CWE category, sharing representations.",
        "Adopt contrastive objectives between security vs. non-security diffs and between pre/post-version subgraphs to sharpen discriminative features."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": "https://github.com/SunLab-GMU/PySecDB",
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Hidden (silent) security fixes lack informative commit messages, complicating detection.",
        "Manual expert verification needed to ensure data quality.",
        "Computational overhead of CPG generation and static analysis (mitigated by slicing).",
        "Limited coverage of CVE-linked commits in Python; many fixes are unindexed."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Construct the first Python security-commit dataset (PySecDB) with 1,258 security and 2,791 non-security commits across 351 repositories and 119 CWEs.",
      "Design a keyword filtering method (via LDA-derived keywords) to locate potential security commits from commit messages.",
      "Propose CommitCPG, a commit-level graph representation merging versioned CPGs with slicing to encode structural and sequential semantics.",
      "Introduce SCOPY, a multi-attributed graph learning model (GCN with multi-head attention, CodeBERT node embeddings) to identify security commit candidates.",
      "Demonstrate improved data collection efficiency: +30 pp (keyword filtering) and +40 pp (SCOPY) over random selection for pilot and augmented datasets.",
      "Perform a case study discovering four common security fix patterns covering over 85% of Python security commits (sanity checks, update API usage, update regular expressions, restrict security properties)."
    ]
  },
  {
    "arxiv_id": "2307.08679v1",
    "title": "Externally validating the IoTDevID device identification methodology using the CIC IoT 2022 Dataset",
    "authors": "Kahraman Kostas; Mike Just; Michael A. Lones",
    "abstract": "In the era of rapid IoT device proliferation, recognizing, diagnosing, and securing these devices are crucial tasks. The IoTDevID method (IEEE Internet of Things 2022) proposes a machine learning approach for device identification using network packet features. In this article we present a validation study of the IoTDevID method by testing core components, namely its feature set and its aggregation algorithm, on a new dataset. The new dataset (CIC-IoT-2022) offers several advantages over earlier datasets, including a larger number of devices, multiple instances of the same device, both IP and non-IP device data, normal (benign) usage data, and diverse usage profiles, such as active and idle states. Using this independent dataset, we explore the validity of IoTDevID's core components, and also examine the impacts of the new data on model performance. Our results indicate that data diversity is important to model performance. For example, models trained with active usage data outperformed those trained with idle usage data, and multiple usage data similarly improved performance. Results for IoTDevID were strong with a 92.50 F1 score for 31 IP-only device classes, similar to our results on previous datasets. In all cases, the IoTDevID aggregation algorithm improved model performance. For non-IP devices we obtained a 78.80 F1 score for 40 device classes, though with much less data, confirming that data quantity is also important to model performance.",
    "published_date": "2023-07-03",
    "pdf_link": "https://arxiv.org/pdf/2307.08679v1",
    "paper_types": [
      "empirical_analysis",
      "reproducibility"
    ],
    "security_domain": {
      "primary": "IoT Security",
      "subdomain": "Device Identification",
      "specific_problem": "Classifying IoT device make/model (device classes) from network packet features, including both IP and non-IP protocols, with MAC-based post-hoc aggregation",
      "attack_types": []
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Decision Tree",
        "specific": null,
        "novel_contribution": "Used as the main classifier in external validation experiments on CIC-IoT-22"
      },
      {
        "type": "primary",
        "category": "Rule-based aggregation/voting",
        "specific": "MAC-address-based two-step aggregation (dominant-MAC determination and MAC-group majority label assignment with exception list)",
        "novel_contribution": "Validation of IoTDevID’s aggregation algorithm showing consistent performance gains over individual packet predictions"
      },
      {
        "type": "primary",
        "category": "Evolutionary Algorithm",
        "specific": "Genetic algorithm for feature selection (from prior IoTDevID work)",
        "novel_contribution": "This study validates the previously selected feature set on a new dataset rather than introducing a new selection method"
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "CIC-IoT-2022 (CIC-IoT-22)",
        "type": "public",
        "domain": "network_traffic",
        "link": "http://205.174.165.80/IOTDataset/CIC_IOT_Dataset2022",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Aalto IoT dataset",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "UNSW IoT dataset (Sivanathan et al.)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "IoTDevID (individual packets, no aggregation) vs IoTDevID + Aggregation (AA condition)",
        "paper_reference": null,
        "metric": "F1-score",
        "their_result": "0.773",
        "baseline_result": "0.702"
      },
      {
        "method_name": "IoTDevID (individual packets, no aggregation) vs IoTDevID + Aggregation (AI condition)",
        "paper_reference": null,
        "metric": "F1-score",
        "their_result": "0.780",
        "baseline_result": "0.714"
      },
      {
        "method_name": "IoTDevID (individual packets, no aggregation) vs IoTDevID + Aggregation (IA condition)",
        "paper_reference": null,
        "metric": "F1-score",
        "their_result": "0.744",
        "baseline_result": "0.679"
      },
      {
        "method_name": "IoTDevID (individual packets, no aggregation) vs IoTDevID + Aggregation (II condition)",
        "paper_reference": null,
        "metric": "F1-score",
        "their_result": "0.778",
        "baseline_result": "0.720"
      }
    ],
    "performance_metrics_used": [
      "F1-score",
      "Accuracy"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Do IoTDevID’s core components (feature set and aggregation algorithm) generalize to an independent dataset (CIC-IoT-22)?",
        "How does data diversity (e.g., active vs idle device usage; multiple usage profiles) impact device identification performance?",
        "Does the aggregation algorithm consistently improve performance over individual packet predictions on CIC-IoT-22?"
      ],
      "gaps_identified": [
        "Data leakage due to improper train/test separation in prior work (e.g., session-dependent features like ports, TCP sequence/ack used during feature extraction)",
        "Use of overly specific or network-dependent features that harm generalizability (e.g., IP/MAC counts, flow statistics tied to a specific network)",
        "Selective device testing and lack of transparency (e.g., excluding devices without justification; lack of datasets/code)",
        "Transfer problem when concatenating packets by MAC/IP where one MAC/IP can represent multiple devices (e.g., gateway re-encapsulation in Aalto)",
        "Non-IP device coverage and normal usage data are often missing in prior datasets"
      ],
      "limitations": [
        "Evaluation assumes benign network conditions; aggregation may group malicious packets that spoof IP/MAC addresses",
        "Non-IP device data is limited in CIC-IoT-22; Z-Wave data is not in pcap format, and ZigBee/Z-Wave lack normal usage data",
        "Not all devices generate data in every session; sessions differ in device presence, requiring matching-session comparisons",
        "Discrepancy in available sessions: only 24 active sessions found versus 30 reported in the CIC-IoT-22 paper",
        "Only features selected in prior IoTDevID work were used; no re-optimization of feature set on CIC-IoT-22"
      ],
      "future_work": [],
      "motivation": "Provide an external validation of IoTDevID’s methodology on a richer, independent dataset to assess robustness and generalizability, and to understand the impact of data diversity (active/idle, multiple usage profiles) on model performance while adhering to transparent, reproducible practices.",
      "potential_research_ideas": [
        "Evaluate and harden the aggregation algorithm against adversarial traffic that spoofs MAC/IP to induce misaggregation (e.g., attack-aware aggregation or trust-weighted grouping).",
        "Cross-network generalization study: train on one network topology and evaluate on another to quantify network-dependence and develop domain adaptation methods.",
        "Leverage self-supervised or contrastive pretraining on large volumes of idle traffic to improve packet-level representations under data scarcity.",
        "Protocol-agnostic packet embeddings using sequence models over header fields and limited payload statistics to better handle non-IP devices.",
        "Multi-instance learning over packet bags to reduce reliance on MAC-based grouping while remaining robust to transfer problems.",
        "Active learning strategies to efficiently label underrepresented devices or states (non-IP, rare active behaviors).",
        "Comprehensive benchmark including Aalto, UNSW, and CIC-IoT-22 with standardized splits preventing data leakage and transfer issues."
      ],
      "architectural_improvement_recommendations": [
        "Replace or augment MAC-based aggregation with learned clustering of packet embeddings per temporal window, with exception handling for potential transfer scenarios.",
        "Incorporate domain adaptation (e.g., CORAL, adversarial domain alignment) across sessions/states to mitigate active/idle distribution shift.",
        "Introduce uncertainty-aware aggregation (e.g., confidence-weighted voting) and out-of-distribution detection for unseen devices or malicious spoofing.",
        "Re-run feature selection on CIC-IoT-22 with genetic algorithms or embedded methods and compare to the prior feature set for potential gains.",
        "Explore lightweight sequence models (e.g., tree ensembles with temporal features or small transformers over packet sequences) that remain protocol-agnostic."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/kahramankostas/IoTDevID-CIC",
      "frameworks": [
        "Python",
        "Scapy",
        "Wireshark"
      ],
      "reproducibility_score": "high",
      "computational_requirements": "Decision Tree training ~1.5s, testing ~0.17–0.19s per experiment; aggregation ~7.0–7.5s; CPU-based."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": "Testing ~0.17–0.19s per experiment; aggregation ~7.0–7.5s",
      "deployment_challenges": [
        "Aggregation may misgroup malicious spoofed traffic in non-benign environments.",
        "Transfer problem in networks where a single MAC/IP can represent multiple devices (e.g., gateway re-encapsulation).",
        "Limited data for non-IP devices and inconsistent device presence across sessions.",
        "Ensuring generalization across active and idle states and across different networks."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "External validation of IoTDevID’s feature set and aggregation algorithm on CIC-IoT-22.",
      "Quantified impact of data diversity: \"models trained with active usage data outperformed those trained with idle usage data, and multiple usage data similarly improved performance.\"",
      "Consistent gains from aggregation over individual packet predictions across all session conditions (e.g., AA F1 0.773 vs 0.702).",
      "Reported strong performance: \"92.50 F1 score for 31 IP-only device classes\" and \"78.80 F1 score for 40 device classes\" (non-IP).",
      "Provided transparent methodology with shared scripts and feature list (Python/Scapy/Wireshark; GitHub link).",
      "Analyzed CIC-IoT-22 data quality and highlighted session/device coverage limitations relevant for model training/testing."
    ]
  },
  {
    "arxiv_id": "2307.00501v2",
    "title": "Classifying World War II Era Ciphers with Machine Learning",
    "authors": "Brooke Dalton; Mark Stamp",
    "abstract": "We determine the accuracy with which machine learning and deep learning techniques can classify selected World War II era ciphers when only ciphertext is available. The specific ciphers considered are Enigma, M-209, Sigaba, Purple, and Typex. We experiment with three classic machine learning models, namely, Support Vector Machines (SVM), $k$-Nearest Neighbors ($k$-NN), and Random Forest (RF). We also experiment with four deep learning neural network-based models: Multi-Layer Perceptrons (MLP), Long Short-Term Memory (LSTM), Extreme Learning Machines (ELM), and Convolutional Neural Networks (CNN). Each model is trained on features consisting of histograms, digrams, and raw ciphertext letter sequences. Furthermore, the classification problem is considered under four distinct scenarios: Fixed plaintext with fixed keys, random plaintext with fixed keys, fixed plaintext with random keys, and random plaintext with random keys. Under the most realistic scenario, given 1000 characters per ciphertext, we are able to distinguish the ciphers with greater than 97% accuracy. In addition, we consider the accuracy of a subset of the learning techniques as a function of the length of the ciphertext messages. Somewhat surprisingly, our classic machine learning models perform at least as well as our deep learning models. We also find that ciphers that are more similar in design are somewhat more challenging to distinguish, but not as difficult as might be expected.",
    "published_date": "2023-07-02",
    "pdf_link": "https://arxiv.org/pdf/2307.00501v2",
    "paper_types": [
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Cryptanalysis",
      "subdomain": "Cipher Classification",
      "specific_problem": "Identify which WWII-era cipher machine (Enigma, M-209, Sigaba, Purple, Typex) produced a ciphertext, given ciphertext-only",
      "attack_types": [
        "ciphertext-only"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "SVM",
        "specific": "Linear, RBF, Polynomial, Sigmoid kernels",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Instance-based",
        "specific": "k-Nearest Neighbors (k-NN)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Ensemble/Tree-based",
        "specific": "Random Forest",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Feedforward Neural Network",
        "specific": "Multilayer Perceptron (MLP)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "RNN",
        "specific": "Long Short-Term Memory (LSTM)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "ELM",
        "specific": "Extreme Learning Machine (single hidden layer, random hidden weights)",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "CNN",
        "specific": "Convolutional Neural Network",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Multiclass Classification"
    ],
    "datasets": [
      {
        "name": "CT2-generated WWII cipher ciphertext dataset (Enigma, M-209, Sigaba, Purple, Typex) under four scenarios",
        "type": "synthetic",
        "domain": "ciphertext",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Brown Corpus (plaintext source for generating ciphertext)",
        "type": "public",
        "domain": "natural_language_corpus",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "Accuracy",
      "Precision",
      "Recall",
      "F1-score"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can machine learning and deep learning techniques classify selected World War II era cipher machines when only ciphertext is available?",
        "How does classification accuracy change across four scenarios: fixed plaintext/fixed keys, random plaintext/fixed keys, fixed plaintext/random keys, and random plaintext/random keys?",
        "How does the length of the ciphertext message affect classification accuracy?",
        "Are ciphers with more similar designs more challenging to distinguish?"
      ],
      "gaps_identified": [
        "Previous research has classified classic ciphers with ML, but, per the authors, no prior work focused on the WWII-era cipher machines (Enigma, M-209, Sigaba, Purple, Typex)."
      ],
      "limitations": [
        "\"Under the most realistic scenario, given 1000 characters per ciphertext, we are able to distinguish the ciphers with greater than 97% accuracy.\" (indicates long ciphertext is needed for highest accuracy)",
        "\"ciphers that are more similar in design are somewhat more challenging to distinguish\""
      ],
      "future_work": [],
      "motivation": "Although these WWII ciphers have been broken and modern cryptography is stronger, accurately classifying ciphertext from these machines is an interesting challenge that had not been specifically studied for these machines.",
      "potential_research_ideas": [
        "Evaluate modern sequence models (e.g., character-level Transformers) for ciphertext-only cipher classification and compare to classic ML and LSTM/CNN baselines.",
        "Adversarial or out-of-distribution detection to identify ciphertext from unseen cipher types or settings (open-set classification).",
        "Robustness analysis under channel noise, character errors, and mixed-language/plaintext domains to approximate real-world conditions.",
        "Few-shot and short-text classification: methods that perform well with very short ciphertexts (e.g., <200 characters), such as metric learning or prototypical networks on n-gram features.",
        "Multi-task learning to jointly classify cipher family and estimate properties (e.g., rotor count, stepping irregularity) to improve interpretability.",
        "Feature learning that fuses statistical n-gram features with sequence encoders via multi-branch architectures.",
        "Cross-corpus generalization: train on one plaintext source and test on others; domain adaptation to handle different language distributions.",
        "Automated cipher-similarity analysis to quantify design similarity and its impact on confusability; use this to guide curriculum training."
      ],
      "architectural_improvement_recommendations": [
        "Hybrid model combining engineered features (histograms, digrams, trigrams) via gradient boosting with a character-level Transformer encoder for raw sequences.",
        "Use lightweight 1D CNNs with dilated convolutions or Temporal Convolutional Networks to capture n-gram patterns efficiently.",
        "Incorporate positional or cycle-aware embeddings that reflect rotor stepping periodicities to help distinguish rotor machines.",
        "Calibrated probabilistic outputs and confidence estimation (e.g., temperature scaling) to support open-set rejection.",
        "Data augmentation: vary plaintext sources, include multilingual text, inject spacing/punctuation/noise to improve robustness.",
        "Meta-learning or contrastive learning on synthetic families of ciphers to enable rapid adaptation to unseen cipher variants."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [
        "scikit-learn"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "High accuracy reported with relatively long ciphertexts (about 1000 characters).",
        "Classification becomes more challenging when ciphers are similar in design."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Comprehensive empirical study of classic ML (SVM, k-NN, Random Forest) and deep learning (MLP, LSTM, ELM, CNN) methods to classify five WWII-era cipher machines from ciphertext-only.",
      "Evaluation across four key scenarios: fixed plaintext/fixed keys; random plaintext/fixed keys; fixed plaintext/random keys; random plaintext/random keys.",
      "Feature sets include letter histograms, digrams, and raw ciphertext character sequences.",
      "Quantifies effect of ciphertext length on accuracy.",
      "Key finding: \"Somewhat surprisingly, our classic machine learning models perform at least as well as our deep learning models.\"",
      "Result: \"Under the most realistic scenario, given 1000 characters per ciphertext, we are able to distinguish the ciphers with greater than 97% accuracy.\"",
      "Observation: \"ciphers that are more similar in design are somewhat more challenging to distinguish.\""
    ]
  },
  {
    "arxiv_id": "2307.08195v1",
    "title": "Covert Communication in Autoencoder Wireless Systems",
    "authors": "Ali Mohammadi Teshnizi; Majid Ghaderi; Dennis Goeckel",
    "abstract": "Hiding the wireless communication by transmitter Alice to intended receiver Bob from a capable and attentive adversary Willie has been widely studied under the moniker \"covert communications\". However, when such covert communication is done in the presence of allowable system communications, there has been little study of both hiding the signal and preserving the performance of those allowable communications. Here, by treating Alice, Bob, and Willie as a generator, decoder, and discriminator neural network, we perform joint training in an adversarial setting to yield a covert communication scheme that can be added to any normal autoencoder. The method does not depend on the characteristics of the cover signal or the type of channel and it is developed for both single-user and multi-user systems. Numerical results indicate that we are able to establish a reliable undetectable channel between Alice and Bob, regardless of the cover signal or type of fading, and that the signal causes almost no disturbance to the ongoing normal operation of the system.",
    "published_date": "2023-07-17",
    "pdf_link": "https://arxiv.org/pdf/2307.08195v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Wireless Security",
      "subdomain": "Physical-layer Security / Covert Communication",
      "specific_problem": "Embedding and decoding covert messages in autoencoder-based wireless systems while remaining undetectable and preserving normal communication performance",
      "attack_types": [
        "covert channel creation",
        "steganalysis evasion",
        "evasion of detector (Willie)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "GAN",
        "specific": null,
        "novel_contribution": "Input-agnostic generator (Alice) and discriminator (Willie) jointly trained with a decoder (Bob) in an adversarial setting to produce undetectable covert signals independent of cover signals and channel type"
      },
      {
        "type": "primary",
        "category": "Autoencoder",
        "specific": null,
        "novel_contribution": "Integration of a covert channel into any normal autoencoder-based wireless system with training that preserves the BLER of normal users"
      },
      {
        "type": "primary",
        "category": "Neural Receiver / Learned Equalization",
        "specific": null,
        "novel_contribution": "Blind channel estimation via a preliminary parameter estimation layer and transformation layer to equalize fading channels prior to decoding"
      }
    ],
    "learning_paradigm": [
      "Adversarial",
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Simulated wireless baseband signals over AWGN, Rayleigh, and Rician channels",
        "type": "synthetic",
        "domain": "wireless_baseband_signals",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [
      "BLER (Block Error Rate) for normal users",
      "Detection performance of Willie (e.g., probability of detection / detection accuracy)",
      "SNR (Signal-to-Noise Ratio) sweeps",
      "Communication rate / covert rate"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can a covert communication scheme be jointly trained with an autoencoder wireless system to remain undetectable while preserving the performance (e.g., BLER) of allowable communications?",
        "Can such a scheme be made independent of cover signal characteristics and channel models (AWGN, Rayleigh, Rician)?",
        "How to control the trade-off between covertness and communication performance in single-user and multi-user scenarios?"
      ],
      "gaps_identified": [
        "Prior practical covert communication methods often rely on external factors (hardware impairments, cooperative jammer/relay) or strong assumptions (e.g., knowledge of modulation, noise uncertainty) reducing generality",
        "Many works limit analysis to AWGN channels and neglect fading scenarios",
        "Impact of covert signaling on normal communication performance is often neglected",
        "Covert schemes that induce noticeable statistical deviations can be detected by steganalysis"
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Hide the very existence of communication in wireless systems while maintaining normal system operation; address the lack of joint consideration of covertness and preservation of allowable communications in autoencoder-based wireless systems.",
      "potential_research_ideas": [
        "Evaluate the proposed scheme over-the-air with real RF hardware (e.g., USRPs) including oscillator offsets, IQ imbalance, and non-linear PA effects to assess real-world covertness and reliability",
        "Extend to OFDM and time/frequency-selective MIMO channels with differentiable channel models and pilot structures; study mobility and time variation",
        "Harden against advanced detectors by training against ensembles of Willies including classical hypothesis tests and modern steganalysis/CNN/RNN-based detectors; measure AUC/ROC under distribution shift",
        "Provide formal covertness guarantees (e.g., bounds on total variation/KL divergence between with/without covert signaling) and relate to the square-root law under fading",
        "Design explicit keying/initialization protocols for the implicit shared secret formed via joint training; explore key rotation and resilience to model extraction",
        "Optimize for transmitter/receiver computational and energy constraints to enable on-device deployment; explore knowledge distillation or lightweight architectures",
        "Incorporate privacy-preserving or information-theoretic objectives (e.g., mutual information minimization with Willie's observations) in the loss"
      ],
      "architectural_improvement_recommendations": [
        "Adversarial training against an ensemble of discriminators (different architectures, input features such as time, frequency, cyclostationary statistics) to improve worst-case stealth",
        "Multi-objective loss with explicit divergence penalties (e.g., MMD, Wasserstein distance) between distributions with and without covert signaling to regularize statistical indistinguishability",
        "Differentiable OFDM and MIMO channel layers with pilot-aided or learned channel estimation blocks; incorporate temporal correlation models",
        "Lightweight generator/decoder via pruning/quantization and knowledge distillation for low-latency inference",
        "Curriculum training across SNRs and channel conditions to improve generalization under variable environments"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Input and Channel Independent GAN-based Covert Model: “We propose a novel covert communication approach using generative adversarial networks (GANs) that utilizes an input-agnostic generator and discriminator… independent of cover signals, waveforms, and modulation types… trained on three channel models (AWGN, Rayleigh Fading, and Rician Fading).”",
      "Controllable trade-off between covertness and performance via a regularized loss enabling flexible prioritization (covert vs. rate) in single- and multi-user systems",
      "Comprehensive experiments in single- and multi-user scenarios over AWGN/Rayleigh/Rician showing “our (8,1) covert model is demonstrated to have a negligible impact on the normal users’ BLER, while establishing a reliable covert communication link and consistently deceiving the detector at various SNRs.”",
      "Observation of a degree-of-freedom effect where increasing the number of users affects covert and normal performance in fading channels",
      "Method can be added to any normal autoencoder and does not depend on cover signal characteristics or channel type; claims applicability to conventional systems as well"
    ]
  },
  {
    "arxiv_id": "2307.10583v4",
    "title": "Deep fused flow and topology features for botnet detection basing on pretrained GCN",
    "authors": "Meng Xiaoyuan; Lang bo; Yanxi Liu; Yuhao Yan",
    "abstract": "Nowadays, botnets have become one of the major threats to cyber security. The characteristics of botnets are mainly reflected in bots network behavior and their intercommunication relationships. Existing botnet detection methods use flow features or topology features individually, which overlook the other type of feature. This affects model performance. In this paper, we propose a botnet detection model which uses graph convolutional network (GCN) to deeply fuse flow features and topology features for the first time. We construct communication graphs from network traffic and represent nodes with flow features. Due to the imbalance of existing public traffic flow datasets, it is impossible to train a GCN model on these datasets. Therefore, we use a balanced public communication graph dataset to pretrain a GCN model, thereby guaranteeing its capacity for identify topology features. We then feed the communication graph with flow features into the pretrained GCN. The output from the last hidden layer is treated as the fusion of flow and topology features. Additionally, by adjusting the number of layers in the GCN network, the model can effectively detect botnets under both C2 and P2P structures. Validated on the public ISCX2014 dataset, our approach achieves a remarkable recall rate 92.90% and F1-score 92.76% for C2 botnets, alongside recall rate 94.66% and F1-score of 92.35% for P2P botnets. These results not only demonstrate the effectiveness of our method, but also outperform the performance of the currently leading detection models.",
    "published_date": "2023-07-20",
    "pdf_link": "https://arxiv.org/pdf/2307.10583v4",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Botnet Detection",
      "specific_problem": "Node-level bot detection on communication graphs by fusing flow features with topological features via a pretrained GCN and classifying nodes",
      "attack_types": [
        "C2 botnets",
        "P2P botnets"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "GNN",
        "specific": "GCN (with residual connections)",
        "novel_contribution": "Uses a GCN to deeply fuse node flow features with learned topological features; GCN is pretrained on a balanced artificial communication-graph dataset and then frozen; number of GCN layers is tuned for C2 vs P2P detection."
      },
      {
        "type": "primary",
        "category": "Ensemble Trees",
        "specific": "Extra Trees (Extremely Randomized Trees)",
        "novel_contribution": "Final classifier trained on fused features from the last hidden layer of the pretrained GCN to improve performance on imbalanced real datasets."
      },
      {
        "type": "baseline",
        "category": "SVM",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "MLP",
        "specific": null,
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Transfer Learning"
    ],
    "datasets": [
      {
        "name": "ISCX 2014 Botnet Dataset",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "CTU-13",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Balanced artificial communication graph dataset (from Zhou et al. [17])",
        "type": "synthetic",
        "domain": "communication_graph",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "GCN trained on artificial balanced graph datasets (Zhou et al. [17])",
        "paper_reference": "[17]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "GIN with grouped reversible residual connections (Lo et al. [16])",
        "paper_reference": "[16]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Flow-based Random Forest (e.g., Udiyono et al. [34])",
        "paper_reference": "[34]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Naive Bayes on small-packet features (Kirubavathi et al. [11])",
        "paper_reference": "[11]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "DNN on TCP/UDP packet features (van Roosmalen et al. [33])",
        "paper_reference": "[33]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Decision tree feature selection + NN classifier (Alauthaman et al. [19])",
        "paper_reference": "[19]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Recall",
      "F1-score"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can a GCN be used to deeply fuse flow features and topological features to improve botnet detection?",
        "Can pretraining a GCN on a balanced communication-graph dataset mitigate the training difficulties caused by extreme class imbalance in real traffic datasets?",
        "Does adjusting the number of GCN layers improve detection for different botnet architectures (C2 vs P2P)?"
      ],
      "gaps_identified": [
        "Most existing methods use either flow features or topology features alone, missing complementary information.",
        "Topology feature engineering (e.g., degrees, PageRank) is time-consuming and does not scale well for real-time detection.",
        "Severe class imbalance in public botnet datasets makes training GCNs directly on real traffic impractical (e.g., CTU-13 has ~1:25000 malicious:benign).",
        "Artificially balanced graph datasets may not reflect characteristics of real traffic, degrading performance when deployed.",
        "Existing GNN-based methods generally do not use traffic flow features."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Fuse complementary flow and topology information for real-time botnet detection while overcoming class imbalance that hampers GCN training on real datasets.",
      "potential_research_ideas": [
        "Self-supervised or contrastive pretraining on large unlabeled real communication graphs to reduce reliance on artificial datasets.",
        "Domain adaptation or transfer learning techniques to bridge distribution shift between artificial pretraining graphs and real network traffic graphs.",
        "Temporal/dynamic GNNs that model evolving communication over sliding windows (e.g., TGN/TGAT) to capture bot behavior changes.",
        "Incorporate edge attributes (protocol, ports, bytes, direction, timing) and edge weights into GNN for richer topology-flow fusion.",
        "Adaptive layer-depth selection (e.g., Jumping Knowledge or layer-wise attention) to automatically tailor receptive field for C2 vs P2P.",
        "Imbalance-aware end-to-end training (focal loss, class-balanced loss, cost-sensitive learning) to reduce need for separate Extra Trees stage.",
        "Explainability modules (e.g., GNNExplainer) to highlight flows/neighbors driving bot classification and aid SOC triage.",
        "Adversarially robust training to resist flow-mimicry and topology perturbation attacks."
      ],
      "architectural_improvement_recommendations": [
        "Replace vanilla GCN with GraphSAGE or GAT for better neighborhood aggregation and handling of heterophily.",
        "Use Jumping Knowledge networks or residual/dense connections plus normalization to mitigate over-smoothing at higher layers.",
        "Introduce edge features and weighted adjacency; consider GATv2 or EGNN-style message functions with edge attributes.",
        "Add a learned gating/fusion module to combine raw flow features and topological embeddings before classification.",
        "Train end-to-end with an imbalance-aware loss; compare Extra Trees vs. a small MLP or calibrated linear head on fused embeddings.",
        "Incorporate temporal encodings and build a temporal GNN over time-sliced graphs.",
        "Apply post-hoc calibration (e.g., temperature scaling) for better decision thresholds in imbalanced settings."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Extreme class imbalance in real enterprise networks.",
        "Selection of sliding window size/stride affects detection and latency.",
        "Requirement to continuously construct communication graphs from high-volume traffic.",
        "Potential distribution shift between training/pretraining graphs and live traffic."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes a GCN-based method to deeply fuse flow features and topological features for node-level bot detection; defines simple, efficiently-computable five flow features and corresponding selection rules.",
      "Introduces a pretrained-GCN framework: pretrain GCN on a balanced artificial communication-graph dataset for topology representation, freeze it, and train an Extra Trees classifier on fused features from real (imbalanced) datasets.",
      "Demonstrates strong results on ISCX2014 and reports that the approach outperforms current state-of-the-art models; shows the number of GCN layers can be adjusted to effectively detect both C2 and P2P botnets."
    ]
  },
  {
    "arxiv_id": "2307.08309v3",
    "title": "LogPrécis: Unleashing Language Models for Automated Malicious Log Analysis",
    "authors": "Matteo Boffa; Rodolfo Vieira Valentim; Luca Vassio; Danilo Giordano; Idilio Drago; Marco Mellia; Zied Ben Houidi",
    "abstract": "The collection of security-related logs holds the key to understanding attack behaviors and diagnosing vulnerabilities. Still, their analysis remains a daunting challenge. Recently, Language Models (LMs) have demonstrated unmatched potential in understanding natural and programming languages. The question arises whether and how LMs could be also useful for security experts since their logs contain intrinsically confused and obfuscated information. In this paper, we systematically study how to benefit from the state-of-the-art in LM to automatically analyze text-like Unix shell attack logs. We present a thorough design methodology that leads to LogPrécis. It receives as input raw shell sessions and automatically identifies and assigns the attacker tactic to each portion of the session, i.e., unveiling the sequence of the attacker's goals. We demonstrate LogPrécis capability to support the analysis of two large datasets containing about 400,000 unique Unix shell attacks. LogPrécis reduces them into about 3,000 fingerprints, each grouping sessions with the same sequence of tactics. The abstraction it provides lets the analyst better understand attacks, identify fingerprints, detect novelty, link similar attacks, and track families and mutations. Overall, LogPrécis, released as open source, paves the way for better and more responsive defense against cyberattacks.",
    "published_date": "2023-07-17",
    "pdf_link": "https://arxiv.org/pdf/2307.08309v3",
    "paper_types": [
      "empirical_analysis",
      "benchmark",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Security Operations & Monitoring",
      "subdomain": "Log Analysis and Threat Hunting",
      "specific_problem": "Automated labeling of malicious Unix shell sessions with MITRE ATT&CK tactics and deriving attack fingerprints to support analysis at scale",
      "attack_types": [
        "Execution",
        "Persistence",
        "Discovery",
        "Impact",
        "Defense Evasion"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Transformer",
        "specific": null,
        "novel_contribution": "LogPrécis: a PLM-based token/statement classification pipeline that maps raw shell sessions to sequences of MITRE ATT&CK tactics (attack fingerprints) using few-shot fine-tuning (≈360 labeled sessions) and design-space exploration (chunking and pretraining strategies)."
      },
      {
        "type": "primary",
        "category": "Few-shot Fine-tuning",
        "specific": null,
        "novel_contribution": "Supervised few-shot fine-tuning of PLMs for MITRE tactic labeling on shell logs."
      },
      {
        "type": "primary",
        "category": "Domain Adaptation",
        "specific": null,
        "novel_contribution": "Unsupervised domain adaptation of generic PLMs to malicious shell log distribution prior to fine-tuning."
      },
      {
        "type": "baseline",
        "category": "Word2Vec/Classic NLP",
        "specific": "Word2Vec",
        "novel_contribution": "Used to illustrate limitations of non-contextual embeddings for tactic classification of shell logs."
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Self-supervised",
      "Few-shot"
    ],
    "datasets": [
      {
        "name": "Two honeypot Unix shell attack session datasets (2-year deployment; ~400k unique attacks)",
        "type": "proprietary",
        "domain": "log_files",
        "link": "https://github.com/SmartData-Polito/logprecis",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "LogPrécis labeled dataset for MITRE tactic tagging (~360 labeled sessions)",
        "type": "synthetic",
        "domain": "log_files",
        "link": "https://github.com/SmartData-Polito/logprecis",
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "LogPrécis trained models (HuggingFace)",
        "type": "public",
        "domain": "log_files",
        "link": "https://huggingface.co/SmartDataPolito",
        "is_new_contribution": true,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Word2Vec-based approach (non-contextual embeddings)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Static rules/blocklisting (conceptual comparison)",
        "paper_reference": null,
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can PLMs/LLMs pre-trained on natural language and code be successfully applied to malicious shell logs that are obfuscated and distribution-shifted?",
        "Does fine-tuning or domain adaptation on security logs improve performance or harm original PLM knowledge?",
        "Would training a specialized model from scratch on security logs match or exceed starting from pre-trained models?",
        "Which PLM size/type is best suited for this task (are very large models necessary)?",
        "How should performance be evaluated given the lack of universally accepted benchmarks/tasks for PLM-based log analysis?"
      ],
      "gaps_identified": [
        "No PLMs specifically pre-trained on security logs.",
        "Lack of standardized benchmarks and evaluation tasks for PLM-based log analysis.",
        "Static rules/blocklisting are brittle, expensive to maintain, and struggle with obfuscation/novelty.",
        "Classic NLP (e.g., Word2Vec) lacks contextualization needed for complex shell logs.",
        "Severe domain shift: malicious shell logs differ significantly from natural language and even programming languages (high OOV, randomization)."
      ],
      "limitations": [
        "Focus on Unix shell attack logs from Cowrie honeypots; generalization to other log types not empirically demonstrated.",
        "Fine-tuning relies on a relatively small labeled dataset (~360 sessions).",
        "Results and head-to-head metrics against alternative PLMs or large LLMs are not detailed in the provided text."
      ],
      "future_work": [
        "Extend LogPrécis methodology to other log sources (e.g., Windows event logs, application logs, cloud logs).",
        "Use the released labeled dataset as a benchmark to catalyze standardized evaluation.",
        "Investigate continual/domain-adaptive training with drift detection in operational settings."
      ],
      "motivation": "Automate the analysis of messy, obfuscated security logs by leveraging PLMs to extract attacker intent (MITRE tactics), aiding analysts in understanding, grouping, and tracking attacks at scale.",
      "potential_research_ideas": [
        "Pre-train or continue-pretrain a PLM on large-scale security log corpora to create a security-log-native LM.",
        "Build a comprehensive multi-source benchmark for tactic labeling across diverse log types with standardized splits and metrics.",
        "Incorporate CRF or structured prediction layers for sequence-consistent tactic labeling over tokens/statements.",
        "Hierarchical modeling of sessions (tokens→statements→session) to better capture context and ordering constraints.",
        "Active learning and weak supervision to efficiently expand labeled data from large unlabeled honeypot corpora.",
        "Adversarial data augmentation (obfuscations, randomizations) to improve robustness to attacker evasion.",
        "Knowledge-augmented models that retrieve MITRE ATT&CK knowledge or known TTP exemplars during inference.",
        "Online/continual learning with drift detection and safe model updates in production SIEM pipelines.",
        "Cross-modal fusion with network flow metadata, file artifacts, or sandbox outcomes to enrich tactic inference.",
        "Explainability tooling (attention/attribution over commands) to support analyst trust and validation."
      ],
      "architectural_improvement_recommendations": [
        "Add a CRF layer on top of the token classification head to enforce label consistency across statements.",
        "Adopt a hierarchical transformer that encodes statements first and then aggregates at the session level.",
        "Perform domain-adaptive pretraining on large unlabeled shell/honeypot logs before supervised fine-tuning.",
        "Leverage code-specialized PLMs (e.g., CodeBERT/CodeT5) and compare with general PLMs for shell-specific syntax.",
        "Use retrieval-augmented inference with a MITRE tactic exemplar bank to guide predictions on rare/novel commands.",
        "Introduce adversarial training and obfuscation-aware tokenization (e.g., byte-level or unigram tokenizers) to handle OOV/randomized tokens.",
        "Implement active learning loops with uncertainty sampling to prioritize labeling of ambiguous sessions."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": true,
      "code_link": "https://github.com/SmartData-Polito/logprecis",
      "frameworks": [
        "HuggingFace Transformers",
        "PyTorch"
      ],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Honeypot (Cowrie) environment; analysis of multi-year malicious shell sessions",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Domain shift between natural/code text and malicious shell logs; high OOV and randomization.",
        "Label scarcity for supervised fine-tuning; need for efficient annotation strategies.",
        "Model/size selection trade-offs vs. cost; large LLMs may be unnecessary.",
        "Generalization to other log formats and environments not yet validated.",
        "Operational drift and evolving attacker obfuscation tactics."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduce LogPrécis, a PLM-based system that labels shell session entities with MITRE ATT&CK tactics and derives attack fingerprints.",
      "Design-space exploration for malicious log analysis (chunking policies, pretraining strategies, PLM choices).",
      "Few-shot fine-tuning with only ~360 labeled sessions to achieve automated tactic labeling.",
      "Large-scale application to two multi-year honeypot datasets (~400k unique Unix shell attacks), reducing them to ~3,000 tactic-sequence fingerprints.",
      "Release of code, trained models, and labeled dataset to serve as a community benchmark.",
      "Empirical evidence and rationale that classical NLP (e.g., Word2Vec) is insufficient for contextual tactic classification of shell logs."
    ]
  },
  {
    "arxiv_id": "2307.01965v1",
    "title": "An analysis of scam baiting calls: Identifying and extracting scam stages and scripts",
    "authors": "Ian Wood; Michal Kepkowski; Leron Zinatullin; Travis Darnley; Mohamed Ali Kaafar",
    "abstract": "Phone scams remain a difficult problem to tackle due to the combination of protocol limitations, legal enforcement challenges and advances in technology enabling attackers to hide their identities and reduce costs. Scammers use social engineering techniques to manipulate victims into revealing their personal details, purchasing online vouchers or transferring funds, causing significant financial losses. This paper aims to establish a methodology with which to semi-automatically analyze scam calls and infer information about scammers, their scams and their strategies at scale. Obtaining data for the study of scam calls is challenging, as true scam victims do not in general record their conversations. Instead, we draw from the community of ``scam baiters'' on YouTube: individuals who interact knowingly with phone scammers and publicly publish their conversations. These can not be considered as true scam calls, however they do provide a valuable opportunity to study scammer scripts and techniques, as the scammers are unaware that they are not speaking to a true scam victim for the bulk of the call. We applied topic and time series modeling alongside emotion recognition to scammer utterances and found clear evidence of scripted scam progressions that matched our expectations from close reading. We identified social engineering techniques associated with identified script stages including the apparent use of emotion as a social engineering tool. Our analyses provide new insights into strategies used by scammers and presents an effective methodology to infer such at scale. This work serves as a first step in building a better understanding of phone scam techniques, forming the ground work for more effective detection and prevention mechanisms that draw on a deeper understanding of the phone scam phenomenon.",
    "published_date": "2023-07-05",
    "pdf_link": "https://arxiv.org/pdf/2307.01965v1",
    "paper_types": [
      "new_dataset",
      "empirical_analysis"
    ],
    "security_domain": {
      "primary": "Social Engineering Defense",
      "subdomain": "Vishing/Phone Scam Analysis",
      "specific_problem": "Semi-automatic identification of scam call types, stages, and scripts from scammer utterances in phone calls",
      "attack_types": [
        "social security number scams",
        "refund scams",
        "tech support scams",
        "reward/gift card scams",
        "family member impersonation scams",
        "tax scams",
        "charity scams",
        "impostor scams"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Topic Model",
        "specific": "Contextualized Topic Model (CTM)",
        "novel_contribution": "Applied to scammer utterances to uncover scam themes and provide inputs to temporal modeling of scam stages"
      },
      {
        "type": "primary",
        "category": "Probabilistic Graphical Model",
        "specific": "Hidden Markov Model (HMM)",
        "novel_contribution": "Modeling progression of scam stages over time from topic sequences; validated against manual annotations; used for live state inference"
      },
      {
        "type": "primary",
        "category": "Text Classification",
        "specific": null,
        "novel_contribution": "Early-turn supervised classification to recognize scam type with limited initial utterances"
      },
      {
        "type": "primary",
        "category": "Emotion Recognition",
        "specific": "Publicly available emotion detection model (unspecified)",
        "novel_contribution": "Emotion recognition over scammer utterances to link affect with social engineering techniques"
      },
      {
        "type": "baseline",
        "category": "Baseline/Heuristic",
        "specific": "Random model",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "Scam baiting phone call transcripts (YouTube) — 341 annotated conversations",
        "type": "public",
        "domain": "phone_call_transcripts",
        "link": null,
        "is_new_contribution": true,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Random model (state inference)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "“an increase of 45% accuracy over a random model” and “around 80% accuracy when we accept predictions that are one step ahead or behind.”",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Accuracy"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can scam call types be identified early in a call using only the first few scammer utterances?",
        "Do scam calls follow consistent, scripted stages that can be automatically inferred from conversation content?",
        "Which social engineering techniques and emotional cues are associated with different scam stages?"
      ],
      "gaps_identified": [
        "Scarcity of large-scale, content-rich datasets of real scam calls; victims rarely record conversations",
        "Existing datasets often limited to call metadata or very small samples of content",
        "Legal/regulatory constraints hinder large-scale call recording (telephony honeypots)",
        "Lack of scalable, automated analysis methods for understanding evolving scam scripts"
      ],
      "limitations": [
        "“These can not be considered as true scam calls” (scam baiters, not genuine victims)",
        "“cannot be considered comprehensive” due to data source and scope",
        "Primarily US-centric scam types; potential sampling bias",
        "ASR-based transcripts and manual cleaning may introduce transcription/annotation noise"
      ],
      "future_work": [
        "“first step toward future mitigation strategies and tools such as real time early detection and public education”",
        "Use insights to improve conversational AI ‘victim bots’ with better situational awareness",
        "Expand and update datasets to track evolving scam scripts"
      ],
      "motivation": "Establish a methodology to semi-automatically analyze scam calls at scale, infer scammers’ scripts and strategies, and enable more effective detection, prevention, and AI-based countermeasures.",
      "potential_research_ideas": [
        "Collect and curate a privacy-preserving dataset of real victim–scammer calls to validate and extend script/stage models",
        "Cross-lingual and multilingual modeling of scam scripts to capture global variations",
        "Early-warning detectors that trigger within the first few utterances using multimodal cues (text + prosody)",
        "Longitudinal tracking of script evolution and campaign drift using dynamic topic models",
        "Automatic discovery of new scam types using unsupervised clustering and novelty detection",
        "Human-in-the-loop systems to refine stage annotations and bootstrap semi-supervised learning",
        "Integrate call-graph and infrastructure signals (number reputation, CDR patterns) with content models for holistic detection"
      ],
      "architectural_improvement_recommendations": [
        "Replace or augment HMM with neural sequence models (BiLSTM/GRU/Transformer + CRF) for state inference",
        "Hierarchical modeling: utterance-level encoders feeding a conversation-level temporal model",
        "Use dynamic/temporal topic models or BERTopic with class-based TF-IDF for more stable, interpretable topics",
        "Multimodal fusion of lexical content with acoustic/prosodic features and emotion trajectories",
        "Online/streaming inference to support real-time stage tracking with uncertainty estimates (Bayesian HMM/HSMM)",
        "Semi-supervised training via weak labels from topic states to reduce annotation burden"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Data acquisition: real victims rarely record calls; legal/regulatory barriers to recording",
        "Evolving scam scripts require continuous updates and monitoring",
        "Telephony ecosystem limitations (legacy networks, caller ID spoofing) complicate end-to-end defenses",
        "Generalizing from scam-baiter interactions to real victims’ behavior",
        "Potential jurisdictional and privacy constraints for large-scale deployment"
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Collected and collated a dataset of 341 scam-baiter–scammer conversations annotated by scam type (≈90 hours).",
      "Developed a semi-automated framework combining contextualized topic modeling, HMM-based stage inference, and emotion recognition to identify scam scripts and persuasion techniques.",
      "Demonstrated early scam-type recognition: “identification of the scam type of a call is effective with just one utterance (80–90% accuracy) and highly accurate with 5 or 6 utterances (92–98%).”",
      "Constructed a live-call state inference model achieving “an increase of 45% accuracy over a random model” and “around 80% accuracy when we accept predictions that are one step ahead or behind.”",
      "Provided qualitative insights into scammers’ social engineering strategies and consistent scripted progressions."
    ]
  },
  {
    "arxiv_id": "2307.06932v2",
    "title": "PHOENI2X -- A European Cyber Resilience Framework With Artificial-Intelligence-Assisted Orchestration, Automation and Response Capabilities for Business Continuity and Recovery, Incident Response, and Information Exchange",
    "authors": "Konstantinos Fysarakis; Alexios Lekidis; Vasileios Mavroeidis; Konstantinos Lampropoulos; George Lyberopoulos; Ignasi Garcia-Milà Vidal; José Carles Terés i Casals; Eva Rodriguez Luna; Alejandro Antonio Moreno Sancho; Antonios Mavrelos; Marinos Tsantekidis; Sebastian Pape; Argyro Chatzopoulou; Christina Nanou; George Drivas; Vangelis Photiou; George Spanoudakis; Odysseas Koufopavlou",
    "abstract": "As digital technologies become more pervasive in society and the economy, cybersecurity incidents become more frequent and impactful. According to the NIS and NIS2 Directives, EU Member States and their Operators of Essential Services must establish a minimum baseline set of cybersecurity capabilities and engage in cross-border coordination and cooperation. However, this is only a small step towards European cyber resilience. In this landscape, preparedness, shared situational awareness, and coordinated incident response are essential for effective cyber crisis management and resilience. Motivated by the above, this paper presents PHOENI2X, an EU-funded project aiming to design, develop, and deliver a Cyber Resilience Framework providing Artificial-Intelligence-assisted orchestration, automation and response capabilities for business continuity and recovery, incident response, and information exchange, tailored to the needs of Operators of Essential Services and the EU Member State authorities entrusted with cybersecurity.",
    "published_date": "2023-07-13",
    "pdf_link": "https://arxiv.org/pdf/2307.06932v2",
    "paper_types": [
      "position"
    ],
    "security_domain": {
      "primary": "Security Operations",
      "subdomain": "Incident Response and Cyber Resilience",
      "specific_problem": "AI-assisted orchestration, automation, and response for business continuity, recovery, incident response, and threat information exchange among Operators of Essential Services (OES) and EU authorities",
      "attack_types": [
        "Advanced Persistent Threats (APTs)",
        "Ransomware",
        "Insider threats",
        "Social engineering/phishing",
        "Network intrusions",
        "General cyber-physical attacks on critical infrastructure"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "AutoML",
        "specific": null,
        "novel_contribution": "AutoML-based pipelines for UEBA and CTI discovery/analysis, including model selection and integration of explainability features"
      },
      {
        "type": "primary",
        "category": "Knowledge Graphs and Ontology-based Reasoning",
        "specific": "STIX 2.1 TAC ontology (extended)",
        "novel_contribution": "Integration of CTI knowledge graph linked to TIPs (MISP/OpenCTI) with reasoning for near real-time inference and threat hunting; plan to extend TAC ontology for end-user infrastructures"
      },
      {
        "type": "primary",
        "category": "NLP (Information Extraction and Sentiment Analysis)",
        "specific": "Use of tools like VADER, LIWC, SentiStrength (evaluated)",
        "novel_contribution": "Pipeline to extract CTI from surface/dark web, reports, social media; contextualization and sentiment analysis to assess relevance/actionability"
      },
      {
        "type": "primary",
        "category": "Anomaly Detection / UEBA",
        "specific": "Unsupervised ML; integration of human-behaviour persuasion signals",
        "novel_contribution": "UEBA enriched with human persuasion indicators from training/cyber range outputs to detect social-engineering-driven threats"
      },
      {
        "type": "primary",
        "category": "Predictive Modeling (Supervised and Unsupervised)",
        "specific": "Attack Categorisation Modelling (ATM) and Predictive Maintenance (PMEM)",
        "novel_contribution": "ATM to analyze system behavior and predict impacts; PMEM to propose proactive/response actions customized per sector/scenario"
      },
      {
        "type": "primary",
        "category": "Risk Modeling / Rule-based + ML",
        "specific": "DEXi (qualitative) plus new quantitative model",
        "novel_contribution": "Near real-time risk impact assessment engine combining qualitative DEXi rules with a new quantitative model for alert/response prioritization"
      },
      {
        "type": "baseline",
        "category": "RNN/LSTM",
        "specific": "LSTM",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "GAN",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Logistic Regression",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Transfer Learning",
        "specific": "Pre-trained language models for exploitability prediction (cited background)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Self-Organizing Maps",
        "specific": null,
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised",
      "Unsupervised",
      "Transfer Learning",
      "Deep Learning",
      "AutoML"
    ],
    "datasets": [
      {
        "name": "MITRE ATT&CK Framework",
        "type": "public",
        "domain": "threat_intelligence",
        "link": "https://attack.mitre.org",
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "STIX 2.1 Threat Actor Context (TAC) Ontology",
        "type": "public",
        "domain": "threat_intelligence_ontology",
        "link": "https://www.oasis-open.org/committees/tac",
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [],
    "performance_metrics_used": [],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [],
      "gaps_identified": [
        "CTI shared via TIPs is “too voluminous and complex to be actioned.”",
        "There is “limited technology enablement in threat triage and relevancy determination.”",
        "There are “trust-related issues” in current CTI sharing.",
        "“Qualities of shared threat data and TIP limitations” indicate the need for more context in shared data.",
        "Need to go beyond SotA by offering qualitative and quantitative business risk assessment with prioritized mitigation measures.",
        "UEBA lacks incorporation of human behaviour aspects (e.g., persuasion in social engineering).",
        "Challenge: contextualize heterogeneous CTI with infrastructure state and real-time alerts to score relevance/actionability."
      ],
      "limitations": [
        "Introduction of PHOENI2X into OES environments may increase the attack surface; thus a continuous evidence-based security assurance and certification solution is required.",
        "Several components are to be developed/integrated (e.g., quantitative risk model, ATM/PMEM, ontology extensions), implying current absence of empirical validation in the paper."
      ],
      "future_work": [
        "Develop and integrate a quantitative risk model alongside DEXi for near real-time risk assessment and prioritization.",
        "Design and implement Attack Categorisation Modelling (ATM) and Predictive Maintenance (PMEM) tools.",
        "Extend the STIX 2.1 TAC ontology to end-user infrastructures and use cases.",
        "Define and operationalize AutoML-based pipelines for CTI extraction, sentiment analysis, and contextualization.",
        "Adopt and extend OASIS CACAO for machine-processable Resilience Playbooks (RPs) and build the orchestration engine.",
        "Validate the PHOENI2X CRF in OES-focused use cases (energy, transport, healthcare)."
      ],
      "motivation": "Enhance European cyber resilience by providing AI-assisted situational awareness, orchestration/automation for business continuity and incident response, and standardized information exchange tailored to OES and national authorities within the NIS/NIS2 regulatory landscape.",
      "potential_research_ideas": [
        "Create a benchmark and labeled datasets for UEBA with social-engineering persuasion signals to evaluate human-factor-aware anomaly detection.",
        "Evaluate and compare different sentiment/contextualization methods for CTI actionability scoring; develop a standardized metric for CTI relevance and timeliness.",
        "Design privacy-preserving CTI sharing (e.g., federated learning over knowledge graphs, differential privacy for indicators) to address trust-related issues.",
        "Develop adversarially robust UEBA and CTI extractors resilient to data poisoning and evasion on surface/dark web sources.",
        "Use causal inference models to link observed alerts/CTI to predicted business impact, improving risk-based prioritization.",
        "Optimize Resilience Playbooks via reinforcement learning or bandit algorithms using cyber range feedback loops.",
        "Formal verification of CACAO playbooks and ROAR workflows to ensure safety and compliance before deployment.",
        "Graph neural networks over CTI knowledge graphs for threat hunting and attack path prediction."
      ],
      "architectural_improvement_recommendations": [
        "Introduce a streaming data fabric (e.g., Kafka + schema registry) with strict schema governance (STIX 2.1 profiles) to ensure scalable, low-latency CTI/context ingestion.",
        "Add active learning loops for the AutoML components (UEBA, CTI extraction) to leverage analyst feedback from SOC/CSIRT workflows.",
        "Integrate policy-as-code and formal verification for CACAO playbooks; enforce pre-deployment checks and runtime guards.",
        "Adopt a hybrid privacy layer (federated queries over knowledge graphs + DP noise for sensitive attributes) for cross-border CTI sharing.",
        "Employ graph neural networks on the CTI knowledge graph and asset graphs to improve attack path prediction and response recommendations.",
        "Implement red-team/blue-team simulation on the cyber range with automated RP evaluation metrics (MTTD/MTTR, business impact) to drive model/playbook updates."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "low",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "PHOENI2X Cyber Resilience Centres (CRCs) deployed at OES premises (energy, transport, healthcare) with supporting cloud platform for NFV/MEC orchestration",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Integration with diverse OES infrastructures and legacy SOC tooling (SIEM/EDR/NDR).",
        "Interoperability and standardization across CTI/TIP platforms (MISP, OpenCTI, STIX/TAXII).",
        "Ensuring trust, privacy, and legal compliance in cross-border information sharing under NIS2.",
        "Managing increased attack surface introduced by new components; need for continuous assurance/certification.",
        "Data quality, labeling scarcity, and drift in UEBA/CTI models; need for explainability and auditor trust.",
        "Operationalizing ROAR playbooks safely without disrupting critical services.",
        "Cloud/edge orchestration complexity (NFV/MEC) and resilience under crisis scenarios."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposal of the PHOENI2X Cyber Resilience Framework (CRF) and Cyber Resilience Centres (CRCs) for OES with AI-assisted situational awareness, ROAR, preparedness, and standardized information exchange.",
      "Integration blueprint combining baseline prevention/detection/response toolset (e.g., OpenVAS, Wazuh, OpenCTI, OSSEC, Suricata, Dionaea, GRR) with AI enablers.",
      "Adoption and planned extension of OASIS CACAO for machine-processable Resilience Playbooks and a purpose-built execution/orchestration engine.",
      "CTI knowledge graph approach aligned with STIX 2.1 TAC ontology, interoperable with MISP/OpenCTI, including NLP-based extraction and reasoning for near real-time threat hunting.",
      "Human-factor-aware UEBA via AutoML with explainability, incorporating persuasion indicators from social engineering training outputs.",
      "Attack Categorisation Modelling (ATM) and Predictive Maintenance (PMEM) concepts for proactive response and impact-aware recommendations.",
      "Near real-time risk assessment and prioritization engine combining qualitative (DEXi) and a new quantitative model for business risk impact.",
      "Continuous, evidence-based security assurance and certification for the CRCs to mitigate added attack surface.",
      "Preparedness via Resilience Cyber Range and Serious Games for assessment/training of business continuity, recovery, and IR processes.",
      "Cloud-based network infrastructure management and orchestration supporting NFV and MEC deployments."
    ]
  },
  {
    "arxiv_id": "2307.11079v3",
    "title": "3D-IDS: Doubly Disentangled Dynamic Intrusion Detection",
    "authors": "Chenyang Qiu; Yingsheng Geng; Junrui Lu; Kaida Chen; Shitong Zhu; Ya Su; Guoshun Nan; Can Zhang; Junsong Fu; Qimei Cui; Xiaofeng Tao",
    "abstract": "Network-based intrusion detection system (NIDS) monitors network traffic for malicious activities, forming the frontline defense against increasing attacks over information infrastructures. Although promising, our quantitative analysis shows that existing methods perform inconsistently in declaring various unknown attacks (e.g., 9% and 35% F1 respectively for two distinct unknown threats for an SVM-based method) or detecting diverse known attacks (e.g., 31% F1 for the Backdoor and 93% F1 for DDoS by a GCN-based state-of-the-art method), and reveals that the underlying cause is entangled distributions of flow features. This motivates us to propose 3D-IDS, a novel method that aims to tackle the above issues through two-step feature disentanglements and a dynamic graph diffusion scheme. Specifically, we first disentangle traffic features by a non-parameterized optimization based on mutual information, automatically differentiating tens and hundreds of complex features of various attacks. Such differentiated features will be fed into a memory model to generate representations, which are further disentangled to highlight the attack-specific features. Finally, we use a novel graph diffusion method that dynamically fuses the network topology for spatial-temporal aggregation in evolving data streams. By doing so, we can effectively identify various attacks in encrypted traffics, including unknown threats and known ones that are not easily detected. Experiments show the superiority of our 3D-IDS. We also demonstrate that our two-step feature disentanglements benefit the explainability of NIDS.",
    "published_date": "2023-07-02",
    "pdf_link": "https://arxiv.org/pdf/2307.11079v3",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Network-based intrusion detection on encrypted traffic with open/unknown attack detection via disentangled feature learning and dynamic graph diffusion",
      "attack_types": [
        "MITM",
        "DDoS",
        "DoS",
        "Backdoor",
        "Unknown threats"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Optimization/Information-theoretic",
        "specific": "Non-parametric optimization via SMT to minimize mutual information among feature elements",
        "novel_contribution": "Statistical disentanglement of traffic features using a constrained non-parametric optimization (mutual-information minimizing with SMT constraints) to automatically differentiate complex flow features"
      },
      {
        "type": "primary",
        "category": "GNN",
        "specific": "Multi-layer dynamic graph diffusion (inspired by GIND) with network-layer fusion",
        "novel_contribution": "Dynamic multi-layer graph diffusion that fuses network topology for spatial-temporal aggregation in evolving data streams, while preserving disentangled representations"
      },
      {
        "type": "primary",
        "category": "RNN",
        "specific": "GRU-based memory model",
        "novel_contribution": "Temporal memory that integrates statistically disentangled edge features to form node representations, followed by a representational disentanglement regularizer"
      },
      {
        "type": "primary",
        "category": "Regularization",
        "specific": "Orthogonality-style disentanglement loss",
        "novel_contribution": "Representational disentanglement via loss L_Dis that encourages element-wise near-orthogonality, highlighting attack-specific features"
      },
      {
        "type": "baseline",
        "category": "GCN",
        "specific": "E-GraphSAGE",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "SVM",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Logistic Regression",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Decision Tree",
        "specific": null,
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "CTC-ToN-IOT",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "SVM-based method",
        "paper_reference": "[31] (as cited in the paper)",
        "metric": "F1-score",
        "their_result": null,
        "baseline_result": "9% F1 for one unknown threat (MITM) and 35% F1 for another unknown threat on CTC-ToN-IOT"
      },
      {
        "method_name": "E-GraphSAGE",
        "paper_reference": "[46] (as cited in the paper)",
        "metric": "F1-score",
        "their_result": null,
        "baseline_result": "31% F1 for Backdoor and 93% F1 for DDoS"
      },
      {
        "method_name": "E-GraphSAGE",
        "paper_reference": "[46] (as cited in the paper)",
        "metric": "F1-score",
        "their_result": null,
        "baseline_result": "<20% F1 for MITM on CTC-ToN-IOT"
      }
    ],
    "performance_metrics_used": [
      "F1-score"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "\"How can an intrusion detection model automatically address the above issue, i.e., two entangled distributions, to benefit the detection of both unknown and known attacks\""
      ],
      "gaps_identified": [
        "Existing anomaly-based NIDS perform inconsistently across attack types due to entangled distributions of flow features and representations",
        "Lack of methods in NIDS to automatically disentangle tens to hundreds of complex traffic features without prior distributional knowledge",
        "Existing deep NIDS often use static or snapshot/time-windowed graphs, losing fine-grained continuous spatio-temporal information in evolving streams",
        "Prior disentanglement methods focus on object-level representation learning in CV/NLP and are not directly applicable to intrusion detection",
        "Correlation entanglement in learned representations is linked to poor detection for certain attacks (e.g., MITM)"
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Improve NIDS reliability across diverse known and unknown attacks by addressing entangled statistical and representational feature distributions, and by better leveraging evolving network topology.",
      "potential_research_ideas": [
        "Replace SMT-based non-parametric disentanglement with a differentiable mutual information minimization module (e.g., neural MI estimators) to enable end-to-end training",
        "Incorporate uncertainty estimation and open-set calibration (e.g., energy-based scores or EVT) for more principled unknown attack detection",
        "Develop online/continual learning with concept drift detection to adapt representations and diffusion parameters in non-stationary traffic",
        "Add contrastive/self-supervised pretraining on large unlabeled traffic to improve generalization to novel attacks",
        "Integrate cross-layer attention mechanisms to adaptively weight network layers during diffusion",
        "Combine the model with flow metadata enrichment (e.g., TLS handshake features, side-channel timing) under privacy constraints to improve encrypted traffic detection"
      ],
      "architectural_improvement_recommendations": [
        "Parameterize the statistical disentanglement with a lightweight neural module trained to minimize MI (InfoNCE/MINE) under constraints, removing external SMT solving",
        "Augment representational disentanglement with spectral decorrelation penalties and class-conditional contrastive losses",
        "Introduce adaptive graph diffusion coefficients conditioned on node/edge context via attention or gating",
        "Use memory-augmented transformers for temporal updates to capture longer dependencies than GRU without sacrificing efficiency via linear attention",
        "Incorporate uncertainty-aware output heads with temperature scaling and outlier exposure for open-set robustness"
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": []
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes 3D-IDS to mitigate entangled distributions of flow features for NIDS and quantitatively analyzes the cause of inconsistent performance",
      "Introduces a double-feature disentanglement scheme: statistical disentanglement via non-parametric MI-based optimization and representational disentanglement via regularization to highlight attack-specific features",
      "Presents a novel multi-layer dynamic graph diffusion method that fuses network topology for spatial-temporal aggregation in evolving streams, demonstrating superior performance and improved explainability on multiple benchmarks"
    ]
  },
  {
    "arxiv_id": "2307.03911v1",
    "title": "A Novel Pseudo-Random Number Generator Based on Multi-Objective Optimization for Image-Cryptographic Applications",
    "authors": "Takreem Haider; Saúl A. Blanco; Umar Hayat",
    "abstract": "Pseudo-random number generators (PRNGs) play an important role to ensure the security and confidentiality of image cryptographic algorithms. Their primary function is to generate a sequence of numbers that possesses unpredictability and randomness, which is crucial for the algorithms to work effectively and provide the desired level of security. However, traditional PRNGs frequently encounter limitations like insufficient randomness, predictability, and vulnerability to cryptanalysis attacks. To overcome these limitations, we propose a novel method namely an elliptic curve genetic algorithm (ECGA) for the construction of an image-dependent pseudo-random number generator (IDPRNG) that merges elliptic curves (ECs) and a multi-objective genetic algorithm (MOGA). The ECGA consists of two primary stages. First, we generate an EC-based initial sequence of random numbers using pixels of a plain-image and parameters of an EC, that depart from traditional methods of population initialization. In our proposed approach, the image itself serves as the seed for the initial population in the genetic algorithm optimization, taking into account the image-dependent nature of cryptographic applications. This allows the PRNG to adapt its behavior to the unique characteristics of the input image, leading to enhanced security and improved resistance against differential attacks. Furthermore, the use of a good initial population reduces the number of generations required by a genetic algorithm, which results in decreased computational cost. In the second stage, we use well-known operations of a genetic algorithm to optimize the generated sequence by maximizing a multi-objective fitness function that is based on both the information entropy and the period of the PRNG. By combining elliptic curves and genetic algorithms, we enhance the randomness and security of the ECGA.",
    "published_date": "2023-07-08",
    "pdf_link": "https://arxiv.org/pdf/2307.03911v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Cryptography",
      "subdomain": "Randomness Generation",
      "specific_problem": "Image-dependent pseudo-random number generator (PRNG) design for image cryptographic applications",
      "attack_types": [
        "differential attacks",
        "predictability attacks",
        "cryptanalysis (general)",
        "brute-force (resistance emphasized)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Evolutionary Algorithm",
        "specific": "Genetic Algorithm",
        "novel_contribution": "Image-dependent seeding using elliptic-curve-derived sequences and SHA-256 hashed parameters; multi-objective fitness maximizing entropy and period"
      },
      {
        "type": "primary",
        "category": "Multi-objective Optimization",
        "specific": "MOGA (multi-objective genetic algorithm)",
        "novel_contribution": "Fitness function jointly maximizes information entropy H(Ω) and period T(Ω) for PRNG sequences"
      },
      {
        "type": "primary",
        "category": "Elliptic Curve Arithmetic",
        "specific": null,
        "novel_contribution": "Elliptic curve-based initialization to generate candidate sequences used as GA population; image and EC parameters combined via bit-mixing with SHA-256 digests"
      }
    ],
    "learning_paradigm": [
      "Evolutionary Optimization",
      "Metaheuristic",
      "Multi-objective Optimization"
    ],
    "datasets": [
      {
        "name": "Benchmark images (unspecified)",
        "type": "public",
        "domain": "images",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Logistic-map based PRNG (improved 1D logistic map)",
        "paper_reference": "[21] Murillo et al.",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Chen chaotic system PRNG for images",
        "paper_reference": "[13] Hamza",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Controlled digital chaotic system PRNG",
        "paper_reference": "[35] Xia and Zheng",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Improved Henon map PRNG",
        "paper_reference": "[20] Meranza et al.",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Generalized Newton complex map PRNG",
        "paper_reference": "[6] Barani et al.",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Hyper-chaotic system PRNG",
        "paper_reference": "[39] Zhao et al.",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Logistic chaotic system PRNG",
        "paper_reference": "[33] Wang et al.",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Residue number system PRNG",
        "paper_reference": "[8] Gayoso et al.",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Neural-network-structured PRNG (Hopfield-like)",
        "paper_reference": "[9]",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Chaotic system + improved Hopfield NN PRNG",
        "paper_reference": "[36] Yu et al.",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Generalized conservative Sprott-A chaotic system PRNG",
        "paper_reference": "[7] Cang et al.",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Cascade fractal function PRNG",
        "paper_reference": "[3] Agarwal et al.",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Baker chaotic map PRNG",
        "paper_reference": "[27] Shi and Deng",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Complex polynomial chaotic maps PRNG",
        "paper_reference": "[38] Zang et al.",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Ordered elliptic curves PRNG",
        "paper_reference": "[14] Hayat and Azam",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Mordell elliptic curve PRNG",
        "paper_reference": "[32] Ullah et al.",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Isomorphic EC-based PRNG",
        "paper_reference": "[12] Haider et al.",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Large-prime EC PRNG (LSB of y-coordinate)",
        "paper_reference": "[2] Adhikari and Karforma",
        "metric": null,
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Information entropy H(Ω)",
      "Period T(Ω)",
      "NIST SP 800-22 test suite",
      "Uniformity"
    ],
    "claims_sota": true,
    "research_scope": {
      "research_questions": [
        "Can combining elliptic curve arithmetic with a multi-objective genetic algorithm produce a PRNG with improved randomness and security for image cryptography?",
        "Does using the input image to seed the GA population improve resistance to differential attacks in image-cryptographic applications?",
        "Can a fitness function that jointly maximizes information entropy and period yield higher-quality PRNS than existing PRNGs?"
      ],
      "gaps_identified": [
        "Traditional PRNGs exhibit insufficient randomness, predictability, and vulnerability to cryptanalysis attacks.",
        "Chaos-based PRNGs operate over real numbers leading to quantization/round-off issues in finite-field cryptographic settings, potentially creating irreversibility and decryption problems.",
        "Existing EC-based PRNGs face high time/space complexity and/or incompatibility with ECs over large primes.",
        "Prior EC-based approaches do not guarantee PRNs with security levels close to theoretically optimal values."
      ],
      "limitations": [],
      "future_work": [],
      "motivation": "Design a PRNG that achieves optimal randomness and improved cryptographic strength for image applications by leveraging elliptic curves for initialization and a multi-objective genetic algorithm to maximize entropy and period.",
      "potential_research_ideas": [
        "Evaluate the ECGA-based PRNG across a standardized and named image corpus and diverse modalities (e.g., medical, satellite) and report detailed NIST test pass rates and p-values.",
        "Incorporate advanced multi-objective evolutionary algorithms (e.g., NSGA-II/III, MOEA/D) and compare convergence and randomness quality versus the current GA.",
        "Add cryptographic salting and nonces (e.g., per-session salt) to the image-dependent seed to mitigate identical-image reuse and enable controlled re-keying.",
        "Formal security analysis under well-defined adversarial models (e.g., reduction-style arguments relating to ECDLP assumptions) beyond empirical tests.",
        "Hardware-oriented implementation (e.g., FPGA/ASIC) to measure throughput, latency, and energy for real-time image encryption pipelines.",
        "Extend to video streams with temporal-aware seeding and assess NPCR/UACI, correlation, and key sensitivity across frames.",
        "Explore alternative EC groups (e.g., Montgomery/Edwards curves) and fast scalar arithmetic to further reduce initialization cost.",
        "Hybridize with standardized DRBG constructions (e.g., Hash_DRBG/CTR_DRBG) by using ECGA as an entropy source reseeding mechanism."
      ],
      "architectural_improvement_recommendations": [
        "Replace basic GA with NSGA-II/III or MOEA/D and add elitism and crowding distance to preserve diverse high-entropy, long-period solutions.",
        "Adopt adaptive operators: self-adaptive mutation/crossover rates driven by on-line entropy/period gains to accelerate convergence.",
        "Use cryptographic hash-based key derivation (HKDF) over SHA-256 image digests with per-session salt and personalization strings for robust, non-repeating seeds.",
        "Evaluate different EC forms (Montgomery, Edwards) and precomputation (windowed methods) to speed up point generation for large primes.",
        "Introduce multi-source seeding (image + device TRNG + timestamp) with a mixing function (e.g., XOF like SHAKE) to improve unpredictability.",
        "Implement rigorous statistical testing pipeline (Dieharder, TestU01) alongside NIST SP 800-22 and report full p-value distributions."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Selection and validation of elliptic curve parameters and base points for large primes",
        "Tuning GA hyperparameters (population size, crossover/mutation rates, generations) versus computational budget",
        "Ensuring robust, non-repeating image-dependent seeds across deployments"
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Introduces ECGA: a novel image-dependent PRNG combining elliptic curves and a multi-objective genetic algorithm.",
      "Uses the input image and EC parameters to generate an EC-based initial population, reducing the number of GA generations and computational cost.",
      "Designs a multi-objective fitness function maximizing both information entropy and period of the PRNS.",
      "Claims enhanced security and resistance to differential attacks due to image-dependent seeding.",
      "Provides experimental evaluation on benchmark images using standard tests including the NIST test suite and reports superiority in uniformity, randomness, and cryptographic strength."
    ]
  },
  {
    "arxiv_id": "2307.14161v1",
    "title": "ICCPS: Impact discovery using causal inference for cyber attacks in CPSs",
    "authors": "Rajib Ranjan Maiti; Sridhar Adepu; Emil Lupu",
    "abstract": "We propose a new method to quantify the impact of cyber attacks in Cyber Physical Systems (CPSs). In particular, our method allows to identify the Design Parameter (DPs) affected due to a cyber attack launched on a different set of DPs in the same CPS. To achieve this, we adopt causal graphs to causally link DPs with each other and quantify the impact of one DP on another. Using SWaT, a real world testbed of a water treatment system, we demonstrate that causal graphs can be build in two ways: i) using domain knowledge of the control logic and the physical connectivity structure of the DPs, we call these causal domain graphs and ii) learning from operational data logs, we call these causal learnt graphs. We then compare these graphs when a same set of DPs is used. Our analysis shows a common set of edges between the causal domain graphs and the causal learnt graphs exists, which helps validate the causal learnt graphs. Additionally, we show that the learnt graphs can discover new causal relations, not initially considered in the domain graphs, that help significantly characterising the impact of the attack. We use causal domain graphs to estimate the parameters of the graphs, and the causal learnt graphs for causal inference. To learn the structure of the causal learnt graphs in all the six-stages of SWaT, we experiment with three learning algorithms: Peter Clarke (PC), Hill Climb (HC) search and Chow-Lie (CH). Finally, we demonstrate how causal graphs can be used to analyse the impact of cyber attacks by analysing nine well known cyber attacks on the SWaT test bed. We find that by using causal learnt graphs the DPs impacted by the attacks are correctly discovered with a probability greater than 0.9.",
    "published_date": "2023-07-26",
    "pdf_link": "https://arxiv.org/pdf/2307.14161v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Industrial Control Systems (ICS) Security",
      "subdomain": "Cyber-Physical Systems (CPS) Attack Impact Analysis",
      "specific_problem": "Discovering and quantifying impacted design parameters (DPs) in CPS after cyber attacks using causal inference",
      "attack_types": [
        "sensor spoofing",
        "actuator manipulation",
        "false data injection"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Causal Graphs / Structural Causal Models (Bayesian Networks)",
        "specific": null,
        "novel_contribution": "Formulate and use causal domain graphs (from control logic and physical coupling) and causal learnt graphs (from data) to perform impact discovery of cyber attacks in CPS"
      },
      {
        "type": "primary",
        "category": "Causal Discovery (Constraint-based)",
        "specific": "PC algorithm",
        "novel_contribution": "Used to learn causal structure (causal learnt graphs) across all six SWaT stages; then used for causal inference of attack impact"
      },
      {
        "type": "primary",
        "category": "Causal Discovery (Score-based)",
        "specific": "Hill Climb (HC) search",
        "novel_contribution": "Alternative structure learning to PC for causal learnt graphs; compared in terms of overlap with domain graphs and ability to discover impacts"
      },
      {
        "type": "primary",
        "category": "Causal Discovery (Tree-based approximation)",
        "specific": "Chow-Liu (CH) tree",
        "novel_contribution": "Explored as a third structure learning method for all six SWaT stages"
      },
      {
        "type": "primary",
        "category": "Causal Inference",
        "specific": null,
        "novel_contribution": "Algorithm that leverages parameters estimated from causal domain graphs and performs inference on causal learnt graphs to identify impacted DPs and quantify impact"
      },
      {
        "type": "primary",
        "category": "Data preprocessing",
        "specific": "Discretization of continuous variables",
        "novel_contribution": "Procedure to convert continuous sensor time series into discrete variables to enable structure learning"
      }
    ],
    "learning_paradigm": [
      "Unsupervised"
    ],
    "datasets": [
      {
        "name": "SWaT (Secure Water Treatment) historian data logs",
        "type": "public",
        "domain": "log_files (time series of sensors and actuators in ICS/CPS)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "SWaT cyber attack scenarios (nine well-known attacks on SWaT)",
        "type": "public",
        "domain": "log_files (attack time series in ICS/CPS)",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Causal domain graphs (domain-knowledge-based) versus causal learnt graphs (PC/HC)",
        "paper_reference": null,
        "metric": "Probability of correctly discovering impacted DPs for nine SWaT attacks",
        "their_result": "“we show that, in 7 out of the 9 attacks, the impacted DPs due to an attack on the targeted DPs can be determined with a probability higher than 0.90. Also, in 4 out of 9 attacks, domain knowledge cannot help to discover the impacted DPs, whereas the causal learnt graphs using PC and HC algorithms can determine their impacts.”",
        "baseline_result": "Domain knowledge could not determine impacted DPs in 4 out of 9 attacks"
      }
    ],
    "performance_metrics_used": [
      "probability of edge existence in causal domain graphs (edges > 0.95)",
      "probability of correctly discovering impacted DPs (> 0.90 in 7/9 attacks)",
      "overlap of edges between domain and learnt graphs",
      "qualitative discovery of new causal relations beyond domain graphs"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can we quantify the impact of a cyber attack in CPS by identifying the set of impacted DPs when an attack targets another set of DPs?",
        "Can causal graphs built from domain knowledge (control logic and physical coupling) and those learnt from data agree, and can learnt graphs reveal additional causal relations?",
        "Can causal inference on learnt graphs accurately discover impacted DPs for known cyber attacks on SWaT?"
      ],
      "gaps_identified": [
        "Existing approaches do not quantify which DPs are impacted when an attack targets other DPs in CPS.",
        "Correlation-based methods cannot capture non-linear or directional dependencies necessary for impact discovery.",
        "Public historian logs may omit DP functionality mappings and may include only a subset of DPs, complicating causal analysis."
      ],
      "limitations": [
        "Exact functionality of DPs in the historian logs is not revealed, only symbolic identifiers are available.",
        "Published datasets may contain only a subset of DPs in the CPS, which can limit completeness of causal discovery.",
        "Continuous sensor variables must be discretized to apply the chosen structure learning algorithms, which may introduce information loss."
      ],
      "future_work": [],
      "motivation": "Attacks on one DP can causally affect other DPs due to structural coupling and control dependencies; discovering and quantifying impacted DPs strengthens CPS security response and mitigation.",
      "potential_research_ideas": [
        "Extend to dynamic/temporal causal models (e.g., Dynamic Bayesian Networks or PCMCI) to capture time-lagged causal effects across stages.",
        "Incorporate counterfactual reasoning to simulate alternative attack scenarios and quantify potential mitigations.",
        "Calibrate and compare continuous-variable causal discovery (e.g., NOTEARS, GES with continuous scores) to avoid discretization.",
        "Combine expert domain constraints with structure learning via constrained or hybrid causal discovery to reduce spurious edges.",
        "Evaluate and generalize across multiple CPS testbeds (e.g., gas pipeline, power grid) to assess transferability.",
        "Integrate with online anomaly detection to provide real-time impact prediction and prioritization of remediation.",
        "Robustness analysis under adversarial manipulation of historian logs (evasion/poisoning) and develop robust causal discovery.",
        "Uncertainty quantification on discovered impacts (e.g., bootstrap CIs for edge existence and effect sizes)."
      ],
      "architectural_improvement_recommendations": [
        "Adopt time-lagged edges and temporal windows in the causal graph to reflect process dynamics across SWaT stages.",
        "Use continuous scoring functions and mixed-variable causal discovery to avoid discretization artifacts.",
        "Apply structure learning with prior knowledge constraints (whitelists/blacklists) derived from control logic and physical topology.",
        "Leverage ensemble causal discovery (aggregate PC, HC, CH results) with stability selection to increase reliability of edges.",
        "Incorporate interventional data (safe experiments) on SWaT to validate and orient ambiguous edges (equivalence classes).",
        "Quantify causal effect sizes via structural equations fitted with regularization and report uncertainty bounds."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "ICS water treatment testbed (SWaT), six-stage CPS",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Limited access to full set of DPs and lack of DP functionality mappings in shared logs",
        "Need for time synchronization and data quality in historian logs",
        "Choice of discretization thresholds impacts structure learning",
        "Computational complexity and equivalence class ambiguity in causal structure learning",
        "Non-stationarity and process changes across CPS operating conditions"
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Adopt causal dependency to build causal domain graphs from control logic and physical coupling; parameters estimated show edge probabilities generally > 0.95.",
      "Convert continuous historian variables into discrete variables and learn causal structures (PC, HC, CH) across all six SWaT stages; identify significant overlap with domain graphs and discover new relations not in domain graphs.",
      "Develop an inference algorithm that uses parameters from causal domain graphs and performs causal inference on learnt graphs to discover attack impacts.",
      "Evaluate on nine well-known SWaT cyber attacks; impacted DPs discovered with probability > 0.90 in 7/9 attacks; in 4/9 attacks, domain knowledge could not discover impacts but learnt graphs (PC/HC) could."
    ]
  },
  {
    "arxiv_id": "2307.11032v1",
    "title": "A Natural Language Processing Approach to Malware Classification",
    "authors": "Ritik Mehta; Olha Jurečková; Mark Stamp",
    "abstract": "Many different machine learning and deep learning techniques have been successfully employed for malware detection and classification. Examples of popular learning techniques in the malware domain include Hidden Markov Models (HMM), Random Forests (RF), Convolutional Neural Networks (CNN), Support Vector Machines (SVM), and Recurrent Neural Networks (RNN) such as Long Short-Term Memory (LSTM) networks. In this research, we consider a hybrid architecture, where HMMs are trained on opcode sequences, and the resulting hidden states of these trained HMMs are used as feature vectors in various classifiers. In this context, extracting the HMM hidden state sequences can be viewed as a form of feature engineering that is somewhat analogous to techniques that are commonly employed in Natural Language Processing (NLP). We find that this NLP-based approach outperforms other popular techniques on a challenging malware dataset, with an HMM-Random Forrest model yielding the best results.",
    "published_date": "2023-07-07",
    "pdf_link": "https://arxiv.org/pdf/2307.11032v1",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Malware Analysis",
      "subdomain": "Malware Classification",
      "specific_problem": "Static malware family classification from opcode sequences using HMM hidden-state features with a Random Forest classifier",
      "attack_types": [
        "ZeroAccess",
        "Winwebsec",
        "SecurityShield",
        "Zbot",
        "Cridex",
        "SmartHDD",
        "Harebot"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Hidden Markov Model",
        "specific": null,
        "novel_contribution": "Train one HMM per malware family on opcode sequences and extract hidden state sequences; use these state sequences as engineered features (NLP-inspired) for downstream classification"
      },
      {
        "type": "primary",
        "category": "Random Forest",
        "specific": null,
        "novel_contribution": "RF trained on concatenated, scaled hidden-state sequences from class-specific HMMs (feature length = 7×L); hyperparameter grid search to optimize performance"
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Malicia dataset",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "MalImg",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "Microsoft malware dataset (as referenced in paper)",
        "type": "public",
        "domain": "malware_binaries",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "HMM-RF (this paper)",
        "paper_reference": null,
        "metric": "accuracy",
        "their_result": "0.9758 accuracy (80/20 split on Malicia subset)",
        "baseline_result": null
      },
      {
        "method_name": "Random Forest on grayscale malware images",
        "paper_reference": "Garcia and Muga II [14]",
        "metric": "accuracy",
        "their_result": null,
        "baseline_result": "0.9562 accuracy (dataset in [14])"
      },
      {
        "method_name": "SVM on malware features",
        "paper_reference": "Kruczkowski et al. [19]",
        "metric": "accuracy (cross-validation); F1-score",
        "their_result": null,
        "baseline_result": "0.9398 accuracy (CV); 0.9552 F1-score"
      },
      {
        "method_name": "LSTM with Word2Vec opcode embeddings",
        "paper_reference": "R. Lu [21]",
        "metric": "AUC",
        "their_result": null,
        "baseline_result": "0.987 AUC on a dataset of 969 malware and 123 benign files"
      },
      {
        "method_name": "M-CNN (VGG-16-based) on malware images",
        "paper_reference": "Kalash et al. [18]",
        "metric": "accuracy",
        "their_result": null,
        "baseline_result": "0.9852 on MalImg; 0.9997 on a Microsoft dataset"
      }
    ],
    "performance_metrics_used": [
      "accuracy",
      "confusion matrix"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can HMM hidden-state sequences, used as NLP-style engineered features, improve static malware family classification performance?",
        "What HMM and RF hyperparameters (e.g., number of hidden states N, sequence length L, RF settings) yield the best accuracy on Malicia?",
        "How does performance vary across highly imbalanced malware families?"
      ],
      "gaps_identified": [
        "Signature-based AV methods are limited to known malware and vulnerable to obfuscation.",
        "Prior ML work in malware often uses opcode n-grams, direct HMM scoring, or image/CNN approaches; using HMM hidden-state sequences as features analogous to NLP has not been previously employed in the malware domain (as stated by the authors).",
        "Static-feature methods are efficient but may be less robust than dynamic-feature approaches; balancing efficiency and robustness remains a gap."
      ],
      "limitations": [
        "Only static features (opcode sequences) are used; no dynamic analysis.",
        "Evaluation limited to a subset of Malicia (7 families with at least 50 samples); class imbalance persists.",
        "Samples with fewer than L opcodes were dropped.",
        "Training cost is significant: average of ~5 hours to train each HMM; 7 HMMs total.",
        "Performance on small classes (e.g., Cridex, Harebot) is weaker, with minimal effect on overall accuracy due to imbalance.",
        "Truncation to the first L opcodes may omit informative content for some samples."
      ],
      "future_work": [],
      "motivation": "Improve malware family classification by leveraging an NLP-inspired feature engineering step: extract HMM hidden-state sequences from opcode streams and feed them to a classifier to outperform popular techniques on a challenging dataset.",
      "potential_research_ideas": [
        "Augment hidden-state features with dynamic features (e.g., API call sequences) for a hybrid static-dynamic model.",
        "Pretrain opcode embeddings (e.g., Word2Vec/skip-gram or Transformer tokenizers) and concatenate with HMM state features for richer representations.",
        "Replace discrete HMMs with continuous-emission HMMs, GMM-HMMs, or HSMMs, and compare feature quality.",
        "Learn class-agnostic HMMs (shared or hierarchical HMMs) to reduce per-class training cost and possibly improve generalization to rare families.",
        "Use sequence models (BiLSTM/Transformer) on the hidden-state sequences to capture higher-order dependencies before classification.",
        "Apply cost-sensitive learning or focal loss adaptations (via RF class weights or alternative classifiers) to improve minority-family performance.",
        "Adopt semi-supervised/self-supervised pretraining on large unlabeled opcode corpora and fine-tune with limited labels.",
        "Evaluate adversarial robustness to opcode-level perturbations (instruction substitution, dead-code insertion) and develop defenses."
      ],
      "architectural_improvement_recommendations": [
        "Incorporate class weighting or balanced subsampling in RF to mitigate class imbalance.",
        "Jointly use HMM log-likelihood scores and hidden-state sequences as concatenated features to capture both generative fitness and latent structure.",
        "Switch to GMM-HMM or HSMM to better model opcode emission distributions and durations; compare to discrete HMM features.",
        "Add learned opcode embeddings and positional encodings; then stack a lightweight sequence encoder (e.g., 1D-CNN or BiLSTM) on top of hidden-state sequences.",
        "Learn a shared HMM (or a small set of HMMs) and a class-specific attention mechanism to reduce training time while maintaining discriminability.",
        "Perform automated model selection for L and N (e.g., Bayesian optimization) rather than fixed grid search.",
        "Calibrate the RF outputs (e.g., Platt scaling or isotonic regression) for better decision thresholds in deployment."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": "Average ~5 hours to train each HMM (7 total); HMM with N=20 hidden states, M=426 unique opcodes; average 10.43 Baum-Welch iterations; best L=50; RF grid search over n_estimators, criterion, and max_features."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": null,
      "scalability_discussed": false,
      "inference_time": null,
      "deployment_challenges": [
        "Strong class imbalance leads to weaker performance on small families.",
        "Preprocessing requires reliable opcode extraction and sequence truncation decisions (choice of L).",
        "Training cost for multiple per-class HMMs is high; scaling to many families increases cost."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Proposes an NLP-inspired hybrid architecture: train class-specific HMMs on opcode sequences and use their hidden-state sequences as engineered features.",
      "Introduces HMM-RF pipeline that concatenates per-class hidden-state sequences (length L) and trains a Random Forest on the scaled vectors.",
      "Empirical study on the Malicia dataset subset (7 families) with 80/20 split; best accuracy reported: 0.9758.",
      "Hyperparameter exploration for HMM (N) and RF (n_estimators, criterion, max_features), and sequence length L; best observed with N=20 and L=50.",
      "Provides confusion-matrix analysis; notes strong performance on large classes and weaker results on very small classes."
    ]
  },
  {
    "arxiv_id": "2307.10252v1",
    "title": "A Machine Learning based Empirical Evaluation of Cyber Threat Actors High Level Attack Patterns over Low level Attack Patterns in Attributing Attacks",
    "authors": "Umara Noor; Sawera Shahid; Rimsha Kanwal; Zahid Rashid",
    "abstract": "Cyber threat attribution is the process of identifying the actor of an attack incident in cyberspace. An accurate and timely threat attribution plays an important role in deterring future attacks by applying appropriate and timely defense mechanisms. Manual analysis of attack patterns gathered by honeypot deployments, intrusion detection systems, firewalls, and via trace-back procedures is still the preferred method of security analysts for cyber threat attribution. Such attack patterns are low-level Indicators of Compromise (IOC). They represent Tactics, Techniques, Procedures (TTP), and software tools used by the adversaries in their campaigns. The adversaries rarely re-use them. They can also be manipulated, resulting in false and unfair attribution. To empirically evaluate and compare the effectiveness of both kinds of IOC, there are two problems that need to be addressed. The first problem is that in recent research works, the ineffectiveness of low-level IOC for cyber threat attribution has been discussed intuitively. An empirical evaluation for the measure of the effectiveness of low-level IOC based on a real-world dataset is missing. The second problem is that the available dataset for high-level IOC has a single instance for each predictive class label that cannot be used directly for training machine learning models. To address these problems in this research work, we empirically evaluate the effectiveness of low-level IOC based on a real-world dataset that is specifically built for comparative analysis with high-level IOC. The experimental results show that the high-level IOC trained models effectively attribute cyberattacks with an accuracy of 95% as compared to the low-level IOC trained models where accuracy is 40%.",
    "published_date": "2023-07-17",
    "pdf_link": "https://arxiv.org/pdf/2307.10252v1",
    "paper_types": [
      "empirical_analysis",
      "new_dataset"
    ],
    "security_domain": {
      "primary": "Threat Intelligence",
      "subdomain": "Cyber Threat Attribution",
      "specific_problem": "Attributing cyber attacks to threat actors using Indicators of Compromise (IOCs), comparing high-level (TTP/attack pattern) IOCs vs low-level (hashes, IPs, domains) IOCs",
      "attack_types": [
        "Advanced Persistent Threat (APT) campaigns",
        "Cyber espionage",
        "Financial cybercrime",
        "Data breaches"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "Random Forest",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Naive Bayes",
        "specific": "Kernel Naive Bayes",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Naive Bayes",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Decision Tree",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "KNN",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Neural Network (ANN/MLP)",
        "specific": "ANN",
        "novel_contribution": "Identified as the most effective among tested algorithms for attribution in this study"
      },
      {
        "type": "primary",
        "category": "Deep Neural Network",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Gradient Boosted Trees",
        "specific": null,
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Generalized Linear Model",
        "specific": "Logistic Regression",
        "novel_contribution": null
      },
      {
        "type": "primary",
        "category": "Ensemble",
        "specific": "Ensemble Learning Models",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "Low-level IOC Dataset for Cyber Threat Attribution",
        "type": "public",
        "domain": "threat_intel_iocs (file_hashes, ip_addresses, domains) with threat actor labels",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "High-level IOC Multi-instance Dataset (MITRE ATT&CK-based attack patterns)",
        "type": "public",
        "domain": "attack_patterns/TTPs mapped to threat actors",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Low-level IOC trained models (various classifiers)",
        "paper_reference": null,
        "metric": "Accuracy",
        "their_result": "95%",
        "baseline_result": "40%"
      },
      {
        "method_name": "LSI-based high-level IOC attribution (Noor et al.)",
        "paper_reference": "[8]",
        "metric": "Accuracy",
        "their_result": null,
        "baseline_result": "94%"
      },
      {
        "method_name": "Fuzzy pattern tree / multi-modal fuzzy for malware attribution (Haddadpajouh et al.)",
        "paper_reference": "[31]",
        "metric": "Accuracy",
        "their_result": null,
        "baseline_result": "95.2%"
      },
      {
        "method_name": "SIMVER (Word2Vec) + Neural Networks on CTI reports (Naveen et al.)",
        "paper_reference": "[33]",
        "metric": "Accuracy",
        "their_result": null,
        "baseline_result": "86.5%"
      },
      {
        "method_name": "Naive Bayes posterior for attribution (Sentuna et al.)",
        "paper_reference": "[35]",
        "metric": "Accuracy",
        "their_result": null,
        "baseline_result": "95%"
      }
    ],
    "performance_metrics_used": [
      "Accuracy"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "How effective are low-level IOCs for cyber threat attribution when evaluated empirically on a real-world dataset?",
        "Do high-level IOCs (TTP/attack patterns) enable more accurate attribution than low-level IOCs, and by how much?"
      ],
      "gaps_identified": [
        "Lack of empirical evaluation of low-level IOC effectiveness for attribution on a real-world dataset.",
        "Existing high-level IOC datasets had a single instance per class, unsuitable for training machine learning models.",
        "Low-level IOCs (file hashes, IPs, domains) are short-lived, easily manipulated/spoofed, and rarely reused, leading to unfair or false attribution."
      ],
      "limitations": [
        "Scalability and feasibility limitations are highlighted by the authors; technological improvements are suggested (details in Section 6, not provided in excerpt).",
        "Manual extraction of low-level IOCs from CTI documents was required to carefully construct the dataset."
      ],
      "future_work": [
        "Improve scalability and feasibility of the approach with technological enhancements.",
        "Extend datasets and continue comparative evaluation; apply the approach to additional real-world incidents."
      ],
      "motivation": "Provide an empirical, machine-learning-based comparison of high-level vs low-level IOCs for cyber threat attribution and construct datasets enabling such evaluation.",
      "potential_research_ideas": [
        "Automate extraction and normalization of both low-level and high-level IOCs from CTI reports using transformer-based NLP to reduce manual effort and improve scalability.",
        "Model temporal evolution of actor TTPs to handle concept drift and campaign phases (e.g., sequence models over ATT&CK techniques).",
        "Multi-modal fusion of CTI-derived high-level IOCs with operational telemetry (network flow, EDR logs) for stronger attribution signals.",
        "Open-set and few-shot attribution to handle emerging or previously unseen actors; incorporate uncertainty quantification.",
        "Adversarial evaluation of attribution models under deceptive IOCs (poisoned/false flags) and development of robustness defenses.",
        "Graph-based actor-TTP-technique co-occurrence modeling (GNNs over ATT&CK graphs) for explainable attribution.",
        "Self-supervised pretraining on large CTI corpora (reports, STIX/TAXII feeds) to learn domain-specific representations of TTPs and entities."
      ],
      "architectural_improvement_recommendations": [
        "Use a TTP-sequence encoder (Transformer) over ATT&CK technique sequences, with hierarchical pooling at tactic->technique->sub-technique levels.",
        "Construct a heterogeneous graph (actors, techniques, tools, campaigns, indicators) and apply graph neural networks with meta-path attention for attribution.",
        "Implement contrastive learning between CTI text spans and structured ATT&CK nodes to align unstructured and structured indicators.",
        "Calibrate classifier outputs and add open-set detection (e.g., energy-based or ODIN) to avoid overconfident misattribution.",
        "Employ data augmentation for TTP permutations and synonym mapping; integrate MITRE mappings to normalize technique aliases across reports.",
        "Build a weakly supervised pipeline leveraging STIX relationships to expand labeled instances with noise-robust training (e.g., co-teaching)."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": null,
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Low-level IOCs are volatile and easily manipulated (spoofed IPs, changed domains/hashes), leading to unreliable attribution.",
        "Manual IOC extraction from CTI documents impacts scalability.",
        "Dataset construction and coverage (actors, campaigns) require continuous curation from diverse sources."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Constructed a new low-level IOC training dataset for attribution by associating adversaries with file hashes, IP addresses, and domains from publicly available CTI reports.",
      "Built a multi-instance high-level IOC training dataset from MITRE ATT&CK adversary attack pattern taxonomy suitable for ML training.",
      "Applied statistical techniques to analyze characteristics of both datasets.",
      "Performed empirical comparative analysis of ML models on both datasets, showing high-level IOC models achieve 95% accuracy vs 40% for low-level IOC models.",
      "Applied the high-level IOC model to attribute a recent Red Cross data breach incident with unknown actor."
    ]
  },
  {
    "arxiv_id": "2307.04561v1",
    "title": "Performance comparison of timing-based anomaly detectors for Controller Area Network: a reproducible study",
    "authors": "Francesco Pollicino; Dario Stabili; Mirco Marchetti",
    "abstract": "This work presents an experimental evaluation of the detection performance of eight different algorithms for anomaly detection on the Controller Area Network (CAN) bus of modern vehicles based on the analysis of the timing or frequency of CAN messages. This work solves the current limitations of related scientific literature, that is based on private dataset, lacks of open implementations, and detailed description of the detection algorithms. These drawback prevent the reproducibility of published results, and makes it impossible to compare a novel proposal against related work, thus hindering the advancement of science. This paper solves these issues by publicly releasing implementations, labeled datasets and by describing an unbiased experimental comparisons.",
    "published_date": "2023-07-10",
    "pdf_link": "https://arxiv.org/pdf/2307.04561v1",
    "paper_types": [
      "benchmark",
      "reproducibility",
      "empirical_analysis",
      "new_dataset"
    ],
    "security_domain": {
      "primary": "Automotive Security",
      "subdomain": "In-vehicle Networks / CAN Bus Intrusion Detection",
      "specific_problem": "Timing- and frequency-based anomaly detection on the CAN bus with a reproducible, unbiased comparison",
      "attack_types": [
        "Message injection (fabrication)",
        "Denial of Service (DoS)",
        "Fuzzing",
        "Message removal / ECU shutdown / ECU inhibition",
        "Spoofing"
      ]
    },
    "ml_techniques": [
      {
        "type": "baseline",
        "category": "Statistical / Rule-based (timing)",
        "specific": "Delayed-decision cycle detection (thresholding on cycle time)",
        "novel_contribution": "Reference reimplementation of Otsuka14 with clarified assumptions/parameters"
      },
      {
        "type": "baseline",
        "category": "Statistical test over time-series",
        "specific": "Sliding-window t-test with logarithmic summed score",
        "novel_contribution": "Reference reimplementation of Taylor15; unified parameterization and evaluation"
      },
      {
        "type": "baseline",
        "category": "Time-series modeling",
        "specific": "RLS (Recursive Least Squares) clock-skew baseline + CUSUM with adaptive threshold",
        "novel_contribution": "Reference reimplementation of Cho16 (per-message variant) due to absent DBC mapping"
      },
      {
        "type": "baseline",
        "category": "Rule-based (frequency thresholding)",
        "specific": "Δt < 0.5 * cycle time heuristic",
        "novel_contribution": "Reference reimplementation of Gmiden16 (originally theoretical) for empirical comparison"
      },
      {
        "type": "baseline",
        "category": "Rule-based (timing thresholds)",
        "specific": "Δt < 0.5 * ct (injection) and Δt < 0.2 ms counter with threshold (DoS)",
        "novel_contribution": "Reference reimplementation of Song16 with unified evaluation"
      },
      {
        "type": "baseline",
        "category": "Rule-based (timing deviation with learned margin)",
        "specific": "Threshold on |Δt - ct| with margin m; three consecutive violations",
        "novel_contribution": "Reference reimplementation of Moore17"
      },
      {
        "type": "baseline",
        "category": "Rule-based (missing-message detection)",
        "specific": "Timeout-based ct × k per-ID missing message detection",
        "novel_contribution": "Reference reimplementation of Stabili19"
      },
      {
        "type": "baseline",
        "category": "Specification-based / Supervised threshold learning",
        "specific": "SAIDu-CANT: learn per-ID min/max inter-arrival, period, jitter; flag arrivals outside [Pi - Ji, Pi + Ji]",
        "novel_contribution": "Reference reimplementation of Olufowobi20"
      }
    ],
    "learning_paradigm": [
      "Unsupervised",
      "Supervised",
      "Specification-based",
      "Rule-based",
      "Statistical"
    ],
    "datasets": [
      {
        "name": "New CAN timing anomaly dataset (this paper)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": true,
        "availability": "publicly_available"
      },
      {
        "name": "Public CAN bus dataset (unspecified in excerpt)",
        "type": "public",
        "domain": "network_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      }
    ],
    "baselines": [
      {
        "method_name": "Otsuka14 (delayed-decision cycle detection)",
        "paper_reference": "Otsuka et al., 2014",
        "metric": "FPR, FNR (original); unified metrics in this study",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Taylor15 (sliding-window t-test + log-sum score)",
        "paper_reference": "Taylor et al., 2015",
        "metric": "ROC, AUC (original); unified metrics in this study",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Cho16 (CIDS: RLS + CUSUM)",
        "paper_reference": "Cho and Shin, 2016",
        "metric": "Detection over W-frame windows; unified metrics in this study",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Gmiden16 (frequency-based thresholding)",
        "paper_reference": "Gmiden and Trabelsi, 2016",
        "metric": "N/A in original; unified metrics in this study",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Song16 (Δt thresholding + DoS counter)",
        "paper_reference": "Song et al., 2016",
        "metric": "Accuracy (original); unified metrics in this study",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Moore17 (deviation margin with 3-strike rule)",
        "paper_reference": "Moore et al., 2017",
        "metric": "TPR, FPR, FNR (original); unified metrics in this study",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Stabili19 (missing-message detector ct×k)",
        "paper_reference": "Stabili et al., 2019",
        "metric": "F-measure (original); unified metrics in this study",
        "their_result": null,
        "baseline_result": null
      },
      {
        "method_name": "Olufowobi20 (SAIDu-CANT)",
        "paper_reference": "Olufowobi et al., 2020",
        "metric": "TN, TP, FP, FN, Accuracy, Recall, Precision, F1 (original); unified metrics in this study",
        "their_result": null,
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "FPR",
      "FNR",
      "TPR",
      "ROC",
      "AUC",
      "Accuracy",
      "Precision",
      "Recall",
      "F1"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "What is the comparative detection performance of eight timing/frequency-based CAN anomaly detectors under a unified, unbiased evaluation?",
        "How do attack parameters (e.g., cycle times and injection frequencies) affect detector performance?",
        "How do results differ between a new, well-specified dataset and a commonly used public dataset?",
        "How can we improve reproducibility in CAN IDS research via open implementations and datasets?"
      ],
      "gaps_identified": [
        "Use of private datasets that hinder reproducibility and fair comparison",
        "Lack of open reference implementations and missing algorithmic details",
        "Inconsistent evaluation metrics across studies",
        "Insufficient tuning/training detail disclosure affecting performance replication",
        "Public datasets may be limited for timing-based evaluations (e.g., inadequate threat models or timing characteristics)",
        "Timing-based methods only work for cyclic messages and cannot detect attacks targeting non-cyclic messages"
      ],
      "limitations": [
        "Evaluation restricted to timing/frequency-based detectors; content-based features not considered",
        "Detectors are applicable only to cyclic messages; attacks on non-cyclic messages may evade detection",
        "For Cho16, only the per-message variant could be applied due to lack of DBC/ECU-to-ID mapping",
        "Comparisons are limited to two datasets (one new, one public) in the presented experiments"
      ],
      "future_work": [
        "Define and evaluate comprehensive threat models including varied cycle times, injection frequencies, and broader attack types",
        "Extend comparisons to include content-based and hybrid detectors",
        "Curate and release more realistic public datasets with rich timing fidelity and labels"
      ],
      "motivation": "Enable reproducible, unbiased comparison of CAN timing-based anomaly detectors by releasing open implementations and labeled datasets, addressing the prevalent lack of public data, code, and consistent evaluation.",
      "potential_research_ideas": [
        "Design hybrid CAN IDS combining timing models with payload/content models for improved coverage of non-cyclic messages.",
        "Develop adaptive, per-ID probabilistic timing models (e.g., Bayesian or state-space) that update online to cope with jitter and concept drift.",
        "Create standardized, multi-vehicle CAN-FD/CAN datasets with controlled, parameterized attack campaigns for benchmarking.",
        "Investigate transfer learning/domain adaptation to generalize timing models across vehicle platforms and environmental conditions.",
        "Build an adversarial evaluation framework to stress-test timing-based IDS against adaptive attackers (e.g., stealthy injections aligned to timing distributions).",
        "Ensemble different timing detectors with meta-learning to optimize per-ID detector selection and thresholds."
      ],
      "architectural_improvement_recommendations": [
        "Use Kalman/Bayesian filters for per-ID period and jitter estimation with uncertainty-aware thresholds.",
        "Introduce robust statistics (median/MAD, trimmed means) for cycle-time estimation to resist outliers and stealthy injections.",
        "Employ online calibration and drift detection to adapt thresholds without retraining.",
        "Leverage cross-ID correlations (messages from same ECU) via lightweight graph or co-scheduling models when DBC/mapping is available.",
        "Implement hierarchical detection (fast O(1) prefilter + windowed RLS/CUSUM) to balance latency and accuracy.",
        "Augment datasets with precise clock synchronization and timestamp quality metrics to improve reproducibility and comparability."
      ],
      "ease_of_improvement": "high"
    },
    "implementation": {
      "code_available": true,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "high",
      "computational_requirements": "Lightweight CPU-only; most methods O(1) per message (Otsuka14, Taylor15, Gmiden16, Song16, Moore17, Olufowobi20), Stabili19 O(n) per message-ID set, Cho16 RLS+CUSUM O(N^2) per window."
    },
    "practical_aspects": {
      "real_world_tested": false,
      "deployment_environment": "Automotive CAN bus (in-vehicle network), offline evaluation from CAN traces",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Timing-based detection does not cover non-cyclic messages.",
        "Need for DBC/ECU-ID mapping to use some variants (e.g., Cho16 message-pairwise).",
        "Parameter tuning per-ID and per-vehicle; sensitivity to jitter and environmental changes.",
        "Variability across vehicle platforms/datasets affects generalization.",
        "Timestamp precision/synchronization quality can impact timing-based detection reliability."
      ]
    },
    "concerns": {
      "explainability": false,
      "adversarial_robustness": false,
      "privacy_preserving": false,
      "fairness_bias": false
    },
    "contributions": [
      "Unbiased empirical comparison of eight timing/frequency-based CAN anomaly detectors.",
      "Public release of reference implementations for all evaluated detectors.",
      "Introduction and release of a new labeled CAN timing anomaly dataset; use of an additional public dataset.",
      "Unified, detailed description of algorithms with clarified design choices to improve reproducibility.",
      "Analysis highlighting limitations of existing public datasets and the importance of a comprehensive threat model (e.g., varying cycle times and injection frequencies)."
    ]
  },
  {
    "arxiv_id": "2307.04358v2",
    "title": "False Sense of Security: Leveraging XAI to Analyze the Reasoning and True Performance of Context-less DGA Classifiers",
    "authors": "Arthur Drichel; Ulrike Meyer",
    "abstract": "The problem of revealing botnet activity through Domain Generation Algorithm (DGA) detection seems to be solved, considering that available deep learning classifiers achieve accuracies of over 99.9%. However, these classifiers provide a false sense of security as they are heavily biased and allow for trivial detection bypass. In this work, we leverage explainable artificial intelligence (XAI) methods to analyze the reasoning of deep learning classifiers and to systematically reveal such biases. We show that eliminating these biases from DGA classifiers considerably deteriorates their performance. Nevertheless we are able to design a context-aware detection system that is free of the identified biases and maintains the detection rate of state-of-the art deep learning classifiers. In this context, we propose a visual analysis system that helps to better understand a classifier's reasoning, thereby increasing trust in and transparency of detection methods and facilitating decision-making.",
    "published_date": "2023-07-10",
    "pdf_link": "https://arxiv.org/pdf/2307.04358v2",
    "paper_types": [
      "empirical_analysis",
      "new_technique"
    ],
    "security_domain": {
      "primary": "Network Security",
      "subdomain": "Intrusion Detection",
      "specific_problem": "Detection of DGA-generated domains (botnet C2) from DNS NX-traffic; analysis and mitigation of biases in context-less DGA classifiers",
      "attack_types": [
        "Botnet Command-and-Control via DGA",
        "Evasion via bias exploitation (spurious correlations, sampling biases)"
      ]
    },
    "ml_techniques": [
      {
        "type": "primary",
        "category": "XAI/Attribution",
        "specific": "B-Cos transform (as one explainability-related modification); plus a broad set of 20 XAI methods (names not all specified in excerpt)",
        "novel_contribution": "Systematic use of XAI to reveal biases in DGA classifiers; visual analysis system to interpret classifier reasoning"
      },
      {
        "type": "primary",
        "category": "Context-aware detection system",
        "specific": null,
        "novel_contribution": "Design of a context-aware DGA detection system that removes identified biases while maintaining detection rates of state-of-the-art context-less deep learning classifiers"
      },
      {
        "type": "baseline",
        "category": "CNN/ResNet",
        "specific": "B-ResNet (binary), M-ResNet (multiclass)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "Random Forest",
        "specific": "EXPLAIN (one-vs-rest RF, 76 handcrafted features)",
        "novel_contribution": null
      },
      {
        "type": "baseline",
        "category": "CNN with interpretable layers",
        "specific": "M-ResNet + B-Cos",
        "novel_contribution": null
      }
    ],
    "learning_paradigm": [
      "Supervised"
    ],
    "datasets": [
      {
        "name": "DGArchive OSINT feed",
        "type": "public",
        "domain": "domain_names/DNS",
        "link": null,
        "is_new_contribution": false,
        "availability": "publicly_available"
      },
      {
        "name": "RWTH Aachen University NX-traffic (benign)",
        "type": "private",
        "domain": "dns_nx_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "Siemens AG enterprise NX-traffic (benign, real-world eval)",
        "type": "private",
        "domain": "dns_nx_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "DSmod (balanced train/test set for SOTA reproduction)",
        "type": "proprietary",
        "domain": "dns_nx_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      },
      {
        "name": "DSex (held-out explainability and bias analysis set)",
        "type": "proprietary",
        "domain": "dns_nx_traffic",
        "link": null,
        "is_new_contribution": false,
        "availability": "not_available"
      }
    ],
    "baselines": [
      {
        "method_name": "B-ResNet (binary ResNet)",
        "paper_reference": "Drichel et al. [14]",
        "metric": "ACC/TPR/FPR (macro as applicable)",
        "their_result": "ACC 0.99864, TPR 0.99982, FPR 0.00255",
        "baseline_result": null
      },
      {
        "method_name": "M-ResNet (multiclass ResNet)",
        "paper_reference": "Drichel et al. [14]",
        "metric": "F1 / Precision / Recall (macro)",
        "their_result": "F1 0.78682, Precision 0.80058, Recall 0.79690",
        "baseline_result": null
      },
      {
        "method_name": "EXPLAIN (RF, 76 handcrafted features)",
        "paper_reference": "EXPLAIN [13]",
        "metric": "F1 / Precision / Recall (macro)",
        "their_result": "F1 0.76733, Precision 0.78604, Recall 0.76685",
        "baseline_result": null
      },
      {
        "method_name": "M-ResNet + B-Cos",
        "paper_reference": "Bohle et al. [11] (B-Cos transform applied to M-ResNet)",
        "metric": "F1 / Precision / Recall (macro)",
        "their_result": "F1 0.76990, Precision 0.79555, Recall 0.77250",
        "baseline_result": null
      }
    ],
    "performance_metrics_used": [
      "Accuracy",
      "True Positive Rate (TPR/Recall)",
      "False Positive Rate (FPR)",
      "F1-score (macro-averaged)",
      "Precision (macro-averaged)",
      "Recall (macro-averaged)"
    ],
    "claims_sota": false,
    "research_scope": {
      "research_questions": [
        "Can XAI systematically reveal biases and reasoning shortcuts in state-of-the-art context-less DGA classifiers?",
        "What is the impact on performance when identified biases are eliminated from DGA classifiers?",
        "Can we design a context-aware detection system that is free of these biases while maintaining SOTA detection rates?",
        "Which explainability methods are most effective and stable across models for DGA detection?",
        "Do DGA classifiers generalize across networks and remain time-robust under real-world, bias-mitigated evaluation settings?"
      ],
      "gaps_identified": [
        "Context-less DGA classifiers achieve high reported accuracy but are heavily biased and vulnerable to trivial bypass via spurious correlations and artifacts.",
        "Common experimental biases (spatial/class imbalance and temporal leakage) distort evaluation, leading to a base-rate fallacy in deployment-relevant settings.",
        "Use of artificial benign data (e.g., top sites) and resolving DNS introduces additional bias versus NX-traffic.",
        "Opaque deep models hinder understanding of learned features and potential vulnerabilities.",
        "Preprocessing and data construction choices (e.g., padding, balancing) can inject shortcuts unrelated to the underlying task."
      ],
      "limitations": [
        "The DSmod/DSex methodology (balanced classes, cross-validation) inherently introduces spatial and temporal experimental biases; a separate real-world evaluation is required to mitigate these.",
        "Benign NXDs, even after OSINT filtering, may still contain mislabeled samples.",
        "Underrepresented DGA classes with very few samples are included, which may affect per-class stability.",
        "No institutional ethics review board was involved; privacy mitigations were applied but formal oversight was absent.",
        "Details of the proposed context-aware system architecture and full list of XAI methods are not provided in the excerpt."
      ],
      "future_work": [
        "Apply the XAI-driven bias analysis methodology to other security tasks (phishing, malware detection, vulnerability discovery, general intrusion detection).",
        "Further study time robustness and cross-network generalization with longer horizons and additional environments.",
        "Develop systematic strategies to mitigate identified biases without degrading performance."
      ],
      "motivation": "High reported accuracy of context-less DGA classifiers masks heavy biases and vulnerability to trivial evasion; leveraging XAI can expose and address these issues to build trustworthy, transparent, and robust detection.",
      "potential_research_ideas": [
        "Design a standardized XAI benchmark suite for security ML with curated datasets containing known artifacts to evaluate explanation fidelity and bias detection capability.",
        "Causal or invariance-based training (e.g., IRM-style objectives) to learn features stable across networks/time and robust to spurious correlations in DGA detection.",
        "Adversarial example generation tailored to DGA strings (e.g., gradient-based domain mutations or generative DGAs) to stress-test and harden classifiers.",
        "Context-aware architectures that incorporate privacy-preserving network statistics (e.g., per-client NX ratios, temporal patterns) via federated or split learning.",
        "Prototype- or concept-based interpretable models (e.g., concept bottlenecks) for DGA families to align explanations with human-understandable patterns.",
        "Data-centric validation: active learning to identify and relabel mislabeled NXDs and detect drift and time-dependent artifacts.",
        "Calibration and threshold optimization for extreme base-rate settings to reduce operational false positives in SOCs."
      ],
      "architectural_improvement_recommendations": [
        "Integrate interpretable components (e.g., B-Cos layers, prototype networks) into CNN backbones to constrain shortcut learning.",
        "Add context channels (temporal features, per-host statistics, resolver-side aggregates) in a multimodal model while maintaining privacy via aggregation.",
        "Use domain/time adversarial objectives (domain adversarial training) to enforce invariance across networks and time periods.",
        "Apply data augmentation and domain randomization over character distributions, lengths, and token patterns to reduce reliance on spurious cues.",
        "Introduce cost-sensitive learning and calibration tuned for low base rates typical of enterprise DNS NX-traffic."
      ],
      "ease_of_improvement": "medium"
    },
    "implementation": {
      "code_available": false,
      "code_link": null,
      "frameworks": [],
      "reproducibility_score": "medium",
      "computational_requirements": null
    },
    "practical_aspects": {
      "real_world_tested": true,
      "deployment_environment": "Enterprise DNS resolvers (company network) and campus network; SOC analysis support",
      "scalability_discussed": true,
      "inference_time": null,
      "deployment_challenges": [
        "Strong class imbalance in real DNS traffic leading to base-rate fallacy if not addressed.",
        "Temporal drift and cross-network generalization challenges.",
        "Model biases enabling trivial evasion if explanations and debiasing are not applied.",
        "Privacy and data access constraints when incorporating context.",
        "Operational integration of explainability into SOC workflows."
      ]
    },
    "concerns": {
      "explainability": true,
      "adversarial_robustness": false,
      "privacy_preserving": true,
      "fairness_bias": false
    },
    "contributions": [
      "Systematic application of XAI to analyze the reasoning of context-less DGA classifiers and reveal multiple biases.",
      "Demonstration that eliminating identified biases substantially deteriorates performance of context-less classifiers.",
      "Design of a context-aware DGA detection system that is free of identified biases and maintains SOTA detection rates.",
      "Proposal of a visual analysis system to understand classifier reasoning, supporting SOC decision-making and transparency.",
      "Extensive evaluation of explainability methods (20+) and debugging of state-of-the-art DGA classifiers.",
      "Reproduction of state-of-the-art results for B-ResNet, M-ResNet, and EXPLAIN, providing a validated evaluation setup.",
      "Secondary contribution: improvements to deep learning- and feature-based approaches for DGA multiclass classification in performance and efficiency.",
      "Real-world, bias-mitigated evaluation using enterprise DNS NX-traffic to assess generalization and time robustness."
    ]
  }
]